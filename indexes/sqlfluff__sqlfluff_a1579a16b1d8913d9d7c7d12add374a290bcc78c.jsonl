{"doc_id": "setup.py::read", "file_path": "setup.py", "class_name": null, "func_name": "read", "text": "文件路径: setup.py\ndef read(*names, **kwargs):\n    \"\"\"Read a file and return the contents as a string.\"\"\"\n    return open(\n        join(dirname(__file__), *names), encoding=kwargs.get(\"encoding\", \"utf8\")\n    ).read()\n", "tokens": ["setup", "py", "def", "read", "names", "kwargs", "read", "a", "file", "and", "return", "the", "contents", "as", "a", "string", "return", "open", "join", "dirname", "__file__", "names", "encoding", "kwargs", "get", "encoding", "utf8", "read"], "doc_len": 28}
{"doc_id": "util.py::cli", "file_path": "util.py", "class_name": null, "func_name": "cli", "text": "文件路径: util.py\ndef cli():\n    \"\"\"Launch the utility cli.\"\"\"\n    pass\n", "tokens": ["util", "py", "def", "cli", "launch", "the", "utility", "cli", "pass"], "doc_len": 9}
{"doc_id": "util.py::clean_tests", "file_path": "util.py", "class_name": null, "func_name": "clean_tests", "text": "文件路径: util.py\ndef clean_tests(path):\n    \"\"\"Clear up the tests directory.\n\n    NB: Using scripts allows platform independence\n    Makes a new one afterward\n    \"\"\"\n    try:\n        shutil.rmtree(path)\n        click.echo(f\"Removed {path!r}...\")\n    # OSError is for python 27\n    # in py36 its FileNotFoundError (but that inherits from IOError, which exists in py27)\n    except OSError:\n        click.echo(f\"Directory {path!r} does not exist. Skipping...\")\n\n    os.mkdir(path)\n    click.echo(f\"Created {path!r}\")\n", "tokens": ["util", "py", "def", "clean_tests", "path", "clear", "up", "the", "tests", "directory", "nb", "using", "scripts", "allows", "platform", "independence", "makes", "a", "new", "one", "afterward", "try", "shutil", "rmtree", "path", "click", "echo", "f", "removed", "path", "r", "oserror", "is", "for", "python", "27", "in", "py36", "its", "filenotfounderror", "but", "that", "inherits", "from", "ioerror", "which", "exists", "in", "py27", "except", "oserror", "click", "echo", "f", "directory", "path", "r", "does", "not", "exist", "skipping", "os", "mkdir", "path", "click", "echo", "f", "created", "path", "r"], "doc_len": 70}
{"doc_id": "util.py::benchmark", "file_path": "util.py", "class_name": null, "func_name": "benchmark", "text": "文件路径: util.py\ndef benchmark(cmd, runs, from_file):\n    \"\"\"Benchmark how long it takes to run a particular command.\"\"\"\n    if from_file:\n        with open(from_file) as yaml_file:\n            parsed = yaml.load(yaml_file.read(), Loader=yaml.FullLoader)\n            benchmarks = parsed[\"benchmarks\"]\n            click.echo(repr(benchmarks))\n    elif cmd:\n        benchmarks = [{\"name\": str(hash(cmd)), \"cmd\": cmd}]\n    else:\n        click.echo(\"No command or file specified!\")\n        sys.exit(1)\n\n    commit_hash = None\n    post_results = False\n    # Try and detect a CI environment\n    if \"CI\" in os.environ:\n        click.echo(\"CI detected!\")\n        # available_vars = [var for var in os.environ.keys()]  # if var.startswith('CIRCLE')\n        # click.echo(\"Available keys: {0!r}\".format(available_vars))\n        commit_hash = os.environ.get(\"GITHUB_SHA\", None)\n        post_results = True\n        click.echo(f\"Commit hash is: {commit_hash!r}\")\n\n    all_results = {}\n    for run_no in range(runs):\n        click.echo(f\"===== Run #{run_no + 1} =====\")\n        results = {}\n        for benchmark in benchmarks:\n            # Iterate through benchmarks\n            click.echo(\"Starting benchmark: {!r}\".format(benchmark[\"name\"]))\n            t0 = time.monotonic()\n            click.echo(\"===START PROCESS OUTPUT===\")\n            process = subprocess.run(benchmark[\"cmd\"])\n            click.echo(\"===END PROCESS OUTPUT===\")\n            t1 = time.monotonic()\n            if process.returncode != 0:\n                if benchmark[\"cmd\"][0] == \"sqlfluff\" and benchmark[\"cmd\"][1] == \"fix\":\n                    # Allow fix to fail as not all our benchmark errors are fixable\n                    click.echo(\n                        f\"Fix command failed with return code: {process.returncode}\"\n                    )\n                else:\n                    click.echo(f\"Command failed with return code: {process.returncode}\")\n                    sys.exit(process.returncode)\n            else:\n                duration = t1 - t0\n                click.echo(f\"Process completed in {duration:.4f}s\")\n                results[benchmark[\"name\"]] = duration\n\n        if post_results:\n            click.echo(f\"Posting results: {results}\")\n            resp = requests.post(\n                \"https://f32cvv8yh3.execute-api.eu-west-1.amazonaws.com/result/gh/{repo}/{commit}\".format(\n                    # TODO: update the stats collector eventually to allow the new repo path\n                    repo=\"alanmcruickshank/sqlfluff\",\n                    commit=commit_hash,\n                ),\n                params={\"key\": \"mtqTC1fVVebVQ5BVREP7jYrKwgjaO0IfRILzyZt\"},\n                json=results,\n            )\n            click.echo(resp.text)\n        all_results[run_no] = results\n    click.echo(\"===== Done =====\")\n    for run_no in all_results:\n        click.echo(\"Run {:>5}: {}\".format(f\"#{run_no}\", all_results[run_no]))\n", "tokens": ["util", "py", "def", "benchmark", "cmd", "runs", "from_file", "benchmark", "how", "long", "it", "takes", "to", "run", "a", "particular", "command", "if", "from_file", "with", "open", "from_file", "as", "yaml_file", "parsed", "yaml", "load", "yaml_file", "read", "loader", "yaml", "fullloader", "benchmarks", "parsed", "benchmarks", "click", "echo", "repr", "benchmarks", "elif", "cmd", "benchmarks", "name", "str", "hash", "cmd", "cmd", "cmd", "else", "click", "echo", "no", "command", "or", "file", "specified", "sys", "exit", "1", "commit_hash", "none", "post_results", "false", "try", "and", "detect", "a", "ci", "environment", "if", "ci", "in", "os", "environ", "click", "echo", "ci", "detected", "available_vars", "var", "for", "var", "in", "os", "environ", "keys", "if", "var", "startswith", "circle", "click", "echo", "available", "keys", "0", "r", "format", "available_vars", "commit_hash", "os", "environ", "get", "github_sha", "none", "post_results", "true", "click", "echo", "f", "commit", "hash", "is", "commit_hash", "r", "all_results", "for", "run_no", "in", "range", "runs", "click", "echo", "f", "run", "run_no", "1", "results", "for", "benchmark", "in", "benchmarks", "iterate", "through", "benchmarks", "click", "echo", "starting", "benchmark", "r", "format", "benchmark", "name", "t0", "time", "monotonic", "click", "echo", "start", "process", "output", "process", "subprocess", "run", "benchmark", "cmd", "click", "echo", "end", "process", "output", "t1", "time", "monotonic", "if", "process", "returncode", "0", "if", "benchmark", "cmd", "0", "sqlfluff", "and", "benchmark", "cmd", "1", "fix", "allow", "fix", "to", "fail", "as", "not", "all", "our", "benchmark", "errors", "are", "fixable", "click", "echo", "f", "fix", "command", "failed", "with", "return", "code", "process", "returncode", "else", "click", "echo", "f", "command", "failed", "with", "return", "code", "process", "returncode", "sys", "exit", "process", "returncode", "else", "duration", "t1", "t0", "click", "echo", "f", "process", "completed", "in", "duration", "4f", "s", "results", "benchmark", "name", "duration", "if", "post_results", "click", "echo", "f", "posting", "results", "results", "resp", "requests", "post", "https", "f32cvv8yh3", "execute", "api", "eu", "west", "1", "amazonaws", "com", "result", "gh", "repo", "commit", "format", "todo", "update", "the", "stats", "collector", "eventually", "to", "allow", "the", "new", "repo", "path", "repo", "alanmcruickshank", "sqlfluff", "commit", "commit_hash", "params", "key", "mtqtc1fvvebvq5bvrep7jyrkwgjao0ifrilzyzt", "json", "results", "click", "echo", "resp", "text", "all_results", "run_no", "results", "click", "echo", "done", "for", "run_no", "in", "all_results", "click", "echo", "run", "5", "format", "f", "run_no", "all_results", "run_no"], "doc_len": 302}
{"doc_id": "docs/source/conf.py::ultimate_replace", "file_path": "docs/source/conf.py", "class_name": null, "func_name": "ultimate_replace", "text": "文件路径: docs/source/conf.py\ndef ultimate_replace(app, docname, source):\n    \"\"\"Replaces variables in docs, including code blocks.\n\n    From: https://github.com/sphinx-doc/sphinx/issues/4054#issuecomment-329097229\n    \"\"\"\n    result = source[0]\n    for key in app.config.ultimate_replacements:\n        result = result.replace(key, app.config.ultimate_replacements[key])\n    source[0] = result\n", "tokens": ["docs", "source", "conf", "py", "def", "ultimate_replace", "app", "docname", "source", "replaces", "variables", "in", "docs", "including", "code", "blocks", "from", "https", "github", "com", "sphinx", "doc", "sphinx", "issues", "4054", "issuecomment", "329097229", "result", "source", "0", "for", "key", "in", "app", "config", "ultimate_replacements", "result", "result", "replace", "key", "app", "config", "ultimate_replacements", "key", "source", "0", "result"], "doc_len": 47}
{"doc_id": "docs/source/conf.py::setup", "file_path": "docs/source/conf.py", "class_name": null, "func_name": "setup", "text": "文件路径: docs/source/conf.py\ndef setup(app):\n    \"\"\"Configures the documentation app.\"\"\"\n    app.add_config_value(\"ultimate_replacements\", {}, True)\n    app.connect(\"source-read\", ultimate_replace)\n", "tokens": ["docs", "source", "conf", "py", "def", "setup", "app", "configures", "the", "documentation", "app", "app", "add_config_value", "ultimate_replacements", "true", "app", "connect", "source", "read", "ultimate_replace"], "doc_len": 20}
{"doc_id": "examples/02_timing_api_steps.py::time_function", "file_path": "examples/02_timing_api_steps.py", "class_name": null, "func_name": "time_function", "text": "文件路径: examples/02_timing_api_steps.py\ndef time_function(func, name, iterations=20):\n    \"\"\"A basic timing function.\"\"\"\n    # Do the timing\n    time = timeit.timeit(func, number=iterations) / iterations\n    # Output the result\n    print(\n        \"{:<35} {:.6}s [{} iterations]\".format(\n            f\"Time to {name}:\",\n            time,\n            iterations,\n        )\n    )\n", "tokens": ["examples", "02_timing_api_steps", "py", "def", "time_function", "func", "name", "iterations", "20", "a", "basic", "timing", "function", "do", "the", "timing", "time", "timeit", "timeit", "func", "number", "iterations", "iterations", "output", "the", "result", "print", "35", "6", "s", "iterations", "format", "f", "time", "to", "name", "time", "iterations"], "doc_len": 38}
{"doc_id": "plugins/sqlfluff-plugin-example/src/example/rules.py::get_rules", "file_path": "plugins/sqlfluff-plugin-example/src/example/rules.py", "class_name": null, "func_name": "get_rules", "text": "文件路径: plugins/sqlfluff-plugin-example/src/example/rules.py\ndef get_rules() -> List[BaseRule]:\n    \"\"\"Get plugin rules.\"\"\"\n    return [Rule_Example_L001]\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "src", "example", "rules", "py", "def", "get_rules", "list", "baserule", "get", "plugin", "rules", "return", "rule_example_l001"], "doc_len": 17}
{"doc_id": "plugins/sqlfluff-plugin-example/src/example/rules.py::load_default_config", "file_path": "plugins/sqlfluff-plugin-example/src/example/rules.py", "class_name": null, "func_name": "load_default_config", "text": "文件路径: plugins/sqlfluff-plugin-example/src/example/rules.py\ndef load_default_config() -> dict:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return ConfigLoader.get_global().load_default_config_file(\n        file_dir=os.path.dirname(__file__),\n        file_name=\"plugin_default_config.cfg\",\n    )\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "src", "example", "rules", "py", "def", "load_default_config", "dict", "loads", "the", "default", "configuration", "for", "the", "plugin", "return", "configloader", "get_global", "load_default_config_file", "file_dir", "os", "path", "dirname", "__file__", "file_name", "plugin_default_config", "cfg"], "doc_len": 30}
{"doc_id": "plugins/sqlfluff-plugin-example/src/example/rules.py::get_configs_info", "file_path": "plugins/sqlfluff-plugin-example/src/example/rules.py", "class_name": null, "func_name": "get_configs_info", "text": "文件路径: plugins/sqlfluff-plugin-example/src/example/rules.py\ndef get_configs_info() -> dict:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return {\n        \"forbidden_columns\": {\"definition\": \"A list of column to forbid\"},\n    }\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "src", "example", "rules", "py", "def", "get_configs_info", "dict", "get", "rule", "config", "validations", "and", "descriptions", "return", "forbidden_columns", "definition", "a", "list", "of", "column", "to", "forbid"], "doc_len": 26}
{"doc_id": "plugins/sqlfluff-plugin-example/src/example/rules.py::Rule_Example_L001.__init__", "file_path": "plugins/sqlfluff-plugin-example/src/example/rules.py", "class_name": "Rule_Example_L001", "func_name": "__init__", "text": "文件路径: plugins/sqlfluff-plugin-example/src/example/rules.py, 类名: Rule_Example_L001\n    def __init__(self, *args, **kwargs):\n        \"\"\"Overwrite __init__ to set config.\"\"\"\n        super().__init__(*args, **kwargs)\n        self.forbidden_columns = [\n            col.strip() for col in self.forbidden_columns.split(\",\")\n        ]\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "src", "example", "rules", "py", "rule_example_l001", "def", "__init__", "self", "args", "kwargs", "overwrite", "__init__", "to", "set", "config", "super", "__init__", "args", "kwargs", "self", "forbidden_columns", "col", "strip", "for", "col", "in", "self", "forbidden_columns", "split"], "doc_len": 33}
{"doc_id": "plugins/sqlfluff-plugin-example/src/example/rules.py::Rule_Example_L001._eval", "file_path": "plugins/sqlfluff-plugin-example/src/example/rules.py", "class_name": "Rule_Example_L001", "func_name": "_eval", "text": "文件路径: plugins/sqlfluff-plugin-example/src/example/rules.py, 类名: Rule_Example_L001\n    def _eval(self, context: RuleContext):\n        \"\"\"We should not use ORDER BY.\"\"\"\n        if context.segment.is_type(\"orderby_clause\"):\n            for seg in context.segment.segments:\n                col_name = seg.raw.lower()\n                if (\n                    seg.is_type(\"column_reference\")\n                    and col_name in self.forbidden_columns\n                ):\n                    return LintResult(\n                        anchor=seg,\n                        description=f\"Column `{col_name}` not allowed in ORDER BY.\",\n                    )\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "src", "example", "rules", "py", "rule_example_l001", "def", "_eval", "self", "context", "rulecontext", "we", "should", "not", "use", "order", "by", "if", "context", "segment", "is_type", "orderby_clause", "for", "seg", "in", "context", "segment", "segments", "col_name", "seg", "raw", "lower", "if", "seg", "is_type", "column_reference", "and", "col_name", "in", "self", "forbidden_columns", "return", "lintresult", "anchor", "seg", "description", "f", "column", "col_name", "not", "allowed", "in", "order", "by"], "doc_len": 57}
{"doc_id": "plugins/sqlfluff-plugin-example/test/rules/rule_test_cases_test.py::test__rule_test_case", "file_path": "plugins/sqlfluff-plugin-example/test/rules/rule_test_cases_test.py", "class_name": null, "func_name": "test__rule_test_case", "text": "文件路径: plugins/sqlfluff-plugin-example/test/rules/rule_test_cases_test.py\ndef test__rule_test_case(test_case):\n    \"\"\"Run the tests.\"\"\"\n    rules__test_helper(test_case)\n", "tokens": ["plugins", "sqlfluff", "plugin", "example", "test", "rules", "rule_test_cases_test", "py", "def", "test__rule_test_case", "test_case", "run", "the", "tests", "rules__test_helper", "test_case"], "doc_len": 16}
{"doc_id": "plugins/sqlfluff-templater-dbt/setup.py::read", "file_path": "plugins/sqlfluff-templater-dbt/setup.py", "class_name": null, "func_name": "read", "text": "文件路径: plugins/sqlfluff-templater-dbt/setup.py\ndef read(*names, **kwargs):\n    \"\"\"Read a file and return the contents as a string.\"\"\"\n    return open(\n        join(dirname(__file__), *names), encoding=kwargs.get(\"encoding\", \"utf8\")\n    ).read()\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "setup", "py", "def", "read", "names", "kwargs", "read", "a", "file", "and", "return", "the", "contents", "as", "a", "string", "return", "open", "join", "dirname", "__file__", "names", "encoding", "kwargs", "get", "encoding", "utf8", "read"], "doc_len": 32}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.__init__", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "__init__", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def __init__(self, **kwargs):\n        self.sqlfluff_config = None\n        self.formatter = None\n        self.project_dir = None\n        self.profiles_dir = None\n        self.working_dir = os.getcwd()\n        self._sequential_fails = 0\n        super().__init__(**kwargs)\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "__init__", "self", "kwargs", "self", "sqlfluff_config", "none", "self", "formatter", "none", "self", "project_dir", "none", "self", "profiles_dir", "none", "self", "working_dir", "os", "getcwd", "self", "_sequential_fails", "0", "super", "__init__", "kwargs"], "doc_len": 34}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.config_pairs", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "config_pairs", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def config_pairs(self):  # pragma: no cover TODO?\n        \"\"\"Returns info about the given templater for output by the cli.\"\"\"\n        return [(\"templater\", self.name), (\"dbt\", self.dbt_version)]\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "config_pairs", "self", "pragma", "no", "cover", "todo", "returns", "info", "about", "the", "given", "templater", "for", "output", "by", "the", "cli", "return", "templater", "self", "name", "dbt", "self", "dbt_version"], "doc_len": 33}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_version", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_version", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_version(self):\n        \"\"\"Gets the dbt version.\"\"\"\n        return DBT_VERSION_STRING\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_version", "self", "gets", "the", "dbt", "version", "return", "dbt_version_string"], "doc_len": 17}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_version_tuple", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_version_tuple", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_version_tuple(self):\n        \"\"\"Gets the dbt version as a tuple on (major, minor).\"\"\"\n        return DBT_VERSION_TUPLE\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_version_tuple", "self", "gets", "the", "dbt", "version", "as", "a", "tuple", "on", "major", "minor", "return", "dbt_version_tuple"], "doc_len": 23}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_config", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_config", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_config(self):\n        \"\"\"Loads the dbt config.\"\"\"\n        self.dbt_config = DbtRuntimeConfig.from_args(\n            DbtConfigArgs(\n                project_dir=self.project_dir,\n                profiles_dir=self.profiles_dir,\n                profile=self._get_profile(),\n            )\n        )\n        register_adapter(self.dbt_config)\n        return self.dbt_config\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_config", "self", "loads", "the", "dbt", "config", "self", "dbt_config", "dbtruntimeconfig", "from_args", "dbtconfigargs", "project_dir", "self", "project_dir", "profiles_dir", "self", "profiles_dir", "profile", "self", "_get_profile", "register_adapter", "self", "dbt_config", "return", "self", "dbt_config"], "doc_len": 35}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_compiler", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_compiler", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_compiler(self):\n        \"\"\"Loads the dbt compiler.\"\"\"\n        self.dbt_compiler = DbtCompiler(self.dbt_config)\n        return self.dbt_compiler\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_compiler", "self", "loads", "the", "dbt", "compiler", "self", "dbt_compiler", "dbtcompiler", "self", "dbt_config", "return", "self", "dbt_compiler"], "doc_len": 23}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_manifest", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_manifest", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_manifest(self):\n        \"\"\"Loads the dbt manifest.\"\"\"\n        # Identity function used for macro hooks\n        def identity(x):\n            return x\n\n        # Set dbt not to run tracking. We don't load\n        # a dull project and so some tracking routines\n        # may fail.\n        from dbt.tracking import do_not_track\n\n        do_not_track()\n\n        if self.dbt_version_tuple <= (0, 19):\n\n            if self.dbt_version_tuple == (0, 17):  # pragma: no cover TODO?\n                # dbt version 0.17.*\n                from dbt.parser.manifest import (\n                    load_internal_manifest as load_macro_manifest,\n                )\n            else:\n                # dbt version 0.18.* & # 0.19.*\n                from dbt.parser.manifest import load_macro_manifest\n\n                load_macro_manifest = partial(load_macro_manifest, macro_hook=identity)\n\n            from dbt.parser.manifest import load_manifest\n\n            dbt_macros_manifest = load_macro_manifest(self.dbt_config)\n            self.dbt_manifest = load_manifest(\n                self.dbt_config, dbt_macros_manifest, macro_hook=identity\n            )\n        else:\n            # dbt 0.20.* and onward\n            from dbt.parser.manifest import ManifestLoader\n\n            projects = self.dbt_config.load_dependencies()\n            loader = ManifestLoader(self.dbt_config, projects, macro_hook=identity)\n            self.dbt_manifest = loader.load()\n\n        return self.dbt_manifest\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_manifest", "self", "loads", "the", "dbt", "manifest", "identity", "function", "used", "for", "macro", "hooks", "def", "identity", "x", "return", "x", "set", "dbt", "not", "to", "run", "tracking", "we", "don", "t", "load", "a", "dull", "project", "and", "so", "some", "tracking", "routines", "may", "fail", "from", "dbt", "tracking", "import", "do_not_track", "do_not_track", "if", "self", "dbt_version_tuple", "0", "19", "if", "self", "dbt_version_tuple", "0", "17", "pragma", "no", "cover", "todo", "dbt", "version", "0", "17", "from", "dbt", "parser", "manifest", "import", "load_internal_manifest", "as", "load_macro_manifest", "else", "dbt", "version", "0", "18", "0", "19", "from", "dbt", "parser", "manifest", "import", "load_macro_manifest", "load_macro_manifest", "partial", "load_macro_manifest", "macro_hook", "identity", "from", "dbt", "parser", "manifest", "import", "load_manifest", "dbt_macros_manifest", "load_macro_manifest", "self", "dbt_config", "self", "dbt_manifest", "load_manifest", "self", "dbt_config", "dbt_macros_manifest", "macro_hook", "identity", "else", "dbt", "0", "20", "and", "onward", "from", "dbt", "parser", "manifest", "import", "manifestloader", "projects", "self", "dbt_config", "load_dependencies", "loader", "manifestloader", "self", "dbt_config", "projects", "macro_hook", "identity", "self", "dbt_manifest", "loader", "load", "return", "self", "dbt_manifest"], "doc_len": 144}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.dbt_selector_method", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "dbt_selector_method", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def dbt_selector_method(self):\n        \"\"\"Loads the dbt selector method.\"\"\"\n        if self.formatter:  # pragma: no cover TODO?\n            self.formatter.dispatch_compilation_header(\n                \"dbt templater\", \"Compiling dbt project...\"\n            )\n\n        if self.dbt_version_tuple == (0, 17):  # pragma: no cover TODO?\n            from dbt.graph.selector import PathSelector\n\n            self.dbt_selector_method = PathSelector(self.dbt_manifest)\n        else:\n            from dbt.graph.selector_methods import (\n                MethodManager as DbtSelectorMethodManager,\n                MethodName as DbtMethodName,\n            )\n\n            selector_methods_manager = DbtSelectorMethodManager(\n                self.dbt_manifest, previous_state=None\n            )\n            self.dbt_selector_method = selector_methods_manager.get_method(\n                DbtMethodName.Path, method_arguments=[]\n            )\n\n        if self.formatter:  # pragma: no cover TODO?\n            self.formatter.dispatch_compilation_header(\n                \"dbt templater\", \"Project Compiled.\"\n            )\n\n        return self.dbt_selector_method\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "dbt_selector_method", "self", "loads", "the", "dbt", "selector", "method", "if", "self", "formatter", "pragma", "no", "cover", "todo", "self", "formatter", "dispatch_compilation_header", "dbt", "templater", "compiling", "dbt", "project", "if", "self", "dbt_version_tuple", "0", "17", "pragma", "no", "cover", "todo", "from", "dbt", "graph", "selector", "import", "pathselector", "self", "dbt_selector_method", "pathselector", "self", "dbt_manifest", "else", "from", "dbt", "graph", "selector_methods", "import", "methodmanager", "as", "dbtselectormethodmanager", "methodname", "as", "dbtmethodname", "selector_methods_manager", "dbtselectormethodmanager", "self", "dbt_manifest", "previous_state", "none", "self", "dbt_selector_method", "selector_methods_manager", "get_method", "dbtmethodname", "path", "method_arguments", "if", "self", "formatter", "pragma", "no", "cover", "todo", "self", "formatter", "dispatch_compilation_header", "dbt", "templater", "project", "compiled", "return", "self", "dbt_selector_method"], "doc_len": 93}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater._get_profiles_dir", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "_get_profiles_dir", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def _get_profiles_dir(self):\n        \"\"\"Get the dbt profiles directory from the configuration.\n\n        The default is `~/.dbt` in 0.17 but we use the\n        PROFILES_DIR variable from the dbt library to\n        support a change of default in the future, as well\n        as to support the same overwriting mechanism as\n        dbt (currently an environment variable).\n        \"\"\"\n        dbt_profiles_dir = os.path.abspath(\n            os.path.expanduser(\n                self.sqlfluff_config.get_section(\n                    (self.templater_selector, self.name, \"profiles_dir\")\n                )\n                or PROFILES_DIR\n            )\n        )\n\n        if not os.path.exists(dbt_profiles_dir):\n            templater_logger.error(\n                f\"dbt_profiles_dir: {dbt_profiles_dir} could not be accessed. Check it exists.\"\n            )\n\n        return dbt_profiles_dir\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "_get_profiles_dir", "self", "get", "the", "dbt", "profiles", "directory", "from", "the", "configuration", "the", "default", "is", "dbt", "in", "0", "17", "but", "we", "use", "the", "profiles_dir", "variable", "from", "the", "dbt", "library", "to", "support", "a", "change", "of", "default", "in", "the", "future", "as", "well", "as", "to", "support", "the", "same", "overwriting", "mechanism", "as", "dbt", "currently", "an", "environment", "variable", "dbt_profiles_dir", "os", "path", "abspath", "os", "path", "expanduser", "self", "sqlfluff_config", "get_section", "self", "templater_selector", "self", "name", "profiles_dir", "or", "profiles_dir", "if", "not", "os", "path", "exists", "dbt_profiles_dir", "templater_logger", "error", "f", "dbt_profiles_dir", "dbt_profiles_dir", "could", "not", "be", "accessed", "check", "it", "exists", "return", "dbt_profiles_dir"], "doc_len": 97}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater._get_project_dir", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "_get_project_dir", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def _get_project_dir(self):\n        \"\"\"Get the dbt project directory from the configuration.\n\n        Defaults to the working directory.\n        \"\"\"\n        dbt_project_dir = os.path.abspath(\n            os.path.expanduser(\n                self.sqlfluff_config.get_section(\n                    (self.templater_selector, self.name, \"project_dir\")\n                )\n                or os.getcwd()\n            )\n        )\n        if not os.path.exists(dbt_project_dir):\n            templater_logger.error(\n                f\"dbt_project_dir: {dbt_project_dir} could not be accessed. Check it exists.\"\n            )\n\n        return dbt_project_dir\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "_get_project_dir", "self", "get", "the", "dbt", "project", "directory", "from", "the", "configuration", "defaults", "to", "the", "working", "directory", "dbt_project_dir", "os", "path", "abspath", "os", "path", "expanduser", "self", "sqlfluff_config", "get_section", "self", "templater_selector", "self", "name", "project_dir", "or", "os", "getcwd", "if", "not", "os", "path", "exists", "dbt_project_dir", "templater_logger", "error", "f", "dbt_project_dir", "dbt_project_dir", "could", "not", "be", "accessed", "check", "it", "exists", "return", "dbt_project_dir"], "doc_len": 62}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater._get_profile", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "_get_profile", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def _get_profile(self):\n        \"\"\"Get a dbt profile name from the configuration.\"\"\"\n        return self.sqlfluff_config.get_section(\n            (self.templater_selector, self.name, \"profile\")\n        )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "_get_profile", "self", "get", "a", "dbt", "profile", "name", "from", "the", "configuration", "return", "self", "sqlfluff_config", "get_section", "self", "templater_selector", "self", "name", "profile"], "doc_len": 28}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.sequence_files", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "sequence_files", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def sequence_files(\n        self, fnames: List[str], config=None, formatter=None\n    ) -> Iterator[str]:\n        \"\"\"Reorder fnames to process dependent files first.\n\n        This avoids errors when an ephemeral model is processed before use.\n        \"\"\"\n        if formatter:  # pragma: no cover\n            formatter.dispatch_compilation_header(\"dbt templater\", \"Sorting Nodes...\")\n\n        # Initialise config if not already done\n        self.sqlfluff_config = config\n        if not self.project_dir:\n            self.project_dir = self._get_project_dir()\n        if not self.profiles_dir:\n            self.profiles_dir = self._get_profiles_dir()\n\n        # Populate full paths for selected files\n        full_paths: Dict[str, str] = {}\n        selected_files = set()\n        for fname in fnames:\n            fpath = os.path.join(self.working_dir, fname)\n            full_paths[fpath] = fname\n            selected_files.add(fpath)\n\n        ephemeral_nodes: Dict[str, Tuple[str, Any]] = {}\n\n        # Extract the ephemeral models\n        for key, node in self.dbt_manifest.nodes.items():\n            if node.config.materialized == \"ephemeral\":\n                # The key is the full filepath.\n                # The value tuple, with the filepath and a list of dependent keys\n                ephemeral_nodes[key] = (\n                    os.path.join(self.project_dir, node.original_file_path),\n                    node.depends_on.nodes,\n                )\n\n        # Yield ephemeral nodes first. We use a Deque for efficient requeing.\n        # We iterate through the deque, yielding any nodes without dependents,\n        # or where those dependents have already yielded, first. The original\n        # mapping is still used to hold the metadata on each key.\n        already_yielded = set()\n        ephemeral_buffer: Deque[str] = deque(ephemeral_nodes.keys())\n        while ephemeral_buffer:\n            key = ephemeral_buffer.popleft()\n            fpath, dependents = ephemeral_nodes[key]\n\n            # If it's not in our selection, skip it\n            if fpath not in selected_files:\n                templater_logger.debug(\"- Purging unselected ephemeral: %r\", fpath)\n            # If there are dependent nodes in the set, don't process it yet.\n            elif any(\n                dependent in ephemeral_buffer for dependent in dependents\n            ):  # pragma: no cover\n                templater_logger.debug(\n                    \"- Requeuing ephemeral with dependents: %r\", fpath\n                )\n                # Requeue it for later\n                ephemeral_buffer.append(key)\n            # Otherwise yield it.\n            else:\n                templater_logger.debug(\"- Yielding Ephemeral: %r\", fpath)\n                yield full_paths[fpath]\n                already_yielded.add(full_paths[fpath])\n\n        for fname in fnames:\n            if fname not in already_yielded:\n                yield fname\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "sequence_files", "self", "fnames", "list", "str", "config", "none", "formatter", "none", "iterator", "str", "reorder", "fnames", "to", "process", "dependent", "files", "first", "this", "avoids", "errors", "when", "an", "ephemeral", "model", "is", "processed", "before", "use", "if", "formatter", "pragma", "no", "cover", "formatter", "dispatch_compilation_header", "dbt", "templater", "sorting", "nodes", "initialise", "config", "if", "not", "already", "done", "self", "sqlfluff_config", "config", "if", "not", "self", "project_dir", "self", "project_dir", "self", "_get_project_dir", "if", "not", "self", "profiles_dir", "self", "profiles_dir", "self", "_get_profiles_dir", "populate", "full", "paths", "for", "selected", "files", "full_paths", "dict", "str", "str", "selected_files", "set", "for", "fname", "in", "fnames", "fpath", "os", "path", "join", "self", "working_dir", "fname", "full_paths", "fpath", "fname", "selected_files", "add", "fpath", "ephemeral_nodes", "dict", "str", "tuple", "str", "any", "extract", "the", "ephemeral", "models", "for", "key", "node", "in", "self", "dbt_manifest", "nodes", "items", "if", "node", "config", "materialized", "ephemeral", "the", "key", "is", "the", "full", "filepath", "the", "value", "tuple", "with", "the", "filepath", "and", "a", "list", "of", "dependent", "keys", "ephemeral_nodes", "key", "os", "path", "join", "self", "project_dir", "node", "original_file_path", "node", "depends_on", "nodes", "yield", "ephemeral", "nodes", "first", "we", "use", "a", "deque", "for", "efficient", "requeing", "we", "iterate", "through", "the", "deque", "yielding", "any", "nodes", "without", "dependents", "or", "where", "those", "dependents", "have", "already", "yielded", "first", "the", "original", "mapping", "is", "still", "used", "to", "hold", "the", "metadata", "on", "each", "key", "already_yielded", "set", "ephemeral_buffer", "deque", "str", "deque", "ephemeral_nodes", "keys", "while", "ephemeral_buffer", "key", "ephemeral_buffer", "popleft", "fpath", "dependents", "ephemeral_nodes", "key", "if", "it", "s", "not", "in", "our", "selection", "skip", "it", "if", "fpath", "not", "in", "selected_files", "templater_logger", "debug", "purging", "unselected", "ephemeral", "r", "fpath", "if", "there", "are", "dependent", "nodes", "in", "the", "set", "don", "t", "process", "it", "yet", "elif", "any", "dependent", "in", "ephemeral_buffer", "for", "dependent", "in", "dependents", "pragma", "no", "cover", "templater_logger", "debug", "requeuing", "ephemeral", "with", "dependents", "r", "fpath", "requeue", "it", "for", "later", "ephemeral_buffer", "append", "key", "otherwise", "yield", "it", "else", "templater_logger", "debug", "yielding", "ephemeral", "r", "fpath", "yield", "full_paths", "fpath", "already_yielded", "add", "full_paths", "fpath", "for", "fname", "in", "fnames", "if", "fname", "not", "in", "already_yielded", "yield", "fname"], "doc_len": 304}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater.process", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "process", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def process(self, *, fname, in_str=None, config=None, formatter=None):\n        \"\"\"Compile a dbt model and return the compiled SQL.\n\n        Args:\n            fname (:obj:`str`): Path to dbt model(s)\n            in_str (:obj:`str`, optional): This is ignored for dbt\n            config (:obj:`FluffConfig`, optional): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n        \"\"\"\n        # Stash the formatter if provided to use in cached methods.\n        self.formatter = formatter\n        self.sqlfluff_config = config\n        self.project_dir = self._get_project_dir()\n        self.profiles_dir = self._get_profiles_dir()\n        fname_absolute_path = os.path.abspath(fname)\n\n        try:\n            os.chdir(self.project_dir)\n            processed_result = self._unsafe_process(fname_absolute_path, in_str, config)\n            # Reset the fail counter\n            self._sequential_fails = 0\n            return processed_result\n        except DbtCompilationException as e:\n            # Increment the counter\n            self._sequential_fails += 1\n            if e.node:\n                return None, [\n                    SQLTemplaterError(\n                        f\"dbt compilation error on file '{e.node.original_file_path}', {e.msg}\",\n                        # It's fatal if we're over the limit\n                        fatal=self._sequential_fails > self.sequential_fail_limit,\n                    )\n                ]\n            else:\n                raise  # pragma: no cover\n        except DbtFailedToConnectException as e:\n            return None, [\n                SQLTemplaterError(\n                    \"dbt tried to connect to the database and failed: \"\n                    \"you could use 'execute' https://docs.getdbt.com/reference/dbt-jinja-functions/execute/ \"\n                    f\"to skip the database calls. Error: {e.msg}\",\n                    fatal=True,\n                )\n            ]\n        # If a SQLFluff error is raised, just pass it through\n        except SQLTemplaterError as e:  # pragma: no cover\n            return None, [e]\n        finally:\n            os.chdir(self.working_dir)\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "process", "self", "fname", "in_str", "none", "config", "none", "formatter", "none", "compile", "a", "dbt", "model", "and", "return", "the", "compiled", "sql", "args", "fname", "obj", "str", "path", "to", "dbt", "model", "s", "in_str", "obj", "str", "optional", "this", "is", "ignored", "for", "dbt", "config", "obj", "fluffconfig", "optional", "a", "specific", "config", "to", "use", "for", "this", "templating", "operation", "only", "necessary", "for", "some", "templaters", "formatter", "obj", "callbackformatter", "optional", "object", "for", "output", "stash", "the", "formatter", "if", "provided", "to", "use", "in", "cached", "methods", "self", "formatter", "formatter", "self", "sqlfluff_config", "config", "self", "project_dir", "self", "_get_project_dir", "self", "profiles_dir", "self", "_get_profiles_dir", "fname_absolute_path", "os", "path", "abspath", "fname", "try", "os", "chdir", "self", "project_dir", "processed_result", "self", "_unsafe_process", "fname_absolute_path", "in_str", "config", "reset", "the", "fail", "counter", "self", "_sequential_fails", "0", "return", "processed_result", "except", "dbtcompilationexception", "as", "e", "increment", "the", "counter", "self", "_sequential_fails", "1", "if", "e", "node", "return", "none", "sqltemplatererror", "f", "dbt", "compilation", "error", "on", "file", "e", "node", "original_file_path", "e", "msg", "it", "s", "fatal", "if", "we", "re", "over", "the", "limit", "fatal", "self", "_sequential_fails", "self", "sequential_fail_limit", "else", "raise", "pragma", "no", "cover", "except", "dbtfailedtoconnectexception", "as", "e", "return", "none", "sqltemplatererror", "dbt", "tried", "to", "connect", "to", "the", "database", "and", "failed", "you", "could", "use", "execute", "https", "docs", "getdbt", "com", "reference", "dbt", "jinja", "functions", "execute", "f", "to", "skip", "the", "database", "calls", "error", "e", "msg", "fatal", "true", "if", "a", "sqlfluff", "error", "is", "raised", "just", "pass", "it", "through", "except", "sqltemplatererror", "as", "e", "pragma", "no", "cover", "return", "none", "e", "finally", "os", "chdir", "self", "working_dir"], "doc_len": 230}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater._find_node", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "_find_node", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def _find_node(self, fname, config=None):\n        if not config:  # pragma: no cover\n            raise ValueError(\n                \"For the dbt templater, the `process()` method requires a config object.\"\n            )\n        if not fname:  # pragma: no cover\n            raise ValueError(\n                \"For the dbt templater, the `process()` method requires a file name\"\n            )\n        elif fname == \"stdin\":  # pragma: no cover\n            raise ValueError(\n                \"The dbt templater does not support stdin input, provide a path instead\"\n            )\n        selected = self.dbt_selector_method.search(\n            included_nodes=self.dbt_manifest.nodes,\n            # Selector needs to be a relative path\n            selector=os.path.relpath(fname, start=os.getcwd()),\n        )\n        results = [self.dbt_manifest.expect(uid) for uid in selected]\n\n        if not results:\n            model_name = os.path.splitext(os.path.basename(fname))[0]\n            disabled_model = self.dbt_manifest.find_disabled_by_name(name=model_name)\n            if disabled_model and os.path.abspath(\n                disabled_model.original_file_path\n            ) == os.path.abspath(fname):\n                raise SQLTemplaterSkipFile(\n                    f\"Skipped file {fname} because the model was disabled\"\n                )\n            raise RuntimeError(\n                \"File %s was not found in dbt project\" % fname\n            )  # pragma: no cover\n        return results[0]\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "_find_node", "self", "fname", "config", "none", "if", "not", "config", "pragma", "no", "cover", "raise", "valueerror", "for", "the", "dbt", "templater", "the", "process", "method", "requires", "a", "config", "object", "if", "not", "fname", "pragma", "no", "cover", "raise", "valueerror", "for", "the", "dbt", "templater", "the", "process", "method", "requires", "a", "file", "name", "elif", "fname", "stdin", "pragma", "no", "cover", "raise", "valueerror", "the", "dbt", "templater", "does", "not", "support", "stdin", "input", "provide", "a", "path", "instead", "selected", "self", "dbt_selector_method", "search", "included_nodes", "self", "dbt_manifest", "nodes", "selector", "needs", "to", "be", "a", "relative", "path", "selector", "os", "path", "relpath", "fname", "start", "os", "getcwd", "results", "self", "dbt_manifest", "expect", "uid", "for", "uid", "in", "selected", "if", "not", "results", "model_name", "os", "path", "splitext", "os", "path", "basename", "fname", "0", "disabled_model", "self", "dbt_manifest", "find_disabled_by_name", "name", "model_name", "if", "disabled_model", "and", "os", "path", "abspath", "disabled_model", "original_file_path", "os", "path", "abspath", "fname", "raise", "sqltemplaterskipfile", "f", "skipped", "file", "fname", "because", "the", "model", "was", "disabled", "raise", "runtimeerror", "file", "s", "was", "not", "found", "in", "dbt", "project", "fname", "pragma", "no", "cover", "return", "results", "0"], "doc_len": 162}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py::DbtTemplater._unsafe_process", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py", "class_name": "DbtTemplater", "func_name": "_unsafe_process", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/templater.py, 类名: DbtTemplater\n    def _unsafe_process(self, fname, in_str=None, config=None):\n        node = self._find_node(fname, config)\n\n        node = self.dbt_compiler.compile_node(\n            node=node,\n            manifest=self.dbt_manifest,\n        )\n\n        if hasattr(node, \"injected_sql\"):\n            # If injected SQL is present, it contains a better picture\n            # of what will actually hit the database (e.g. with tests).\n            # However it's not always present.\n            compiled_sql = node.injected_sql\n        else:\n            compiled_sql = node.compiled_sql\n\n        if not compiled_sql:  # pragma: no cover\n            raise SQLTemplaterError(\n                \"dbt templater compilation failed silently, check your configuration \"\n                \"by running `dbt compile` directly.\"\n            )\n\n        with open(fname) as source_dbt_model:\n            source_dbt_sql = source_dbt_model.read()\n\n        n_trailing_newlines = len(source_dbt_sql) - len(source_dbt_sql.rstrip(\"\\n\"))\n\n        templater_logger.debug(\n            \"    Trailing newline count in source dbt model: %r\", n_trailing_newlines\n        )\n        templater_logger.debug(\"    Raw SQL before compile: %r\", source_dbt_sql)\n        templater_logger.debug(\"    Node raw SQL: %r\", node.raw_sql)\n        templater_logger.debug(\"    Node compiled SQL: %r\", compiled_sql)\n\n        # When using dbt-templater, trailing newlines are ALWAYS REMOVED during\n        # compiling. Unless fixed (like below), this will cause:\n        #    1. L009 linting errors when running \"sqlfluff lint foo_bar.sql\"\n        #       since the linter will use the compiled code with the newlines\n        #       removed.\n        #    2. \"No newline at end of file\" warnings in Git/GitHub since\n        #       sqlfluff uses the compiled SQL to write fixes back to the\n        #       source SQL in the dbt model.\n        # The solution is:\n        #    1. Check for trailing newlines before compiling by looking at the\n        #       raw SQL in the source dbt file, store the count of trailing newlines.\n        #    2. Append the count from #1 above to the node.raw_sql and\n        #       compiled_sql objects, both of which have had the trailing\n        #       newlines removed by the dbt-templater.\n        node.raw_sql = node.raw_sql + \"\\n\" * n_trailing_newlines\n        compiled_sql = compiled_sql + \"\\n\" * n_trailing_newlines\n\n        raw_sliced, sliced_file, templated_sql = self.slice_file(\n            node.raw_sql,\n            compiled_sql,\n            config=config,\n        )\n\n        return (\n            TemplatedFile(\n                source_str=node.raw_sql,\n                templated_str=templated_sql,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            # No violations returned in this way.\n            [],\n        )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "templater", "py", "dbttemplater", "def", "_unsafe_process", "self", "fname", "in_str", "none", "config", "none", "node", "self", "_find_node", "fname", "config", "node", "self", "dbt_compiler", "compile_node", "node", "node", "manifest", "self", "dbt_manifest", "if", "hasattr", "node", "injected_sql", "if", "injected", "sql", "is", "present", "it", "contains", "a", "better", "picture", "of", "what", "will", "actually", "hit", "the", "database", "e", "g", "with", "tests", "however", "it", "s", "not", "always", "present", "compiled_sql", "node", "injected_sql", "else", "compiled_sql", "node", "compiled_sql", "if", "not", "compiled_sql", "pragma", "no", "cover", "raise", "sqltemplatererror", "dbt", "templater", "compilation", "failed", "silently", "check", "your", "configuration", "by", "running", "dbt", "compile", "directly", "with", "open", "fname", "as", "source_dbt_model", "source_dbt_sql", "source_dbt_model", "read", "n_trailing_newlines", "len", "source_dbt_sql", "len", "source_dbt_sql", "rstrip", "n", "templater_logger", "debug", "trailing", "newline", "count", "in", "source", "dbt", "model", "r", "n_trailing_newlines", "templater_logger", "debug", "raw", "sql", "before", "compile", "r", "source_dbt_sql", "templater_logger", "debug", "node", "raw", "sql", "r", "node", "raw_sql", "templater_logger", "debug", "node", "compiled", "sql", "r", "compiled_sql", "when", "using", "dbt", "templater", "trailing", "newlines", "are", "always", "removed", "during", "compiling", "unless", "fixed", "like", "below", "this", "will", "cause", "1", "l009", "linting", "errors", "when", "running", "sqlfluff", "lint", "foo_bar", "sql", "since", "the", "linter", "will", "use", "the", "compiled", "code", "with", "the", "newlines", "removed", "2", "no", "newline", "at", "end", "of", "file", "warnings", "in", "git", "github", "since", "sqlfluff", "uses", "the", "compiled", "sql", "to", "write", "fixes", "back", "to", "the", "source", "sql", "in", "the", "dbt", "model", "the", "solution", "is", "1", "check", "for", "trailing", "newlines", "before", "compiling", "by", "looking", "at", "the", "raw", "sql", "in", "the", "source", "dbt", "file", "store", "the", "count", "of", "trailing", "newlines", "2", "append", "the", "count", "from", "1", "above", "to", "the", "node", "raw_sql", "and", "compiled_sql", "objects", "both", "of", "which", "have", "had", "the", "trailing", "newlines", "removed", "by", "the", "dbt", "templater", "node", "raw_sql", "node", "raw_sql", "n", "n_trailing_newlines", "compiled_sql", "compiled_sql", "n", "n_trailing_newlines", "raw_sliced", "sliced_file", "templated_sql", "self", "slice_file", "node", "raw_sql", "compiled_sql", "config", "config", "return", "templatedfile", "source_str", "node", "raw_sql", "templated_str", "templated_sql", "fname", "fname", "sliced_file", "sliced_file", "raw_sliced", "raw_sliced", "no", "violations", "returned", "in", "this", "way"], "doc_len": 300}
{"doc_id": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/__init__.py::get_templaters", "file_path": "plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/__init__.py", "class_name": null, "func_name": "get_templaters", "text": "文件路径: plugins/sqlfluff-templater-dbt/sqlfluff_templater_dbt/__init__.py\ndef get_templaters():\n    \"\"\"Get templaters.\"\"\"\n    return [DbtTemplater]\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "sqlfluff_templater_dbt", "__init__", "py", "def", "get_templaters", "get", "templaters", "return", "dbttemplater"], "doc_len": 13}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/linter_test.py::test__linter__skip_dbt_model_disabled", "file_path": "plugins/sqlfluff-templater-dbt/test/linter_test.py", "class_name": null, "func_name": "test__linter__skip_dbt_model_disabled", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/linter_test.py\ndef test__linter__skip_dbt_model_disabled(project_dir):  # noqa\n    \"\"\"Test that the linter skips disabled dbt models.\"\"\"\n    conf = FluffConfig(configs=DBT_FLUFF_CONFIG)\n    lntr = Linter(config=conf)\n    model_file_path = os.path.join(\n        project_dir, \"models/my_new_project/disabled_model.sql\"\n    )\n    linted_path = lntr.lint_path(path=model_file_path)\n    # Check that the file is still there\n    assert len(linted_path.files) == 1\n    linted_file = linted_path.files[0]\n    # Normalise paths to control for OS variance\n    assert os.path.normpath(linted_file.path) == os.path.normpath(model_file_path)\n    assert not linted_file.templated_file\n    assert not linted_file.tree\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "linter_test", "py", "def", "test__linter__skip_dbt_model_disabled", "project_dir", "noqa", "test", "that", "the", "linter", "skips", "disabled", "dbt", "models", "conf", "fluffconfig", "configs", "dbt_fluff_config", "lntr", "linter", "config", "conf", "model_file_path", "os", "path", "join", "project_dir", "models", "my_new_project", "disabled_model", "sql", "linted_path", "lntr", "lint_path", "path", "model_file_path", "check", "that", "the", "file", "is", "still", "there", "assert", "len", "linted_path", "files", "1", "linted_file", "linted_path", "files", "0", "normalise", "paths", "to", "control", "for", "os", "variance", "assert", "os", "path", "normpath", "linted_file", "path", "os", "path", "normpath", "model_file_path", "assert", "not", "linted_file", "templated_file", "assert", "not", "linted_file", "tree"], "doc_len": 82}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/rules_test.py::test__rules__std_file_dbt", "file_path": "plugins/sqlfluff-templater-dbt/test/rules_test.py", "class_name": null, "func_name": "test__rules__std_file_dbt", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/rules_test.py\ndef test__rules__std_file_dbt(rule, path, violations, project_dir):  # noqa\n    \"\"\"Test the linter finds the given errors in (and only in) the right places (DBT).\"\"\"\n    assert_rule_raises_violations_in_file(\n        rule=rule,\n        fpath=os.path.join(project_dir, path),\n        violations=violations,\n        fluff_config=FluffConfig(configs=DBT_FLUFF_CONFIG, overrides=dict(rules=rule)),\n    )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "rules_test", "py", "def", "test__rules__std_file_dbt", "rule", "path", "violations", "project_dir", "noqa", "test", "the", "linter", "finds", "the", "given", "errors", "in", "and", "only", "in", "the", "right", "places", "dbt", "assert_rule_raises_violations_in_file", "rule", "rule", "fpath", "os", "path", "join", "project_dir", "path", "violations", "violations", "fluff_config", "fluffconfig", "configs", "dbt_fluff_config", "overrides", "dict", "rules", "rule"], "doc_len": 48}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_missing", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_missing", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_missing(dbt_templater, project_dir):  # noqa: F811\n    \"\"\"Check that a nice error is returned when dbt module is missing.\"\"\"\n    try:\n        import dbt  # noqa: F401\n\n        pytest.skip(msg=\"dbt is installed\")\n    except ModuleNotFoundError:\n        pass\n\n    with pytest.raises(ModuleNotFoundError, match=r\"pip install sqlfluff\\[dbt\\]\"):\n        dbt_templater.process(\n            in_str=\"\",\n            fname=os.path.join(project_dir, \"models/my_new_project/test.sql\"),\n            config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n        )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_missing", "dbt_templater", "project_dir", "noqa", "f811", "check", "that", "a", "nice", "error", "is", "returned", "when", "dbt", "module", "is", "missing", "try", "import", "dbt", "noqa", "f401", "pytest", "skip", "msg", "dbt", "is", "installed", "except", "modulenotfounderror", "pass", "with", "pytest", "raises", "modulenotfounderror", "match", "r", "pip", "install", "sqlfluff", "dbt", "dbt_templater", "process", "in_str", "fname", "os", "path", "join", "project_dir", "models", "my_new_project", "test", "sql", "config", "fluffconfig", "configs", "dbt_fluff_config"], "doc_len": 65}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_profiles_dir_expanded", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_profiles_dir_expanded", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_profiles_dir_expanded(dbt_templater):  # noqa: F811\n    \"\"\"Check that the profiles_dir is expanded.\"\"\"\n    dbt_templater.sqlfluff_config = FluffConfig(\n        configs={\"templater\": {\"dbt\": {\"profiles_dir\": \"~/.dbt\"}}}\n    )\n    profiles_dir = dbt_templater._get_profiles_dir()\n    # Normalise paths to control for OS variance\n    assert os.path.normpath(profiles_dir) == os.path.normpath(\n        os.path.expanduser(\"~/.dbt\")\n    )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_profiles_dir_expanded", "dbt_templater", "noqa", "f811", "check", "that", "the", "profiles_dir", "is", "expanded", "dbt_templater", "sqlfluff_config", "fluffconfig", "configs", "templater", "dbt", "profiles_dir", "dbt", "profiles_dir", "dbt_templater", "_get_profiles_dir", "normalise", "paths", "to", "control", "for", "os", "variance", "assert", "os", "path", "normpath", "profiles_dir", "os", "path", "normpath", "os", "path", "expanduser", "dbt"], "doc_len": 48}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_templating_result", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_templating_result", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_templating_result(\n    project_dir, dbt_templater, fname  # noqa: F811\n):\n    \"\"\"Test that input sql file gets templated into output sql file.\"\"\"\n    templated_file, _ = dbt_templater.process(\n        in_str=\"\",\n        fname=os.path.join(project_dir, \"models/my_new_project/\", fname),\n        config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n    )\n    assert (\n        str(templated_file)\n        == open(\"plugins/sqlfluff-templater-dbt/test/fixtures/dbt/\" + fname).read()\n    )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_templating_result", "project_dir", "dbt_templater", "fname", "noqa", "f811", "test", "that", "input", "sql", "file", "gets", "templated", "into", "output", "sql", "file", "templated_file", "_", "dbt_templater", "process", "in_str", "fname", "os", "path", "join", "project_dir", "models", "my_new_project", "fname", "config", "fluffconfig", "configs", "dbt_fluff_config", "assert", "str", "templated_file", "open", "plugins", "sqlfluff", "templater", "dbt", "test", "fixtures", "dbt", "fname", "read"], "doc_len": 55}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_sequence_files_ephemeral_dependency", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_sequence_files_ephemeral_dependency", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_sequence_files_ephemeral_dependency(\n    project_dir, dbt_templater, fnames_input, fnames_expected_sequence  # noqa: F811\n):\n    \"\"\"Test that dbt templater sequences files based on dependencies.\"\"\"\n    result = dbt_templater.sequence_files(\n        [str(Path(project_dir) / fn) for fn in fnames_input],\n        config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n    )\n    pd = Path(project_dir)\n    expected = [str(pd / fn) for fn in fnames_expected_sequence]\n    assert list(result) == expected\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_sequence_files_ephemeral_dependency", "project_dir", "dbt_templater", "fnames_input", "fnames_expected_sequence", "noqa", "f811", "test", "that", "dbt", "templater", "sequences", "files", "based", "on", "dependencies", "result", "dbt_templater", "sequence_files", "str", "path", "project_dir", "fn", "for", "fn", "in", "fnames_input", "config", "fluffconfig", "configs", "dbt_fluff_config", "pd", "path", "project_dir", "expected", "str", "pd", "fn", "for", "fn", "in", "fnames_expected_sequence", "assert", "list", "result", "expected"], "doc_len": 54}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_slice_file_wrapped_test", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_slice_file_wrapped_test", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_slice_file_wrapped_test(\n    raw_file, templated_file, result, dbt_templater, caplog  # noqa: F811\n):\n    \"\"\"Test that wrapped queries are sliced safely using _check_for_wrapped().\"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.templater\"):\n        _, resp, _ = dbt_templater.slice_file(\n            raw_file,\n            templated_file,\n        )\n    assert resp == result\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_slice_file_wrapped_test", "raw_file", "templated_file", "result", "dbt_templater", "caplog", "noqa", "f811", "test", "that", "wrapped", "queries", "are", "sliced", "safely", "using", "_check_for_wrapped", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "templater", "_", "resp", "_", "dbt_templater", "slice_file", "raw_file", "templated_file", "assert", "resp", "result"], "doc_len": 43}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_templating_test_lex", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_templating_test_lex", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_templating_test_lex(\n    project_dir, dbt_templater, fname  # noqa: F811\n):\n    \"\"\"A test to demonstrate the lexer works on both dbt models (with any # of trailing newlines) and dbt tests.\"\"\"\n    source_fpath = os.path.join(project_dir, fname)\n    with open(source_fpath, \"r\") as source_dbt_model:\n        source_dbt_sql = source_dbt_model.read()\n    n_trailing_newlines = len(source_dbt_sql) - len(source_dbt_sql.rstrip(\"\\n\"))\n    lexer = Lexer(config=FluffConfig(configs=DBT_FLUFF_CONFIG))\n    templated_file, _ = dbt_templater.process(\n        in_str=\"\",\n        fname=os.path.join(project_dir, fname),\n        config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n    )\n    tokens, lex_vs = lexer.lex(templated_file)\n    assert (\n        templated_file.source_str\n        == \"select a\\nfrom table_a\" + \"\\n\" * n_trailing_newlines\n    )\n    assert (\n        templated_file.templated_str\n        == \"select a\\nfrom table_a\" + \"\\n\" * n_trailing_newlines\n    )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_templating_test_lex", "project_dir", "dbt_templater", "fname", "noqa", "f811", "a", "test", "to", "demonstrate", "the", "lexer", "works", "on", "both", "dbt", "models", "with", "any", "of", "trailing", "newlines", "and", "dbt", "tests", "source_fpath", "os", "path", "join", "project_dir", "fname", "with", "open", "source_fpath", "r", "as", "source_dbt_model", "source_dbt_sql", "source_dbt_model", "read", "n_trailing_newlines", "len", "source_dbt_sql", "len", "source_dbt_sql", "rstrip", "n", "lexer", "lexer", "config", "fluffconfig", "configs", "dbt_fluff_config", "templated_file", "_", "dbt_templater", "process", "in_str", "fname", "os", "path", "join", "project_dir", "fname", "config", "fluffconfig", "configs", "dbt_fluff_config", "tokens", "lex_vs", "lexer", "lex", "templated_file", "assert", "templated_file", "source_str", "select", "a", "nfrom", "table_a", "n", "n_trailing_newlines", "assert", "templated_file", "templated_str", "select", "a", "nfrom", "table_a", "n", "n_trailing_newlines"], "doc_len": 99}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_skips_disabled_model", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_skips_disabled_model", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_skips_disabled_model(dbt_templater, project_dir):  # noqa: F811\n    \"\"\"A disabled dbt model should be skipped.\"\"\"\n    with pytest.raises(SQLTemplaterSkipFile, match=r\"model was disabled\"):\n        dbt_templater.process(\n            in_str=\"\",\n            fname=os.path.join(project_dir, \"models/my_new_project/disabled_model.sql\"),\n            config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n        )\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_skips_disabled_model", "dbt_templater", "project_dir", "noqa", "f811", "a", "disabled", "dbt", "model", "should", "be", "skipped", "with", "pytest", "raises", "sqltemplaterskipfile", "match", "r", "model", "was", "disabled", "dbt_templater", "process", "in_str", "fname", "os", "path", "join", "project_dir", "models", "my_new_project", "disabled_model", "sql", "config", "fluffconfig", "configs", "dbt_fluff_config"], "doc_len": 45}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__dbt_templated_models_do_not_raise_lint_error", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__dbt_templated_models_do_not_raise_lint_error", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__dbt_templated_models_do_not_raise_lint_error(\n    project_dir, fname  # noqa: F811\n):\n    \"\"\"Test that templated dbt models do not raise a linting error.\"\"\"\n    lntr = Linter(config=FluffConfig(configs=DBT_FLUFF_CONFIG))\n    lnt = lntr.lint_path(\n        path=os.path.join(project_dir, \"models/my_new_project/\", fname)\n    )\n    violations = lnt.check_tuples()\n    assert len(violations) == 0\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__dbt_templated_models_do_not_raise_lint_error", "project_dir", "fname", "noqa", "f811", "test", "that", "templated", "dbt", "models", "do", "not", "raise", "a", "linting", "error", "lntr", "linter", "config", "fluffconfig", "configs", "dbt_fluff_config", "lnt", "lntr", "lint_path", "path", "os", "path", "join", "project_dir", "models", "my_new_project", "fname", "violations", "lnt", "check_tuples", "assert", "len", "violations", "0"], "doc_len": 48}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__dbt_templated_models_fix_does_not_corrupt_file", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__dbt_templated_models_fix_does_not_corrupt_file", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__dbt_templated_models_fix_does_not_corrupt_file(project_dir):  # noqa: F811\n    \"\"\"Test fix for issue 1608. Previously \"sqlfluff fix\" corrupted the file.\"\"\"\n    lntr = Linter(config=FluffConfig(configs=DBT_FLUFF_CONFIG))\n    lnt = lntr.lint_path(\n        os.path.join(project_dir, \"models/my_new_project/issue_1608.sql\"), fix=True\n    )\n    lnt.persist_changes(fixed_file_suffix=\"FIXED\")\n    with open(\n        os.path.join(project_dir, \"models/my_new_project/issue_1608.sql.after\")\n    ) as f:\n        comp_buff = f.read()\n    with open(\n        os.path.join(project_dir, \"models/my_new_project/issue_1608FIXED.sql\")\n    ) as f:\n        fixed_buff = f.read()\n    assert fixed_buff == comp_buff\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__dbt_templated_models_fix_does_not_corrupt_file", "project_dir", "noqa", "f811", "test", "fix", "for", "issue", "1608", "previously", "sqlfluff", "fix", "corrupted", "the", "file", "lntr", "linter", "config", "fluffconfig", "configs", "dbt_fluff_config", "lnt", "lntr", "lint_path", "os", "path", "join", "project_dir", "models", "my_new_project", "issue_1608", "sql", "fix", "true", "lnt", "persist_changes", "fixed_file_suffix", "fixed", "with", "open", "os", "path", "join", "project_dir", "models", "my_new_project", "issue_1608", "sql", "after", "as", "f", "comp_buff", "f", "read", "with", "open", "os", "path", "join", "project_dir", "models", "my_new_project", "issue_1608fixed", "sql", "as", "f", "fixed_buff", "f", "read", "assert", "fixed_buff", "comp_buff"], "doc_len": 80}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_templating_absolute_path", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_templating_absolute_path", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_templating_absolute_path(\n    project_dir, dbt_templater  # noqa: F811\n):\n    \"\"\"Test that absolute path of input path does not cause RuntimeError.\"\"\"\n    try:\n        dbt_templater.process(\n            in_str=\"\",\n            fname=os.path.abspath(\n                os.path.join(project_dir, \"models/my_new_project/use_var.sql\")\n            ),\n            config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n        )\n    except Exception as e:\n        pytest.fail(f\"Unexpected RuntimeError: {e}\")\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_templating_absolute_path", "project_dir", "dbt_templater", "noqa", "f811", "test", "that", "absolute", "path", "of", "input", "path", "does", "not", "cause", "runtimeerror", "try", "dbt_templater", "process", "in_str", "fname", "os", "path", "abspath", "os", "path", "join", "project_dir", "models", "my_new_project", "use_var", "sql", "config", "fluffconfig", "configs", "dbt_fluff_config", "except", "exception", "as", "e", "pytest", "fail", "f", "unexpected", "runtimeerror", "e"], "doc_len": 54}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__templater_dbt_handle_exceptions", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__templater_dbt_handle_exceptions", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__templater_dbt_handle_exceptions(\n    project_dir, dbt_templater, fname, exception_msg  # noqa: F811\n):\n    \"\"\"Test that exceptions during compilation are returned as violation.\"\"\"\n    from dbt.adapters.factory import get_adapter\n\n    src_fpath = \"plugins/sqlfluff-templater-dbt/test/fixtures/dbt/error_models/\" + fname\n    target_fpath = os.path.abspath(\n        os.path.join(project_dir, \"models/my_new_project/\", fname)\n    )\n    # We move the file that throws an error in and out of the project directory\n    # as dbt throws an error if a node fails to parse while computing the DAG\n    os.rename(src_fpath, target_fpath)\n    try:\n        _, violations = dbt_templater.process(\n            in_str=\"\",\n            fname=target_fpath,\n            config=FluffConfig(configs=DBT_FLUFF_CONFIG),\n        )\n    finally:\n        get_adapter(dbt_templater.dbt_config).connections.release()\n        os.rename(target_fpath, src_fpath)\n    assert violations\n    # NB: Replace slashes to deal with different plaform paths being returned.\n    assert violations[0].desc().replace(\"\\\\\", \"/\").startswith(exception_msg)\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__templater_dbt_handle_exceptions", "project_dir", "dbt_templater", "fname", "exception_msg", "noqa", "f811", "test", "that", "exceptions", "during", "compilation", "are", "returned", "as", "violation", "from", "dbt", "adapters", "factory", "import", "get_adapter", "src_fpath", "plugins", "sqlfluff", "templater", "dbt", "test", "fixtures", "dbt", "error_models", "fname", "target_fpath", "os", "path", "abspath", "os", "path", "join", "project_dir", "models", "my_new_project", "fname", "we", "move", "the", "file", "that", "throws", "an", "error", "in", "and", "out", "of", "the", "project", "directory", "as", "dbt", "throws", "an", "error", "if", "a", "node", "fails", "to", "parse", "while", "computing", "the", "dag", "os", "rename", "src_fpath", "target_fpath", "try", "_", "violations", "dbt_templater", "process", "in_str", "fname", "target_fpath", "config", "fluffconfig", "configs", "dbt_fluff_config", "finally", "get_adapter", "dbt_templater", "dbt_config", "connections", "release", "os", "rename", "target_fpath", "src_fpath", "assert", "violations", "nb", "replace", "slashes", "to", "deal", "with", "different", "plaform", "paths", "being", "returned", "assert", "violations", "0", "desc", "replace", "startswith", "exception_msg"], "doc_len": 127}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/templater_test.py::test__project_dir_does_not_exist_error", "file_path": "plugins/sqlfluff-templater-dbt/test/templater_test.py", "class_name": null, "func_name": "test__project_dir_does_not_exist_error", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/templater_test.py\ndef test__project_dir_does_not_exist_error(dbt_templater, caplog):  # noqa: F811\n    \"\"\"Test that an error is logged if the specified dbt project directory doesn't exist.\"\"\"\n    dbt_templater.sqlfluff_config = FluffConfig(\n        configs={\"templater\": {\"dbt\": {\"project_dir\": \"./non_existing_directory\"}}}\n    )\n    logger = logging.getLogger(\"sqlfluff\")\n    original_propagate_value = logger.propagate\n    try:\n        logger.propagate = True\n        with caplog.at_level(logging.ERROR, logger=\"sqlfluff.templater\"):\n            dbt_project_dir = dbt_templater._get_project_dir()\n        assert (\n            f\"dbt_project_dir: {dbt_project_dir} could not be accessed. Check it exists.\"\n            in caplog.text\n        )\n    finally:\n        logger.propagate = original_propagate_value\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "templater_test", "py", "def", "test__project_dir_does_not_exist_error", "dbt_templater", "caplog", "noqa", "f811", "test", "that", "an", "error", "is", "logged", "if", "the", "specified", "dbt", "project", "directory", "doesn", "t", "exist", "dbt_templater", "sqlfluff_config", "fluffconfig", "configs", "templater", "dbt", "project_dir", "non_existing_directory", "logger", "logging", "getlogger", "sqlfluff", "original_propagate_value", "logger", "propagate", "try", "logger", "propagate", "true", "with", "caplog", "at_level", "logging", "error", "logger", "sqlfluff", "templater", "dbt_project_dir", "dbt_templater", "_get_project_dir", "assert", "f", "dbt_project_dir", "dbt_project_dir", "could", "not", "be", "accessed", "check", "it", "exists", "in", "caplog", "text", "finally", "logger", "propagate", "original_propagate_value"], "doc_len": 76}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py::project_dir", "file_path": "plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py", "class_name": null, "func_name": "project_dir", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py\ndef project_dir():\n    \"\"\"Returns the dbt project directory.\"\"\"\n    return DBT_FLUFF_CONFIG[\"templater\"][\"dbt\"][\"project_dir\"]\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "fixtures", "dbt", "templater", "py", "def", "project_dir", "returns", "the", "dbt", "project", "directory", "return", "dbt_fluff_config", "templater", "dbt", "project_dir"], "doc_len": 21}
{"doc_id": "plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py::dbt_templater", "file_path": "plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py", "class_name": null, "func_name": "dbt_templater", "text": "文件路径: plugins/sqlfluff-templater-dbt/test/fixtures/dbt/templater.py\ndef dbt_templater():\n    \"\"\"Returns an instance of the DbtTemplater.\"\"\"\n    return FluffConfig().get_templater(\"dbt\")\n", "tokens": ["plugins", "sqlfluff", "templater", "dbt", "test", "fixtures", "dbt", "templater", "py", "def", "dbt_templater", "returns", "an", "instance", "of", "the", "dbttemplater", "return", "fluffconfig", "get_templater", "dbt"], "doc_len": 21}
{"doc_id": "src/sqlfluff/diff_quality_plugin.py::SQLFluffViolationReporter.__init__", "file_path": "src/sqlfluff/diff_quality_plugin.py", "class_name": "SQLFluffViolationReporter", "func_name": "__init__", "text": "文件路径: src/sqlfluff/diff_quality_plugin.py, 类名: SQLFluffViolationReporter\n    def __init__(self):\n        \"\"\"Calls the base class constructor to set the object's name.\"\"\"\n        super().__init__(\"sqlfluff\")\n", "tokens": ["src", "sqlfluff", "diff_quality_plugin", "py", "sqlfluffviolationreporter", "def", "__init__", "self", "calls", "the", "base", "class", "constructor", "to", "set", "the", "object", "s", "name", "super", "__init__", "sqlfluff"], "doc_len": 22}
{"doc_id": "src/sqlfluff/diff_quality_plugin.py::SQLFluffViolationReporter.violations", "file_path": "src/sqlfluff/diff_quality_plugin.py", "class_name": "SQLFluffViolationReporter", "func_name": "violations", "text": "文件路径: src/sqlfluff/diff_quality_plugin.py, 类名: SQLFluffViolationReporter\n    def violations(src_path: str) -> List[Violation]:\n        \"\"\"Return list of violations.\n\n        Given the path to a .sql file, analyze it and return a list of\n        violations (i.e. formatting or style issues).\n        \"\"\"\n        linter = Linter(config=FluffConfig.from_root())\n        linted_path = linter.lint_path(src_path, ignore_non_existent_files=True)\n        result = []\n        for violation in linted_path.get_violations():\n            try:\n                # Normal SQLFluff warnings\n                message = f\"{violation.rule_code()}: {violation.description}\"\n            except AttributeError:\n                # Parse errors\n                message = str(violation)\n            result.append(Violation(violation.line_no, message))\n        return result\n", "tokens": ["src", "sqlfluff", "diff_quality_plugin", "py", "sqlfluffviolationreporter", "def", "violations", "src_path", "str", "list", "violation", "return", "list", "of", "violations", "given", "the", "path", "to", "a", "sql", "file", "analyze", "it", "and", "return", "a", "list", "of", "violations", "i", "e", "formatting", "or", "style", "issues", "linter", "linter", "config", "fluffconfig", "from_root", "linted_path", "linter", "lint_path", "src_path", "ignore_non_existent_files", "true", "result", "for", "violation", "in", "linted_path", "get_violations", "try", "normal", "sqlfluff", "warnings", "message", "f", "violation", "rule_code", "violation", "description", "except", "attributeerror", "parse", "errors", "message", "str", "violation", "result", "append", "violation", "violation", "line_no", "message", "return", "result"], "doc_len": 78}
{"doc_id": "src/sqlfluff/diff_quality_plugin.py::SQLFluffViolationReporter.measured_lines", "file_path": "src/sqlfluff/diff_quality_plugin.py", "class_name": "SQLFluffViolationReporter", "func_name": "measured_lines", "text": "文件路径: src/sqlfluff/diff_quality_plugin.py, 类名: SQLFluffViolationReporter\n    def measured_lines(self, src_path: str) -> None:  # pragma: no cover\n        \"\"\"Return list of the lines in src_path that were measured.\"\"\"\n", "tokens": ["src", "sqlfluff", "diff_quality_plugin", "py", "sqlfluffviolationreporter", "def", "measured_lines", "self", "src_path", "str", "none", "pragma", "no", "cover", "return", "list", "of", "the", "lines", "in", "src_path", "that", "were", "measured"], "doc_len": 24}
{"doc_id": "src/sqlfluff/diff_quality_plugin.py::diff_cover_report_quality", "file_path": "src/sqlfluff/diff_quality_plugin.py", "class_name": null, "func_name": "diff_cover_report_quality", "text": "文件路径: src/sqlfluff/diff_quality_plugin.py\ndef diff_cover_report_quality() -> SQLFluffViolationReporter:\n    \"\"\"Returns the SQLFluff plugin.\n\n    This function is registered as a diff_cover entry point. diff-quality calls\n    it in order to \"discover\" the SQLFluff plugin.\n\n    :return: Object that implements the BaseViolationReporter ABC\n    \"\"\"\n    return SQLFluffViolationReporter()\n", "tokens": ["src", "sqlfluff", "diff_quality_plugin", "py", "def", "diff_cover_report_quality", "sqlfluffviolationreporter", "returns", "the", "sqlfluff", "plugin", "this", "function", "is", "registered", "as", "a", "diff_cover", "entry", "point", "diff", "quality", "calls", "it", "in", "order", "to", "discover", "the", "sqlfluff", "plugin", "return", "object", "that", "implements", "the", "baseviolationreporter", "abc", "return", "sqlfluffviolationreporter"], "doc_len": 40}
{"doc_id": "src/sqlfluff/api/info.py::list_rules", "file_path": "src/sqlfluff/api/info.py", "class_name": null, "func_name": "list_rules", "text": "文件路径: src/sqlfluff/api/info.py\ndef list_rules() -> List[RuleTuple]:\n    \"\"\"Return a list of available rule tuples.\"\"\"\n    linter = Linter()\n    return linter.rule_tuples()\n", "tokens": ["src", "sqlfluff", "api", "info", "py", "def", "list_rules", "list", "ruletuple", "return", "a", "list", "of", "available", "rule", "tuples", "linter", "linter", "return", "linter", "rule_tuples"], "doc_len": 21}
{"doc_id": "src/sqlfluff/api/info.py::list_dialects", "file_path": "src/sqlfluff/api/info.py", "class_name": null, "func_name": "list_dialects", "text": "文件路径: src/sqlfluff/api/info.py\ndef list_dialects() -> List[DialectTuple]:\n    \"\"\"Return a list of available dialect info.\"\"\"\n    return list(dialect_readout())\n", "tokens": ["src", "sqlfluff", "api", "info", "py", "def", "list_dialects", "list", "dialecttuple", "return", "a", "list", "of", "available", "dialect", "info", "return", "list", "dialect_readout"], "doc_len": 19}
{"doc_id": "src/sqlfluff/api/simple.py::APIParsingError.__init__", "file_path": "src/sqlfluff/api/simple.py", "class_name": "APIParsingError", "func_name": "__init__", "text": "文件路径: src/sqlfluff/api/simple.py, 类名: APIParsingError\n    def __init__(self, violations, **kwargs):\n        self.violations = violations\n        self.msg = f\"Found {len(violations)} issues while parsing string.\"\n        for viol in violations:\n            self.msg += f\"\\n{viol!s}\"\n        super().__init__(self.msg, **kwargs)\n", "tokens": ["src", "sqlfluff", "api", "simple", "py", "apiparsingerror", "def", "__init__", "self", "violations", "kwargs", "self", "violations", "violations", "self", "msg", "f", "found", "len", "violations", "issues", "while", "parsing", "string", "for", "viol", "in", "violations", "self", "msg", "f", "n", "viol", "s", "super", "__init__", "self", "msg", "kwargs"], "doc_len": 39}
{"doc_id": "src/sqlfluff/api/simple.py::_unify_str_or_file", "file_path": "src/sqlfluff/api/simple.py", "class_name": null, "func_name": "_unify_str_or_file", "text": "文件路径: src/sqlfluff/api/simple.py\ndef _unify_str_or_file(sql):\n    \"\"\"Unify string and files in the same format.\"\"\"\n    if not isinstance(sql, str):\n        try:\n            sql = sql.read()\n        except AttributeError:  # pragma: no cover\n            raise TypeError(\"Value passed as sql is not a string or a readable object.\")\n    return sql\n", "tokens": ["src", "sqlfluff", "api", "simple", "py", "def", "_unify_str_or_file", "sql", "unify", "string", "and", "files", "in", "the", "same", "format", "if", "not", "isinstance", "sql", "str", "try", "sql", "sql", "read", "except", "attributeerror", "pragma", "no", "cover", "raise", "typeerror", "value", "passed", "as", "sql", "is", "not", "a", "string", "or", "a", "readable", "object", "return", "sql"], "doc_len": 46}
{"doc_id": "src/sqlfluff/api/simple.py::lint", "file_path": "src/sqlfluff/api/simple.py", "class_name": null, "func_name": "lint", "text": "文件路径: src/sqlfluff/api/simple.py\ndef lint(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Lint a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`list` of :obj:`dict` for each violation found.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql)\n    result_records = result.as_records()\n    # Return just the violations for this file\n    return [] if not result_records else result_records[0][\"violations\"]\n", "tokens": ["src", "sqlfluff", "api", "simple", "py", "def", "lint", "sql", "dialect", "ansi", "rules", "none", "lint", "a", "sql", "string", "or", "file", "args", "sql", "obj", "str", "or", "file", "like", "object", "the", "sql", "to", "be", "linted", "either", "as", "a", "string", "or", "a", "subclass", "of", "obj", "textiobase", "dialect", "obj", "str", "optional", "a", "reference", "to", "the", "dialect", "of", "the", "sql", "to", "be", "linted", "defaults", "to", "ansi", "rules", "obj", "str", "or", "iterable", "of", "obj", "str", "optional", "a", "subset", "of", "rule", "reference", "to", "lint", "for", "returns", "obj", "list", "of", "obj", "dict", "for", "each", "violation", "found", "sql", "_unify_str_or_file", "sql", "linter", "linter", "dialect", "dialect", "rules", "rules", "result", "linter", "lint_string_wrapped", "sql", "result_records", "result", "as_records", "return", "just", "the", "violations", "for", "this", "file", "return", "if", "not", "result_records", "else", "result_records", "0", "violations"], "doc_len": 117}
{"doc_id": "src/sqlfluff/api/simple.py::fix", "file_path": "src/sqlfluff/api/simple.py", "class_name": null, "func_name": "fix", "text": "文件路径: src/sqlfluff/api/simple.py\ndef fix(sql, dialect=\"ansi\", rules=None):\n    \"\"\"Fix a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n        rules (:obj:`str` or iterable of :obj:`str`, optional): A subset of rule\n            reference to lint for.\n\n    Returns:\n        :obj:`str` for the fixed sql if possible.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect, rules=rules)\n\n    result = linter.lint_string_wrapped(sql, fix=True)\n    fixed_string = result.paths[0].files[0].fix_string()[0]\n    return fixed_string\n", "tokens": ["src", "sqlfluff", "api", "simple", "py", "def", "fix", "sql", "dialect", "ansi", "rules", "none", "fix", "a", "sql", "string", "or", "file", "args", "sql", "obj", "str", "or", "file", "like", "object", "the", "sql", "to", "be", "linted", "either", "as", "a", "string", "or", "a", "subclass", "of", "obj", "textiobase", "dialect", "obj", "str", "optional", "a", "reference", "to", "the", "dialect", "of", "the", "sql", "to", "be", "linted", "defaults", "to", "ansi", "rules", "obj", "str", "or", "iterable", "of", "obj", "str", "optional", "a", "subset", "of", "rule", "reference", "to", "lint", "for", "returns", "obj", "str", "for", "the", "fixed", "sql", "if", "possible", "sql", "_unify_str_or_file", "sql", "linter", "linter", "dialect", "dialect", "rules", "rules", "result", "linter", "lint_string_wrapped", "sql", "fix", "true", "fixed_string", "result", "paths", "0", "files", "0", "fix_string", "0", "return", "fixed_string"], "doc_len": 110}
{"doc_id": "src/sqlfluff/api/simple.py::parse", "file_path": "src/sqlfluff/api/simple.py", "class_name": null, "func_name": "parse", "text": "文件路径: src/sqlfluff/api/simple.py\ndef parse(sql, dialect=\"ansi\"):\n    \"\"\"Parse a sql string or file.\n\n    Args:\n        sql (:obj:`str` or file-like object): The sql to be linted\n            either as a string or a subclass of :obj:`TextIOBase`.\n        dialect (:obj:`str`, optional): A reference to the dialect of the sql\n            to be linted. Defaults to `ansi`.\n\n    Returns:\n        :obj:`ParsedString` containing the parsed structure.\n    \"\"\"\n    sql = _unify_str_or_file(sql)\n    linter = Linter(dialect=dialect)\n    parsed = linter.parse_string(sql)\n    # If we encounter any parsing errors, raise them in a combined issue.\n    if parsed.violations:\n        raise APIParsingError(parsed.violations)\n    return parsed\n", "tokens": ["src", "sqlfluff", "api", "simple", "py", "def", "parse", "sql", "dialect", "ansi", "parse", "a", "sql", "string", "or", "file", "args", "sql", "obj", "str", "or", "file", "like", "object", "the", "sql", "to", "be", "linted", "either", "as", "a", "string", "or", "a", "subclass", "of", "obj", "textiobase", "dialect", "obj", "str", "optional", "a", "reference", "to", "the", "dialect", "of", "the", "sql", "to", "be", "linted", "defaults", "to", "ansi", "returns", "obj", "parsedstring", "containing", "the", "parsed", "structure", "sql", "_unify_str_or_file", "sql", "linter", "linter", "dialect", "dialect", "parsed", "linter", "parse_string", "sql", "if", "we", "encounter", "any", "parsing", "errors", "raise", "them", "in", "a", "combined", "issue", "if", "parsed", "violations", "raise", "apiparsingerror", "parsed", "violations", "return", "parsed"], "doc_len": 96}
{"doc_id": "src/sqlfluff/cli/commands.py::RedWarningsFilter.filter", "file_path": "src/sqlfluff/cli/commands.py", "class_name": "RedWarningsFilter", "func_name": "filter", "text": "文件路径: src/sqlfluff/cli/commands.py, 类名: RedWarningsFilter\n    def filter(self, record: logging.LogRecord) -> bool:\n        \"\"\"Filter any warnings (or above) to turn them red.\"\"\"\n        if record.levelno >= logging.WARNING:\n            record.msg = f\"{colorize(record.msg, Color.red)} \"\n        return True\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "redwarningsfilter", "def", "filter", "self", "record", "logging", "logrecord", "bool", "filter", "any", "warnings", "or", "above", "to", "turn", "them", "red", "if", "record", "levelno", "logging", "warning", "record", "msg", "f", "colorize", "record", "msg", "color", "red", "return", "true"], "doc_len": 37}
{"doc_id": "src/sqlfluff/cli/commands.py::set_logging_level", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "set_logging_level", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef set_logging_level(\n    verbosity: int, logger: Optional[logging.Logger] = None, stderr_output: bool = False\n) -> None:\n    \"\"\"Set up logging for the CLI.\n\n    We either set up global logging based on the verbosity\n    or, if `logger` is specified, we only limit to a single\n    sqlfluff logger. Verbosity is applied in the same way.\n\n    Implementation: If `logger` is not specified, the handler\n    is attached to the `sqlfluff` logger. If it is specified\n    then it attaches the the logger in question. In addition\n    if `logger` is specified, then that logger will also\n    not propagate.\n    \"\"\"\n    fluff_logger = logging.getLogger(\"sqlfluff\")\n    # Don't propagate logging\n    fluff_logger.propagate = False\n\n    # Enable colorama\n    colorama.init()\n\n    # Set up the log handler to log to stdout\n    handler = logging.StreamHandler(stream=sys.stderr if stderr_output else sys.stdout)\n    # NB: the unicode character at the beginning is to squash any badly\n    # tamed ANSI colour statements, and return us to normality.\n    handler.setFormatter(logging.Formatter(\"\\u001b[0m%(levelname)-10s %(message)s\"))\n    # Set up a handler to colour warnings red.\n    handler.addFilter(RedWarningsFilter())\n    if logger:\n        focus_logger = logging.getLogger(f\"sqlfluff.{logger}\")\n        focus_logger.addHandler(handler)\n    else:\n        fluff_logger.addHandler(handler)\n\n    # NB: We treat the parser logger slightly differently because it's noisier.\n    # It's important that we set levels for all each time so\n    # that we don't break tests by changing the granularity\n    # between tests.\n    parser_logger = logging.getLogger(\"sqlfluff.parser\")\n    if verbosity < 3:\n        fluff_logger.setLevel(logging.WARNING)\n        parser_logger.setLevel(logging.NOTSET)\n    elif verbosity == 3:\n        fluff_logger.setLevel(logging.INFO)\n        parser_logger.setLevel(logging.WARNING)\n    elif verbosity == 4:\n        fluff_logger.setLevel(logging.DEBUG)\n        parser_logger.setLevel(logging.INFO)\n    elif verbosity > 4:\n        fluff_logger.setLevel(logging.DEBUG)\n        parser_logger.setLevel(logging.DEBUG)\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "set_logging_level", "verbosity", "int", "logger", "optional", "logging", "logger", "none", "stderr_output", "bool", "false", "none", "set", "up", "logging", "for", "the", "cli", "we", "either", "set", "up", "global", "logging", "based", "on", "the", "verbosity", "or", "if", "logger", "is", "specified", "we", "only", "limit", "to", "a", "single", "sqlfluff", "logger", "verbosity", "is", "applied", "in", "the", "same", "way", "implementation", "if", "logger", "is", "not", "specified", "the", "handler", "is", "attached", "to", "the", "sqlfluff", "logger", "if", "it", "is", "specified", "then", "it", "attaches", "the", "the", "logger", "in", "question", "in", "addition", "if", "logger", "is", "specified", "then", "that", "logger", "will", "also", "not", "propagate", "fluff_logger", "logging", "getlogger", "sqlfluff", "don", "t", "propagate", "logging", "fluff_logger", "propagate", "false", "enable", "colorama", "colorama", "init", "set", "up", "the", "log", "handler", "to", "log", "to", "stdout", "handler", "logging", "streamhandler", "stream", "sys", "stderr", "if", "stderr_output", "else", "sys", "stdout", "nb", "the", "unicode", "character", "at", "the", "beginning", "is", "to", "squash", "any", "badly", "tamed", "ansi", "colour", "statements", "and", "return", "us", "to", "normality", "handler", "setformatter", "logging", "formatter", "u001b", "0m", "levelname", "10s", "message", "s", "set", "up", "a", "handler", "to", "colour", "warnings", "red", "handler", "addfilter", "redwarningsfilter", "if", "logger", "focus_logger", "logging", "getlogger", "f", "sqlfluff", "logger", "focus_logger", "addhandler", "handler", "else", "fluff_logger", "addhandler", "handler", "nb", "we", "treat", "the", "parser", "logger", "slightly", "differently", "because", "it", "s", "noisier", "it", "s", "important", "that", "we", "set", "levels", "for", "all", "each", "time", "so", "that", "we", "don", "t", "break", "tests", "by", "changing", "the", "granularity", "between", "tests", "parser_logger", "logging", "getlogger", "sqlfluff", "parser", "if", "verbosity", "3", "fluff_logger", "setlevel", "logging", "warning", "parser_logger", "setlevel", "logging", "notset", "elif", "verbosity", "3", "fluff_logger", "setlevel", "logging", "info", "parser_logger", "setlevel", "logging", "warning", "elif", "verbosity", "4", "fluff_logger", "setlevel", "logging", "debug", "parser_logger", "setlevel", "logging", "info", "elif", "verbosity", "4", "fluff_logger", "setlevel", "logging", "debug", "parser_logger", "setlevel", "logging", "debug"], "doc_len": 270}
{"doc_id": "src/sqlfluff/cli/commands.py::common_options", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "common_options", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef common_options(f: Callable) -> Callable:\n    \"\"\"Add common options to commands via a decorator.\n\n    These are applied to all of the cli commands.\n    \"\"\"\n    f = click.version_option()(f)\n    f = click.option(\n        \"-v\",\n        \"--verbose\",\n        count=True,\n        help=(\n            \"Verbosity, how detailed should the output be. This is *stackable*, so `-vv`\"\n            \" is more verbose than `-v`. For the most verbose option try `-vvvv` or `-vvvvv`.\"\n        ),\n    )(f)\n    f = click.option(\n        \"-n\",\n        \"--nocolor\",\n        is_flag=True,\n        help=\"No color - if this is set then the output will be without ANSI color codes.\",\n    )(f)\n\n    return f\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "common_options", "f", "callable", "callable", "add", "common", "options", "to", "commands", "via", "a", "decorator", "these", "are", "applied", "to", "all", "of", "the", "cli", "commands", "f", "click", "version_option", "f", "f", "click", "option", "v", "verbose", "count", "true", "help", "verbosity", "how", "detailed", "should", "the", "output", "be", "this", "is", "stackable", "so", "vv", "is", "more", "verbose", "than", "v", "for", "the", "most", "verbose", "option", "try", "vvvv", "or", "vvvvv", "f", "f", "click", "option", "n", "nocolor", "is_flag", "true", "help", "no", "color", "if", "this", "is", "set", "then", "the", "output", "will", "be", "without", "ansi", "color", "codes", "f", "return", "f"], "doc_len": 92}
{"doc_id": "src/sqlfluff/cli/commands.py::core_options", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "core_options", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef core_options(f: Callable) -> Callable:\n    \"\"\"Add core operation options to commands via a decorator.\n\n    These are applied to the main (but not all) cli commands like\n    `parse`, `lint` and `fix`.\n    \"\"\"\n    f = click.option(\n        \"--dialect\", default=None, help=\"The dialect of SQL to lint (default=ansi)\"\n    )(f)\n    f = click.option(\n        \"--templater\", default=None, help=\"The templater to use (default=jinja)\"\n    )(f)\n    f = click.option(\n        \"--rules\",\n        default=None,\n        # short_help='Specify a particular rule, or comma separated rules, to check',\n        help=(\n            \"Narrow the search to only specific rules. For example \"\n            \"specifying `--rules L001` will only search for rule `L001` (Unnecessary \"\n            \"trailing whitespace). Multiple rules can be specified with commas e.g. \"\n            \"`--rules L001,L002` will specify only looking for violations of rule \"\n            \"`L001` and rule `L002`.\"\n        ),\n    )(f)\n    f = click.option(\n        \"--exclude-rules\",\n        default=None,\n        # short_help='Specify a particular rule, or comma separated rules to exclude',\n        help=(\n            \"Exclude specific rules. For example \"\n            \"specifying `--exclude-rules L001` will remove rule `L001` (Unnecessary \"\n            \"trailing whitespace) from the set of considered rules. This could either \"\n            \"be the whitelist, or the general set if there is no specific whitelist. \"\n            \"Multiple rules can be specified with commas e.g. \"\n            \"`--exclude-rules L001,L002` will exclude violations of rule \"\n            \"`L001` and rule `L002`.\"\n        ),\n    )(f)\n    f = click.option(\n        \"--ignore\",\n        default=None,\n        help=(\n            \"Ignore particular families of errors so that they don't cause a failed \"\n            \"run. For example `--ignore parsing` would mean that any parsing errors \"\n            \"are ignored and don't influence the success or fail of a run. Multiple \"\n            \"options are possible if comma separated e.g. `--ignore parsing,templating`.\"\n        ),\n    )(f)\n    f = click.option(\n        \"--bench\",\n        is_flag=True,\n        help=\"Set this flag to engage the benchmarking tool output.\",\n    )(f)\n    f = click.option(\n        \"--logger\",\n        type=click.Choice(\n            [\"templater\", \"lexer\", \"parser\", \"linter\", \"rules\"], case_sensitive=False\n        ),\n        help=\"Choose to limit the logging to one of the loggers.\",\n    )(f)\n    return f\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "core_options", "f", "callable", "callable", "add", "core", "operation", "options", "to", "commands", "via", "a", "decorator", "these", "are", "applied", "to", "the", "main", "but", "not", "all", "cli", "commands", "like", "parse", "lint", "and", "fix", "f", "click", "option", "dialect", "default", "none", "help", "the", "dialect", "of", "sql", "to", "lint", "default", "ansi", "f", "f", "click", "option", "templater", "default", "none", "help", "the", "templater", "to", "use", "default", "jinja", "f", "f", "click", "option", "rules", "default", "none", "short_help", "specify", "a", "particular", "rule", "or", "comma", "separated", "rules", "to", "check", "help", "narrow", "the", "search", "to", "only", "specific", "rules", "for", "example", "specifying", "rules", "l001", "will", "only", "search", "for", "rule", "l001", "unnecessary", "trailing", "whitespace", "multiple", "rules", "can", "be", "specified", "with", "commas", "e", "g", "rules", "l001", "l002", "will", "specify", "only", "looking", "for", "violations", "of", "rule", "l001", "and", "rule", "l002", "f", "f", "click", "option", "exclude", "rules", "default", "none", "short_help", "specify", "a", "particular", "rule", "or", "comma", "separated", "rules", "to", "exclude", "help", "exclude", "specific", "rules", "for", "example", "specifying", "exclude", "rules", "l001", "will", "remove", "rule", "l001", "unnecessary", "trailing", "whitespace", "from", "the", "set", "of", "considered", "rules", "this", "could", "either", "be", "the", "whitelist", "or", "the", "general", "set", "if", "there", "is", "no", "specific", "whitelist", "multiple", "rules", "can", "be", "specified", "with", "commas", "e", "g", "exclude", "rules", "l001", "l002", "will", "exclude", "violations", "of", "rule", "l001", "and", "rule", "l002", "f", "f", "click", "option", "ignore", "default", "none", "help", "ignore", "particular", "families", "of", "errors", "so", "that", "they", "don", "t", "cause", "a", "failed", "run", "for", "example", "ignore", "parsing", "would", "mean", "that", "any", "parsing", "errors", "are", "ignored", "and", "don", "t", "influence", "the", "success", "or", "fail", "of", "a", "run", "multiple", "options", "are", "possible", "if", "comma", "separated", "e", "g", "ignore", "parsing", "templating", "f", "f", "click", "option", "bench", "is_flag", "true", "help", "set", "this", "flag", "to", "engage", "the", "benchmarking", "tool", "output", "f", "f", "click", "option", "logger", "type", "click", "choice", "templater", "lexer", "parser", "linter", "rules", "case_sensitive", "false", "help", "choose", "to", "limit", "the", "logging", "to", "one", "of", "the", "loggers", "f", "return", "f"], "doc_len": 311}
{"doc_id": "src/sqlfluff/cli/commands.py::get_config", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "get_config", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef get_config(**kwargs) -> FluffConfig:\n    \"\"\"Get a config object from kwargs.\"\"\"\n    if \"dialect\" in kwargs:\n        try:\n            # We're just making sure it exists at this stage - it will be fetched properly in the linter\n            dialect_selector(kwargs[\"dialect\"])\n        except SQLFluffUserError as err:\n            click.echo(\n                colorize(\n                    f\"Error loading dialect '{kwargs['dialect']}': {str(err)}\",\n                    color=Color.red,\n                )\n            )\n            sys.exit(66)\n        except KeyError:\n            click.echo(\n                colorize(\n                    f\"Error: Unknown dialect '{kwargs['dialect']}'\", color=Color.red\n                )\n            )\n            sys.exit(66)\n    # Instantiate a config object (filtering out the nulls)\n    overrides = {k: kwargs[k] for k in kwargs if kwargs[k] is not None}\n    try:\n        return FluffConfig.from_root(overrides=overrides)\n    except SQLFluffUserError as err:  # pragma: no cover\n        click.echo(\n            colorize(\n                f\"Error loading config: {str(err)}\",\n                color=Color.red,\n            )\n        )\n        sys.exit(66)\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "get_config", "kwargs", "fluffconfig", "get", "a", "config", "object", "from", "kwargs", "if", "dialect", "in", "kwargs", "try", "we", "re", "just", "making", "sure", "it", "exists", "at", "this", "stage", "it", "will", "be", "fetched", "properly", "in", "the", "linter", "dialect_selector", "kwargs", "dialect", "except", "sqlfluffusererror", "as", "err", "click", "echo", "colorize", "f", "error", "loading", "dialect", "kwargs", "dialect", "str", "err", "color", "color", "red", "sys", "exit", "66", "except", "keyerror", "click", "echo", "colorize", "f", "error", "unknown", "dialect", "kwargs", "dialect", "color", "color", "red", "sys", "exit", "66", "instantiate", "a", "config", "object", "filtering", "out", "the", "nulls", "overrides", "k", "kwargs", "k", "for", "k", "in", "kwargs", "if", "kwargs", "k", "is", "not", "none", "try", "return", "fluffconfig", "from_root", "overrides", "overrides", "except", "sqlfluffusererror", "as", "err", "pragma", "no", "cover", "click", "echo", "colorize", "f", "error", "loading", "config", "str", "err", "color", "color", "red", "sys", "exit", "66"], "doc_len": 129}
{"doc_id": "src/sqlfluff/cli/commands.py::get_linter_and_formatter", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "get_linter_and_formatter", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef get_linter_and_formatter(\n    cfg: FluffConfig, silent: bool = False\n) -> Tuple[Linter, CallbackFormatter]:\n    \"\"\"Get a linter object given a config.\"\"\"\n    try:\n        # We're just making sure it exists at this stage - it will be fetched properly in the linter\n        dialect_selector(cfg.get(\"dialect\"))\n    except KeyError:  # pragma: no cover\n        click.echo(f\"Error: Unknown dialect '{cfg.get('dialect')}'\")\n        sys.exit(66)\n\n    if not silent:\n        # Instantiate the linter and return (with an output function)\n        formatter = CallbackFormatter(\n            callback=lambda m: click.echo(m, color=cfg.get(\"color\")),\n            verbosity=cfg.get(\"verbose\"),\n            output_line_length=cfg.get(\"output_line_length\"),\n        )\n        return Linter(config=cfg, formatter=formatter), formatter\n    else:\n        # Instantiate the linter and return. NB: No formatter\n        # in the Linter and a black formatter otherwise.\n        formatter = CallbackFormatter(callback=lambda m: None, verbosity=0)\n        return Linter(config=cfg), formatter\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "get_linter_and_formatter", "cfg", "fluffconfig", "silent", "bool", "false", "tuple", "linter", "callbackformatter", "get", "a", "linter", "object", "given", "a", "config", "try", "we", "re", "just", "making", "sure", "it", "exists", "at", "this", "stage", "it", "will", "be", "fetched", "properly", "in", "the", "linter", "dialect_selector", "cfg", "get", "dialect", "except", "keyerror", "pragma", "no", "cover", "click", "echo", "f", "error", "unknown", "dialect", "cfg", "get", "dialect", "sys", "exit", "66", "if", "not", "silent", "instantiate", "the", "linter", "and", "return", "with", "an", "output", "function", "formatter", "callbackformatter", "callback", "lambda", "m", "click", "echo", "m", "color", "cfg", "get", "color", "verbosity", "cfg", "get", "verbose", "output_line_length", "cfg", "get", "output_line_length", "return", "linter", "config", "cfg", "formatter", "formatter", "formatter", "else", "instantiate", "the", "linter", "and", "return", "nb", "no", "formatter", "in", "the", "linter", "and", "a", "black", "formatter", "otherwise", "formatter", "callbackformatter", "callback", "lambda", "m", "none", "verbosity", "0", "return", "linter", "config", "cfg", "formatter"], "doc_len": 131}
{"doc_id": "src/sqlfluff/cli/commands.py::cli", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "cli", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef cli():\n    \"\"\"Sqlfluff is a modular sql linter for humans.\"\"\"\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "cli", "sqlfluff", "is", "a", "modular", "sql", "linter", "for", "humans"], "doc_len": 15}
{"doc_id": "src/sqlfluff/cli/commands.py::version", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "version", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef version(**kwargs) -> None:\n    \"\"\"Show the version of sqlfluff.\"\"\"\n    c = get_config(**kwargs)\n    if c.get(\"verbose\") > 0:\n        # Instantiate the linter\n        lnt, formatter = get_linter_and_formatter(c)\n        # Dispatch the detailed config from the linter.\n        formatter.dispatch_config(lnt)\n    else:\n        # Otherwise just output the package version.\n        click.echo(get_package_version(), color=c.get(\"color\"))\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "version", "kwargs", "none", "show", "the", "version", "of", "sqlfluff", "c", "get_config", "kwargs", "if", "c", "get", "verbose", "0", "instantiate", "the", "linter", "lnt", "formatter", "get_linter_and_formatter", "c", "dispatch", "the", "detailed", "config", "from", "the", "linter", "formatter", "dispatch_config", "lnt", "else", "otherwise", "just", "output", "the", "package", "version", "click", "echo", "get_package_version", "color", "c", "get", "color"], "doc_len": 53}
{"doc_id": "src/sqlfluff/cli/commands.py::rules", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "rules", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef rules(**kwargs) -> None:\n    \"\"\"Show the current rules in use.\"\"\"\n    c = get_config(**kwargs)\n    lnt, _ = get_linter_and_formatter(c)\n    click.echo(format_rules(lnt), color=c.get(\"color\"))\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "rules", "kwargs", "none", "show", "the", "current", "rules", "in", "use", "c", "get_config", "kwargs", "lnt", "_", "get_linter_and_formatter", "c", "click", "echo", "format_rules", "lnt", "color", "c", "get", "color"], "doc_len": 30}
{"doc_id": "src/sqlfluff/cli/commands.py::dialects", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "dialects", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef dialects(**kwargs) -> None:\n    \"\"\"Show the current dialects available.\"\"\"\n    c = get_config(**kwargs)\n    click.echo(format_dialects(dialect_readout), color=c.get(\"color\"))\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "dialects", "kwargs", "none", "show", "the", "current", "dialects", "available", "c", "get_config", "kwargs", "click", "echo", "format_dialects", "dialect_readout", "color", "c", "get", "color"], "doc_len": 25}
{"doc_id": "src/sqlfluff/cli/commands.py::lint", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "lint", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef lint(\n    paths: Tuple[str],\n    processes: int,\n    format: str,\n    annotation_level: str,\n    nofail: bool,\n    disregard_sqlfluffignores: bool,\n    logger: Optional[logging.Logger] = None,\n    bench: bool = False,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Lint SQL files via passing a list of files or using stdin.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n\n    Linting SQL files:\n\n        sqlfluff lint path/to/file.sql\n        sqlfluff lint directory/of/sql/files\n\n    Linting a file via stdin (note the lone '-' character):\n\n        cat path/to/file.sql | sqlfluff lint -\n        echo 'select col from tbl' | sqlfluff lint -\n\n    \"\"\"\n    config = get_config(**kwargs)\n    non_human_output = format != FormatType.human.value\n    lnt, formatter = get_linter_and_formatter(config, silent=non_human_output)\n    verbose = config.get(\"verbose\")\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=non_human_output)\n    # add stdin if specified via lone '-'\n    if (\"-\",) == paths:\n        result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\n    else:\n        # Output the results as we go\n        if verbose >= 1:\n            click.echo(format_linting_result_header())\n        try:\n            result = lnt.lint_paths(\n                paths,\n                ignore_non_existent_files=False,\n                ignore_files=not disregard_sqlfluffignores,\n                processes=processes,\n            )\n        except OSError:\n            click.echo(\n                colorize(\n                    f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                    Color.red,\n                )\n            )\n            sys.exit(1)\n        # Output the final stats\n        if verbose >= 1:\n            click.echo(format_linting_stats(result, verbose=verbose))\n\n    if format == FormatType.json.value:\n        click.echo(json.dumps(result.as_records()))\n    elif format == FormatType.yaml.value:\n        click.echo(yaml.dump(result.as_records()))\n    elif format == FormatType.github_annotation.value:\n        github_result = []\n        for record in result.as_records():\n            filepath = record[\"filepath\"]\n            for violation in record[\"violations\"]:\n                # NOTE: The output format is designed for this GitHub action:\n                # https://github.com/yuzutech/annotations-action\n                # It is similar, but not identical, to the native GitHub format:\n                # https://docs.github.com/en/rest/reference/checks#annotations-items\n                github_result.append(\n                    {\n                        \"file\": filepath,\n                        \"line\": violation[\"line_no\"],\n                        \"start_column\": violation[\"line_pos\"],\n                        \"end_column\": violation[\"line_pos\"],\n                        \"title\": \"SQLFluff\",\n                        \"message\": f\"{violation['code']}: {violation['description']}\",\n                        \"annotation_level\": annotation_level,\n                    }\n                )\n        click.echo(json.dumps(github_result))\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    if not nofail:\n        if not non_human_output:\n            _completion_message(config)\n        sys.exit(result.stats()[\"exit code\"])\n    else:\n        sys.exit(0)\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "lint", "paths", "tuple", "str", "processes", "int", "format", "str", "annotation_level", "str", "nofail", "bool", "disregard_sqlfluffignores", "bool", "logger", "optional", "logging", "logger", "none", "bench", "bool", "false", "kwargs", "noreturn", "lint", "sql", "files", "via", "passing", "a", "list", "of", "files", "or", "using", "stdin", "path", "is", "the", "path", "to", "a", "sql", "file", "or", "directory", "to", "lint", "this", "can", "be", "either", "a", "file", "path", "to", "file", "sql", "a", "path", "directory", "of", "sql", "files", "a", "single", "character", "to", "indicate", "reading", "from", "stdin", "or", "a", "dot", "blank", "which", "will", "be", "interpreted", "like", "passing", "the", "current", "working", "directory", "as", "a", "path", "argument", "linting", "sql", "files", "sqlfluff", "lint", "path", "to", "file", "sql", "sqlfluff", "lint", "directory", "of", "sql", "files", "linting", "a", "file", "via", "stdin", "note", "the", "lone", "character", "cat", "path", "to", "file", "sql", "sqlfluff", "lint", "echo", "select", "col", "from", "tbl", "sqlfluff", "lint", "config", "get_config", "kwargs", "non_human_output", "format", "formattype", "human", "value", "lnt", "formatter", "get_linter_and_formatter", "config", "silent", "non_human_output", "verbose", "config", "get", "verbose", "formatter", "dispatch_config", "lnt", "set", "up", "logging", "set_logging_level", "verbosity", "verbose", "logger", "logger", "stderr_output", "non_human_output", "add", "stdin", "if", "specified", "via", "lone", "if", "paths", "result", "lnt", "lint_string_wrapped", "sys", "stdin", "read", "fname", "stdin", "else", "output", "the", "results", "as", "we", "go", "if", "verbose", "1", "click", "echo", "format_linting_result_header", "try", "result", "lnt", "lint_paths", "paths", "ignore_non_existent_files", "false", "ignore_files", "not", "disregard_sqlfluffignores", "processes", "processes", "except", "oserror", "click", "echo", "colorize", "f", "the", "path", "s", "paths", "could", "not", "be", "accessed", "check", "it", "they", "exist", "s", "color", "red", "sys", "exit", "1", "output", "the", "final", "stats", "if", "verbose", "1", "click", "echo", "format_linting_stats", "result", "verbose", "verbose", "if", "format", "formattype", "json", "value", "click", "echo", "json", "dumps", "result", "as_records", "elif", "format", "formattype", "yaml", "value", "click", "echo", "yaml", "dump", "result", "as_records", "elif", "format", "formattype", "github_annotation", "value", "github_result", "for", "record", "in", "result", "as_records", "filepath", "record", "filepath", "for", "violation", "in", "record", "violations", "note", "the", "output", "format", "is", "designed", "for", "this", "github", "action", "https", "github", "com", "yuzutech", "annotations", "action", "it", "is", "similar", "but", "not", "identical", "to", "the", "native", "github", "format", "https", "docs", "github", "com", "en", "rest", "reference", "checks", "annotations", "items", "github_result", "append", "file", "filepath", "line", "violation", "line_no", "start_column", "violation", "line_pos", "end_column", "violation", "line_pos", "title", "sqlfluff", "message", "f", "violation", "code", "violation", "description", "annotation_level", "annotation_level", "click", "echo", "json", "dumps", "github_result", "if", "bench", "click", "echo", "overall", "timings", "click", "echo", "cli_table", "clock", "time", "result", "total_time", "timing_summary", "result", "timing_summary", "for", "step", "in", "timing_summary", "click", "echo", "f", "step", "click", "echo", "cli_table", "timing_summary", "step", "items", "if", "not", "nofail", "if", "not", "non_human_output", "_completion_message", "config", "sys", "exit", "result", "stats", "exit", "code", "else", "sys", "exit", "0"], "doc_len": 397}
{"doc_id": "src/sqlfluff/cli/commands.py::do_fixes", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "do_fixes", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef do_fixes(lnt, result, formatter=None, **kwargs):\n    \"\"\"Actually do the fixes.\"\"\"\n    click.echo(\"Persisting Changes...\")\n    res = result.persist_changes(formatter=formatter, **kwargs)\n    if all(res.values()):\n        click.echo(\"Done. Please check your files to confirm.\")\n        return True\n    # If some failed then return false\n    click.echo(\n        \"Done. Some operations failed. Please check your files to confirm.\"\n    )  # pragma: no cover\n    click.echo(\n        \"Some errors cannot be fixed or there is another error blocking it.\"\n    )  # pragma: no cover\n    return False  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "do_fixes", "lnt", "result", "formatter", "none", "kwargs", "actually", "do", "the", "fixes", "click", "echo", "persisting", "changes", "res", "result", "persist_changes", "formatter", "formatter", "kwargs", "if", "all", "res", "values", "click", "echo", "done", "please", "check", "your", "files", "to", "confirm", "return", "true", "if", "some", "failed", "then", "return", "false", "click", "echo", "done", "some", "operations", "failed", "please", "check", "your", "files", "to", "confirm", "pragma", "no", "cover", "click", "echo", "some", "errors", "cannot", "be", "fixed", "or", "there", "is", "another", "error", "blocking", "it", "pragma", "no", "cover", "return", "false", "pragma", "no", "cover"], "doc_len": 84}
{"doc_id": "src/sqlfluff/cli/commands.py::fix", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "fix", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef fix(\n    force: bool,\n    paths: Tuple[str],\n    processes: int,\n    bench: bool = False,\n    fixed_suffix: str = \"\",\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Fix SQL files.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    # some quick checks\n    fixing_stdin = (\"-\",) == paths\n\n    config = get_config(**kwargs)\n    lnt, formatter = get_linter_and_formatter(config, silent=fixing_stdin)\n    verbose = config.get(\"verbose\")\n    exit_code = 0\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=fixing_stdin)\n\n    # handle stdin case. should output formatted sql to stdout and nothing else.\n    if fixing_stdin:\n        stdin = sys.stdin.read()\n\n        result = lnt.lint_string_wrapped(stdin, fname=\"stdin\", fix=True)\n        templater_error = result.num_violations(types=SQLTemplaterError) > 0\n        unfixable_error = result.num_violations(types=SQLLintError, fixable=False) > 0\n\n        if result.num_violations(types=SQLLintError, fixable=True) > 0:\n            stdout = result.paths[0].files[0].fix_string()[0]\n        else:\n            stdout = stdin\n\n        if templater_error:\n            click.echo(\n                colorize(\n                    \"Fix aborted due to unparseable template variables.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n            click.echo(\n                colorize(\n                    \"Use '--ignore templating' to attempt to fix anyway.\",\n                    Color.red,\n                ),\n                err=True,\n            )\n        if unfixable_error:\n            click.echo(colorize(\"Unfixable violations detected.\", Color.red), err=True)\n\n        click.echo(stdout, nl=False)\n        sys.exit(1 if templater_error or unfixable_error else 0)\n\n    # Lint the paths (not with the fix argument at this stage), outputting as we go.\n    click.echo(\"==== finding fixable violations ====\")\n    try:\n        result = lnt.lint_paths(\n            paths, fix=True, ignore_non_existent_files=False, processes=processes\n        )\n    except OSError:\n        click.echo(\n            colorize(\n                f\"The path(s) '{paths}' could not be accessed. Check it/they exist(s).\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n\n    # NB: We filter to linting violations here, because they're\n    # the only ones which can be potentially fixed.\n    if result.num_violations(types=SQLLintError, fixable=True) > 0:\n        click.echo(\"==== fixing violations ====\")\n        click.echo(\n            f\"{result.num_violations(types=SQLLintError, fixable=True)} fixable linting violations found\"\n        )\n        if force:\n            click.echo(f\"{colorize('FORCE MODE', Color.red)}: Attempting fixes...\")\n            success = do_fixes(\n                lnt,\n                result,\n                formatter,\n                types=SQLLintError,\n                fixed_file_suffix=fixed_suffix,\n            )\n            if not success:\n                sys.exit(1)  # pragma: no cover\n        else:\n            click.echo(\n                \"Are you sure you wish to attempt to fix these? [Y/n] \", nl=False\n            )\n            c = click.getchar().lower()\n            click.echo(\"...\")\n            if c in (\"y\", \"\\r\", \"\\n\"):\n                click.echo(\"Attempting fixes...\")\n                success = do_fixes(\n                    lnt,\n                    result,\n                    formatter,\n                    types=SQLLintError,\n                    fixed_file_suffix=fixed_suffix,\n                )\n                if not success:\n                    sys.exit(1)  # pragma: no cover\n                else:\n                    _completion_message(config)\n            elif c == \"n\":\n                click.echo(\"Aborting...\")\n                exit_code = 1\n            else:  # pragma: no cover\n                click.echo(\"Invalid input, please enter 'Y' or 'N'\")\n                click.echo(\"Aborting...\")\n                exit_code = 1\n    else:\n        click.echo(\"==== no fixable linting violations found ====\")\n        _completion_message(config)\n\n    if result.num_violations(types=SQLLintError, fixable=False) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLLintError, fixable=False)} unfixable linting violations found]\"\n        )\n        exit_code = 1\n\n    if result.num_violations(types=SQLTemplaterError) > 0:\n        click.echo(\n            f\"  [{result.num_violations(types=SQLTemplaterError)} templating errors found]\"\n        )\n        exit_code = 1\n\n    if bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", result.total_time)]))\n        timing_summary = result.timing_summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    sys.exit(exit_code)\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "fix", "force", "bool", "paths", "tuple", "str", "processes", "int", "bench", "bool", "false", "fixed_suffix", "str", "logger", "optional", "logging", "logger", "none", "kwargs", "noreturn", "fix", "sql", "files", "path", "is", "the", "path", "to", "a", "sql", "file", "or", "directory", "to", "lint", "this", "can", "be", "either", "a", "file", "path", "to", "file", "sql", "a", "path", "directory", "of", "sql", "files", "a", "single", "character", "to", "indicate", "reading", "from", "stdin", "or", "a", "dot", "blank", "which", "will", "be", "interpreted", "like", "passing", "the", "current", "working", "directory", "as", "a", "path", "argument", "some", "quick", "checks", "fixing_stdin", "paths", "config", "get_config", "kwargs", "lnt", "formatter", "get_linter_and_formatter", "config", "silent", "fixing_stdin", "verbose", "config", "get", "verbose", "exit_code", "0", "formatter", "dispatch_config", "lnt", "set", "up", "logging", "set_logging_level", "verbosity", "verbose", "logger", "logger", "stderr_output", "fixing_stdin", "handle", "stdin", "case", "should", "output", "formatted", "sql", "to", "stdout", "and", "nothing", "else", "if", "fixing_stdin", "stdin", "sys", "stdin", "read", "result", "lnt", "lint_string_wrapped", "stdin", "fname", "stdin", "fix", "true", "templater_error", "result", "num_violations", "types", "sqltemplatererror", "0", "unfixable_error", "result", "num_violations", "types", "sqllinterror", "fixable", "false", "0", "if", "result", "num_violations", "types", "sqllinterror", "fixable", "true", "0", "stdout", "result", "paths", "0", "files", "0", "fix_string", "0", "else", "stdout", "stdin", "if", "templater_error", "click", "echo", "colorize", "fix", "aborted", "due", "to", "unparseable", "template", "variables", "color", "red", "err", "true", "click", "echo", "colorize", "use", "ignore", "templating", "to", "attempt", "to", "fix", "anyway", "color", "red", "err", "true", "if", "unfixable_error", "click", "echo", "colorize", "unfixable", "violations", "detected", "color", "red", "err", "true", "click", "echo", "stdout", "nl", "false", "sys", "exit", "1", "if", "templater_error", "or", "unfixable_error", "else", "0", "lint", "the", "paths", "not", "with", "the", "fix", "argument", "at", "this", "stage", "outputting", "as", "we", "go", "click", "echo", "finding", "fixable", "violations", "try", "result", "lnt", "lint_paths", "paths", "fix", "true", "ignore_non_existent_files", "false", "processes", "processes", "except", "oserror", "click", "echo", "colorize", "f", "the", "path", "s", "paths", "could", "not", "be", "accessed", "check", "it", "they", "exist", "s", "color", "red", "err", "true", "sys", "exit", "1", "nb", "we", "filter", "to", "linting", "violations", "here", "because", "they", "re", "the", "only", "ones", "which", "can", "be", "potentially", "fixed", "if", "result", "num_violations", "types", "sqllinterror", "fixable", "true", "0", "click", "echo", "fixing", "violations", "click", "echo", "f", "result", "num_violations", "types", "sqllinterror", "fixable", "true", "fixable", "linting", "violations", "found", "if", "force", "click", "echo", "f", "colorize", "force", "mode", "color", "red", "attempting", "fixes", "success", "do_fixes", "lnt", "result", "formatter", "types", "sqllinterror", "fixed_file_suffix", "fixed_suffix", "if", "not", "success", "sys", "exit", "1", "pragma", "no", "cover", "else", "click", "echo", "are", "you", "sure", "you", "wish", "to", "attempt", "to", "fix", "these", "y", "n", "nl", "false", "c", "click", "getchar", "lower", "click", "echo", "if", "c", "in", "y", "r", "n", "click", "echo", "attempting", "fixes", "success", "do_fixes", "lnt", "result", "formatter", "types", "sqllinterror", "fixed_file_suffix", "fixed_suffix", "if", "not", "success", "sys", "exit", "1", "pragma", "no", "cover", "else", "_completion_message", "config", "elif", "c", "n", "click", "echo", "aborting", "exit_code", "1", "else", "pragma", "no", "cover", "click", "echo", "invalid", "input", "please", "enter", "y", "or", "n", "click", "echo", "aborting", "exit_code", "1", "else", "click", "echo", "no", "fixable", "linting", "violations", "found", "_completion_message", "config", "if", "result", "num_violations", "types", "sqllinterror", "fixable", "false", "0", "click", "echo", "f", "result", "num_violations", "types", "sqllinterror", "fixable", "false", "unfixable", "linting", "violations", "found", "exit_code", "1", "if", "result", "num_violations", "types", "sqltemplatererror", "0", "click", "echo", "f", "result", "num_violations", "types", "sqltemplatererror", "templating", "errors", "found", "exit_code", "1", "if", "bench", "click", "echo", "overall", "timings", "click", "echo", "cli_table", "clock", "time", "result", "total_time", "timing_summary", "result", "timing_summary", "for", "step", "in", "timing_summary", "click", "echo", "f", "step", "click", "echo", "cli_table", "timing_summary", "step", "items", "sys", "exit", "exit_code"], "doc_len": 526}
{"doc_id": "src/sqlfluff/cli/commands.py::_completion_message", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "_completion_message", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef _completion_message(config: FluffConfig) -> None:\n    click.echo(f\"All Finished{'' if config.get('nocolor') else ' 📜 🎉'}!\")\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "_completion_message", "config", "fluffconfig", "none", "click", "echo", "f", "all", "finished", "if", "config", "get", "nocolor", "else"], "doc_len": 20}
{"doc_id": "src/sqlfluff/cli/commands.py::quoted_presenter", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "quoted_presenter", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef quoted_presenter(dumper, data):\n    \"\"\"Re-presenter which always double quotes string values needing escapes.\"\"\"\n    if \"\\n\" in data or \"\\t\" in data or \"'\" in data:\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style='\"')\n    else:\n        return dumper.represent_scalar(\"tag:yaml.org,2002:str\", data, style=\"\")\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "quoted_presenter", "dumper", "data", "re", "presenter", "which", "always", "double", "quotes", "string", "values", "needing", "escapes", "if", "n", "in", "data", "or", "t", "in", "data", "or", "in", "data", "return", "dumper", "represent_scalar", "tag", "yaml", "org", "2002", "str", "data", "style", "else", "return", "dumper", "represent_scalar", "tag", "yaml", "org", "2002", "str", "data", "style"], "doc_len": 51}
{"doc_id": "src/sqlfluff/cli/commands.py::parse", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "parse", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef parse(\n    path: str,\n    code_only: bool,\n    include_meta: bool,\n    format: str,\n    profiler: bool,\n    bench: bool,\n    nofail: bool,\n    logger: Optional[logging.Logger] = None,\n    **kwargs,\n) -> NoReturn:\n    \"\"\"Parse SQL files and just spit out the result.\n\n    PATH is the path to a sql file or directory to lint. This can be either a\n    file ('path/to/file.sql'), a path ('directory/of/sql/files'), a single ('-')\n    character to indicate reading from *stdin* or a dot/blank ('.'/' ') which will\n    be interpreted like passing the current working directory as a path argument.\n    \"\"\"\n    c = get_config(**kwargs)\n    # We don't want anything else to be logged if we want json or yaml output\n    non_human_output = format in (FormatType.json.value, FormatType.yaml.value)\n    lnt, formatter = get_linter_and_formatter(c, silent=non_human_output)\n    verbose = c.get(\"verbose\")\n    recurse = c.get(\"recurse\")\n\n    formatter.dispatch_config(lnt)\n\n    # Set up logging.\n    set_logging_level(verbosity=verbose, logger=logger, stderr_output=non_human_output)\n\n    # TODO: do this better\n\n    if profiler:\n        # Set up the profiler if required\n        try:\n            import cProfile\n        except ImportError:  # pragma: no cover\n            click.echo(\"The cProfiler is not available on your platform.\")\n            sys.exit(1)\n        pr = cProfile.Profile()\n        pr.enable()\n\n    try:\n        t0 = time.monotonic()\n\n        # handle stdin if specified via lone '-'\n        if \"-\" == path:\n            parsed_strings = [\n                lnt.parse_string(\n                    sys.stdin.read(), \"stdin\", recurse=recurse, config=lnt.config\n                ),\n            ]\n        else:\n            # A single path must be specified for this command\n            parsed_strings = list(lnt.parse_path(path, recurse=recurse))\n\n        total_time = time.monotonic() - t0\n        violations_count = 0\n\n        # iterative print for human readout\n        if format == FormatType.human.value:\n            violations_count = _print_out_violations_and_timing(\n                bench, code_only, total_time, verbose, parsed_strings\n            )\n        else:\n            parsed_strings_dict = [\n                dict(\n                    filepath=linted_result.fname,\n                    segments=linted_result.tree.as_record(\n                        code_only=code_only, show_raw=True, include_meta=include_meta\n                    )\n                    if linted_result.tree\n                    else None,\n                )\n                for linted_result in parsed_strings\n            ]\n\n            if format == FormatType.yaml.value:\n                # For yaml dumping always dump double quoted strings if they contain tabs or newlines.\n                yaml.add_representer(str, quoted_presenter)\n                click.echo(yaml.dump(parsed_strings_dict))\n            elif format == FormatType.json.value:\n                click.echo(json.dumps(parsed_strings_dict))\n\n    except OSError:  # pragma: no cover\n        click.echo(\n            colorize(\n                f\"The path '{path}' could not be accessed. Check it exists.\",\n                Color.red,\n            ),\n            err=True,\n        )\n        sys.exit(1)\n\n    if profiler:\n        pr.disable()\n        profiler_buffer = StringIO()\n        ps = pstats.Stats(pr, stream=profiler_buffer).sort_stats(\"cumulative\")\n        ps.print_stats()\n        click.echo(\"==== profiler stats ====\")\n        # Only print the first 50 lines of it\n        click.echo(\"\\n\".join(profiler_buffer.getvalue().split(\"\\n\")[:50]))\n\n    if violations_count > 0 and not nofail:\n        sys.exit(66)  # pragma: no cover\n    else:\n        sys.exit(0)\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "parse", "path", "str", "code_only", "bool", "include_meta", "bool", "format", "str", "profiler", "bool", "bench", "bool", "nofail", "bool", "logger", "optional", "logging", "logger", "none", "kwargs", "noreturn", "parse", "sql", "files", "and", "just", "spit", "out", "the", "result", "path", "is", "the", "path", "to", "a", "sql", "file", "or", "directory", "to", "lint", "this", "can", "be", "either", "a", "file", "path", "to", "file", "sql", "a", "path", "directory", "of", "sql", "files", "a", "single", "character", "to", "indicate", "reading", "from", "stdin", "or", "a", "dot", "blank", "which", "will", "be", "interpreted", "like", "passing", "the", "current", "working", "directory", "as", "a", "path", "argument", "c", "get_config", "kwargs", "we", "don", "t", "want", "anything", "else", "to", "be", "logged", "if", "we", "want", "json", "or", "yaml", "output", "non_human_output", "format", "in", "formattype", "json", "value", "formattype", "yaml", "value", "lnt", "formatter", "get_linter_and_formatter", "c", "silent", "non_human_output", "verbose", "c", "get", "verbose", "recurse", "c", "get", "recurse", "formatter", "dispatch_config", "lnt", "set", "up", "logging", "set_logging_level", "verbosity", "verbose", "logger", "logger", "stderr_output", "non_human_output", "todo", "do", "this", "better", "if", "profiler", "set", "up", "the", "profiler", "if", "required", "try", "import", "cprofile", "except", "importerror", "pragma", "no", "cover", "click", "echo", "the", "cprofiler", "is", "not", "available", "on", "your", "platform", "sys", "exit", "1", "pr", "cprofile", "profile", "pr", "enable", "try", "t0", "time", "monotonic", "handle", "stdin", "if", "specified", "via", "lone", "if", "path", "parsed_strings", "lnt", "parse_string", "sys", "stdin", "read", "stdin", "recurse", "recurse", "config", "lnt", "config", "else", "a", "single", "path", "must", "be", "specified", "for", "this", "command", "parsed_strings", "list", "lnt", "parse_path", "path", "recurse", "recurse", "total_time", "time", "monotonic", "t0", "violations_count", "0", "iterative", "print", "for", "human", "readout", "if", "format", "formattype", "human", "value", "violations_count", "_print_out_violations_and_timing", "bench", "code_only", "total_time", "verbose", "parsed_strings", "else", "parsed_strings_dict", "dict", "filepath", "linted_result", "fname", "segments", "linted_result", "tree", "as_record", "code_only", "code_only", "show_raw", "true", "include_meta", "include_meta", "if", "linted_result", "tree", "else", "none", "for", "linted_result", "in", "parsed_strings", "if", "format", "formattype", "yaml", "value", "for", "yaml", "dumping", "always", "dump", "double", "quoted", "strings", "if", "they", "contain", "tabs", "or", "newlines", "yaml", "add_representer", "str", "quoted_presenter", "click", "echo", "yaml", "dump", "parsed_strings_dict", "elif", "format", "formattype", "json", "value", "click", "echo", "json", "dumps", "parsed_strings_dict", "except", "oserror", "pragma", "no", "cover", "click", "echo", "colorize", "f", "the", "path", "path", "could", "not", "be", "accessed", "check", "it", "exists", "color", "red", "err", "true", "sys", "exit", "1", "if", "profiler", "pr", "disable", "profiler_buffer", "stringio", "ps", "pstats", "stats", "pr", "stream", "profiler_buffer", "sort_stats", "cumulative", "ps", "print_stats", "click", "echo", "profiler", "stats", "only", "print", "the", "first", "50", "lines", "of", "it", "click", "echo", "n", "join", "profiler_buffer", "getvalue", "split", "n", "50", "if", "violations_count", "0", "and", "not", "nofail", "sys", "exit", "66", "pragma", "no", "cover", "else", "sys", "exit", "0"], "doc_len": 390}
{"doc_id": "src/sqlfluff/cli/commands.py::_print_out_violations_and_timing", "file_path": "src/sqlfluff/cli/commands.py", "class_name": null, "func_name": "_print_out_violations_and_timing", "text": "文件路径: src/sqlfluff/cli/commands.py\ndef _print_out_violations_and_timing(\n    bench: bool,\n    code_only: bool,\n    total_time: float,\n    verbose: int,\n    parsed_strings: List[ParsedString],\n) -> int:\n    \"\"\"Used by human formatting during the parse.\"\"\"\n    violations_count = 0\n    timing = TimingSummary()\n\n    for parsed_string in parsed_strings:\n        timing.add(parsed_string.time_dict)\n\n        if parsed_string.tree:\n            click.echo(parsed_string.tree.stringify(code_only=code_only))\n        else:\n            # TODO: Make this prettier\n            click.echo(\"...Failed to Parse...\")  # pragma: no cover\n\n        violations_count += len(parsed_string.violations)\n        if parsed_string.violations:\n            click.echo(\"==== parsing violations ====\")  # pragma: no cover\n        for v in parsed_string.violations:\n            click.echo(format_violation(v))  # pragma: no cover\n        if parsed_string.violations and parsed_string.config.get(\"dialect\") == \"ansi\":\n            click.echo(format_dialect_warning())  # pragma: no cover\n\n        if verbose >= 2:\n            click.echo(\"==== timings ====\")\n            click.echo(cli_table(parsed_string.time_dict.items()))\n\n    if verbose >= 2 or bench:\n        click.echo(\"==== overall timings ====\")\n        click.echo(cli_table([(\"Clock time\", total_time)]))\n        timing_summary = timing.summary()\n        for step in timing_summary:\n            click.echo(f\"=== {step} ===\")\n            click.echo(cli_table(timing_summary[step].items()))\n\n    return violations_count\n", "tokens": ["src", "sqlfluff", "cli", "commands", "py", "def", "_print_out_violations_and_timing", "bench", "bool", "code_only", "bool", "total_time", "float", "verbose", "int", "parsed_strings", "list", "parsedstring", "int", "used", "by", "human", "formatting", "during", "the", "parse", "violations_count", "0", "timing", "timingsummary", "for", "parsed_string", "in", "parsed_strings", "timing", "add", "parsed_string", "time_dict", "if", "parsed_string", "tree", "click", "echo", "parsed_string", "tree", "stringify", "code_only", "code_only", "else", "todo", "make", "this", "prettier", "click", "echo", "failed", "to", "parse", "pragma", "no", "cover", "violations_count", "len", "parsed_string", "violations", "if", "parsed_string", "violations", "click", "echo", "parsing", "violations", "pragma", "no", "cover", "for", "v", "in", "parsed_string", "violations", "click", "echo", "format_violation", "v", "pragma", "no", "cover", "if", "parsed_string", "violations", "and", "parsed_string", "config", "get", "dialect", "ansi", "click", "echo", "format_dialect_warning", "pragma", "no", "cover", "if", "verbose", "2", "click", "echo", "timings", "click", "echo", "cli_table", "parsed_string", "time_dict", "items", "if", "verbose", "2", "or", "bench", "click", "echo", "overall", "timings", "click", "echo", "cli_table", "clock", "time", "total_time", "timing_summary", "timing", "summary", "for", "step", "in", "timing_summary", "click", "echo", "f", "step", "click", "echo", "cli_table", "timing_summary", "step", "items", "return", "violations_count"], "doc_len": 148}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_filename", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_filename", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_filename(\n    filename: str, success: Union[str, bool] = False, success_text: str = \"PASS\"\n) -> str:\n    \"\"\"Format filenames.\"\"\"\n    if isinstance(success, str):\n        status_string = success\n    else:\n        status_string = colorize(\n            success_text if success else \"FAIL\",\n            Color.green if success else Color.red,\n        )\n    return f\"== [{colorize(filename, Color.lightgrey)}] {status_string}\"\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_filename", "filename", "str", "success", "union", "str", "bool", "false", "success_text", "str", "pass", "str", "format", "filenames", "if", "isinstance", "success", "str", "status_string", "success", "else", "status_string", "colorize", "success_text", "if", "success", "else", "fail", "color", "green", "if", "success", "else", "color", "red", "return", "f", "colorize", "filename", "color", "lightgrey", "status_string"], "doc_len": 48}
{"doc_id": "src/sqlfluff/cli/formatters.py::split_string_on_spaces", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "split_string_on_spaces", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef split_string_on_spaces(s: str, line_length: int = 100) -> List[str]:\n    \"\"\"Split a string into lines based on whitespace.\"\"\"\n    line_buff = []\n    str_buff = \"\"\n    for token in s.split():\n        # Can we put this token on this line without going over?\n        if str_buff:\n            if len(str_buff) + len(token) > line_length:\n                line_buff.append(str_buff)\n                str_buff = token\n            else:\n                str_buff += \" \" + token\n        else:\n            # In the case that the buffer is already empty, add it without checking,\n            # otherwise there might be things that we might never.\n            str_buff = token\n    # If we have left over buff, add it in\n    if str_buff:\n        line_buff.append(str_buff)\n    return line_buff\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "split_string_on_spaces", "s", "str", "line_length", "int", "100", "list", "str", "split", "a", "string", "into", "lines", "based", "on", "whitespace", "line_buff", "str_buff", "for", "token", "in", "s", "split", "can", "we", "put", "this", "token", "on", "this", "line", "without", "going", "over", "if", "str_buff", "if", "len", "str_buff", "len", "token", "line_length", "line_buff", "append", "str_buff", "str_buff", "token", "else", "str_buff", "token", "else", "in", "the", "case", "that", "the", "buffer", "is", "already", "empty", "add", "it", "without", "checking", "otherwise", "there", "might", "be", "things", "that", "we", "might", "never", "str_buff", "token", "if", "we", "have", "left", "over", "buff", "add", "it", "in", "if", "str_buff", "line_buff", "append", "str_buff", "return", "line_buff"], "doc_len": 97}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_violation", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_violation", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_violation(violation: SQLBaseError, max_line_length: int = 90) -> str:\n    \"\"\"Format a violation.\"\"\"\n    if isinstance(violation, SQLBaseError):\n        desc = violation.desc()\n        if violation.line_no is not None:\n            line_elem = f\"{violation.line_no:4d}\"\n        else:\n            line_elem = \"   -\"  # pragma: no cover\n        if violation.line_pos is not None:\n            pos_elem = f\"{violation.line_pos:4d}\"\n        else:\n            pos_elem = \"   -\"  # pragma: no cover\n    else:  # pragma: no cover\n        raise ValueError(f\"Unexpected violation format: {violation}\")\n\n    if violation.ignore:\n        desc = \"IGNORE: \" + desc  # pragma: no cover\n\n    split_desc = split_string_on_spaces(desc, line_length=max_line_length - 25)\n\n    out_buff = \"\"\n    for idx, line in enumerate(split_desc):\n        if idx == 0:\n            out_buff += colorize(\n                f\"L:{line_elem} | P:{pos_elem} | {violation.rule_code().rjust(4)} | \",\n                # Grey out the violation if we're ignoring it.\n                Color.lightgrey if violation.ignore else Color.blue,\n            )\n        else:\n            out_buff += (\n                \"\\n\"\n                + (\" \" * 23)\n                + colorize(\n                    \"| \",\n                    Color.lightgrey if violation.ignore else Color.blue,\n                )\n            )\n        out_buff += line\n    return out_buff\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_violation", "violation", "sqlbaseerror", "max_line_length", "int", "90", "str", "format", "a", "violation", "if", "isinstance", "violation", "sqlbaseerror", "desc", "violation", "desc", "if", "violation", "line_no", "is", "not", "none", "line_elem", "f", "violation", "line_no", "4d", "else", "line_elem", "pragma", "no", "cover", "if", "violation", "line_pos", "is", "not", "none", "pos_elem", "f", "violation", "line_pos", "4d", "else", "pos_elem", "pragma", "no", "cover", "else", "pragma", "no", "cover", "raise", "valueerror", "f", "unexpected", "violation", "format", "violation", "if", "violation", "ignore", "desc", "ignore", "desc", "pragma", "no", "cover", "split_desc", "split_string_on_spaces", "desc", "line_length", "max_line_length", "25", "out_buff", "for", "idx", "line", "in", "enumerate", "split_desc", "if", "idx", "0", "out_buff", "colorize", "f", "l", "line_elem", "p", "pos_elem", "violation", "rule_code", "rjust", "4", "grey", "out", "the", "violation", "if", "we", "re", "ignoring", "it", "color", "lightgrey", "if", "violation", "ignore", "else", "color", "blue", "else", "out_buff", "n", "23", "colorize", "color", "lightgrey", "if", "violation", "ignore", "else", "color", "blue", "out_buff", "line", "return", "out_buff"], "doc_len": 136}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_linting_stats", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_linting_stats", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_linting_stats(result, verbose=0):\n    \"\"\"Format a set of stats given a `LintingResult`.\"\"\"\n    text_buffer = StringIO()\n    all_stats = result.stats()\n    text_buffer.write(\"==== summary ====\\n\")\n    if verbose >= 2:\n        output_fields = [\n            \"files\",\n            \"violations\",\n            \"clean files\",\n            \"unclean files\",\n            \"avg per file\",\n            \"unclean rate\",\n            \"status\",\n        ]\n        special_formats = {\"unclean rate\": \"{0:.0%}\"}\n    else:\n        output_fields = [\"violations\", \"status\"]\n        special_formats = {}\n    # Generate content tuples, applying special formats for some fields\n    summary_content = [\n        (\n            key,\n            special_formats[key].format(all_stats[key])\n            if key in special_formats\n            else all_stats[key],\n        )\n        for key in output_fields\n    ]\n    # Render it all as a table\n    text_buffer.write(cli_table(summary_content, max_label_width=14))\n    return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_linting_stats", "result", "verbose", "0", "format", "a", "set", "of", "stats", "given", "a", "lintingresult", "text_buffer", "stringio", "all_stats", "result", "stats", "text_buffer", "write", "summary", "n", "if", "verbose", "2", "output_fields", "files", "violations", "clean", "files", "unclean", "files", "avg", "per", "file", "unclean", "rate", "status", "special_formats", "unclean", "rate", "0", "0", "else", "output_fields", "violations", "status", "special_formats", "generate", "content", "tuples", "applying", "special", "formats", "for", "some", "fields", "summary_content", "key", "special_formats", "key", "format", "all_stats", "key", "if", "key", "in", "special_formats", "else", "all_stats", "key", "for", "key", "in", "output_fields", "render", "it", "all", "as", "a", "table", "text_buffer", "write", "cli_table", "summary_content", "max_label_width", "14", "return", "text_buffer", "getvalue"], "doc_len": 95}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_linting_result_header", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_linting_result_header", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_linting_result_header():\n    \"\"\"Format the header of a linting result output.\"\"\"\n    text_buffer = StringIO()\n    text_buffer.write(\"==== readout ====\\n\")\n    return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_linting_result_header", "format", "the", "header", "of", "a", "linting", "result", "output", "text_buffer", "stringio", "text_buffer", "write", "readout", "n", "return", "text_buffer", "getvalue"], "doc_len": 24}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_config_vals", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_config_vals", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_config_vals(config_vals):\n    \"\"\"Format an iterable of config values from a config object.\"\"\"\n    text_buffer = StringIO()\n    for i, k, v in config_vals:\n        val = \"\" if v is None else str(v)\n        text_buffer.write(\n            (\"    \" * i)\n            + colorize(pad_line(str(k) + \":\", 20, \"left\"), color=Color.lightgrey)\n            + pad_line(val, 20, \"left\")\n            + \"\\n\"\n        )\n    return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_config_vals", "config_vals", "format", "an", "iterable", "of", "config", "values", "from", "a", "config", "object", "text_buffer", "stringio", "for", "i", "k", "v", "in", "config_vals", "val", "if", "v", "is", "none", "else", "str", "v", "text_buffer", "write", "i", "colorize", "pad_line", "str", "k", "20", "left", "color", "color", "lightgrey", "pad_line", "val", "20", "left", "n", "return", "text_buffer", "getvalue"], "doc_len": 54}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_rules", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_rules", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_rules(linter: Linter, verbose: int = 0) -> str:\n    \"\"\"Format the a set of rules given a `Linter`.\"\"\"\n    text_buffer = StringIO()\n    text_buffer.write(\"==== sqlfluff - rules ====\\n\")\n    text_buffer.write(\n        cli_table(\n            linter.rule_tuples(),\n            col_width=80,\n            cols=1,\n            label_color=Color.blue,\n            val_align=\"left\",\n        )\n    )\n    return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_rules", "linter", "linter", "verbose", "int", "0", "str", "format", "the", "a", "set", "of", "rules", "given", "a", "linter", "text_buffer", "stringio", "text_buffer", "write", "sqlfluff", "rules", "n", "text_buffer", "write", "cli_table", "linter", "rule_tuples", "col_width", "80", "cols", "1", "label_color", "color", "blue", "val_align", "left", "return", "text_buffer", "getvalue"], "doc_len": 46}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_dialects", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_dialects", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_dialects(dialect_readout, verbose=0):\n    \"\"\"Format the dialects yielded by `dialect_readout`.\"\"\"\n    text_buffer = StringIO()\n    text_buffer.write(\"==== sqlfluff - dialects ====\\n\")\n    readouts = [\n        (\n            dialect.label,\n            f\"{dialect.name} dialect [inherits from '{dialect.inherits_from}']\",\n        )\n        for dialect in dialect_readout()\n    ]\n    text_buffer.write(\n        cli_table(\n            readouts,\n            col_width=60,\n            cols=1,\n            label_color=Color.blue,\n            val_align=\"right\",\n        )\n    )\n    return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_dialects", "dialect_readout", "verbose", "0", "format", "the", "dialects", "yielded", "by", "dialect_readout", "text_buffer", "stringio", "text_buffer", "write", "sqlfluff", "dialects", "n", "readouts", "dialect", "label", "f", "dialect", "name", "dialect", "inherits", "from", "dialect", "inherits_from", "for", "dialect", "in", "dialect_readout", "text_buffer", "write", "cli_table", "readouts", "col_width", "60", "cols", "1", "label_color", "color", "blue", "val_align", "right", "return", "text_buffer", "getvalue"], "doc_len": 54}
{"doc_id": "src/sqlfluff/cli/formatters.py::format_dialect_warning", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": null, "func_name": "format_dialect_warning", "text": "文件路径: src/sqlfluff/cli/formatters.py\ndef format_dialect_warning():  # pragma: no cover\n    \"\"\"Output a warning for parsing errors found on the ansi dialect.\"\"\"\n    return colorize(\n        (\n            \"WARNING: Parsing errors found and dialect is set to \"\n            \"'ansi'. Have you configured your dialect?\"\n        ),\n        Color.lightgrey,\n    )\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "def", "format_dialect_warning", "pragma", "no", "cover", "output", "a", "warning", "for", "parsing", "errors", "found", "on", "the", "ansi", "dialect", "return", "colorize", "warning", "parsing", "errors", "found", "and", "dialect", "is", "set", "to", "ansi", "have", "you", "configured", "your", "dialect", "color", "lightgrey"], "doc_len": 40}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.__init__", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "__init__", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def __init__(\n        self,\n        callback: Callable,\n        verbosity: int = 0,\n        filter_empty: bool = True,\n        output_line_length: int = 80,\n    ):\n        self._callback = callback\n        self._verbosity = verbosity\n        self._filter_empty = filter_empty\n        self.output_line_length = output_line_length\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "__init__", "self", "callback", "callable", "verbosity", "int", "0", "filter_empty", "bool", "true", "output_line_length", "int", "80", "self", "_callback", "callback", "self", "_verbosity", "verbosity", "self", "_filter_empty", "filter_empty", "self", "output_line_length", "output_line_length"], "doc_len": 32}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter._dispatch", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "_dispatch", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def _dispatch(self, s: str) -> None:\n        \"\"\"Dispatch a string to the callback.\n\n        This method is designed as a point for subclassing.\n        \"\"\"\n        # The strip here is to filter out any empty messages\n        if (not self._filter_empty) or s.strip(\" \\n\\t\"):\n            self._callback(s)\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "_dispatch", "self", "s", "str", "none", "dispatch", "a", "string", "to", "the", "callback", "this", "method", "is", "designed", "as", "a", "point", "for", "subclassing", "the", "strip", "here", "is", "to", "filter", "out", "any", "empty", "messages", "if", "not", "self", "_filter_empty", "or", "s", "strip", "n", "t", "self", "_callback", "s"], "doc_len": 49}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter._format_config", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "_format_config", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def _format_config(self, linter: Linter) -> str:\n        \"\"\"Format the config of a `Linter`.\"\"\"\n        text_buffer = StringIO()\n        # Only show version information if verbosity is high enough\n        if self._verbosity > 0:\n            text_buffer.write(\"==== sqlfluff ====\\n\")\n            config_content = [\n                (\"sqlfluff\", get_package_version()),\n                (\"python\", get_python_version()),\n                (\"implementation\", get_python_implementation()),\n                (\"dialect\", linter.dialect.name),\n                (\"verbosity\", self._verbosity),\n            ] + linter.templater.config_pairs()\n            text_buffer.write(\n                cli_table(config_content, col_width=30, max_label_width=15)\n            )\n            text_buffer.write(\"\\n\")\n            if linter.config.get(\"rule_whitelist\"):\n                text_buffer.write(\n                    cli_table(\n                        [(\"rules\", \", \".join(linter.config.get(\"rule_whitelist\")))],\n                        col_width=41,\n                    )\n                )\n            if self._verbosity > 1:\n                text_buffer.write(\"\\n== Raw Config:\\n\")\n                text_buffer.write(format_config_vals(linter.config.iter_vals()))\n        return text_buffer.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "_format_config", "self", "linter", "linter", "str", "format", "the", "config", "of", "a", "linter", "text_buffer", "stringio", "only", "show", "version", "information", "if", "verbosity", "is", "high", "enough", "if", "self", "_verbosity", "0", "text_buffer", "write", "sqlfluff", "n", "config_content", "sqlfluff", "get_package_version", "python", "get_python_version", "implementation", "get_python_implementation", "dialect", "linter", "dialect", "name", "verbosity", "self", "_verbosity", "linter", "templater", "config_pairs", "text_buffer", "write", "cli_table", "config_content", "col_width", "30", "max_label_width", "15", "text_buffer", "write", "n", "if", "linter", "config", "get", "rule_whitelist", "text_buffer", "write", "cli_table", "rules", "join", "linter", "config", "get", "rule_whitelist", "col_width", "41", "if", "self", "_verbosity", "1", "text_buffer", "write", "n", "raw", "config", "n", "text_buffer", "write", "format_config_vals", "linter", "config", "iter_vals", "return", "text_buffer", "getvalue"], "doc_len": 100}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_config", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_config", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_config(self, linter: Linter) -> None:\n        \"\"\"Dispatch configuration output appropriately.\"\"\"\n        self._dispatch(self._format_config(linter))\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_config", "self", "linter", "linter", "none", "dispatch", "configuration", "output", "appropriately", "self", "_dispatch", "self", "_format_config", "linter"], "doc_len": 21}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_persist_filename", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_persist_filename", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_persist_filename(self, filename, result):\n        \"\"\"Dispatch filenames during a persist operation.\"\"\"\n        # Only show the skip records at higher levels of verbosity\n        if self._verbosity >= 2 or result != \"SKIP\":\n            self._dispatch(format_filename(filename=filename, success=result))\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_persist_filename", "self", "filename", "result", "dispatch", "filenames", "during", "a", "persist", "operation", "only", "show", "the", "skip", "records", "at", "higher", "levels", "of", "verbosity", "if", "self", "_verbosity", "2", "or", "result", "skip", "self", "_dispatch", "format_filename", "filename", "filename", "success", "result"], "doc_len": 41}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter._format_path", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "_format_path", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def _format_path(path: str) -> str:\n        \"\"\"Format paths.\"\"\"\n        return f\"=== [ path: {colorize(path, Color.lightgrey)} ] ===\\n\"\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "_format_path", "path", "str", "str", "format", "paths", "return", "f", "path", "colorize", "path", "color", "lightgrey", "n"], "doc_len": 21}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_path", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_path", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_path(self, path: str) -> None:\n        \"\"\"Dispatch paths for display.\"\"\"\n        if self._verbosity > 0:\n            self._dispatch(self._format_path(path))\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_path", "self", "path", "str", "none", "dispatch", "paths", "for", "display", "if", "self", "_verbosity", "0", "self", "_dispatch", "self", "_format_path", "path"], "doc_len": 25}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_template_header", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_template_header", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_template_header(\n        self, fname: str, linter_config: FluffConfig, file_config: FluffConfig\n    ) -> None:\n        \"\"\"Dispatch the header displayed before templating.\"\"\"\n        if self._verbosity > 1:\n            self._dispatch(format_filename(filename=fname, success=\"TEMPLATING\"))\n            # This is where we output config diffs if they exist.\n            if file_config:\n                # Only output config diffs if there is a config to diff to.\n                config_diff = file_config.diff_to(linter_config)\n                if config_diff:  # pragma: no cover\n                    self._dispatch(\"   Config Diff:\")\n                    self._dispatch(\n                        format_config_vals(linter_config.iter_vals(cfg=config_diff))\n                    )\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_template_header", "self", "fname", "str", "linter_config", "fluffconfig", "file_config", "fluffconfig", "none", "dispatch", "the", "header", "displayed", "before", "templating", "if", "self", "_verbosity", "1", "self", "_dispatch", "format_filename", "filename", "fname", "success", "templating", "this", "is", "where", "we", "output", "config", "diffs", "if", "they", "exist", "if", "file_config", "only", "output", "config", "diffs", "if", "there", "is", "a", "config", "to", "diff", "to", "config_diff", "file_config", "diff_to", "linter_config", "if", "config_diff", "pragma", "no", "cover", "self", "_dispatch", "config", "diff", "self", "_dispatch", "format_config_vals", "linter_config", "iter_vals", "cfg", "config_diff"], "doc_len": 77}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_parse_header", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_parse_header", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_parse_header(self, fname: str) -> None:\n        \"\"\"Dispatch the header displayed before parsing.\"\"\"\n        if self._verbosity > 1:\n            self._dispatch(format_filename(filename=fname, success=\"PARSING\"))\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_parse_header", "self", "fname", "str", "none", "dispatch", "the", "header", "displayed", "before", "parsing", "if", "self", "_verbosity", "1", "self", "_dispatch", "format_filename", "filename", "fname", "success", "parsing"], "doc_len": 29}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_lint_header", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_lint_header", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_lint_header(self, fname: str) -> None:\n        \"\"\"Dispatch the header displayed before linting.\"\"\"\n        if self._verbosity > 1:\n            self._dispatch(format_filename(filename=fname, success=\"LINTING\"))\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_lint_header", "self", "fname", "str", "none", "dispatch", "the", "header", "displayed", "before", "linting", "if", "self", "_verbosity", "1", "self", "_dispatch", "format_filename", "filename", "fname", "success", "linting"], "doc_len": 29}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_compilation_header", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_compilation_header", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_compilation_header(self, templater, message):\n        \"\"\"Dispatch the header displayed before linting.\"\"\"\n        self._dispatch(\n            f\"=== [{colorize(templater, Color.lightgrey)}] {message}\"\n        )  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_compilation_header", "self", "templater", "message", "dispatch", "the", "header", "displayed", "before", "linting", "self", "_dispatch", "f", "colorize", "templater", "color", "lightgrey", "message", "pragma", "no", "cover"], "doc_len": 28}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_dialect_warning", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_dialect_warning", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_dialect_warning(self) -> None:\n        \"\"\"Dispatch a warning for dialects.\"\"\"\n        self._dispatch(format_dialect_warning())  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_dialect_warning", "self", "none", "dispatch", "a", "warning", "for", "dialects", "self", "_dispatch", "format_dialect_warning", "pragma", "no", "cover"], "doc_len": 21}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter._format_file_violations", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "_format_file_violations", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def _format_file_violations(\n        self, fname: str, violations: List[SQLBaseError]\n    ) -> str:\n        \"\"\"Format a set of violations in a `LintingResult`.\"\"\"\n        text_buffer = StringIO()\n        # Success is having no violations (which aren't ignored)\n        success = sum(int(not violation.ignore) for violation in violations) == 0\n\n        # Only print the filename if it's either a failure or verbosity > 1\n        if self._verbosity > 0 or not success:\n            text_buffer.write(format_filename(fname, success=success))\n            text_buffer.write(\"\\n\")\n\n        # If we have violations, print them\n        if not success:\n            # sort by position in file (using line number and position)\n            s = sorted(violations, key=lambda v: (v.line_no, v.line_pos))\n            for violation in s:\n                text_buffer.write(\n                    format_violation(violation, max_line_length=self.output_line_length)\n                )\n                text_buffer.write(\"\\n\")\n        str_buffer = text_buffer.getvalue()\n        # Remove the trailing newline if there is one\n        if len(str_buffer) > 0 and str_buffer[-1] == \"\\n\":\n            str_buffer = str_buffer[:-1]\n        return str_buffer\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "_format_file_violations", "self", "fname", "str", "violations", "list", "sqlbaseerror", "str", "format", "a", "set", "of", "violations", "in", "a", "lintingresult", "text_buffer", "stringio", "success", "is", "having", "no", "violations", "which", "aren", "t", "ignored", "success", "sum", "int", "not", "violation", "ignore", "for", "violation", "in", "violations", "0", "only", "print", "the", "filename", "if", "it", "s", "either", "a", "failure", "or", "verbosity", "1", "if", "self", "_verbosity", "0", "or", "not", "success", "text_buffer", "write", "format_filename", "fname", "success", "success", "text_buffer", "write", "n", "if", "we", "have", "violations", "print", "them", "if", "not", "success", "sort", "by", "position", "in", "file", "using", "line", "number", "and", "position", "s", "sorted", "violations", "key", "lambda", "v", "v", "line_no", "v", "line_pos", "for", "violation", "in", "s", "text_buffer", "write", "format_violation", "violation", "max_line_length", "self", "output_line_length", "text_buffer", "write", "n", "str_buffer", "text_buffer", "getvalue", "remove", "the", "trailing", "newline", "if", "there", "is", "one", "if", "len", "str_buffer", "0", "and", "str_buffer", "1", "n", "str_buffer", "str_buffer", "1", "return", "str_buffer"], "doc_len": 141}
{"doc_id": "src/sqlfluff/cli/formatters.py::CallbackFormatter.dispatch_file_violations", "file_path": "src/sqlfluff/cli/formatters.py", "class_name": "CallbackFormatter", "func_name": "dispatch_file_violations", "text": "文件路径: src/sqlfluff/cli/formatters.py, 类名: CallbackFormatter\n    def dispatch_file_violations(\n        self, fname: str, linted_file: LintedFile, only_fixable: bool\n    ) -> None:\n        \"\"\"Dispatch any violations found in a file.\"\"\"\n        s = self._format_file_violations(\n            fname, linted_file.get_violations(fixable=True if only_fixable else None)\n        )\n        self._dispatch(s)\n", "tokens": ["src", "sqlfluff", "cli", "formatters", "py", "callbackformatter", "def", "dispatch_file_violations", "self", "fname", "str", "linted_file", "lintedfile", "only_fixable", "bool", "none", "dispatch", "any", "violations", "found", "in", "a", "file", "s", "self", "_format_file_violations", "fname", "linted_file", "get_violations", "fixable", "true", "if", "only_fixable", "else", "none", "self", "_dispatch", "s"], "doc_len": 38}
{"doc_id": "src/sqlfluff/cli/helpers.py::colorize", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "colorize", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef colorize(s: str, color: Optional[Color] = None) -> str:\n    \"\"\"Use ANSI colour codes to colour a string.\n\n    The name of this function is in American. I'm sorry :(.\n    \"\"\"\n    if color:\n        return f\"{color.value}{s}{Style.RESET_ALL}\"\n    else:\n        return s\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "colorize", "s", "str", "color", "optional", "color", "none", "str", "use", "ansi", "colour", "codes", "to", "colour", "a", "string", "the", "name", "of", "this", "function", "is", "in", "american", "i", "m", "sorry", "if", "color", "return", "f", "color", "value", "s", "style", "reset_all", "else", "return", "s"], "doc_len": 45}
{"doc_id": "src/sqlfluff/cli/helpers.py::get_python_version", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "get_python_version", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef get_python_version() -> str:\n    \"\"\"Get the current python version as a string.\"\"\"\n    return \"{0[0]}.{0[1]}.{0[2]}\".format(sys.version_info)\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "get_python_version", "str", "get", "the", "current", "python", "version", "as", "a", "string", "return", "0", "0", "0", "1", "0", "2", "format", "sys", "version_info"], "doc_len": 26}
{"doc_id": "src/sqlfluff/cli/helpers.py::get_python_implementation", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "get_python_implementation", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef get_python_implementation() -> str:\n    \"\"\"Get the current python implementation as a string.\n\n    This is useful if testing in pypy or similar.\n    \"\"\"\n    return sys.implementation.name\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "get_python_implementation", "str", "get", "the", "current", "python", "implementation", "as", "a", "string", "this", "is", "useful", "if", "testing", "in", "pypy", "or", "similar", "return", "sys", "implementation", "name"], "doc_len": 29}
{"doc_id": "src/sqlfluff/cli/helpers.py::get_package_version", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "get_package_version", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef get_package_version() -> str:\n    \"\"\"Get the current version of the sqlfluff package.\"\"\"\n    return pkg_version\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "get_package_version", "str", "get", "the", "current", "version", "of", "the", "sqlfluff", "package", "return", "pkg_version"], "doc_len": 18}
{"doc_id": "src/sqlfluff/cli/helpers.py::wrap_elem", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "wrap_elem", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef wrap_elem(s: str, width: int) -> List[str]:\n    \"\"\"Take a string, and attempt to wrap into a list of strings all less than <width>.\"\"\"\n    return textwrap.wrap(s, width=width)\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "wrap_elem", "s", "str", "width", "int", "list", "str", "take", "a", "string", "and", "attempt", "to", "wrap", "into", "a", "list", "of", "strings", "all", "less", "than", "width", "return", "textwrap", "wrap", "s", "width", "width"], "doc_len": 35}
{"doc_id": "src/sqlfluff/cli/helpers.py::wrap_field", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "wrap_field", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef wrap_field(\n    label: str, val: str, width: int, max_label_width: int = 10, sep_char: str = \": \"\n) -> Dict:\n    \"\"\"Wrap a field (label, val).\n\n    Returns:\n        A dict of {label_list, val_list, sep_char, lines}\n\n    \"\"\"\n    if len(label) > max_label_width:\n        label_list = wrap_elem(label, width=max_label_width)\n        label_width = max(len(line) for line in label_list)\n    else:\n        label_width = len(label)\n        label_list = [label]\n\n    max_val_width = width - len(sep_char) - label_width\n    val_list = wrap_elem(val, width=max_val_width)\n    return dict(\n        label_list=label_list,\n        val_list=val_list,\n        sep_char=sep_char,\n        lines=max(len(label_list), len(val_list)),\n        label_width=label_width,\n        val_width=max_val_width,\n    )\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "wrap_field", "label", "str", "val", "str", "width", "int", "max_label_width", "int", "10", "sep_char", "str", "dict", "wrap", "a", "field", "label", "val", "returns", "a", "dict", "of", "label_list", "val_list", "sep_char", "lines", "if", "len", "label", "max_label_width", "label_list", "wrap_elem", "label", "width", "max_label_width", "label_width", "max", "len", "line", "for", "line", "in", "label_list", "else", "label_width", "len", "label", "label_list", "label", "max_val_width", "width", "len", "sep_char", "label_width", "val_list", "wrap_elem", "val", "width", "max_val_width", "return", "dict", "label_list", "label_list", "val_list", "val_list", "sep_char", "sep_char", "lines", "max", "len", "label_list", "len", "val_list", "label_width", "label_width", "val_width", "max_val_width"], "doc_len": 83}
{"doc_id": "src/sqlfluff/cli/helpers.py::pad_line", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "pad_line", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef pad_line(s: str, width: int, align: str = \"left\") -> str:\n    \"\"\"Pad a string with a given alignment to a specific width with spaces.\"\"\"\n    gap = width - len(s)\n    if gap <= 0:\n        return s\n    elif align == \"left\":\n        return s + (\" \" * gap)\n    elif align == \"right\":\n        return (\" \" * gap) + s\n    else:\n        raise ValueError(f\"Unknown alignment: {align}\")  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "pad_line", "s", "str", "width", "int", "align", "str", "left", "str", "pad", "a", "string", "with", "a", "given", "alignment", "to", "a", "specific", "width", "with", "spaces", "gap", "width", "len", "s", "if", "gap", "0", "return", "s", "elif", "align", "left", "return", "s", "gap", "elif", "align", "right", "return", "gap", "s", "else", "raise", "valueerror", "f", "unknown", "alignment", "align", "pragma", "no", "cover"], "doc_len": 59}
{"doc_id": "src/sqlfluff/cli/helpers.py::cli_table_row", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "cli_table_row", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef cli_table_row(\n    fields: List[Tuple[str, str]],\n    col_width,\n    max_label_width=10,\n    sep_char=\": \",\n    divider_char=\" \",\n    label_color=Color.lightgrey,\n    val_align=\"right\",\n) -> str:\n    \"\"\"Make a row of a CLI table, using wrapped values.\"\"\"\n    # Do some intel first\n    cols = len(fields)\n    last_col_idx = cols - 1\n    wrapped_fields = [\n        wrap_field(\n            field[0],\n            field[1],\n            width=col_width,\n            max_label_width=max_label_width,\n            sep_char=sep_char,\n        )\n        for field in fields\n    ]\n    max_lines = max(fld[\"lines\"] for fld in wrapped_fields)\n    last_line_idx = max_lines - 1\n    # Make some text\n    buff = StringIO()\n    for line_idx in range(max_lines):\n        for col_idx in range(cols):\n            # Assume we pad labels left and values right\n            fld = wrapped_fields[col_idx]\n            ll = fld[\"label_list\"]\n            vl = fld[\"val_list\"]\n            buff.write(\n                colorize(\n                    pad_line(\n                        ll[line_idx] if line_idx < len(ll) else \"\",\n                        width=fld[\"label_width\"],\n                    ),\n                    color=label_color,\n                )\n            )\n            if line_idx == 0:\n                buff.write(sep_char)\n            else:\n                buff.write(\" \" * len(sep_char))\n            buff.write(\n                pad_line(\n                    vl[line_idx] if line_idx < len(vl) else \"\",\n                    width=fld[\"val_width\"],\n                    align=val_align,\n                )\n            )\n            if col_idx != last_col_idx:\n                buff.write(divider_char)\n            elif line_idx != last_line_idx:\n                buff.write(\"\\n\")\n    return buff.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "cli_table_row", "fields", "list", "tuple", "str", "str", "col_width", "max_label_width", "10", "sep_char", "divider_char", "label_color", "color", "lightgrey", "val_align", "right", "str", "make", "a", "row", "of", "a", "cli", "table", "using", "wrapped", "values", "do", "some", "intel", "first", "cols", "len", "fields", "last_col_idx", "cols", "1", "wrapped_fields", "wrap_field", "field", "0", "field", "1", "width", "col_width", "max_label_width", "max_label_width", "sep_char", "sep_char", "for", "field", "in", "fields", "max_lines", "max", "fld", "lines", "for", "fld", "in", "wrapped_fields", "last_line_idx", "max_lines", "1", "make", "some", "text", "buff", "stringio", "for", "line_idx", "in", "range", "max_lines", "for", "col_idx", "in", "range", "cols", "assume", "we", "pad", "labels", "left", "and", "values", "right", "fld", "wrapped_fields", "col_idx", "ll", "fld", "label_list", "vl", "fld", "val_list", "buff", "write", "colorize", "pad_line", "ll", "line_idx", "if", "line_idx", "len", "ll", "else", "width", "fld", "label_width", "color", "label_color", "if", "line_idx", "0", "buff", "write", "sep_char", "else", "buff", "write", "len", "sep_char", "buff", "write", "pad_line", "vl", "line_idx", "if", "line_idx", "len", "vl", "else", "width", "fld", "val_width", "align", "val_align", "if", "col_idx", "last_col_idx", "buff", "write", "divider_char", "elif", "line_idx", "last_line_idx", "buff", "write", "n", "return", "buff", "getvalue"], "doc_len": 159}
{"doc_id": "src/sqlfluff/cli/helpers.py::cli_table", "file_path": "src/sqlfluff/cli/helpers.py", "class_name": null, "func_name": "cli_table", "text": "文件路径: src/sqlfluff/cli/helpers.py\ndef cli_table(\n    fields,\n    col_width=20,\n    cols=2,\n    divider_char=\" \",\n    sep_char=\": \",\n    label_color=Color.lightgrey,\n    float_format=\"{0:.2f}\",\n    max_label_width=10,\n    val_align=\"right\",\n) -> str:\n    \"\"\"Make a crude ascii table, assuming that `fields` is an iterable of (label, value) pairs.\"\"\"\n    # First format all the values into strings\n    formatted_fields = []\n    for label, value in fields:\n        label = str(label)\n        if isinstance(value, float):\n            value = float_format.format(value)\n        else:\n            value = str(value)\n        formatted_fields.append((label, value))\n\n    # Set up a buffer to hold the whole table\n    buff = StringIO()\n    while len(formatted_fields) > 0:\n        row_buff: List[Tuple[str, str]] = []\n        while len(row_buff) < cols and len(formatted_fields) > 0:\n            row_buff.append(formatted_fields.pop(0))\n        buff.write(\n            cli_table_row(\n                row_buff,\n                col_width=col_width,\n                max_label_width=max_label_width,\n                sep_char=sep_char,\n                divider_char=divider_char,\n                label_color=label_color,\n                val_align=val_align,\n            )\n        )\n        if len(formatted_fields) > 0:\n            buff.write(\"\\n\")\n    return buff.getvalue()\n", "tokens": ["src", "sqlfluff", "cli", "helpers", "py", "def", "cli_table", "fields", "col_width", "20", "cols", "2", "divider_char", "sep_char", "label_color", "color", "lightgrey", "float_format", "0", "2f", "max_label_width", "10", "val_align", "right", "str", "make", "a", "crude", "ascii", "table", "assuming", "that", "fields", "is", "an", "iterable", "of", "label", "value", "pairs", "first", "format", "all", "the", "values", "into", "strings", "formatted_fields", "for", "label", "value", "in", "fields", "label", "str", "label", "if", "isinstance", "value", "float", "value", "float_format", "format", "value", "else", "value", "str", "value", "formatted_fields", "append", "label", "value", "set", "up", "a", "buffer", "to", "hold", "the", "whole", "table", "buff", "stringio", "while", "len", "formatted_fields", "0", "row_buff", "list", "tuple", "str", "str", "while", "len", "row_buff", "cols", "and", "len", "formatted_fields", "0", "row_buff", "append", "formatted_fields", "pop", "0", "buff", "write", "cli_table_row", "row_buff", "col_width", "col_width", "max_label_width", "max_label_width", "sep_char", "sep_char", "divider_char", "divider_char", "label_color", "label_color", "val_align", "val_align", "if", "len", "formatted_fields", "0", "buff", "write", "n", "return", "buff", "getvalue"], "doc_len": 131}
{"doc_id": "src/sqlfluff/core/config.py::coerce_value", "file_path": "src/sqlfluff/core/config.py", "class_name": null, "func_name": "coerce_value", "text": "文件路径: src/sqlfluff/core/config.py\ndef coerce_value(val: str) -> Any:\n    \"\"\"Try to coerce to a more specific type.\"\"\"\n    # Try to coerce it to a more specific type,\n    # otherwise just make it a string.\n    try:\n        v: Any = int(val)\n    except ValueError:\n        try:\n            v = float(val)\n        except ValueError:\n            cleaned_val = val.strip().lower()\n            if cleaned_val in [\"true\"]:\n                v = True\n            elif cleaned_val in [\"false\"]:\n                v = False\n            elif cleaned_val in [\"none\"]:\n                v = None\n            else:\n                v = val\n    return v\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "def", "coerce_value", "val", "str", "any", "try", "to", "coerce", "to", "a", "more", "specific", "type", "try", "to", "coerce", "it", "to", "a", "more", "specific", "type", "otherwise", "just", "make", "it", "a", "string", "try", "v", "any", "int", "val", "except", "valueerror", "try", "v", "float", "val", "except", "valueerror", "cleaned_val", "val", "strip", "lower", "if", "cleaned_val", "in", "true", "v", "true", "elif", "cleaned_val", "in", "false", "v", "false", "elif", "cleaned_val", "in", "none", "v", "none", "else", "v", "val", "return", "v"], "doc_len": 73}
{"doc_id": "src/sqlfluff/core/config.py::nested_combine", "file_path": "src/sqlfluff/core/config.py", "class_name": null, "func_name": "nested_combine", "text": "文件路径: src/sqlfluff/core/config.py\ndef nested_combine(*dicts: dict) -> dict:\n    \"\"\"Combine an iterable of dictionaries.\n\n    Each dictionary is combined into a result dictionary. For\n    each key in the first dictionary, it will be overwritten\n    by any same-named key in any later dictionaries in the\n    iterable. If the element at that key is a dictionary, rather\n    than just overwriting we use the same function to combine\n    those dictionaries.\n\n    Args:\n        *dicts: An iterable of dictionaries to be combined.\n\n    Returns:\n        `dict`: A combined dictionary from the input dictionaries.\n\n    \"\"\"\n    r: dict = {}\n    for d in dicts:\n        for k in d:\n            if k in r and isinstance(r[k], dict):\n                if isinstance(d[k], dict):\n                    r[k] = nested_combine(r[k], d[k])\n                else:  # pragma: no cover\n                    raise ValueError(\n                        \"Key {!r} is a dict in one config but not another! PANIC: {!r}\".format(\n                            k, d[k]\n                        )\n                    )\n            else:\n                r[k] = d[k]\n    return r\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "def", "nested_combine", "dicts", "dict", "dict", "combine", "an", "iterable", "of", "dictionaries", "each", "dictionary", "is", "combined", "into", "a", "result", "dictionary", "for", "each", "key", "in", "the", "first", "dictionary", "it", "will", "be", "overwritten", "by", "any", "same", "named", "key", "in", "any", "later", "dictionaries", "in", "the", "iterable", "if", "the", "element", "at", "that", "key", "is", "a", "dictionary", "rather", "than", "just", "overwriting", "we", "use", "the", "same", "function", "to", "combine", "those", "dictionaries", "args", "dicts", "an", "iterable", "of", "dictionaries", "to", "be", "combined", "returns", "dict", "a", "combined", "dictionary", "from", "the", "input", "dictionaries", "r", "dict", "for", "d", "in", "dicts", "for", "k", "in", "d", "if", "k", "in", "r", "and", "isinstance", "r", "k", "dict", "if", "isinstance", "d", "k", "dict", "r", "k", "nested_combine", "r", "k", "d", "k", "else", "pragma", "no", "cover", "raise", "valueerror", "key", "r", "is", "a", "dict", "in", "one", "config", "but", "not", "another", "panic", "r", "format", "k", "d", "k", "else", "r", "k", "d", "k", "return", "r"], "doc_len": 147}
{"doc_id": "src/sqlfluff/core/config.py::dict_diff", "file_path": "src/sqlfluff/core/config.py", "class_name": null, "func_name": "dict_diff", "text": "文件路径: src/sqlfluff/core/config.py\ndef dict_diff(left: dict, right: dict, ignore: Optional[List[str]] = None) -> dict:\n    \"\"\"Work out the difference between to dictionaries.\n\n    Returns a dictionary which represents elements in the `left`\n    dictionary which aren't in the `right` or are different to\n    those in the `right`. If the element is a dictionary, we\n    recursively look for differences in those dictionaries,\n    likewise only returning the differing elements.\n\n    NOTE: If an element is in the `right` but not in the `left`\n    at all (i.e. an element has been *removed*) then it will\n    not show up in the comparison.\n\n    Args:\n        left (:obj:`dict`): The object containing the *new* elements\n            which will be compared against the other.\n        right (:obj:`dict`): The object to compare against.\n\n    Returns:\n        `dict`: A dictionary representing the difference.\n\n    \"\"\"\n    buff: dict = {}\n    for k in left:\n        if ignore and k in ignore:\n            continue\n        # Is the key there at all?\n        if k not in right:\n            buff[k] = left[k]\n        # Is the content the same?\n        elif left[k] == right[k]:\n            continue\n        # If it's not the same but both are dicts, then compare\n        elif isinstance(left[k], dict) and isinstance(right[k], dict):\n            diff = dict_diff(left[k], right[k], ignore=ignore)\n            # Only if the difference is not ignored it do we include it.\n            if diff:\n                buff[k] = diff\n        # It's just different\n        else:\n            buff[k] = left[k]\n    return buff\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "def", "dict_diff", "left", "dict", "right", "dict", "ignore", "optional", "list", "str", "none", "dict", "work", "out", "the", "difference", "between", "to", "dictionaries", "returns", "a", "dictionary", "which", "represents", "elements", "in", "the", "left", "dictionary", "which", "aren", "t", "in", "the", "right", "or", "are", "different", "to", "those", "in", "the", "right", "if", "the", "element", "is", "a", "dictionary", "we", "recursively", "look", "for", "differences", "in", "those", "dictionaries", "likewise", "only", "returning", "the", "differing", "elements", "note", "if", "an", "element", "is", "in", "the", "right", "but", "not", "in", "the", "left", "at", "all", "i", "e", "an", "element", "has", "been", "removed", "then", "it", "will", "not", "show", "up", "in", "the", "comparison", "args", "left", "obj", "dict", "the", "object", "containing", "the", "new", "elements", "which", "will", "be", "compared", "against", "the", "other", "right", "obj", "dict", "the", "object", "to", "compare", "against", "returns", "dict", "a", "dictionary", "representing", "the", "difference", "buff", "dict", "for", "k", "in", "left", "if", "ignore", "and", "k", "in", "ignore", "continue", "is", "the", "key", "there", "at", "all", "if", "k", "not", "in", "right", "buff", "k", "left", "k", "is", "the", "content", "the", "same", "elif", "left", "k", "right", "k", "continue", "if", "it", "s", "not", "the", "same", "but", "both", "are", "dicts", "then", "compare", "elif", "isinstance", "left", "k", "dict", "and", "isinstance", "right", "k", "dict", "diff", "dict_diff", "left", "k", "right", "k", "ignore", "ignore", "only", "if", "the", "difference", "is", "not", "ignored", "it", "do", "we", "include", "it", "if", "diff", "buff", "k", "diff", "it", "s", "just", "different", "else", "buff", "k", "left", "k", "return", "buff"], "doc_len": 228}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.__init__", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def __init__(self):\n        # TODO: check that this cache implementation is actually useful\n        self._config_cache: dict = {}\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "__init__", "self", "todo", "check", "that", "this", "cache", "implementation", "is", "actually", "useful", "self", "_config_cache", "dict"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.get_global", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "get_global", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def get_global(cls) -> \"ConfigLoader\":\n        \"\"\"Get the singleton loader.\"\"\"\n        global global_loader\n        if not global_loader:\n            global_loader = cls()\n        return global_loader\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "get_global", "cls", "configloader", "get", "the", "singleton", "loader", "global", "global_loader", "if", "not", "global_loader", "global_loader", "cls", "return", "global_loader"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader._walk_toml", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "_walk_toml", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def _walk_toml(cls, config: Dict[str, Any], base_key=()):\n        \"\"\"Recursively walk the nested config inside a TOML file.\"\"\"\n        buff: List[tuple] = []\n        for k, v in config.items():\n            key = base_key + (k,)\n            if isinstance(v, dict):\n                buff.extend(cls._walk_toml(v, key))\n            else:\n                buff.append((key, v))\n\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "_walk_toml", "cls", "config", "dict", "str", "any", "base_key", "recursively", "walk", "the", "nested", "config", "inside", "a", "toml", "file", "buff", "list", "tuple", "for", "k", "v", "in", "config", "items", "key", "base_key", "k", "if", "isinstance", "v", "dict", "buff", "extend", "cls", "_walk_toml", "v", "key", "else", "buff", "append", "key", "v", "return", "buff"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader._get_config_elems_from_toml", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "_get_config_elems_from_toml", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def _get_config_elems_from_toml(cls, fpath: str) -> List[Tuple[tuple, Any]]:\n        \"\"\"Load a config from a TOML file and return a list of tuples.\n\n        The return value is a list of tuples, were each tuple has two elements,\n        the first is a tuple of paths, the second is the value at that path.\n        \"\"\"\n        config = toml.load(fpath)\n        tool = config.get(\"tool\", {}).get(\"sqlfluff\", {})\n\n        return cls._walk_toml(tool)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "_get_config_elems_from_toml", "cls", "fpath", "str", "list", "tuple", "tuple", "any", "load", "a", "config", "from", "a", "toml", "file", "and", "return", "a", "list", "of", "tuples", "the", "return", "value", "is", "a", "list", "of", "tuples", "were", "each", "tuple", "has", "two", "elements", "the", "first", "is", "a", "tuple", "of", "paths", "the", "second", "is", "the", "value", "at", "that", "path", "config", "toml", "load", "fpath", "tool", "config", "get", "tool", "get", "sqlfluff", "return", "cls", "_walk_toml", "tool"], "doc_len": 71}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader._get_config_elems_from_file", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "_get_config_elems_from_file", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def _get_config_elems_from_file(fpath: str) -> List[Tuple[tuple, Any]]:\n        \"\"\"Load a config from a file and return a list of tuples.\n\n        The return value is a list of tuples, were each tuple has two elements,\n        the first is a tuple of paths, the second is the value at that path.\n\n        Note:\n            Unlike most cfg file readers, sqlfluff is case-sensitive in how\n            it reads config files.\n\n        Note:\n            Any variable names ending with `_path` or `_dir`, will be attempted to be\n            resolved as relative paths to this config file. If that fails the\n            string value will remain.\n\n        \"\"\"\n        buff: List[Tuple[tuple, Any]] = []\n        # Disable interpolation so we can load macros\n        kw: Dict = {}\n        kw[\"interpolation\"] = None\n        config = configparser.ConfigParser(**kw)\n        # NB: We want to be case sensitive in how we read from files,\n        # because jinja is also case sensitive. To do this we override\n        # the optionxform attribute.\n        config.optionxform = lambda option: option  # type: ignore\n        config.read(fpath)\n        for k in config.sections():\n            if k == \"sqlfluff\":\n                key: Tuple = (\"core\",)\n            elif k.startswith(\"sqlfluff:\"):\n                # Return a tuple of nested values\n                key = tuple(k[len(\"sqlfluff:\") :].split(\":\"))\n            else:  # pragma: no cover\n                # if it doesn't start with sqlfluff, then don't go\n                # further on this iteration\n                continue\n\n            for name, val in config.items(section=k):\n                # Try to coerce it to a more specific type,\n                # otherwise just make it a string.\n                v = coerce_value(val)\n\n                # Attempt to resolve paths\n                if name.lower().endswith((\"_path\", \"_dir\")):\n                    # Try to resolve the path.\n                    # Make the referenced path.\n                    ref_path = os.path.join(os.path.dirname(fpath), val)\n                    # Check if it exists, and if it does, replace the value with the path.\n                    if os.path.exists(ref_path):\n                        v = ref_path\n                # Add the name to the end of the key\n                buff.append((key + (name,), v))\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "_get_config_elems_from_file", "fpath", "str", "list", "tuple", "tuple", "any", "load", "a", "config", "from", "a", "file", "and", "return", "a", "list", "of", "tuples", "the", "return", "value", "is", "a", "list", "of", "tuples", "were", "each", "tuple", "has", "two", "elements", "the", "first", "is", "a", "tuple", "of", "paths", "the", "second", "is", "the", "value", "at", "that", "path", "note", "unlike", "most", "cfg", "file", "readers", "sqlfluff", "is", "case", "sensitive", "in", "how", "it", "reads", "config", "files", "note", "any", "variable", "names", "ending", "with", "_path", "or", "_dir", "will", "be", "attempted", "to", "be", "resolved", "as", "relative", "paths", "to", "this", "config", "file", "if", "that", "fails", "the", "string", "value", "will", "remain", "buff", "list", "tuple", "tuple", "any", "disable", "interpolation", "so", "we", "can", "load", "macros", "kw", "dict", "kw", "interpolation", "none", "config", "configparser", "configparser", "kw", "nb", "we", "want", "to", "be", "case", "sensitive", "in", "how", "we", "read", "from", "files", "because", "jinja", "is", "also", "case", "sensitive", "to", "do", "this", "we", "override", "the", "optionxform", "attribute", "config", "optionxform", "lambda", "option", "option", "type", "ignore", "config", "read", "fpath", "for", "k", "in", "config", "sections", "if", "k", "sqlfluff", "key", "tuple", "core", "elif", "k", "startswith", "sqlfluff", "return", "a", "tuple", "of", "nested", "values", "key", "tuple", "k", "len", "sqlfluff", "split", "else", "pragma", "no", "cover", "if", "it", "doesn", "t", "start", "with", "sqlfluff", "then", "don", "t", "go", "further", "on", "this", "iteration", "continue", "for", "name", "val", "in", "config", "items", "section", "k", "try", "to", "coerce", "it", "to", "a", "more", "specific", "type", "otherwise", "just", "make", "it", "a", "string", "v", "coerce_value", "val", "attempt", "to", "resolve", "paths", "if", "name", "lower", "endswith", "_path", "_dir", "try", "to", "resolve", "the", "path", "make", "the", "referenced", "path", "ref_path", "os", "path", "join", "os", "path", "dirname", "fpath", "val", "check", "if", "it", "exists", "and", "if", "it", "does", "replace", "the", "value", "with", "the", "path", "if", "os", "path", "exists", "ref_path", "v", "ref_path", "add", "the", "name", "to", "the", "end", "of", "the", "key", "buff", "append", "key", "name", "v", "return", "buff"], "doc_len": 297}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader._incorporate_vals", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "_incorporate_vals", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def _incorporate_vals(ctx: dict, vals: List[Tuple[Tuple[str, ...], Any]]) -> dict:\n        \"\"\"Take a list of tuples and incorporate it into a dictionary.\"\"\"\n        for k, v in vals:\n            # Keep a ref we can use for recursion\n            r = ctx\n            # Get the name of the variable\n            n = k[-1]\n            # Get the path\n            pth = k[:-1]\n            for dp in pth:\n                # Does this path exist?\n                if dp in r:\n                    if isinstance(r[dp], dict):\n                        r = r[dp]\n                    else:  # pragma: no cover\n                        raise ValueError(f\"Overriding config value with section! [{k}]\")\n                else:\n                    r[dp] = {}\n                    r = r[dp]\n            # Deal with the value itself\n            r[n] = v\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "_incorporate_vals", "ctx", "dict", "vals", "list", "tuple", "tuple", "str", "any", "dict", "take", "a", "list", "of", "tuples", "and", "incorporate", "it", "into", "a", "dictionary", "for", "k", "v", "in", "vals", "keep", "a", "ref", "we", "can", "use", "for", "recursion", "r", "ctx", "get", "the", "name", "of", "the", "variable", "n", "k", "1", "get", "the", "path", "pth", "k", "1", "for", "dp", "in", "pth", "does", "this", "path", "exist", "if", "dp", "in", "r", "if", "isinstance", "r", "dp", "dict", "r", "r", "dp", "else", "pragma", "no", "cover", "raise", "valueerror", "f", "overriding", "config", "value", "with", "section", "k", "else", "r", "dp", "r", "r", "dp", "deal", "with", "the", "value", "itself", "r", "n", "v", "return", "ctx"], "doc_len": 107}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.load_default_config_file", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "load_default_config_file", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def load_default_config_file(self, file_dir: str, file_name: str) -> dict:\n        \"\"\"Load the default config file.\"\"\"\n        if file_name == \"pyproject.toml\":\n            elems = self._get_config_elems_from_toml(os.path.join(file_dir, file_name))\n        else:\n            elems = self._get_config_elems_from_file(os.path.join(file_dir, file_name))\n        return self._incorporate_vals({}, elems)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "load_default_config_file", "self", "file_dir", "str", "file_name", "str", "dict", "load", "the", "default", "config", "file", "if", "file_name", "pyproject", "toml", "elems", "self", "_get_config_elems_from_toml", "os", "path", "join", "file_dir", "file_name", "else", "elems", "self", "_get_config_elems_from_file", "os", "path", "join", "file_dir", "file_name", "return", "self", "_incorporate_vals", "elems"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.load_config_at_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "load_config_at_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def load_config_at_path(self, path: str) -> dict:\n        \"\"\"Load config from a given path.\"\"\"\n        # First check the cache\n        if str(path) in self._config_cache:\n            return self._config_cache[str(path)]\n\n        # The potential filenames we would look for at this path.\n        # NB: later in this list overwrites earlier\n        filename_options = [\n            \"setup.cfg\",\n            \"tox.ini\",\n            \"pep8.ini\",\n            \".sqlfluff\",\n            \"pyproject.toml\",\n        ]\n\n        configs: dict = {}\n\n        if os.path.isdir(path):\n            p = path\n        else:\n            p = os.path.dirname(path)\n\n        d = os.listdir(os.path.expanduser(p))\n        # iterate this way round to make sure things overwrite is the right direction\n        for fname in filename_options:\n            if fname in d:\n                if fname == \"pyproject.toml\":\n                    elems = self._get_config_elems_from_toml(os.path.join(p, fname))\n                else:\n                    elems = self._get_config_elems_from_file(os.path.join(p, fname))\n                configs = self._incorporate_vals(configs, elems)\n\n        # Store in the cache\n        self._config_cache[str(path)] = configs\n        return configs\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "load_config_at_path", "self", "path", "str", "dict", "load", "config", "from", "a", "given", "path", "first", "check", "the", "cache", "if", "str", "path", "in", "self", "_config_cache", "return", "self", "_config_cache", "str", "path", "the", "potential", "filenames", "we", "would", "look", "for", "at", "this", "path", "nb", "later", "in", "this", "list", "overwrites", "earlier", "filename_options", "setup", "cfg", "tox", "ini", "pep8", "ini", "sqlfluff", "pyproject", "toml", "configs", "dict", "if", "os", "path", "isdir", "path", "p", "path", "else", "p", "os", "path", "dirname", "path", "d", "os", "listdir", "os", "path", "expanduser", "p", "iterate", "this", "way", "round", "to", "make", "sure", "things", "overwrite", "is", "the", "right", "direction", "for", "fname", "in", "filename_options", "if", "fname", "in", "d", "if", "fname", "pyproject", "toml", "elems", "self", "_get_config_elems_from_toml", "os", "path", "join", "p", "fname", "else", "elems", "self", "_get_config_elems_from_file", "os", "path", "join", "p", "fname", "configs", "self", "_incorporate_vals", "configs", "elems", "store", "in", "the", "cache", "self", "_config_cache", "str", "path", "configs", "return", "configs"], "doc_len": 140}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader._get_user_config_dir_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "_get_user_config_dir_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def _get_user_config_dir_path() -> str:\n        appname = \"sqlfluff\"\n        appauthor = \"sqlfluff\"\n\n        # On Mac OSX follow Linux XDG base dirs\n        # https://github.com/sqlfluff/sqlfluff/issues/889\n        user_config_dir_path = os.path.expanduser(\"~/.config/sqlfluff\")\n        if appdirs.system == \"darwin\":\n            appdirs.system = \"linux2\"\n            user_config_dir_path = appdirs.user_config_dir(appname, appauthor)\n            appdirs.system = \"darwin\"\n\n        if not os.path.exists(user_config_dir_path):\n            user_config_dir_path = appdirs.user_config_dir(appname, appauthor)\n\n        return user_config_dir_path\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "_get_user_config_dir_path", "str", "appname", "sqlfluff", "appauthor", "sqlfluff", "on", "mac", "osx", "follow", "linux", "xdg", "base", "dirs", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "889", "user_config_dir_path", "os", "path", "expanduser", "config", "sqlfluff", "if", "appdirs", "system", "darwin", "appdirs", "system", "linux2", "user_config_dir_path", "appdirs", "user_config_dir", "appname", "appauthor", "appdirs", "system", "darwin", "if", "not", "os", "path", "exists", "user_config_dir_path", "user_config_dir_path", "appdirs", "user_config_dir", "appname", "appauthor", "return", "user_config_dir_path"], "doc_len": 62}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.load_user_appdir_config", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "load_user_appdir_config", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def load_user_appdir_config(self) -> dict:\n        \"\"\"Load the config from the user's OS specific appdir config directory.\"\"\"\n        user_config_dir_path = self._get_user_config_dir_path()\n        if os.path.exists(user_config_dir_path):\n            return self.load_config_at_path(user_config_dir_path)\n        else:\n            return {}\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "load_user_appdir_config", "self", "dict", "load", "the", "config", "from", "the", "user", "s", "os", "specific", "appdir", "config", "directory", "user_config_dir_path", "self", "_get_user_config_dir_path", "if", "os", "path", "exists", "user_config_dir_path", "return", "self", "load_config_at_path", "user_config_dir_path", "else", "return"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.load_user_config", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "load_user_config", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def load_user_config(self) -> dict:\n        \"\"\"Load the config from the user's home directory.\"\"\"\n        user_home_path = os.path.expanduser(\"~\")\n        return self.load_config_at_path(user_home_path)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "load_user_config", "self", "dict", "load", "the", "config", "from", "the", "user", "s", "home", "directory", "user_home_path", "os", "path", "expanduser", "return", "self", "load_config_at_path", "user_home_path"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.load_config_up_to_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "load_config_up_to_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def load_config_up_to_path(self, path: str) -> dict:\n        \"\"\"Loads a selection of config files from both the path and its parent paths.\"\"\"\n        user_appdir_config = self.load_user_appdir_config()\n        user_config = self.load_user_config()\n        config_paths = self.iter_config_locations_up_to_path(path)\n        config_stack = [self.load_config_at_path(p) for p in config_paths]\n        return nested_combine(user_appdir_config, user_config, *config_stack)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "load_config_up_to_path", "self", "path", "str", "dict", "loads", "a", "selection", "of", "config", "files", "from", "both", "the", "path", "and", "its", "parent", "paths", "user_appdir_config", "self", "load_user_appdir_config", "user_config", "self", "load_user_config", "config_paths", "self", "iter_config_locations_up_to_path", "path", "config_stack", "self", "load_config_at_path", "p", "for", "p", "in", "config_paths", "return", "nested_combine", "user_appdir_config", "user_config", "config_stack"], "doc_len": 49}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.find_ignore_config_files", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "find_ignore_config_files", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def find_ignore_config_files(\n        cls, path, working_path=os.getcwd(), ignore_file_name=\".sqlfluffignore\"\n    ):\n        \"\"\"Finds sqlfluff ignore files from both the path and its parent paths.\"\"\"\n        return set(\n            filter(\n                os.path.isfile,\n                map(\n                    lambda x: os.path.join(x, ignore_file_name),\n                    cls.iter_config_locations_up_to_path(\n                        path=path, working_path=working_path\n                    ),\n                ),\n            )\n        )\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "find_ignore_config_files", "cls", "path", "working_path", "os", "getcwd", "ignore_file_name", "sqlfluffignore", "finds", "sqlfluff", "ignore", "files", "from", "both", "the", "path", "and", "its", "parent", "paths", "return", "set", "filter", "os", "path", "isfile", "map", "lambda", "x", "os", "path", "join", "x", "ignore_file_name", "cls", "iter_config_locations_up_to_path", "path", "path", "working_path", "working_path"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/config.py::ConfigLoader.iter_config_locations_up_to_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "ConfigLoader", "func_name": "iter_config_locations_up_to_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: ConfigLoader\n    def iter_config_locations_up_to_path(path, working_path=Path.cwd()):\n        \"\"\"Finds config locations from both the path and its parent paths.\n\n        The lowest priority is the user appdir, then home dir, then increasingly\n        the configs closest to the file being directly linted.\n        \"\"\"\n        given_path = Path(path).resolve()\n        working_path = Path(working_path).resolve()\n\n        # If we've been passed a file and not a directory,\n        # then go straight to the directory.\n        if not given_path.is_dir():\n            given_path = given_path.parent\n\n        common_path = Path(os.path.commonpath([working_path, given_path]))\n\n        # we have a sub path! We can load nested paths\n        path_to_visit = common_path\n        while path_to_visit != given_path:\n            yield str(path_to_visit.resolve())\n            next_path_to_visit = (\n                path_to_visit / given_path.relative_to(path_to_visit).parts[0]\n            )\n            if next_path_to_visit == path_to_visit:  # pragma: no cover\n                # we're not making progress...\n                # [prevent infinite loop]\n                break\n            path_to_visit = next_path_to_visit\n\n        yield str(given_path.resolve())\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "configloader", "def", "iter_config_locations_up_to_path", "path", "working_path", "path", "cwd", "finds", "config", "locations", "from", "both", "the", "path", "and", "its", "parent", "paths", "the", "lowest", "priority", "is", "the", "user", "appdir", "then", "home", "dir", "then", "increasingly", "the", "configs", "closest", "to", "the", "file", "being", "directly", "linted", "given_path", "path", "path", "resolve", "working_path", "path", "working_path", "resolve", "if", "we", "ve", "been", "passed", "a", "file", "and", "not", "a", "directory", "then", "go", "straight", "to", "the", "directory", "if", "not", "given_path", "is_dir", "given_path", "given_path", "parent", "common_path", "path", "os", "path", "commonpath", "working_path", "given_path", "we", "have", "a", "sub", "path", "we", "can", "load", "nested", "paths", "path_to_visit", "common_path", "while", "path_to_visit", "given_path", "yield", "str", "path_to_visit", "resolve", "next_path_to_visit", "path_to_visit", "given_path", "relative_to", "path_to_visit", "parts", "0", "if", "next_path_to_visit", "path_to_visit", "pragma", "no", "cover", "we", "re", "not", "making", "progress", "prevent", "infinite", "loop", "break", "path_to_visit", "next_path_to_visit", "yield", "str", "given_path", "resolve"], "doc_len": 130}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.__init__", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def __init__(\n        self,\n        configs: Optional[dict] = None,\n        overrides: Optional[dict] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n    ):\n        self._overrides = overrides  # We only store this for child configs\n\n        # Fetch a fresh plugin manager if we weren't provided with one\n        self._plugin_manager = plugin_manager or get_plugin_manager()\n\n        defaults = nested_combine(*self._plugin_manager.hook.load_default_config())\n        self._configs = nested_combine(\n            defaults, configs or {\"core\": {}}, {\"core\": overrides or {}}\n        )\n        # Some configs require special treatment\n        self._configs[\"core\"][\"color\"] = (\n            False if self._configs[\"core\"].get(\"nocolor\", False) else None\n        )\n        # Deal with potential ignore parameters\n        if self._configs[\"core\"].get(\"ignore\", None):\n            self._configs[\"core\"][\"ignore\"] = self._split_comma_separated_string(\n                self._configs[\"core\"][\"ignore\"]\n            )\n        else:\n            self._configs[\"core\"][\"ignore\"] = []\n        # Whitelists and blacklists\n        if self._configs[\"core\"].get(\"rules\", None):\n            self._configs[\"core\"][\n                \"rule_whitelist\"\n            ] = self._split_comma_separated_string(self._configs[\"core\"][\"rules\"])\n        else:\n            self._configs[\"core\"][\"rule_whitelist\"] = None\n        if self._configs[\"core\"].get(\"exclude_rules\", None):\n            self._configs[\"core\"][\n                \"rule_blacklist\"\n            ] = self._split_comma_separated_string(\n                self._configs[\"core\"][\"exclude_rules\"]\n            )\n        else:\n            self._configs[\"core\"][\"rule_blacklist\"] = None\n        # Configure Recursion\n        if self._configs[\"core\"].get(\"recurse\", 0) == 0:\n            self._configs[\"core\"][\"recurse\"] = True\n\n        # Dialect and Template selection.\n        # NB: We import here to avoid a circular references.\n        from sqlfluff.core.dialects import dialect_selector\n\n        self._configs[\"core\"][\"dialect_obj\"] = dialect_selector(\n            self._configs[\"core\"][\"dialect\"]\n        )\n        self._configs[\"core\"][\"templater_obj\"] = self.get_templater(\n            self._configs[\"core\"][\"templater\"]\n        )\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "__init__", "self", "configs", "optional", "dict", "none", "overrides", "optional", "dict", "none", "plugin_manager", "optional", "pluggy", "pluginmanager", "none", "self", "_overrides", "overrides", "we", "only", "store", "this", "for", "child", "configs", "fetch", "a", "fresh", "plugin", "manager", "if", "we", "weren", "t", "provided", "with", "one", "self", "_plugin_manager", "plugin_manager", "or", "get_plugin_manager", "defaults", "nested_combine", "self", "_plugin_manager", "hook", "load_default_config", "self", "_configs", "nested_combine", "defaults", "configs", "or", "core", "core", "overrides", "or", "some", "configs", "require", "special", "treatment", "self", "_configs", "core", "color", "false", "if", "self", "_configs", "core", "get", "nocolor", "false", "else", "none", "deal", "with", "potential", "ignore", "parameters", "if", "self", "_configs", "core", "get", "ignore", "none", "self", "_configs", "core", "ignore", "self", "_split_comma_separated_string", "self", "_configs", "core", "ignore", "else", "self", "_configs", "core", "ignore", "whitelists", "and", "blacklists", "if", "self", "_configs", "core", "get", "rules", "none", "self", "_configs", "core", "rule_whitelist", "self", "_split_comma_separated_string", "self", "_configs", "core", "rules", "else", "self", "_configs", "core", "rule_whitelist", "none", "if", "self", "_configs", "core", "get", "exclude_rules", "none", "self", "_configs", "core", "rule_blacklist", "self", "_split_comma_separated_string", "self", "_configs", "core", "exclude_rules", "else", "self", "_configs", "core", "rule_blacklist", "none", "configure", "recursion", "if", "self", "_configs", "core", "get", "recurse", "0", "0", "self", "_configs", "core", "recurse", "true", "dialect", "and", "template", "selection", "nb", "we", "import", "here", "to", "avoid", "a", "circular", "references", "from", "sqlfluff", "core", "dialects", "import", "dialect_selector", "self", "_configs", "core", "dialect_obj", "dialect_selector", "self", "_configs", "core", "dialect", "self", "_configs", "core", "templater_obj", "self", "get_templater", "self", "_configs", "core", "templater"], "doc_len": 213}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.__getstate__", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "__getstate__", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def __getstate__(self):\n        # Copy the object's state from self.__dict__ which contains\n        # all our instance attributes. Always use the dict.copy()\n        # method to avoid modifying the original state.\n        state = self.__dict__.copy()\n        # Remove the unpicklable entries.\n        del state[\"_plugin_manager\"]\n        return state\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "__getstate__", "self", "copy", "the", "object", "s", "state", "from", "self", "__dict__", "which", "contains", "all", "our", "instance", "attributes", "always", "use", "the", "dict", "copy", "method", "to", "avoid", "modifying", "the", "original", "state", "state", "self", "__dict__", "copy", "remove", "the", "unpicklable", "entries", "del", "state", "_plugin_manager", "return", "state"], "doc_len": 48}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.__setstate__", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "__setstate__", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def __setstate__(self, state):\n        # Restore instance attributes\n        self.__dict__.update(state)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "__setstate__", "self", "state", "restore", "instance", "attributes", "self", "__dict__", "update", "state"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.from_root", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "from_root", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def from_root(cls, overrides: Optional[dict] = None) -> \"FluffConfig\":\n        \"\"\"Loads a config object just based on the root directory.\"\"\"\n        loader = ConfigLoader.get_global()\n        c = loader.load_config_up_to_path(path=\".\")\n        return cls(configs=c, overrides=overrides)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "from_root", "cls", "overrides", "optional", "dict", "none", "fluffconfig", "loads", "a", "config", "object", "just", "based", "on", "the", "root", "directory", "loader", "configloader", "get_global", "c", "loader", "load_config_up_to_path", "path", "return", "cls", "configs", "c", "overrides", "overrides"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.from_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "from_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def from_path(\n        cls,\n        path: str,\n        overrides: Optional[dict] = None,\n        plugin_manager: Optional[pluggy.PluginManager] = None,\n    ) -> \"FluffConfig\":\n        \"\"\"Loads a config object given a particular path.\"\"\"\n        loader = ConfigLoader.get_global()\n        c = loader.load_config_up_to_path(path=path)\n        return cls(configs=c, overrides=overrides, plugin_manager=plugin_manager)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "from_path", "cls", "path", "str", "overrides", "optional", "dict", "none", "plugin_manager", "optional", "pluggy", "pluginmanager", "none", "fluffconfig", "loads", "a", "config", "object", "given", "a", "particular", "path", "loader", "configloader", "get_global", "c", "loader", "load_config_up_to_path", "path", "path", "return", "cls", "configs", "c", "overrides", "overrides", "plugin_manager", "plugin_manager"], "doc_len": 45}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.from_kwargs", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "from_kwargs", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def from_kwargs(\n        cls,\n        config: Optional[\"FluffConfig\"] = None,\n        dialect: Optional[str] = None,\n        rules: Optional[Union[str, List[str]]] = None,\n    ) -> \"FluffConfig\":\n        \"\"\"Instantiate a config from either an existing config or kwargs.\n\n        This is a convenience method for the ways that the public classes\n        like Linter(), Parser() and Lexer() can be instantiated with a\n        FluffConfig or with the convenience kwargs: dialect & rules.\n        \"\"\"\n        if (dialect or rules) and config:  # pragma: no cover\n            raise ValueError(\n                \"Cannot specify `config` with `dialect` or `rules`. Any config object \"\n                \"specifies its own dialect and rules.\"\n            )\n        elif config:\n            return config\n\n        overrides = {}\n        if dialect:\n            overrides[\"dialect\"] = dialect\n        if rules:\n            # If it's a string, make it a list\n            if isinstance(rules, str):\n                rules = [rules]\n            # Make a comma separated string to pass in as override\n            overrides[\"rules\"] = \",\".join(rules)\n        return cls(overrides=overrides)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "from_kwargs", "cls", "config", "optional", "fluffconfig", "none", "dialect", "optional", "str", "none", "rules", "optional", "union", "str", "list", "str", "none", "fluffconfig", "instantiate", "a", "config", "from", "either", "an", "existing", "config", "or", "kwargs", "this", "is", "a", "convenience", "method", "for", "the", "ways", "that", "the", "public", "classes", "like", "linter", "parser", "and", "lexer", "can", "be", "instantiated", "with", "a", "fluffconfig", "or", "with", "the", "convenience", "kwargs", "dialect", "rules", "if", "dialect", "or", "rules", "and", "config", "pragma", "no", "cover", "raise", "valueerror", "cannot", "specify", "config", "with", "dialect", "or", "rules", "any", "config", "object", "specifies", "its", "own", "dialect", "and", "rules", "elif", "config", "return", "config", "overrides", "if", "dialect", "overrides", "dialect", "dialect", "if", "rules", "if", "it", "s", "a", "string", "make", "it", "a", "list", "if", "isinstance", "rules", "str", "rules", "rules", "make", "a", "comma", "separated", "string", "to", "pass", "in", "as", "override", "overrides", "rules", "join", "rules", "return", "cls", "overrides", "overrides"], "doc_len": 137}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.get_templater", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "get_templater", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def get_templater(self, templater_name=\"jinja\", **kwargs):\n        \"\"\"Fetch a templater by name.\"\"\"\n        templater_lookup = {\n            templater.name: templater\n            for templater in chain.from_iterable(\n                self._plugin_manager.hook.get_templaters()\n            )\n        }\n        try:\n            cls = templater_lookup[templater_name]\n            # Instantiate here, optionally with kwargs\n            return cls(**kwargs)\n        except KeyError:\n            if templater_name == \"dbt\":  # pragma: no cover\n                config_logger.warning(\n                    \"Starting in sqlfluff version 0.7.0 the dbt templater is distributed as a \"\n                    \"seperate python package. Please pip install sqlfluff-templater-dbt to use it.\"\n                )\n            raise SQLFluffUserError(\n                \"Requested templater {!r} which is not currently available. Try one of {}\".format(\n                    templater_name, \", \".join(templater_lookup.keys())\n                )\n            )\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "get_templater", "self", "templater_name", "jinja", "kwargs", "fetch", "a", "templater", "by", "name", "templater_lookup", "templater", "name", "templater", "for", "templater", "in", "chain", "from_iterable", "self", "_plugin_manager", "hook", "get_templaters", "try", "cls", "templater_lookup", "templater_name", "instantiate", "here", "optionally", "with", "kwargs", "return", "cls", "kwargs", "except", "keyerror", "if", "templater_name", "dbt", "pragma", "no", "cover", "config_logger", "warning", "starting", "in", "sqlfluff", "version", "0", "7", "0", "the", "dbt", "templater", "is", "distributed", "as", "a", "seperate", "python", "package", "please", "pip", "install", "sqlfluff", "templater", "dbt", "to", "use", "it", "raise", "sqlfluffusererror", "requested", "templater", "r", "which", "is", "not", "currently", "available", "try", "one", "of", "format", "templater_name", "join", "templater_lookup", "keys"], "doc_len": 96}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.make_child_from_path", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "make_child_from_path", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def make_child_from_path(self, path: str) -> \"FluffConfig\":\n        \"\"\"Make a new child config at a path but pass on overrides.\"\"\"\n        return self.from_path(\n            path, overrides=self._overrides, plugin_manager=self._plugin_manager\n        )\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "make_child_from_path", "self", "path", "str", "fluffconfig", "make", "a", "new", "child", "config", "at", "a", "path", "but", "pass", "on", "overrides", "return", "self", "from_path", "path", "overrides", "self", "_overrides", "plugin_manager", "self", "_plugin_manager"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.diff_to", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "diff_to", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def diff_to(self, other: \"FluffConfig\") -> dict:\n        \"\"\"Compare this config to another.\n\n        Args:\n            other (:obj:`FluffConfig`): Another config object to compare\n                against. We will return keys from *this* object that are\n                not in `other` or are different to those in `other`.\n\n        Returns:\n            A filtered dict of items in this config that are not in the other\n            or are different to the other.\n\n        \"\"\"\n        # We igonre some objects which are not meaningful in the comparison\n        # e.g. dialect_obj, which is generated on the fly.\n        return dict_diff(self._configs, other._configs, ignore=[\"dialect_obj\"])\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "diff_to", "self", "other", "fluffconfig", "dict", "compare", "this", "config", "to", "another", "args", "other", "obj", "fluffconfig", "another", "config", "object", "to", "compare", "against", "we", "will", "return", "keys", "from", "this", "object", "that", "are", "not", "in", "other", "or", "are", "different", "to", "those", "in", "other", "returns", "a", "filtered", "dict", "of", "items", "in", "this", "config", "that", "are", "not", "in", "the", "other", "or", "are", "different", "to", "the", "other", "we", "igonre", "some", "objects", "which", "are", "not", "meaningful", "in", "the", "comparison", "e", "g", "dialect_obj", "which", "is", "generated", "on", "the", "fly", "return", "dict_diff", "self", "_configs", "other", "_configs", "ignore", "dialect_obj"], "doc_len": 95}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.get", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "get", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def get(\n        self, val: str, section: Union[str, Iterable[str]] = \"core\", default: Any = None\n    ):\n        \"\"\"Get a particular value from the config.\"\"\"\n        return self._configs[section].get(val, default)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "get", "self", "val", "str", "section", "union", "str", "iterable", "str", "core", "default", "any", "none", "get", "a", "particular", "value", "from", "the", "config", "return", "self", "_configs", "section", "get", "val", "default"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.get_section", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "get_section", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def get_section(self, section: Union[str, Iterable[str]]) -> Union[dict, None]:\n        \"\"\"Return a whole section of config as a dict.\n\n        If the element found at the address is a value and not\n        a section, it is still returned and so this can be used\n        as a more advanced from of the basic `get` method.\n\n        Args:\n            section: An iterable or string. If it's a string\n                we load that root section. If it's an iterable\n                of strings, then we treat it as a path within\n                the dictionary structure.\n\n        \"\"\"\n        if isinstance(section, str):\n            return self._configs.get(section, None)\n        else:\n            # Try iterating\n            buff = self._configs\n            for sec in section:\n                buff = buff.get(sec, None)\n                if buff is None:\n                    return None\n            return buff\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "get_section", "self", "section", "union", "str", "iterable", "str", "union", "dict", "none", "return", "a", "whole", "section", "of", "config", "as", "a", "dict", "if", "the", "element", "found", "at", "the", "address", "is", "a", "value", "and", "not", "a", "section", "it", "is", "still", "returned", "and", "so", "this", "can", "be", "used", "as", "a", "more", "advanced", "from", "of", "the", "basic", "get", "method", "args", "section", "an", "iterable", "or", "string", "if", "it", "s", "a", "string", "we", "load", "that", "root", "section", "if", "it", "s", "an", "iterable", "of", "strings", "then", "we", "treat", "it", "as", "a", "path", "within", "the", "dictionary", "structure", "if", "isinstance", "section", "str", "return", "self", "_configs", "get", "section", "none", "else", "try", "iterating", "buff", "self", "_configs", "for", "sec", "in", "section", "buff", "buff", "get", "sec", "none", "if", "buff", "is", "none", "return", "none", "return", "buff"], "doc_len": 127}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.set_value", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "set_value", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def set_value(self, config_path: Iterable[str], val: Any):\n        \"\"\"Set a value at a given path.\"\"\"\n        # Make the path a list so we can index on it\n        config_path = list(config_path)\n        # Coerce the value into something more useful.\n        config_val = coerce_value(val)\n        # Sort out core if not there\n        if len(config_path) == 1:  # pragma: no cover TODO?\n            config_path = [\"core\"] + config_path\n        # Current section:\n        dict_buff = [self._configs]\n        for elem in config_path[:-1]:\n            dict_buff.append(dict_buff[-1][elem])\n        # Set the value\n        dict_buff[-1][config_path[-1]] = config_val\n        # Rebuild the config\n        for elem in reversed(config_path[:-1]):\n            dict_elem = dict_buff.pop()\n            dict_buff[-1][elem] = dict_elem\n        self._configs = dict_buff[0]\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "set_value", "self", "config_path", "iterable", "str", "val", "any", "set", "a", "value", "at", "a", "given", "path", "make", "the", "path", "a", "list", "so", "we", "can", "index", "on", "it", "config_path", "list", "config_path", "coerce", "the", "value", "into", "something", "more", "useful", "config_val", "coerce_value", "val", "sort", "out", "core", "if", "not", "there", "if", "len", "config_path", "1", "pragma", "no", "cover", "todo", "config_path", "core", "config_path", "current", "section", "dict_buff", "self", "_configs", "for", "elem", "in", "config_path", "1", "dict_buff", "append", "dict_buff", "1", "elem", "set", "the", "value", "dict_buff", "1", "config_path", "1", "config_val", "rebuild", "the", "config", "for", "elem", "in", "reversed", "config_path", "1", "dict_elem", "dict_buff", "pop", "dict_buff", "1", "elem", "dict_elem", "self", "_configs", "dict_buff", "0"], "doc_len": 105}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.iter_vals", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "iter_vals", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def iter_vals(self, cfg: Optional[dict] = None) -> Iterable[tuple]:\n        \"\"\"Return an iterable of tuples representing keys.\n\n        We show values before dicts, the tuple contains an indent\n        value to know what level of the dict we're in. Dict labels\n        will be returned as a blank value before their content.\n        \"\"\"\n        cfg = cfg or self._configs\n\n        # Get keys and sort\n        keys = sorted(cfg.keys())\n        # First iterate values (alphabetically):\n        for k in keys:\n            if (\n                not isinstance(cfg[k], dict)\n                and cfg[k] is not None\n                and k not in self.private_vals\n            ):\n                yield (0, k, cfg[k])\n\n        # Then iterate dicts (alphabetically (but `core` comes first if it exists))\n        for k in keys:\n            if isinstance(cfg[k], dict):\n                # First yield the dict label\n                yield (0, k, \"\")\n                # Then yield its content\n                for idnt, key, val in self.iter_vals(cfg=cfg[k]):\n                    yield (idnt + 1, key, val)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "iter_vals", "self", "cfg", "optional", "dict", "none", "iterable", "tuple", "return", "an", "iterable", "of", "tuples", "representing", "keys", "we", "show", "values", "before", "dicts", "the", "tuple", "contains", "an", "indent", "value", "to", "know", "what", "level", "of", "the", "dict", "we", "re", "in", "dict", "labels", "will", "be", "returned", "as", "a", "blank", "value", "before", "their", "content", "cfg", "cfg", "or", "self", "_configs", "get", "keys", "and", "sort", "keys", "sorted", "cfg", "keys", "first", "iterate", "values", "alphabetically", "for", "k", "in", "keys", "if", "not", "isinstance", "cfg", "k", "dict", "and", "cfg", "k", "is", "not", "none", "and", "k", "not", "in", "self", "private_vals", "yield", "0", "k", "cfg", "k", "then", "iterate", "dicts", "alphabetically", "but", "core", "comes", "first", "if", "it", "exists", "for", "k", "in", "keys", "if", "isinstance", "cfg", "k", "dict", "first", "yield", "the", "dict", "label", "yield", "0", "k", "then", "yield", "its", "content", "for", "idnt", "key", "val", "in", "self", "iter_vals", "cfg", "cfg", "k", "yield", "idnt", "1", "key", "val"], "doc_len": 146}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.process_inline_config", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "process_inline_config", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def process_inline_config(self, config_line: str):\n        \"\"\"Process an inline config command and update self.\"\"\"\n        # Strip preceding comment marks\n        if config_line.startswith(\"--\"):\n            config_line = config_line[2:].strip()\n        # Strip preceding sqlfluff line.\n        if not config_line.startswith(\"sqlfluff:\"):  # pragma: no cover\n            config_logger.warning(\n                \"Unable to process inline config statement: %r\", config_line\n            )\n            return\n        config_line = config_line[9:].strip()\n        # Divide on colons\n        config_path = [elem.strip() for elem in config_line.split(\":\")]\n        # Set the value\n        self.set_value(config_path[:-1], config_path[-1])\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "process_inline_config", "self", "config_line", "str", "process", "an", "inline", "config", "command", "and", "update", "self", "strip", "preceding", "comment", "marks", "if", "config_line", "startswith", "config_line", "config_line", "2", "strip", "strip", "preceding", "sqlfluff", "line", "if", "not", "config_line", "startswith", "sqlfluff", "pragma", "no", "cover", "config_logger", "warning", "unable", "to", "process", "inline", "config", "statement", "r", "config_line", "return", "config_line", "config_line", "9", "strip", "divide", "on", "colons", "config_path", "elem", "strip", "for", "elem", "in", "config_line", "split", "set", "the", "value", "self", "set_value", "config_path", "1", "config_path", "1"], "doc_len": 77}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig.process_raw_file_for_config", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "process_raw_file_for_config", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def process_raw_file_for_config(self, raw_str: str):\n        \"\"\"Process a full raw file for inline config and update self.\"\"\"\n        # Scan the raw file for config commands.\n        for raw_line in raw_str.splitlines():\n            if raw_line.startswith(\"-- sqlfluff\"):\n                # Found a in-file config command\n                self.process_inline_config(raw_line)\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "process_raw_file_for_config", "self", "raw_str", "str", "process", "a", "full", "raw", "file", "for", "inline", "config", "and", "update", "self", "scan", "the", "raw", "file", "for", "config", "commands", "for", "raw_line", "in", "raw_str", "splitlines", "if", "raw_line", "startswith", "sqlfluff", "found", "a", "in", "file", "config", "command", "self", "process_inline_config", "raw_line"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/config.py::FluffConfig._split_comma_separated_string", "file_path": "src/sqlfluff/core/config.py", "class_name": "FluffConfig", "func_name": "_split_comma_separated_string", "text": "文件路径: src/sqlfluff/core/config.py, 类名: FluffConfig\n    def _split_comma_separated_string(raw_str: str) -> List[str]:\n        return [s.strip() for s in raw_str.split(\",\") if s.strip()]\n", "tokens": ["src", "sqlfluff", "core", "config", "py", "fluffconfig", "def", "_split_comma_separated_string", "raw_str", "str", "list", "str", "return", "s", "strip", "for", "s", "in", "raw_str", "split", "if", "s", "strip"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.__init__", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def __init__(\n        self,\n        *args,\n        pos=None,\n        line_no=0,\n        line_pos=0,\n        ignore=False,\n        fatal=False,\n        **kwargs\n    ):\n        self.fatal = fatal\n        self.ignore = ignore\n        if pos:\n            self.line_no, self.line_pos = pos.source_position()\n        else:\n            self.line_no = line_no\n            self.line_pos = line_pos\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "__init__", "self", "args", "pos", "none", "line_no", "0", "line_pos", "0", "ignore", "false", "fatal", "false", "kwargs", "self", "fatal", "fatal", "self", "ignore", "ignore", "if", "pos", "self", "line_no", "self", "line_pos", "pos", "source_position", "else", "self", "line_no", "line_no", "self", "line_pos", "line_pos", "super", "__init__", "args", "kwargs"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.fixable", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "fixable", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        return False\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "fixable", "self", "should", "this", "error", "be", "considered", "fixable", "return", "false"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.rule_code", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "rule_code", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def rule_code(self):\n        \"\"\"Fetch the code of the rule which cause this error.\n\n        NB: This only returns a real code for some subclasses of\n        error, (the ones with a `rule` attribute), but otherwise\n        returns a placeholder value which can be used instead.\n        \"\"\"\n        if hasattr(self, \"rule\"):\n            return self.rule.code\n        else:\n            return self._code or \"????\"\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "rule_code", "self", "fetch", "the", "code", "of", "the", "rule", "which", "cause", "this", "error", "nb", "this", "only", "returns", "a", "real", "code", "for", "some", "subclasses", "of", "error", "the", "ones", "with", "a", "rule", "attribute", "but", "otherwise", "returns", "a", "placeholder", "value", "which", "can", "be", "used", "instead", "if", "hasattr", "self", "rule", "return", "self", "rule", "code", "else", "return", "self", "_code", "or"], "doc_len": 61}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.desc", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "desc", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def desc(self):\n        \"\"\"Fetch a description of this violation.\n\n        NB: For violations which don't directly implement a rule\n        this attempts to return the error message linked to whatever\n        caused the violation. Optionally some errors may have their\n        description set directly.\n        \"\"\"\n        if hasattr(self, \"description\") and self.description:\n            # This can only override if it's present AND\n            # if it's non-null.\n            return self.description\n        elif hasattr(self, \"rule\"):\n            return self.rule.description\n        else:\n            # Return the first element - probably a string message\n            if len(self.args) > 1:  # pragma: no cover TODO?\n                return self.args\n            elif len(self.args) == 1:\n                return self.args[0]\n            else:  # pragma: no cover TODO?\n                return self.__class__.__name__\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "desc", "self", "fetch", "a", "description", "of", "this", "violation", "nb", "for", "violations", "which", "don", "t", "directly", "implement", "a", "rule", "this", "attempts", "to", "return", "the", "error", "message", "linked", "to", "whatever", "caused", "the", "violation", "optionally", "some", "errors", "may", "have", "their", "description", "set", "directly", "if", "hasattr", "self", "description", "and", "self", "description", "this", "can", "only", "override", "if", "it", "s", "present", "and", "if", "it", "s", "non", "null", "return", "self", "description", "elif", "hasattr", "self", "rule", "return", "self", "rule", "description", "else", "return", "the", "first", "element", "probably", "a", "string", "message", "if", "len", "self", "args", "1", "pragma", "no", "cover", "todo", "return", "self", "args", "elif", "len", "self", "args", "1", "return", "self", "args", "0", "else", "pragma", "no", "cover", "todo", "return", "self", "__class__", "__name__"], "doc_len": 118}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.get_info_dict", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "get_info_dict", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def get_info_dict(self):\n        \"\"\"Return a dict of properties.\n\n        This is useful in the API for outputting violations.\n        \"\"\"\n        return {\n            \"line_no\": self.line_no,\n            \"line_pos\": self.line_pos,\n            \"code\": self.rule_code(),\n            \"description\": self.desc(),\n        }\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "get_info_dict", "self", "return", "a", "dict", "of", "properties", "this", "is", "useful", "in", "the", "api", "for", "outputting", "violations", "return", "line_no", "self", "line_no", "line_pos", "self", "line_pos", "code", "self", "rule_code", "description", "self", "desc"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/errors.py::SQLBaseError.ignore_if_in", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLBaseError", "func_name": "ignore_if_in", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLBaseError\n    def ignore_if_in(self, ignore_iterable):\n        \"\"\"Ignore this violation if it matches the iterable.\"\"\"\n        # Type conversion\n        if isinstance(ignore_iterable, str):  # pragma: no cover TODO?\n            ignore_iterable = []\n        # Ignoring\n        if self._identifier in ignore_iterable:\n            self.ignore = True\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlbaseerror", "def", "ignore_if_in", "self", "ignore_iterable", "ignore", "this", "violation", "if", "it", "matches", "the", "iterable", "type", "conversion", "if", "isinstance", "ignore_iterable", "str", "pragma", "no", "cover", "todo", "ignore_iterable", "ignoring", "if", "self", "_identifier", "in", "ignore_iterable", "self", "ignore", "true"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/errors.py::SQLParseError.__init__", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLParseError", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLParseError\n    def __init__(self, *args, segment=None, **kwargs):\n        # Store the segment on creation - we might need it later\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqlparseerror", "def", "__init__", "self", "args", "segment", "none", "kwargs", "store", "the", "segment", "on", "creation", "we", "might", "need", "it", "later", "self", "segment", "segment", "if", "self", "segment", "kwargs", "pos", "self", "segment", "pos_marker", "super", "__init__", "args", "kwargs"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/errors.py::SQLLintError.__init__", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLLintError", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLLintError\n    def __init__(\n        self, *args, segment=None, rule=None, fixes=None, description=None, **kwargs\n    ):\n        # Something about position, message and fix?\n        self.segment = segment\n        if self.segment:\n            kwargs[\"pos\"] = self.segment.pos_marker\n        self.rule = rule\n        self.fixes = fixes or []\n        self.description = description\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqllinterror", "def", "__init__", "self", "args", "segment", "none", "rule", "none", "fixes", "none", "description", "none", "kwargs", "something", "about", "position", "message", "and", "fix", "self", "segment", "segment", "if", "self", "segment", "kwargs", "pos", "self", "segment", "pos_marker", "self", "rule", "rule", "self", "fixes", "fixes", "or", "self", "description", "description", "super", "__init__", "args", "kwargs"], "doc_len": 50}
{"doc_id": "src/sqlfluff/core/errors.py::SQLLintError.fixable", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLLintError", "func_name": "fixable", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLLintError\n    def fixable(self):\n        \"\"\"Should this error be considered fixable?\"\"\"\n        if self.fixes:\n            return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqllinterror", "def", "fixable", "self", "should", "this", "error", "be", "considered", "fixable", "if", "self", "fixes", "return", "true", "return", "false"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/errors.py::SQLLintError.check_tuple", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLLintError", "func_name": "check_tuple", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLLintError\n    def check_tuple(self) -> CheckTuple:\n        \"\"\"Get a tuple representing this error. Mostly for testing.\"\"\"\n        return (\n            self.rule.code,\n            self.line_no,\n            self.line_pos,\n        )\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqllinterror", "def", "check_tuple", "self", "checktuple", "get", "a", "tuple", "representing", "this", "error", "mostly", "for", "testing", "return", "self", "rule", "code", "self", "line_no", "self", "line_pos"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/errors.py::SQLLintError.__repr__", "file_path": "src/sqlfluff/core/errors.py", "class_name": "SQLLintError", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/errors.py, 类名: SQLLintError\n    def __repr__(self):\n        return \"<SQLLintError: rule {} pos:{!r}, #fixes: {}, description: {}>\".format(\n            self.rule_code(),\n            (self.line_no, self.line_pos),\n            len(self.fixes),\n            self.description,\n        )\n", "tokens": ["src", "sqlfluff", "core", "errors", "py", "sqllinterror", "def", "__repr__", "self", "return", "sqllinterror", "rule", "pos", "r", "fixes", "description", "format", "self", "rule_code", "self", "line_no", "self", "line_pos", "len", "self", "fixes", "self", "description"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/file_helpers.py::get_encoding", "file_path": "src/sqlfluff/core/file_helpers.py", "class_name": null, "func_name": "get_encoding", "text": "文件路径: src/sqlfluff/core/file_helpers.py\ndef get_encoding(fname: str, config: FluffConfig) -> str:\n    \"\"\"Get the encoding of the file (autodetect).\"\"\"\n    encoding_config = config.get(\"encoding\", default=\"autodetect\")\n\n    if encoding_config == \"autodetect\":\n        with open(fname, \"rb\") as f:\n            data = f.read()\n        return chardet.detect(data)[\"encoding\"]\n\n    return encoding_config\n", "tokens": ["src", "sqlfluff", "core", "file_helpers", "py", "def", "get_encoding", "fname", "str", "config", "fluffconfig", "str", "get", "the", "encoding", "of", "the", "file", "autodetect", "encoding_config", "config", "get", "encoding", "default", "autodetect", "if", "encoding_config", "autodetect", "with", "open", "fname", "rb", "as", "f", "data", "f", "read", "return", "chardet", "detect", "data", "encoding", "return", "encoding_config"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/string_helpers.py::frame_msg", "file_path": "src/sqlfluff/core/string_helpers.py", "class_name": null, "func_name": "frame_msg", "text": "文件路径: src/sqlfluff/core/string_helpers.py\ndef frame_msg(msg: str) -> str:\n    \"\"\"Frame a message with hashes so that it covers five lines.\"\"\"\n    return f\"\\n###\\n#\\n# {msg}\\n#\\n###\"\n", "tokens": ["src", "sqlfluff", "core", "string_helpers", "py", "def", "frame_msg", "msg", "str", "str", "frame", "a", "message", "with", "hashes", "so", "that", "it", "covers", "five", "lines", "return", "f", "n", "n", "n", "msg", "n", "n"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/string_helpers.py::curtail_string", "file_path": "src/sqlfluff/core/string_helpers.py", "class_name": null, "func_name": "curtail_string", "text": "文件路径: src/sqlfluff/core/string_helpers.py\ndef curtail_string(s: str, length=20) -> str:\n    \"\"\"Trim a string nicely to length.\"\"\"\n    if len(s) > length:\n        return s[:length] + \"...\"\n    else:\n        return s\n", "tokens": ["src", "sqlfluff", "core", "string_helpers", "py", "def", "curtail_string", "s", "str", "length", "20", "str", "trim", "a", "string", "nicely", "to", "length", "if", "len", "s", "length", "return", "s", "length", "else", "return", "s"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/string_helpers.py::findall", "file_path": "src/sqlfluff/core/string_helpers.py", "class_name": null, "func_name": "findall", "text": "文件路径: src/sqlfluff/core/string_helpers.py\ndef findall(substr: str, in_str: str) -> Iterator[int]:\n    \"\"\"Yields all the positions sbstr within in_str.\n\n    https://stackoverflow.com/questions/4664850/how-to-find-all-occurrences-of-a-substring\n    \"\"\"\n    # Return nothing if one of the inputs is trivial\n    if not substr or not in_str:\n        return\n    idx = in_str.find(substr)\n    while idx != -1:\n        yield idx\n        idx = in_str.find(substr, idx + 1)\n", "tokens": ["src", "sqlfluff", "core", "string_helpers", "py", "def", "findall", "substr", "str", "in_str", "str", "iterator", "int", "yields", "all", "the", "positions", "sbstr", "within", "in_str", "https", "stackoverflow", "com", "questions", "4664850", "how", "to", "find", "all", "occurrences", "of", "a", "substring", "return", "nothing", "if", "one", "of", "the", "inputs", "is", "trivial", "if", "not", "substr", "or", "not", "in_str", "return", "idx", "in_str", "find", "substr", "while", "idx", "1", "yield", "idx", "idx", "in_str", "find", "substr", "idx", "1"], "doc_len": 64}
{"doc_id": "src/sqlfluff/core/timing.py::TimingSummary.__init__", "file_path": "src/sqlfluff/core/timing.py", "class_name": "TimingSummary", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/timing.py, 类名: TimingSummary\n    def __init__(self, steps: Optional[List[str]] = None):\n        self.steps = steps\n        self._timings: List[Dict[str, float]] = []\n", "tokens": ["src", "sqlfluff", "core", "timing", "py", "timingsummary", "def", "__init__", "self", "steps", "optional", "list", "str", "none", "self", "steps", "steps", "self", "_timings", "list", "dict", "str", "float"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/timing.py::TimingSummary.add", "file_path": "src/sqlfluff/core/timing.py", "class_name": "TimingSummary", "func_name": "add", "text": "文件路径: src/sqlfluff/core/timing.py, 类名: TimingSummary\n    def add(self, timing_dict: Dict[str, float]):\n        \"\"\"Add a timing dictionary to the summary.\"\"\"\n        self._timings.append(timing_dict)\n        if not self.steps:\n            self.steps = list(timing_dict.keys())\n", "tokens": ["src", "sqlfluff", "core", "timing", "py", "timingsummary", "def", "add", "self", "timing_dict", "dict", "str", "float", "add", "a", "timing", "dictionary", "to", "the", "summary", "self", "_timings", "append", "timing_dict", "if", "not", "self", "steps", "self", "steps", "list", "timing_dict", "keys"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/timing.py::TimingSummary.summary", "file_path": "src/sqlfluff/core/timing.py", "class_name": "TimingSummary", "func_name": "summary", "text": "文件路径: src/sqlfluff/core/timing.py, 类名: TimingSummary\n    def summary(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Generate a summary for display.\"\"\"\n        vals: Dict[str, List[float]] = defaultdict(list)\n        if not self.steps:  # pragma: no cover\n            return {}\n\n        for timing_dict in self._timings:\n            for step in self.steps:\n                if step in timing_dict:\n                    vals[step].append(timing_dict[step])\n        summary = {}\n        for step in self.steps:\n            if vals[step]:\n                summary[step] = {\n                    \"cnt\": len(vals[step]),\n                    \"sum\": sum(vals[step]),\n                    \"min\": min(vals[step]),\n                    \"max\": max(vals[step]),\n                    \"avg\": sum(vals[step]) / len(vals[step]),\n                }\n        return summary\n", "tokens": ["src", "sqlfluff", "core", "timing", "py", "timingsummary", "def", "summary", "self", "dict", "str", "dict", "str", "float", "generate", "a", "summary", "for", "display", "vals", "dict", "str", "list", "float", "defaultdict", "list", "if", "not", "self", "steps", "pragma", "no", "cover", "return", "for", "timing_dict", "in", "self", "_timings", "for", "step", "in", "self", "steps", "if", "step", "in", "timing_dict", "vals", "step", "append", "timing_dict", "step", "summary", "for", "step", "in", "self", "steps", "if", "vals", "step", "summary", "step", "cnt", "len", "vals", "step", "sum", "sum", "vals", "step", "min", "min", "vals", "step", "max", "max", "vals", "step", "avg", "sum", "vals", "step", "len", "vals", "step", "return", "summary"], "doc_len": 89}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.__init__", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def __init__(\n        self,\n        name,\n        lexer_matchers=None,\n        library=None,\n        sets=None,\n        inherits_from=None,\n        root_segment_name=None,\n    ):\n        self._library = library or {}\n        self.name = name\n        self.lexer_matchers = lexer_matchers\n        self.expanded = False\n        self._sets = sets or {}\n        self.inherits_from = inherits_from\n        self.root_segment_name = root_segment_name\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "__init__", "self", "name", "lexer_matchers", "none", "library", "none", "sets", "none", "inherits_from", "none", "root_segment_name", "none", "self", "_library", "library", "or", "self", "name", "name", "self", "lexer_matchers", "lexer_matchers", "self", "expanded", "false", "self", "_sets", "sets", "or", "self", "inherits_from", "inherits_from", "self", "root_segment_name", "root_segment_name"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.__repr__", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def __repr__(self):  # pragma: no cover\n        return f\"<Dialect: {self.name}>\"\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "__repr__", "self", "pragma", "no", "cover", "return", "f", "dialect", "self", "name"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.expand", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "expand", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def expand(self) -> \"Dialect\":\n        \"\"\"Expand any callable references to concrete ones.\n\n        This must be called before using the dialect. But\n        allows more flexible definitions to happen at runtime.\n\n        NOTE: This method returns a copy of the current dialect\n        so that we don't pollute the original dialect and get\n        dependency issues.\n\n\n        Returns:\n            :obj:`Dialect`: a copy of the given dialect but\n                with expanded references.\n        \"\"\"\n        # Are we already expanded?\n        if self.expanded:  # pragma: no cover\n            raise ValueError(\"Attempted to re-expand an already expanded dialect.\")\n\n        expanded_copy = self.copy_as(name=self.name)\n        # Expand any callable elements of the dialect.\n        for key in expanded_copy._library:\n            if isinstance(expanded_copy._library[key], SegmentGenerator):\n                # If the element is callable, call it passing the current\n                # dialect and store the result in its place.\n                # Use the .replace() method for its error handling.\n                expanded_copy.replace(\n                    **{key: expanded_copy._library[key].expand(expanded_copy)}\n                )\n        # Expand any keyword sets.\n        for keyword_set in [\n            \"unreserved_keywords\",\n            \"reserved_keywords\",\n        ]:  # e.g. reserved_keywords, (JOIN, ...)\n            # Make sure the values are available as KeywordSegments\n            for kw in expanded_copy.sets(keyword_set):\n                n = kw.capitalize() + \"KeywordSegment\"\n                if n not in expanded_copy._library:\n                    expanded_copy._library[n] = StringParser(kw.lower(), KeywordSegment)\n        expanded_copy.expanded = True\n        return expanded_copy\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "expand", "self", "dialect", "expand", "any", "callable", "references", "to", "concrete", "ones", "this", "must", "be", "called", "before", "using", "the", "dialect", "but", "allows", "more", "flexible", "definitions", "to", "happen", "at", "runtime", "note", "this", "method", "returns", "a", "copy", "of", "the", "current", "dialect", "so", "that", "we", "don", "t", "pollute", "the", "original", "dialect", "and", "get", "dependency", "issues", "returns", "obj", "dialect", "a", "copy", "of", "the", "given", "dialect", "but", "with", "expanded", "references", "are", "we", "already", "expanded", "if", "self", "expanded", "pragma", "no", "cover", "raise", "valueerror", "attempted", "to", "re", "expand", "an", "already", "expanded", "dialect", "expanded_copy", "self", "copy_as", "name", "self", "name", "expand", "any", "callable", "elements", "of", "the", "dialect", "for", "key", "in", "expanded_copy", "_library", "if", "isinstance", "expanded_copy", "_library", "key", "segmentgenerator", "if", "the", "element", "is", "callable", "call", "it", "passing", "the", "current", "dialect", "and", "store", "the", "result", "in", "its", "place", "use", "the", "replace", "method", "for", "its", "error", "handling", "expanded_copy", "replace", "key", "expanded_copy", "_library", "key", "expand", "expanded_copy", "expand", "any", "keyword", "sets", "for", "keyword_set", "in", "unreserved_keywords", "reserved_keywords", "e", "g", "reserved_keywords", "join", "make", "sure", "the", "values", "are", "available", "as", "keywordsegments", "for", "kw", "in", "expanded_copy", "sets", "keyword_set", "n", "kw", "capitalize", "keywordsegment", "if", "n", "not", "in", "expanded_copy", "_library", "expanded_copy", "_library", "n", "stringparser", "kw", "lower", "keywordsegment", "expanded_copy", "expanded", "true", "return", "expanded_copy"], "doc_len": 198}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.sets", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "sets", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def sets(self, label):\n        \"\"\"Allows access to sets belonging to this dialect.\n\n        These sets belong to the dialect and are copied for sub\n        dialects. These are used in combination with late-bound\n        dialect objects to create some of the bulk-produced rules.\n\n        \"\"\"\n        if label not in self._sets:\n            self._sets[label] = set()\n        return self._sets[label]\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "sets", "self", "label", "allows", "access", "to", "sets", "belonging", "to", "this", "dialect", "these", "sets", "belong", "to", "the", "dialect", "and", "are", "copied", "for", "sub", "dialects", "these", "are", "used", "in", "combination", "with", "late", "bound", "dialect", "objects", "to", "create", "some", "of", "the", "bulk", "produced", "rules", "if", "label", "not", "in", "self", "_sets", "self", "_sets", "label", "set", "return", "self", "_sets", "label"], "doc_len": 63}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.copy_as", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "copy_as", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def copy_as(self, name):\n        \"\"\"Copy this dialect and create a new one with a different name.\n\n        This is the primary method for inheritance, after which, the\n        `replace` method can be used to override particular rules.\n        \"\"\"\n        # Are we already expanded?\n        if self.expanded:  # pragma: no cover\n            # If we copy an already expanded dialect then any SegmentGenerators\n            # won't respond. This is most likely a mistake.\n            raise ValueError(\"Attempted to copy an already expanded dialect.\")\n\n        # Copy sets if they are passed, so they can be mutated independently\n        new_sets = {}\n        for label in self._sets:\n            new_sets[label] = self._sets[label].copy()\n\n        return self.__class__(\n            name=name,\n            library=self._library.copy(),\n            lexer_matchers=self.lexer_matchers.copy(),\n            sets=new_sets,\n            inherits_from=self.name,\n            root_segment_name=self.root_segment_name,\n        )\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "copy_as", "self", "name", "copy", "this", "dialect", "and", "create", "a", "new", "one", "with", "a", "different", "name", "this", "is", "the", "primary", "method", "for", "inheritance", "after", "which", "the", "replace", "method", "can", "be", "used", "to", "override", "particular", "rules", "are", "we", "already", "expanded", "if", "self", "expanded", "pragma", "no", "cover", "if", "we", "copy", "an", "already", "expanded", "dialect", "then", "any", "segmentgenerators", "won", "t", "respond", "this", "is", "most", "likely", "a", "mistake", "raise", "valueerror", "attempted", "to", "copy", "an", "already", "expanded", "dialect", "copy", "sets", "if", "they", "are", "passed", "so", "they", "can", "be", "mutated", "independently", "new_sets", "for", "label", "in", "self", "_sets", "new_sets", "label", "self", "_sets", "label", "copy", "return", "self", "__class__", "name", "name", "library", "self", "_library", "copy", "lexer_matchers", "self", "lexer_matchers", "copy", "sets", "new_sets", "inherits_from", "self", "name", "root_segment_name", "self", "root_segment_name"], "doc_len": 125}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.segment", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "segment", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def segment(self, replace=False):\n        \"\"\"This is the decorator for elements, it should be called as a method.\n\n        e.g.\n        @dialect.segment()\n        class SomeSegment(BaseSegment):\n            blah blah blah\n\n        \"\"\"\n\n        def segment_wrap(cls):\n            \"\"\"Wrap a segment and register it against the dialect.\"\"\"\n            n = cls.__name__\n            if replace:\n                if n not in self._library:  # pragma: no cover\n                    raise ValueError(f\"{n!r} is not already registered in {self!r}\")\n            else:\n                if n in self._library:  # pragma: no cover\n                    raise ValueError(f\"{n!r} is already registered in {self!r}\")\n            self._library[n] = cls\n            # Pass it back after registering it\n            return cls\n\n        # return the wrapping function\n        return segment_wrap\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "segment", "self", "replace", "false", "this", "is", "the", "decorator", "for", "elements", "it", "should", "be", "called", "as", "a", "method", "e", "g", "dialect", "segment", "class", "somesegment", "basesegment", "blah", "blah", "blah", "def", "segment_wrap", "cls", "wrap", "a", "segment", "and", "register", "it", "against", "the", "dialect", "n", "cls", "__name__", "if", "replace", "if", "n", "not", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "n", "r", "is", "not", "already", "registered", "in", "self", "r", "else", "if", "n", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "n", "r", "is", "already", "registered", "in", "self", "r", "self", "_library", "n", "cls", "pass", "it", "back", "after", "registering", "it", "return", "cls", "return", "the", "wrapping", "function", "return", "segment_wrap"], "doc_len": 111}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.add", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "add", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def add(self, **kwargs: DialectElementType):\n        \"\"\"Add a segment to the dialect directly.\n\n        This is the alternative to the decorator route, most useful for segments\n        defined using `make`. Segments are passed in as kwargs.\n\n        e.g.\n        dialect.add(SomeSegment=StringParser(\"blah\", KeywordSegment))\n\n        Note that multiple segments can be added in the same call as this method\n        will iterate through the kwargs\n        \"\"\"\n        for n in kwargs:\n            if n in self._library:  # pragma: no cover\n                raise ValueError(f\"{n!r} is already registered in {self!r}\")\n            self._library[n] = kwargs[n]\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "add", "self", "kwargs", "dialectelementtype", "add", "a", "segment", "to", "the", "dialect", "directly", "this", "is", "the", "alternative", "to", "the", "decorator", "route", "most", "useful", "for", "segments", "defined", "using", "make", "segments", "are", "passed", "in", "as", "kwargs", "e", "g", "dialect", "add", "somesegment", "stringparser", "blah", "keywordsegment", "note", "that", "multiple", "segments", "can", "be", "added", "in", "the", "same", "call", "as", "this", "method", "will", "iterate", "through", "the", "kwargs", "for", "n", "in", "kwargs", "if", "n", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "n", "r", "is", "already", "registered", "in", "self", "r", "self", "_library", "n", "kwargs", "n"], "doc_len": 95}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.replace", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "replace", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def replace(self, **kwargs: DialectElementType):\n        \"\"\"Override a segment on the dialect directly.\n\n        Usage is very similar to add, but elements specified must already exist.\n        \"\"\"\n        for n in kwargs:\n            if n not in self._library:  # pragma: no cover\n                raise ValueError(f\"{n!r} is not already registered in {self!r}\")\n            self._library[n] = kwargs[n]\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "replace", "self", "kwargs", "dialectelementtype", "override", "a", "segment", "on", "the", "dialect", "directly", "usage", "is", "very", "similar", "to", "add", "but", "elements", "specified", "must", "already", "exist", "for", "n", "in", "kwargs", "if", "n", "not", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "n", "r", "is", "not", "already", "registered", "in", "self", "r", "self", "_library", "n", "kwargs", "n"], "doc_len": 61}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.get_grammar", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "get_grammar", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def get_grammar(self, name: str) -> BaseGrammar:\n        \"\"\"Allow access to grammars pre-expansion.\n\n        This is typically for dialect inheritance. This method\n        also validates that the result is a grammar.\n        \"\"\"\n        if name not in self._library:  # pragma: no cover\n            raise ValueError(f\"Element {name} not found in dialect.\")\n        if not isinstance(self._library[name], BaseGrammar):  # pragma: no cover\n            raise TypeError(\n                f\"Attempted to fetch non grammar [{name}] with get_grammar.\"\n            )\n        return self._library[name]\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "get_grammar", "self", "name", "str", "basegrammar", "allow", "access", "to", "grammars", "pre", "expansion", "this", "is", "typically", "for", "dialect", "inheritance", "this", "method", "also", "validates", "that", "the", "result", "is", "a", "grammar", "if", "name", "not", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "element", "name", "not", "found", "in", "dialect", "if", "not", "isinstance", "self", "_library", "name", "basegrammar", "pragma", "no", "cover", "raise", "typeerror", "f", "attempted", "to", "fetch", "non", "grammar", "name", "with", "get_grammar", "return", "self", "_library", "name"], "doc_len": 78}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.get_segment", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "get_segment", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def get_segment(self, name: str) -> Type[\"BaseSegment\"]:\n        \"\"\"Allow access to segments pre-expansion.\n\n        This is typically for dialect inheritance. This method\n        also validates that the result is a segment.\n        \"\"\"\n        if name not in self._library:  # pragma: no cover\n            raise ValueError(f\"Element {name} not found in dialect.\")\n        if not issubclass(self._library[name], BaseSegment):  # pragma: no cover\n            raise TypeError(\n                f\"Attempted to fetch non segment [{name}] with get_segment.\"\n            )\n        return self._library[name]\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "get_segment", "self", "name", "str", "type", "basesegment", "allow", "access", "to", "segments", "pre", "expansion", "this", "is", "typically", "for", "dialect", "inheritance", "this", "method", "also", "validates", "that", "the", "result", "is", "a", "segment", "if", "name", "not", "in", "self", "_library", "pragma", "no", "cover", "raise", "valueerror", "f", "element", "name", "not", "found", "in", "dialect", "if", "not", "issubclass", "self", "_library", "name", "basesegment", "pragma", "no", "cover", "raise", "typeerror", "f", "attempted", "to", "fetch", "non", "segment", "name", "with", "get_segment", "return", "self", "_library", "name"], "doc_len": 79}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.ref", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "ref", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def ref(self, name: str) -> ExpandedDialectElementType:\n        \"\"\"Return an object which acts as a late binding reference to the element named.\n\n        NB: This requires the dialect to be expanded, and only returns Matchables\n        as a result.\n\n        \"\"\"\n        if not self.expanded:  # pragma: no cover\n            raise RuntimeError(\"Dialect must be expanded before use.\")\n\n        if name in self._library:\n            res = self._library[name]\n            if res:\n                return res\n            else:  # pragma: no cover\n                raise ValueError(\n                    \"Unexpected Null response while fetching {!r} from {}\".format(\n                        name, self.name\n                    )\n                )\n        else:  # pragma: no cover\n            raise RuntimeError(\n                \"Grammar refers to {!r} which was not found in the {} dialect\".format(\n                    name, self.name\n                )\n            )\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "ref", "self", "name", "str", "expandeddialectelementtype", "return", "an", "object", "which", "acts", "as", "a", "late", "binding", "reference", "to", "the", "element", "named", "nb", "this", "requires", "the", "dialect", "to", "be", "expanded", "and", "only", "returns", "matchables", "as", "a", "result", "if", "not", "self", "expanded", "pragma", "no", "cover", "raise", "runtimeerror", "dialect", "must", "be", "expanded", "before", "use", "if", "name", "in", "self", "_library", "res", "self", "_library", "name", "if", "res", "return", "res", "else", "pragma", "no", "cover", "raise", "valueerror", "unexpected", "null", "response", "while", "fetching", "r", "from", "format", "name", "self", "name", "else", "pragma", "no", "cover", "raise", "runtimeerror", "grammar", "refers", "to", "r", "which", "was", "not", "found", "in", "the", "dialect", "format", "name", "self", "name"], "doc_len": 108}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.set_lexer_matchers", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "set_lexer_matchers", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def set_lexer_matchers(self, lexer_matchers):\n        \"\"\"Set the lexer struct for the dialect.\n\n        This is what is used for base dialects. For derived dialects\n        (which don't exist yet) the assumption is that we'll introduce\n        some kind of *patch* function which could be used to mutate\n        an existing `lexer_matchers`.\n        \"\"\"\n        self.lexer_matchers = lexer_matchers\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "set_lexer_matchers", "self", "lexer_matchers", "set", "the", "lexer", "struct", "for", "the", "dialect", "this", "is", "what", "is", "used", "for", "base", "dialects", "for", "derived", "dialects", "which", "don", "t", "exist", "yet", "the", "assumption", "is", "that", "we", "ll", "introduce", "some", "kind", "of", "patch", "function", "which", "could", "be", "used", "to", "mutate", "an", "existing", "lexer_matchers", "self", "lexer_matchers", "lexer_matchers"], "doc_len": 58}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.get_lexer_matchers", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "get_lexer_matchers", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def get_lexer_matchers(self):\n        \"\"\"Fetch the lexer struct for this dialect.\"\"\"\n        if self.lexer_matchers:\n            return self.lexer_matchers\n        else:  # pragma: no cover\n            raise ValueError(f\"Lexing struct has not been set for dialect {self}\")\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "get_lexer_matchers", "self", "fetch", "the", "lexer", "struct", "for", "this", "dialect", "if", "self", "lexer_matchers", "return", "self", "lexer_matchers", "else", "pragma", "no", "cover", "raise", "valueerror", "f", "lexing", "struct", "has", "not", "been", "set", "for", "dialect", "self"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.patch_lexer_matchers", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "patch_lexer_matchers", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def patch_lexer_matchers(self, lexer_patch):\n        \"\"\"Patch an existing lexer struct.\n\n        Used to edit the lexer of a sub-dialect.\n        \"\"\"\n        buff = []\n        if not self.lexer_matchers:  # pragma: no cover\n            raise ValueError(\"Lexer struct must be defined before it can be patched!\")\n\n        # Make a new data struct for lookups\n        patch_dict = {elem.name: elem for elem in lexer_patch}\n\n        for elem in self.lexer_matchers:\n            if elem.name in patch_dict:\n                buff.append(patch_dict[elem.name])\n            else:\n                buff.append(elem)\n        # Overwrite with the buffer once we're done\n        self.lexer_matchers = buff\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "patch_lexer_matchers", "self", "lexer_patch", "patch", "an", "existing", "lexer", "struct", "used", "to", "edit", "the", "lexer", "of", "a", "sub", "dialect", "buff", "if", "not", "self", "lexer_matchers", "pragma", "no", "cover", "raise", "valueerror", "lexer", "struct", "must", "be", "defined", "before", "it", "can", "be", "patched", "make", "a", "new", "data", "struct", "for", "lookups", "patch_dict", "elem", "name", "elem", "for", "elem", "in", "lexer_patch", "for", "elem", "in", "self", "lexer_matchers", "if", "elem", "name", "in", "patch_dict", "buff", "append", "patch_dict", "elem", "name", "else", "buff", "append", "elem", "overwrite", "with", "the", "buffer", "once", "we", "re", "done", "self", "lexer_matchers", "buff"], "doc_len": 90}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.insert_lexer_matchers", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "insert_lexer_matchers", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def insert_lexer_matchers(self, lexer_patch, before):\n        \"\"\"Insert new records into an existing lexer struct.\n\n        Used to edit the lexer of a sub-dialect. The patch is\n        inserted *before* whichever element is named in `before`.\n        \"\"\"\n        buff = []\n        found = False\n        if not self.lexer_matchers:  # pragma: no cover\n            raise ValueError(\"Lexer struct must be defined before it can be patched!\")\n\n        for elem in self.lexer_matchers:\n            if elem.name == before:\n                found = True\n                for patch in lexer_patch:\n                    buff.append(patch)\n                buff.append(elem)\n            else:\n                buff.append(elem)\n\n        if not found:  # pragma: no cover\n            raise ValueError(\n                \"Lexer struct insert before '%s' failed because tag never found.\"\n            )\n        # Overwrite with the buffer once we're done\n        self.lexer_matchers = buff\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "insert_lexer_matchers", "self", "lexer_patch", "before", "insert", "new", "records", "into", "an", "existing", "lexer", "struct", "used", "to", "edit", "the", "lexer", "of", "a", "sub", "dialect", "the", "patch", "is", "inserted", "before", "whichever", "element", "is", "named", "in", "before", "buff", "found", "false", "if", "not", "self", "lexer_matchers", "pragma", "no", "cover", "raise", "valueerror", "lexer", "struct", "must", "be", "defined", "before", "it", "can", "be", "patched", "for", "elem", "in", "self", "lexer_matchers", "if", "elem", "name", "before", "found", "true", "for", "patch", "in", "lexer_patch", "buff", "append", "patch", "buff", "append", "elem", "else", "buff", "append", "elem", "if", "not", "found", "pragma", "no", "cover", "raise", "valueerror", "lexer", "struct", "insert", "before", "s", "failed", "because", "tag", "never", "found", "overwrite", "with", "the", "buffer", "once", "we", "re", "done", "self", "lexer_matchers", "buff"], "doc_len": 116}
{"doc_id": "src/sqlfluff/core/dialects/base.py::Dialect.get_root_segment", "file_path": "src/sqlfluff/core/dialects/base.py", "class_name": "Dialect", "func_name": "get_root_segment", "text": "文件路径: src/sqlfluff/core/dialects/base.py, 类名: Dialect\n    def get_root_segment(self):\n        \"\"\"Get the root segment of the dialect.\"\"\"\n        return self.ref(self.root_segment_name)\n", "tokens": ["src", "sqlfluff", "core", "dialects", "base", "py", "dialect", "def", "get_root_segment", "self", "get", "the", "root", "segment", "of", "the", "dialect", "return", "self", "ref", "self", "root_segment_name"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/dialects/__init__.py::load_raw_dialect", "file_path": "src/sqlfluff/core/dialects/__init__.py", "class_name": null, "func_name": "load_raw_dialect", "text": "文件路径: src/sqlfluff/core/dialects/__init__.py\ndef load_raw_dialect(label: str, base_module: str = \"sqlfluff.dialects\") -> Any:\n    \"\"\"Dynamically load a dialect.\"\"\"\n    if label in _legacy_dialects:\n        raise SQLFluffUserError(_legacy_dialects[label])\n    module, name = _dialect_lookup[label]\n    return getattr(import_module(f\"{base_module}.{module}\"), name)\n", "tokens": ["src", "sqlfluff", "core", "dialects", "__init__", "py", "def", "load_raw_dialect", "label", "str", "base_module", "str", "sqlfluff", "dialects", "any", "dynamically", "load", "a", "dialect", "if", "label", "in", "_legacy_dialects", "raise", "sqlfluffusererror", "_legacy_dialects", "label", "module", "name", "_dialect_lookup", "label", "return", "getattr", "import_module", "f", "base_module", "module", "name"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/dialects/__init__.py::dialect_readout", "file_path": "src/sqlfluff/core/dialects/__init__.py", "class_name": null, "func_name": "dialect_readout", "text": "文件路径: src/sqlfluff/core/dialects/__init__.py\ndef dialect_readout() -> Iterator[DialectTuple]:\n    \"\"\"Generate a readout of available dialects.\"\"\"\n    for dialect_label in sorted(_dialect_lookup):\n        dialect = load_raw_dialect(dialect_label)\n        yield DialectTuple(\n            label=dialect_label,\n            name=dialect.name,\n            inherits_from=dialect.inherits_from or \"nothing\",\n        )\n", "tokens": ["src", "sqlfluff", "core", "dialects", "__init__", "py", "def", "dialect_readout", "iterator", "dialecttuple", "generate", "a", "readout", "of", "available", "dialects", "for", "dialect_label", "in", "sorted", "_dialect_lookup", "dialect", "load_raw_dialect", "dialect_label", "yield", "dialecttuple", "label", "dialect_label", "name", "dialect", "name", "inherits_from", "dialect", "inherits_from", "or", "nothing"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/dialects/__init__.py::dialect_selector", "file_path": "src/sqlfluff/core/dialects/__init__.py", "class_name": null, "func_name": "dialect_selector", "text": "文件路径: src/sqlfluff/core/dialects/__init__.py\ndef dialect_selector(s: str) -> Dialect:\n    \"\"\"Return a dialect given its name.\"\"\"\n    dialect = load_raw_dialect(s or \"ansi\")\n    # Expand any callable references at this point.\n    # NOTE: The result of .expand() is a new class.\n    return dialect.expand()\n", "tokens": ["src", "sqlfluff", "core", "dialects", "__init__", "py", "def", "dialect_selector", "s", "str", "dialect", "return", "a", "dialect", "given", "its", "name", "dialect", "load_raw_dialect", "s", "or", "ansi", "expand", "any", "callable", "references", "at", "this", "point", "note", "the", "result", "of", "expand", "is", "a", "new", "class", "return", "dialect", "expand"], "doc_len": 41}
{"doc_id": "src/sqlfluff/core/linter/common.py::EnrichedFixPatch.dedupe_tuple", "file_path": "src/sqlfluff/core/linter/common.py", "class_name": "EnrichedFixPatch", "func_name": "dedupe_tuple", "text": "文件路径: src/sqlfluff/core/linter/common.py, 类名: EnrichedFixPatch\n    def dedupe_tuple(self):\n        \"\"\"Generate a tuple of this fix for deduping.\"\"\"\n        return (self.source_slice, self.fixed_raw)\n", "tokens": ["src", "sqlfluff", "core", "linter", "common", "py", "enrichedfixpatch", "def", "dedupe_tuple", "self", "generate", "a", "tuple", "of", "this", "fix", "for", "deduping", "return", "self", "source_slice", "self", "fixed_raw"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.__init__", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def __init__(self, path: str) -> None:\n        self.files: List[LintedFile] = []\n        self.path: str = path\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "__init__", "self", "path", "str", "none", "self", "files", "list", "lintedfile", "self", "path", "str", "path"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.add", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "add", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def add(self, file: LintedFile) -> None:\n        \"\"\"Add a file to this path.\"\"\"\n        self.files.append(file)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "add", "self", "file", "lintedfile", "none", "add", "a", "file", "to", "this", "path", "self", "files", "append", "file"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.check_tuples", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def check_tuples(\n        self, by_path: Literal[False]\n    ) -> List[CheckTuple]:  # pragma: no cover\n        \"\"\"Return a List of CheckTuples when by_path is False.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "check_tuples", "self", "by_path", "literal", "false", "list", "checktuple", "pragma", "no", "cover", "return", "a", "list", "of", "checktuples", "when", "by_path", "is", "false"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.check_tuples", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def check_tuples(\n        self, by_path: Literal[True]\n    ) -> Dict[str, List[CheckTuple]]:  # pragma: no cover\n        \"\"\"Return a Dict of paths and CheckTuples when by_path is True.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "check_tuples", "self", "by_path", "literal", "true", "dict", "str", "list", "checktuple", "pragma", "no", "cover", "return", "a", "dict", "of", "paths", "and", "checktuples", "when", "by_path", "is", "true"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.check_tuples", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def check_tuples(self, by_path: bool = False):  # pragma: no cover\n        \"\"\"Default overload method.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "check_tuples", "self", "by_path", "bool", "false", "pragma", "no", "cover", "default", "overload", "method"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.check_tuples", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def check_tuples(self, by_path=False, raise_on_non_linting_violations=True):\n        \"\"\"Compress all the tuples into one list.\n\n        NB: This is a little crude, as you can't tell which\n        file the violations are from. Good for testing though.\n        For more control set the `by_path` argument to true.\n        \"\"\"\n        if by_path:\n            return {\n                file.path: file.check_tuples(\n                    raise_on_non_linting_violations=raise_on_non_linting_violations\n                )\n                for file in self.files\n            }\n        else:\n            tuple_buffer: List[CheckTuple] = []\n            for file in self.files:\n                tuple_buffer += file.check_tuples(\n                    raise_on_non_linting_violations=raise_on_non_linting_violations\n                )\n            return tuple_buffer\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "check_tuples", "self", "by_path", "false", "raise_on_non_linting_violations", "true", "compress", "all", "the", "tuples", "into", "one", "list", "nb", "this", "is", "a", "little", "crude", "as", "you", "can", "t", "tell", "which", "file", "the", "violations", "are", "from", "good", "for", "testing", "though", "for", "more", "control", "set", "the", "by_path", "argument", "to", "true", "if", "by_path", "return", "file", "path", "file", "check_tuples", "raise_on_non_linting_violations", "raise_on_non_linting_violations", "for", "file", "in", "self", "files", "else", "tuple_buffer", "list", "checktuple", "for", "file", "in", "self", "files", "tuple_buffer", "file", "check_tuples", "raise_on_non_linting_violations", "raise_on_non_linting_violations", "return", "tuple_buffer"], "doc_len": 81}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.num_violations", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "num_violations", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def num_violations(self, **kwargs) -> int:\n        \"\"\"Count the number of violations in the path.\"\"\"\n        return sum(file.num_violations(**kwargs) for file in self.files)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "num_violations", "self", "kwargs", "int", "count", "the", "number", "of", "violations", "in", "the", "path", "return", "sum", "file", "num_violations", "kwargs", "for", "file", "in", "self", "files"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.get_violations", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "get_violations", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def get_violations(self, **kwargs) -> list:\n        \"\"\"Return a list of violations in the path.\"\"\"\n        buff: list = []\n        for file in self.files:\n            buff += file.get_violations(**kwargs)\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "get_violations", "self", "kwargs", "list", "return", "a", "list", "of", "violations", "in", "the", "path", "buff", "list", "for", "file", "in", "self", "files", "buff", "file", "get_violations", "kwargs", "return", "buff"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.violation_dict", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "violation_dict", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def violation_dict(self, **kwargs) -> Dict[str, list]:\n        \"\"\"Return a dict of violations by file path.\"\"\"\n        return {file.path: file.get_violations(**kwargs) for file in self.files}\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "violation_dict", "self", "kwargs", "dict", "str", "list", "return", "a", "dict", "of", "violations", "by", "file", "path", "return", "file", "path", "file", "get_violations", "kwargs", "for", "file", "in", "self", "files"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.stats", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "stats", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def stats(self) -> Dict[str, int]:\n        \"\"\"Return a dict containing linting stats about this path.\"\"\"\n        return dict(\n            files=len(self.files),\n            clean=sum(file.is_clean() for file in self.files),\n            unclean=sum(not file.is_clean() for file in self.files),\n            violations=sum(file.num_violations() for file in self.files),\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "stats", "self", "dict", "str", "int", "return", "a", "dict", "containing", "linting", "stats", "about", "this", "path", "return", "dict", "files", "len", "self", "files", "clean", "sum", "file", "is_clean", "for", "file", "in", "self", "files", "unclean", "sum", "not", "file", "is_clean", "for", "file", "in", "self", "files", "violations", "sum", "file", "num_violations", "for", "file", "in", "self", "files"], "doc_len": 56}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.persist_changes", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "persist_changes", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def persist_changes(\n        self, formatter: Any = None, fixed_file_suffix: str = \"\", **kwargs\n    ) -> Dict[str, Union[bool, str]]:\n        \"\"\"Persist changes to files in the given path.\n\n        This also logs the output as we go using the formatter if present.\n        \"\"\"\n        # Run all the fixes for all the files and return a dict\n        buffer: Dict[str, Union[bool, str]] = {}\n        for file in self.files:\n            if file.num_violations(fixable=True, **kwargs) > 0:\n                buffer[file.path] = file.persist_tree(suffix=fixed_file_suffix)\n                result = buffer[file.path]\n            else:  # pragma: no cover TODO?\n                buffer[file.path] = True\n                result = \"SKIP\"\n\n            if formatter:\n                formatter.dispatch_persist_filename(filename=file.path, result=result)\n        return buffer\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "persist_changes", "self", "formatter", "any", "none", "fixed_file_suffix", "str", "kwargs", "dict", "str", "union", "bool", "str", "persist", "changes", "to", "files", "in", "the", "given", "path", "this", "also", "logs", "the", "output", "as", "we", "go", "using", "the", "formatter", "if", "present", "run", "all", "the", "fixes", "for", "all", "the", "files", "and", "return", "a", "dict", "buffer", "dict", "str", "union", "bool", "str", "for", "file", "in", "self", "files", "if", "file", "num_violations", "fixable", "true", "kwargs", "0", "buffer", "file", "path", "file", "persist_tree", "suffix", "fixed_file_suffix", "result", "buffer", "file", "path", "else", "pragma", "no", "cover", "todo", "buffer", "file", "path", "true", "result", "skip", "if", "formatter", "formatter", "dispatch_persist_filename", "filename", "file", "path", "result", "result", "return", "buffer"], "doc_len": 105}
{"doc_id": "src/sqlfluff/core/linter/linted_dir.py::LintedDir.tree", "file_path": "src/sqlfluff/core/linter/linted_dir.py", "class_name": "LintedDir", "func_name": "tree", "text": "文件路径: src/sqlfluff/core/linter/linted_dir.py, 类名: LintedDir\n    def tree(self) -> Optional[BaseSegment]:\n        \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n        if len(self.files) > 1:  # pragma: no cover\n            raise ValueError(\n                \".tree() cannot be called when a LintedDir contains more than one file.\"\n            )\n        return self.files[0].tree\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_dir", "py", "linteddir", "def", "tree", "self", "optional", "basesegment", "a", "convenience", "method", "for", "when", "there", "is", "only", "one", "file", "and", "we", "want", "the", "tree", "if", "len", "self", "files", "1", "pragma", "no", "cover", "raise", "valueerror", "tree", "cannot", "be", "called", "when", "a", "linteddir", "contains", "more", "than", "one", "file", "return", "self", "files", "0", "tree"], "doc_len": 54}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.check_tuples", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def check_tuples(self, raise_on_non_linting_violations=True) -> List[CheckTuple]:\n        \"\"\"Make a list of check_tuples.\n\n        This assumes that all the violations found are\n        linting violations (and therefore implement `check_tuple()`).\n        If they don't then this function raises that error.\n        \"\"\"\n        vs: List[CheckTuple] = []\n        v: SQLLintError\n        for v in self.get_violations():\n            if hasattr(v, \"check_tuple\"):\n                vs.append(v.check_tuple())\n            elif raise_on_non_linting_violations:\n                raise v\n        return vs\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "check_tuples", "self", "raise_on_non_linting_violations", "true", "list", "checktuple", "make", "a", "list", "of", "check_tuples", "this", "assumes", "that", "all", "the", "violations", "found", "are", "linting", "violations", "and", "therefore", "implement", "check_tuple", "if", "they", "don", "t", "then", "this", "function", "raises", "that", "error", "vs", "list", "checktuple", "v", "sqllinterror", "for", "v", "in", "self", "get_violations", "if", "hasattr", "v", "check_tuple", "vs", "append", "v", "check_tuple", "elif", "raise_on_non_linting_violations", "raise", "v", "return", "vs"], "doc_len": 67}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.get_violations", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "get_violations", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def get_violations(\n        self,\n        rules: Optional[Union[str, Tuple[str, ...]]] = None,\n        types: Optional[Union[Type[SQLBaseError], Iterable[Type[SQLBaseError]]]] = None,\n        filter_ignore: bool = True,\n        fixable: bool = None,\n    ) -> list:\n        \"\"\"Get a list of violations, respecting filters and ignore options.\n\n        Optionally now with filters.\n        \"\"\"\n        violations = self.violations\n        # Filter types\n        if types:\n            # If it's a singular type, make it a single item in a tuple\n            # otherwise coerce to tuple normally so that we can use it with\n            # isinstance.\n            if isinstance(types, type) and issubclass(types, SQLBaseError):\n                types = (types,)\n            else:\n                types = tuple(types)  # pragma: no cover TODO?\n            violations = [v for v in violations if isinstance(v, types)]\n        # Filter rules\n        if rules:\n            if isinstance(rules, str):\n                rules = (rules,)\n            else:\n                rules = tuple(rules)\n            violations = [v for v in violations if v.rule_code() in rules]\n        # Filter fixable\n        if fixable is not None:\n            # Assume that fixable is true or false if not None\n            violations = [v for v in violations if v.fixable is fixable]\n        # Filter ignorable violations\n        if filter_ignore:\n            violations = [v for v in violations if not v.ignore]\n            # Ignore any rules in the ignore mask\n            if self.ignore_mask:\n                violations = self.ignore_masked_violations(violations, self.ignore_mask)\n        return violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "get_violations", "self", "rules", "optional", "union", "str", "tuple", "str", "none", "types", "optional", "union", "type", "sqlbaseerror", "iterable", "type", "sqlbaseerror", "none", "filter_ignore", "bool", "true", "fixable", "bool", "none", "list", "get", "a", "list", "of", "violations", "respecting", "filters", "and", "ignore", "options", "optionally", "now", "with", "filters", "violations", "self", "violations", "filter", "types", "if", "types", "if", "it", "s", "a", "singular", "type", "make", "it", "a", "single", "item", "in", "a", "tuple", "otherwise", "coerce", "to", "tuple", "normally", "so", "that", "we", "can", "use", "it", "with", "isinstance", "if", "isinstance", "types", "type", "and", "issubclass", "types", "sqlbaseerror", "types", "types", "else", "types", "tuple", "types", "pragma", "no", "cover", "todo", "violations", "v", "for", "v", "in", "violations", "if", "isinstance", "v", "types", "filter", "rules", "if", "rules", "if", "isinstance", "rules", "str", "rules", "rules", "else", "rules", "tuple", "rules", "violations", "v", "for", "v", "in", "violations", "if", "v", "rule_code", "in", "rules", "filter", "fixable", "if", "fixable", "is", "not", "none", "assume", "that", "fixable", "is", "true", "or", "false", "if", "not", "none", "violations", "v", "for", "v", "in", "violations", "if", "v", "fixable", "is", "fixable", "filter", "ignorable", "violations", "if", "filter_ignore", "violations", "v", "for", "v", "in", "violations", "if", "not", "v", "ignore", "ignore", "any", "rules", "in", "the", "ignore", "mask", "if", "self", "ignore_mask", "violations", "self", "ignore_masked_violations", "violations", "self", "ignore_mask", "return", "violations"], "doc_len": 195}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile._ignore_masked_violations_single_line", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "_ignore_masked_violations_single_line", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def _ignore_masked_violations_single_line(\n        violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-specific directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives with\n        action=None.\n        \"\"\"\n        for ignore in ignore_mask:\n            violations = [\n                v\n                for v in violations\n                if not (\n                    v.line_no == ignore.line_no\n                    and (ignore.rules is None or v.rule_code() in ignore.rules)\n                )\n            ]\n        return violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "_ignore_masked_violations_single_line", "violations", "list", "sqlbaseerror", "ignore_mask", "list", "noqadirective", "returns", "whether", "to", "ignore", "error", "for", "line", "specific", "directives", "the", "ignore", "list", "is", "assumed", "to", "only", "contain", "noqadirectives", "with", "action", "none", "for", "ignore", "in", "ignore_mask", "violations", "v", "for", "v", "in", "violations", "if", "not", "v", "line_no", "ignore", "line_no", "and", "ignore", "rules", "is", "none", "or", "v", "rule_code", "in", "ignore", "rules", "return", "violations"], "doc_len": 65}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile._should_ignore_violation_line_range", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "_should_ignore_violation_line_range", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def _should_ignore_violation_line_range(\n        line_no: int, ignore_rule: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore a violation at line_no.\"\"\"\n        # Loop through the NoQaDirectives to find the state of things at\n        # line_no. Assumptions about \"ignore_rule\":\n        # - Contains directives for only ONE RULE, i.e. the rule that was\n        #   violated at line_no\n        # - Sorted in ascending order by line number\n        disable = False\n        for ignore in ignore_rule:\n            if ignore.line_no > line_no:\n                break\n            disable = ignore.action == \"disable\"\n        return disable\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "_should_ignore_violation_line_range", "line_no", "int", "ignore_rule", "list", "noqadirective", "returns", "whether", "to", "ignore", "a", "violation", "at", "line_no", "loop", "through", "the", "noqadirectives", "to", "find", "the", "state", "of", "things", "at", "line_no", "assumptions", "about", "ignore_rule", "contains", "directives", "for", "only", "one", "rule", "i", "e", "the", "rule", "that", "was", "violated", "at", "line_no", "sorted", "in", "ascending", "order", "by", "line", "number", "disable", "false", "for", "ignore", "in", "ignore_rule", "if", "ignore", "line_no", "line_no", "break", "disable", "ignore", "action", "disable", "return", "disable"], "doc_len": 76}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile._ignore_masked_violations_line_range", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "_ignore_masked_violations_line_range", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def _ignore_masked_violations_line_range(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ):\n        \"\"\"Returns whether to ignore error for line-range directives.\n\n        The \"ignore\" list is assumed to ONLY contain NoQaDirectives where\n        action is \"enable\" or \"disable\".\n        \"\"\"\n        result = []\n        for v in violations:\n            # Find the directives that affect the violated rule \"v\", either\n            # because they specifically reference it or because they don't\n            # specify a list of rules, thus affecting ALL rules.\n            ignore_rule = sorted(\n                (\n                    ignore\n                    for ignore in ignore_mask\n                    if not ignore.rules\n                    or (v.rule_code() in cast(Tuple[str, ...], ignore.rules))\n                ),\n                key=lambda ignore: ignore.line_no,\n            )\n            # Determine whether to ignore the violation, based on the relevant\n            # enable/disable directives.\n            if not cls._should_ignore_violation_line_range(v.line_no, ignore_rule):\n                result.append(v)\n        return result\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "_ignore_masked_violations_line_range", "cls", "violations", "list", "sqlbaseerror", "ignore_mask", "list", "noqadirective", "returns", "whether", "to", "ignore", "error", "for", "line", "range", "directives", "the", "ignore", "list", "is", "assumed", "to", "only", "contain", "noqadirectives", "where", "action", "is", "enable", "or", "disable", "result", "for", "v", "in", "violations", "find", "the", "directives", "that", "affect", "the", "violated", "rule", "v", "either", "because", "they", "specifically", "reference", "it", "or", "because", "they", "don", "t", "specify", "a", "list", "of", "rules", "thus", "affecting", "all", "rules", "ignore_rule", "sorted", "ignore", "for", "ignore", "in", "ignore_mask", "if", "not", "ignore", "rules", "or", "v", "rule_code", "in", "cast", "tuple", "str", "ignore", "rules", "key", "lambda", "ignore", "ignore", "line_no", "determine", "whether", "to", "ignore", "the", "violation", "based", "on", "the", "relevant", "enable", "disable", "directives", "if", "not", "cls", "_should_ignore_violation_line_range", "v", "line_no", "ignore_rule", "result", "append", "v", "return", "result"], "doc_len": 124}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.ignore_masked_violations", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "ignore_masked_violations", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def ignore_masked_violations(\n        cls, violations: List[SQLBaseError], ignore_mask: List[NoQaDirective]\n    ) -> List[SQLBaseError]:\n        \"\"\"Remove any violations specified by ignore_mask.\n\n        This involves two steps:\n        1. Filter out violations affected by single-line \"noqa\" directives.\n        2. Filter out violations affected by disable/enable \"noqa\" directives.\n        \"\"\"\n        ignore_specific = [ignore for ignore in ignore_mask if not ignore.action]\n        ignore_range = [ignore for ignore in ignore_mask if ignore.action]\n        violations = cls._ignore_masked_violations_single_line(\n            violations, ignore_specific\n        )\n        violations = cls._ignore_masked_violations_line_range(violations, ignore_range)\n        return violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "ignore_masked_violations", "cls", "violations", "list", "sqlbaseerror", "ignore_mask", "list", "noqadirective", "list", "sqlbaseerror", "remove", "any", "violations", "specified", "by", "ignore_mask", "this", "involves", "two", "steps", "1", "filter", "out", "violations", "affected", "by", "single", "line", "noqa", "directives", "2", "filter", "out", "violations", "affected", "by", "disable", "enable", "noqa", "directives", "ignore_specific", "ignore", "for", "ignore", "in", "ignore_mask", "if", "not", "ignore", "action", "ignore_range", "ignore", "for", "ignore", "in", "ignore_mask", "if", "ignore", "action", "violations", "cls", "_ignore_masked_violations_single_line", "violations", "ignore_specific", "violations", "cls", "_ignore_masked_violations_line_range", "violations", "ignore_range", "return", "violations"], "doc_len": 79}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.num_violations", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "num_violations", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def num_violations(self, **kwargs) -> int:\n        \"\"\"Count the number of violations.\n\n        Optionally now with filters.\n        \"\"\"\n        violations = self.get_violations(**kwargs)\n        return len(violations)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "num_violations", "self", "kwargs", "int", "count", "the", "number", "of", "violations", "optionally", "now", "with", "filters", "violations", "self", "get_violations", "kwargs", "return", "len", "violations"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.is_clean", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "is_clean", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def is_clean(self) -> bool:\n        \"\"\"Return True if there are no ignorable violations.\"\"\"\n        return not any(self.get_violations(filter_ignore=True))\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "is_clean", "self", "bool", "return", "true", "if", "there", "are", "no", "ignorable", "violations", "return", "not", "any", "self", "get_violations", "filter_ignore", "true"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile._log_hints", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "_log_hints", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def _log_hints(\n        patch: Union[EnrichedFixPatch, FixPatch], templated_file: TemplatedFile\n    ):\n        \"\"\"Log hints for debugging during patch generation.\"\"\"\n        # This next bit is ALL FOR LOGGING AND DEBUGGING\n        max_log_length = 10\n        if patch.templated_slice.start >= max_log_length:\n            pre_hint = templated_file.templated_str[\n                patch.templated_slice.start\n                - max_log_length : patch.templated_slice.start\n            ]\n        else:\n            pre_hint = templated_file.templated_str[: patch.templated_slice.start]\n        if patch.templated_slice.stop + max_log_length < len(\n            templated_file.templated_str\n        ):\n            post_hint = templated_file.templated_str[\n                patch.templated_slice.stop : patch.templated_slice.stop + max_log_length\n            ]\n        else:\n            post_hint = templated_file.templated_str[patch.templated_slice.stop :]\n        linter_logger.debug(\n            \"        Templated Hint: ...%r <> %r...\", pre_hint, post_hint\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "_log_hints", "patch", "union", "enrichedfixpatch", "fixpatch", "templated_file", "templatedfile", "log", "hints", "for", "debugging", "during", "patch", "generation", "this", "next", "bit", "is", "all", "for", "logging", "and", "debugging", "max_log_length", "10", "if", "patch", "templated_slice", "start", "max_log_length", "pre_hint", "templated_file", "templated_str", "patch", "templated_slice", "start", "max_log_length", "patch", "templated_slice", "start", "else", "pre_hint", "templated_file", "templated_str", "patch", "templated_slice", "start", "if", "patch", "templated_slice", "stop", "max_log_length", "len", "templated_file", "templated_str", "post_hint", "templated_file", "templated_str", "patch", "templated_slice", "stop", "patch", "templated_slice", "stop", "max_log_length", "else", "post_hint", "templated_file", "templated_str", "patch", "templated_slice", "stop", "linter_logger", "debug", "templated", "hint", "r", "r", "pre_hint", "post_hint"], "doc_len": 88}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.fix_string", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "fix_string", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def fix_string(self) -> Tuple[Any, bool]:\n        \"\"\"Obtain the changes to a path as a string.\n\n        We use the source mapping features of TemplatedFile\n        to generate a list of \"patches\" which cover the non\n        templated parts of the file and refer back to the locations\n        in the original file.\n\n        NB: This is MUCH FASTER than the original approach\n        using difflib in pre 0.4.0.\n\n        There is an important distinction here between Slices and\n        Segments. A Slice is a portion of a file which is determined\n        by the templater based on which portions of the source file\n        are templated or not, and therefore before Lexing and so is\n        completely dialect agnostic. A Segment is determined by the\n        Lexer from portions of strings after templating.\n        \"\"\"\n        linter_logger.debug(\"Original Tree: %r\", self.templated_file.templated_str)\n        assert self.tree\n        linter_logger.debug(\"Fixed Tree: %r\", self.tree.raw)\n\n        # The sliced file is contiguous in the TEMPLATED space.\n        # NB: It has gaps and repeats in the source space.\n        # It's also not the FIXED file either.\n        linter_logger.debug(\"### Templated File.\")\n        for idx, file_slice in enumerate(self.templated_file.sliced_file):\n            t_str = self.templated_file.templated_str[file_slice.templated_slice]\n            s_str = self.templated_file.source_str[file_slice.source_slice]\n            if t_str == s_str:\n                linter_logger.debug(\n                    \"    File slice: %s %r [invariant]\", idx, file_slice\n                )\n            else:\n                linter_logger.debug(\"    File slice: %s %r\", idx, file_slice)\n                linter_logger.debug(\"    \\t\\t\\ttemplated: %r\\tsource: %r\", t_str, s_str)\n\n        original_source = self.templated_file.source_str\n\n        # Make sure no patches overlap and divide up the source file into slices.\n        # Any Template tags in the source file are off limits.\n        source_only_slices = self.templated_file.source_only_slices()\n\n        linter_logger.debug(\"Source-only slices: %s\", source_only_slices)\n\n        # Iterate patches, filtering and translating as we go:\n        linter_logger.debug(\"### Beginning Patch Iteration.\")\n        filtered_source_patches = []\n        dedupe_buffer = []\n        # We use enumerate so that we get an index for each patch. This is entirely\n        # so when debugging logs we can find a given patch again!\n        patch: Union[EnrichedFixPatch, FixPatch]\n        for idx, patch in enumerate(\n            self.tree.iter_patches(templated_str=self.templated_file.templated_str)\n        ):\n            linter_logger.debug(\"  %s Yielded patch: %s\", idx, patch)\n            self._log_hints(patch, self.templated_file)\n\n            # Attempt to convert to source space.\n            try:\n                source_slice = self.templated_file.templated_slice_to_source_slice(\n                    patch.templated_slice,\n                )\n            except ValueError:\n                linter_logger.info(\n                    \"      - Skipping. Source space Value Error. i.e. attempted insertion within templated section.\"\n                )\n                # If we try and slice within a templated section, then we may fail\n                # in which case, we should skip this patch.\n                continue\n\n            # Check for duplicates\n            dedupe_tuple = (source_slice, patch.fixed_raw)\n            if dedupe_tuple in dedupe_buffer:\n                linter_logger.info(\n                    \"      - Skipping. Source space Duplicate: %s\", dedupe_tuple\n                )\n                continue\n\n            # We now evaluate patches in the source-space for whether they overlap\n            # or disrupt any templated sections.\n            # The intent here is that unless explicitly stated, a fix should never\n            # disrupt a templated section.\n            # NOTE: We rely here on the patches being sorted.\n            # TODO: Implement a mechanism for doing templated section fixes. For\n            # now it's just not allowed.\n\n            # Get the affected raw slices.\n            local_raw_slices = self.templated_file.raw_slices_spanning_source_slice(\n                source_slice\n            )\n            local_type_list = [slc.slice_type for slc in local_raw_slices]\n\n            enriched_patch = EnrichedFixPatch(\n                source_slice=source_slice,\n                templated_slice=patch.templated_slice,\n                patch_category=patch.patch_category,\n                fixed_raw=patch.fixed_raw,\n                templated_str=self.templated_file.templated_str[patch.templated_slice],\n                source_str=self.templated_file.source_str[source_slice],\n            )\n\n            # Deal with the easy case of only literals\n            if set(local_type_list) == {\"literal\"}:\n                linter_logger.info(\n                    \"      * Keeping patch on literal-only section: %s\", enriched_patch\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # Is it a zero length patch.\n            elif (\n                enriched_patch.source_slice.start == enriched_patch.source_slice.stop\n                and enriched_patch.source_slice.start == local_raw_slices[0].source_idx\n            ):\n                linter_logger.info(\n                    \"      * Keeping insertion patch on slice boundary: %s\",\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())\n            # If it's ONLY templated then we should skip it.\n            elif \"literal\" not in local_type_list:\n                linter_logger.info(\n                    \"      - Skipping patch over templated section: %s\", enriched_patch\n                )\n            # If we span more than two slices then we should just skip it. Too Hard.\n            elif len(local_raw_slices) > 2:\n                linter_logger.info(\n                    \"      - Skipping patch over more than two raw slices: %s\",\n                    enriched_patch,\n                )\n            # If it's an insertion (i.e. the string in the pre-fix template is '') then we\n            # won't be able to place it, so skip.\n            elif not enriched_patch.templated_str:  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping insertion patch in templated section: %s\",\n                    enriched_patch,\n                )\n            # If the string from the templated version isn't in the source, then we can't fix it.\n            elif (\n                enriched_patch.templated_str not in enriched_patch.source_str\n            ):  # pragma: no cover TODO?\n                linter_logger.info(\n                    \"      - Skipping edit patch on templated content: %s\",\n                    enriched_patch,\n                )\n            else:\n                # Identify all the places the string appears in the source content.\n                positions = list(\n                    findall(enriched_patch.templated_str, enriched_patch.source_str)\n                )\n                if len(positions) != 1:\n                    linter_logger.debug(\n                        \"        - Skipping edit patch on non-unique templated content: %s\",\n                        enriched_patch,\n                    )\n                    continue\n                # We have a single occurrence of the thing we want to patch. This\n                # means we can use its position to place our patch.\n                new_source_slice = slice(  # pragma: no cover\n                    enriched_patch.source_slice.start + positions[0],\n                    enriched_patch.source_slice.start\n                    + positions[0]\n                    + len(enriched_patch.templated_str),\n                )\n                enriched_patch = EnrichedFixPatch(  # pragma: no cover\n                    source_slice=new_source_slice,\n                    templated_slice=enriched_patch.templated_slice,\n                    patch_category=enriched_patch.patch_category,\n                    fixed_raw=enriched_patch.fixed_raw,\n                    templated_str=enriched_patch.templated_str,\n                    source_str=enriched_patch.source_str,\n                )\n                linter_logger.debug(  # pragma: no cover\n                    \"      * Keeping Tricky Case. Positions: %s, New Slice: %s, Patch: %s\",\n                    positions,\n                    new_source_slice,\n                    enriched_patch,\n                )\n                filtered_source_patches.append(enriched_patch)  # pragma: no cover\n                dedupe_buffer.append(enriched_patch.dedupe_tuple())  # pragma: no cover\n                continue  # pragma: no cover\n\n        # Sort the patches before building up the file.\n        filtered_source_patches = sorted(\n            filtered_source_patches, key=lambda x: x.source_slice.start\n        )\n        # We now slice up the file using the patches and any source only slices.\n        # This gives us regions to apply changes to.\n        slice_buff = []\n        source_idx = 0\n        for patch in filtered_source_patches:\n            # Are there templated slices at or before the start of this patch?\n            while (\n                source_only_slices\n                and source_only_slices[0].source_idx < patch.source_slice.start\n            ):\n                next_so_slice = source_only_slices.pop(0).source_slice()\n                # Add a pre-slice before the next templated slices if needed.\n                if next_so_slice.start > source_idx:\n                    slice_buff.append(slice(source_idx, next_so_slice.start))\n                # Add the templated slice.\n                slice_buff.append(next_so_slice)\n                source_idx = next_so_slice.stop\n\n            # Is there a gap between current position and this patch?\n            if patch.source_slice.start > source_idx:\n                # Add a slice up to this patch.\n                slice_buff.append(slice(source_idx, patch.source_slice.start))\n\n            # Is this patch covering an area we've already covered?\n            if patch.source_slice.start < source_idx:\n                linter_logger.info(\n                    \"Skipping overlapping patch at Index %s, Patch: %s\",\n                    source_idx,\n                    patch,\n                )\n                # Ignore the patch for now...\n                continue\n\n            # Add this patch.\n            slice_buff.append(patch.source_slice)\n            source_idx = patch.source_slice.stop\n        # Add a tail slice.\n        if source_idx < len(self.templated_file.source_str):\n            slice_buff.append(slice(source_idx, len(self.templated_file.source_str)))\n\n        linter_logger.debug(\"Final slice buffer: %s\", slice_buff)\n\n        # Iterate through the patches, building up the new string.\n        str_buff = \"\"\n        for source_slice in slice_buff:\n            # Is it one in the patch buffer:\n            for patch in filtered_source_patches:\n                if patch.source_slice == source_slice:\n                    # Use the patched version\n                    linter_logger.debug(\n                        \"%-30s    %s    %r > %r\",\n                        f\"Appending {patch.patch_category} Patch:\",\n                        patch.source_slice,\n                        patch.source_str,\n                        patch.fixed_raw,\n                    )\n                    str_buff += patch.fixed_raw\n                    break\n            else:\n                # Use the raw string\n                linter_logger.debug(\n                    \"Appending Raw:                    %s     %r\",\n                    source_slice,\n                    self.templated_file.source_str[source_slice],\n                )\n                str_buff += self.templated_file.source_str[source_slice]\n\n        # The success metric here is whether anything ACTUALLY changed.\n        return str_buff, str_buff != original_source\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "fix_string", "self", "tuple", "any", "bool", "obtain", "the", "changes", "to", "a", "path", "as", "a", "string", "we", "use", "the", "source", "mapping", "features", "of", "templatedfile", "to", "generate", "a", "list", "of", "patches", "which", "cover", "the", "non", "templated", "parts", "of", "the", "file", "and", "refer", "back", "to", "the", "locations", "in", "the", "original", "file", "nb", "this", "is", "much", "faster", "than", "the", "original", "approach", "using", "difflib", "in", "pre", "0", "4", "0", "there", "is", "an", "important", "distinction", "here", "between", "slices", "and", "segments", "a", "slice", "is", "a", "portion", "of", "a", "file", "which", "is", "determined", "by", "the", "templater", "based", "on", "which", "portions", "of", "the", "source", "file", "are", "templated", "or", "not", "and", "therefore", "before", "lexing", "and", "so", "is", "completely", "dialect", "agnostic", "a", "segment", "is", "determined", "by", "the", "lexer", "from", "portions", "of", "strings", "after", "templating", "linter_logger", "debug", "original", "tree", "r", "self", "templated_file", "templated_str", "assert", "self", "tree", "linter_logger", "debug", "fixed", "tree", "r", "self", "tree", "raw", "the", "sliced", "file", "is", "contiguous", "in", "the", "templated", "space", "nb", "it", "has", "gaps", "and", "repeats", "in", "the", "source", "space", "it", "s", "also", "not", "the", "fixed", "file", "either", "linter_logger", "debug", "templated", "file", "for", "idx", "file_slice", "in", "enumerate", "self", "templated_file", "sliced_file", "t_str", "self", "templated_file", "templated_str", "file_slice", "templated_slice", "s_str", "self", "templated_file", "source_str", "file_slice", "source_slice", "if", "t_str", "s_str", "linter_logger", "debug", "file", "slice", "s", "r", "invariant", "idx", "file_slice", "else", "linter_logger", "debug", "file", "slice", "s", "r", "idx", "file_slice", "linter_logger", "debug", "t", "t", "ttemplated", "r", "tsource", "r", "t_str", "s_str", "original_source", "self", "templated_file", "source_str", "make", "sure", "no", "patches", "overlap", "and", "divide", "up", "the", "source", "file", "into", "slices", "any", "template", "tags", "in", "the", "source", "file", "are", "off", "limits", "source_only_slices", "self", "templated_file", "source_only_slices", "linter_logger", "debug", "source", "only", "slices", "s", "source_only_slices", "iterate", "patches", "filtering", "and", "translating", "as", "we", "go", "linter_logger", "debug", "beginning", "patch", "iteration", "filtered_source_patches", "dedupe_buffer", "we", "use", "enumerate", "so", "that", "we", "get", "an", "index", "for", "each", "patch", "this", "is", "entirely", "so", "when", "debugging", "logs", "we", "can", "find", "a", "given", "patch", "again", "patch", "union", "enrichedfixpatch", "fixpatch", "for", "idx", "patch", "in", "enumerate", "self", "tree", "iter_patches", "templated_str", "self", "templated_file", "templated_str", "linter_logger", "debug", "s", "yielded", "patch", "s", "idx", "patch", "self", "_log_hints", "patch", "self", "templated_file", "attempt", "to", "convert", "to", "source", "space", "try", "source_slice", "self", "templated_file", "templated_slice_to_source_slice", "patch", "templated_slice", "except", "valueerror", "linter_logger", "info", "skipping", "source", "space", "value", "error", "i", "e", "attempted", "insertion", "within", "templated", "section", "if", "we", "try", "and", "slice", "within", "a", "templated", "section", "then", "we", "may", "fail", "in", "which", "case", "we", "should", "skip", "this", "patch", "continue", "check", "for", "duplicates", "dedupe_tuple", "source_slice", "patch", "fixed_raw", "if", "dedupe_tuple", "in", "dedupe_buffer", "linter_logger", "info", "skipping", "source", "space", "duplicate", "s", "dedupe_tuple", "continue", "we", "now", "evaluate", "patches", "in", "the", "source", "space", "for", "whether", "they", "overlap", "or", "disrupt", "any", "templated", "sections", "the", "intent", "here", "is", "that", "unless", "explicitly", "stated", "a", "fix", "should", "never", "disrupt", "a", "templated", "section", "note", "we", "rely", "here", "on", "the", "patches", "being", "sorted", "todo", "implement", "a", "mechanism", "for", "doing", "templated", "section", "fixes", "for", "now", "it", "s", "just", "not", "allowed", "get", "the", "affected", "raw", "slices", "local_raw_slices", "self", "templated_file", "raw_slices_spanning_source_slice", "source_slice", "local_type_list", "slc", "slice_type", "for", "slc", "in", "local_raw_slices", "enriched_patch", "enrichedfixpatch", "source_slice", "source_slice", "templated_slice", "patch", "templated_slice", "patch_category", "patch", "patch_category", "fixed_raw", "patch", "fixed_raw", "templated_str", "self", "templated_file", "templated_str", "patch", "templated_slice", "source_str", "self", "templated_file", "source_str", "source_slice", "deal", "with", "the", "easy", "case", "of", "only", "literals", "if", "set", "local_type_list", "literal", "linter_logger", "info", "keeping", "patch", "on", "literal", "only", "section", "s", "enriched_patch", "filtered_source_patches", "append", "enriched_patch", "dedupe_buffer", "append", "enriched_patch", "dedupe_tuple", "is", "it", "a", "zero", "length", "patch", "elif", "enriched_patch", "source_slice", "start", "enriched_patch", "source_slice", "stop", "and", "enriched_patch", "source_slice", "start", "local_raw_slices", "0", "source_idx", "linter_logger", "info", "keeping", "insertion", "patch", "on", "slice", "boundary", "s", "enriched_patch", "filtered_source_patches", "append", "enriched_patch", "dedupe_buffer", "append", "enriched_patch", "dedupe_tuple", "if", "it", "s", "only", "templated", "then", "we", "should", "skip", "it", "elif", "literal", "not", "in", "local_type_list", "linter_logger", "info", "skipping", "patch", "over", "templated", "section", "s", "enriched_patch", "if", "we", "span", "more", "than", "two", "slices", "then", "we", "should", "just", "skip", "it", "too", "hard", "elif", "len", "local_raw_slices", "2", "linter_logger", "info", "skipping", "patch", "over", "more", "than", "two", "raw", "slices", "s", "enriched_patch", "if", "it", "s", "an", "insertion", "i", "e", "the", "string", "in", "the", "pre", "fix", "template", "is", "then", "we", "won", "t", "be", "able", "to", "place", "it", "so", "skip", "elif", "not", "enriched_patch", "templated_str", "pragma", "no", "cover", "todo", "linter_logger", "info", "skipping", "insertion", "patch", "in", "templated", "section", "s", "enriched_patch", "if", "the", "string", "from", "the", "templated", "version", "isn", "t", "in", "the", "source", "then", "we", "can", "t", "fix", "it", "elif", "enriched_patch", "templated_str", "not", "in", "enriched_patch", "source_str", "pragma", "no", "cover", "todo", "linter_logger", "info", "skipping", "edit", "patch", "on", "templated", "content", "s", "enriched_patch", "else", "identify", "all", "the", "places", "the", "string", "appears", "in", "the", "source", "content", "positions", "list", "findall", "enriched_patch", "templated_str", "enriched_patch", "source_str", "if", "len", "positions", "1", "linter_logger", "debug", "skipping", "edit", "patch", "on", "non", "unique", "templated", "content", "s", "enriched_patch", "continue", "we", "have", "a", "single", "occurrence", "of", "the", "thing", "we", "want", "to", "patch", "this", "means", "we", "can", "use", "its", "position", "to", "place", "our", "patch", "new_source_slice", "slice", "pragma", "no", "cover", "enriched_patch", "source_slice", "start", "positions", "0", "enriched_patch", "source_slice", "start", "positions", "0", "len", "enriched_patch", "templated_str", "enriched_patch", "enrichedfixpatch", "pragma", "no", "cover", "source_slice", "new_source_slice", "templated_slice", "enriched_patch", "templated_slice", "patch_category", "enriched_patch", "patch_category", "fixed_raw", "enriched_patch", "fixed_raw", "templated_str", "enriched_patch", "templated_str", "source_str", "enriched_patch", "source_str", "linter_logger", "debug", "pragma", "no", "cover", "keeping", "tricky", "case", "positions", "s", "new", "slice", "s", "patch", "s", "positions", "new_source_slice", "enriched_patch", "filtered_source_patches", "append", "enriched_patch", "pragma", "no", "cover", "dedupe_buffer", "append", "enriched_patch", "dedupe_tuple", "pragma", "no", "cover", "continue", "pragma", "no", "cover", "sort", "the", "patches", "before", "building", "up", "the", "file", "filtered_source_patches", "sorted", "filtered_source_patches", "key", "lambda", "x", "x", "source_slice", "start", "we", "now", "slice", "up", "the", "file", "using", "the", "patches", "and", "any", "source", "only", "slices", "this", "gives", "us", "regions", "to", "apply", "changes", "to", "slice_buff", "source_idx", "0", "for", "patch", "in", "filtered_source_patches", "are", "there", "templated", "slices", "at", "or", "before", "the", "start", "of", "this", "patch", "while", "source_only_slices", "and", "source_only_slices", "0", "source_idx", "patch", "source_slice", "start", "next_so_slice", "source_only_slices", "pop", "0", "source_slice", "add", "a", "pre", "slice", "before", "the", "next", "templated", "slices", "if", "needed", "if", "next_so_slice", "start", "source_idx", "slice_buff", "append", "slice", "source_idx", "next_so_slice", "start", "add", "the", "templated", "slice", "slice_buff", "append", "next_so_slice", "source_idx", "next_so_slice", "stop", "is", "there", "a", "gap", "between", "current", "position", "and", "this", "patch", "if", "patch", "source_slice", "start", "source_idx", "add", "a", "slice", "up", "to", "this", "patch", "slice_buff", "append", "slice", "source_idx", "patch", "source_slice", "start", "is", "this", "patch", "covering", "an", "area", "we", "ve", "already", "covered", "if", "patch", "source_slice", "start", "source_idx", "linter_logger", "info", "skipping", "overlapping", "patch", "at", "index", "s", "patch", "s", "source_idx", "patch", "ignore", "the", "patch", "for", "now", "continue", "add", "this", "patch", "slice_buff", "append", "patch", "source_slice", "source_idx", "patch", "source_slice", "stop", "add", "a", "tail", "slice", "if", "source_idx", "len", "self", "templated_file", "source_str", "slice_buff", "append", "slice", "source_idx", "len", "self", "templated_file", "source_str", "linter_logger", "debug", "final", "slice", "buffer", "s", "slice_buff", "iterate", "through", "the", "patches", "building", "up", "the", "new", "string", "str_buff", "for", "source_slice", "in", "slice_buff", "is", "it", "one", "in", "the", "patch", "buffer", "for", "patch", "in", "filtered_source_patches", "if", "patch", "source_slice", "source_slice", "use", "the", "patched", "version", "linter_logger", "debug", "30s", "s", "r", "r", "f", "appending", "patch", "patch_category", "patch", "patch", "source_slice", "patch", "source_str", "patch", "fixed_raw", "str_buff", "patch", "fixed_raw", "break", "else", "use", "the", "raw", "string", "linter_logger", "debug", "appending", "raw", "s", "r", "source_slice", "self", "templated_file", "source_str", "source_slice", "str_buff", "self", "templated_file", "source_str", "source_slice", "the", "success", "metric", "here", "is", "whether", "anything", "actually", "changed", "return", "str_buff", "str_buff", "original_source"], "doc_len": 1136}
{"doc_id": "src/sqlfluff/core/linter/linted_file.py::LintedFile.persist_tree", "file_path": "src/sqlfluff/core/linter/linted_file.py", "class_name": "LintedFile", "func_name": "persist_tree", "text": "文件路径: src/sqlfluff/core/linter/linted_file.py, 类名: LintedFile\n    def persist_tree(self, suffix: str = \"\") -> bool:\n        \"\"\"Persist changes to the given path.\"\"\"\n        write_buff, success = self.fix_string()\n\n        if success:\n            fname = self.path\n            # If there is a suffix specified, then use it.s\n            if suffix:\n                root, ext = os.path.splitext(fname)\n                fname = root + suffix + ext\n            # Actually write the file.\n            with open(fname, \"w\", encoding=self.encoding) as f:\n                f.write(write_buff)\n        return success\n", "tokens": ["src", "sqlfluff", "core", "linter", "linted_file", "py", "lintedfile", "def", "persist_tree", "self", "suffix", "str", "bool", "persist", "changes", "to", "the", "given", "path", "write_buff", "success", "self", "fix_string", "if", "success", "fname", "self", "path", "if", "there", "is", "a", "suffix", "specified", "then", "use", "it", "s", "if", "suffix", "root", "ext", "os", "path", "splitext", "fname", "fname", "root", "suffix", "ext", "actually", "write", "the", "file", "with", "open", "fname", "w", "encoding", "self", "encoding", "as", "f", "f", "write", "write_buff", "return", "success"], "doc_len": 68}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.__init__", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        formatter: Any = None,\n        dialect: Optional[str] = None,\n        rules: Optional[Union[str, List[str]]] = None,\n        user_rules: Optional[Union[str, List[str]]] = None,\n    ) -> None:\n        # Store the config object\n        self.config = FluffConfig.from_kwargs(\n            config=config, dialect=dialect, rules=rules\n        )\n        # Get the dialect and templater\n        self.dialect = self.config.get(\"dialect_obj\")\n        self.templater = self.config.get(\"templater_obj\")\n        # Store the formatter for output\n        self.formatter = formatter\n        # Store references to user rule classes\n        self.user_rules = user_rules or []\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "__init__", "self", "config", "optional", "fluffconfig", "none", "formatter", "any", "none", "dialect", "optional", "str", "none", "rules", "optional", "union", "str", "list", "str", "none", "user_rules", "optional", "union", "str", "list", "str", "none", "none", "store", "the", "config", "object", "self", "config", "fluffconfig", "from_kwargs", "config", "config", "dialect", "dialect", "rules", "rules", "get", "the", "dialect", "and", "templater", "self", "dialect", "self", "config", "get", "dialect_obj", "self", "templater", "self", "config", "get", "templater_obj", "store", "the", "formatter", "for", "output", "self", "formatter", "formatter", "store", "references", "to", "user", "rule", "classes", "self", "user_rules", "user_rules", "or"], "doc_len": 85}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.get_ruleset", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "get_ruleset", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def get_ruleset(self, config: Optional[FluffConfig] = None) -> List[BaseRule]:\n        \"\"\"Get hold of a set of rules.\"\"\"\n        rs = get_ruleset()\n        # Register any user rules\n        for rule in self.user_rules:\n            rs.register(rule)\n        cfg = config or self.config\n        return rs.get_rulelist(config=cfg)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "get_ruleset", "self", "config", "optional", "fluffconfig", "none", "list", "baserule", "get", "hold", "of", "a", "set", "of", "rules", "rs", "get_ruleset", "register", "any", "user", "rules", "for", "rule", "in", "self", "user_rules", "rs", "register", "rule", "cfg", "config", "or", "self", "config", "return", "rs", "get_rulelist", "config", "cfg"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.rule_tuples", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "rule_tuples", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def rule_tuples(self) -> List[RuleTuple]:\n        \"\"\"A simple pass through to access the rule tuples of the rule set.\"\"\"\n        rs = self.get_ruleset()\n        return [RuleTuple(rule.code, rule.description) for rule in rs]\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "rule_tuples", "self", "list", "ruletuple", "a", "simple", "pass", "through", "to", "access", "the", "rule", "tuples", "of", "the", "rule", "set", "rs", "self", "get_ruleset", "return", "ruletuple", "rule", "code", "rule", "description", "for", "rule", "in", "rs"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter._load_raw_file_and_config", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "_load_raw_file_and_config", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def _load_raw_file_and_config(fname, root_config):\n        \"\"\"Load a raw file and the associated config.\"\"\"\n        file_config = root_config.make_child_from_path(fname)\n        encoding = get_encoding(fname=fname, config=file_config)\n        with open(fname, encoding=encoding, errors=\"backslashreplace\") as target_file:\n            raw_file = target_file.read()\n        # Scan the raw file for config commands.\n        file_config.process_raw_file_for_config(raw_file)\n        # Return the raw file and config\n        return raw_file, file_config, encoding\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "_load_raw_file_and_config", "fname", "root_config", "load", "a", "raw", "file", "and", "the", "associated", "config", "file_config", "root_config", "make_child_from_path", "fname", "encoding", "get_encoding", "fname", "fname", "config", "file_config", "with", "open", "fname", "encoding", "encoding", "errors", "backslashreplace", "as", "target_file", "raw_file", "target_file", "read", "scan", "the", "raw", "file", "for", "config", "commands", "file_config", "process_raw_file_for_config", "raw_file", "return", "the", "raw", "file", "and", "config", "return", "raw_file", "file_config", "encoding"], "doc_len": 61}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter._lex_templated_file", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "_lex_templated_file", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def _lex_templated_file(\n        templated_file: TemplatedFile, config: FluffConfig\n    ) -> Tuple[Optional[Sequence[BaseSegment]], List[SQLLexError], FluffConfig]:\n        \"\"\"Lex a templated file.\n\n        NOTE: This potentially mutates the config, so make sure to\n        use the returned one.\n        \"\"\"\n        violations = []\n        linter_logger.info(\"LEXING RAW (%s)\", templated_file.fname)\n        # Get the lexer\n        lexer = Lexer(config=config)\n        # Lex the file and log any problems\n        try:\n            tokens, lex_vs = lexer.lex(templated_file)\n            # We might just get the violations as a list\n            violations += lex_vs\n            linter_logger.info(\n                \"Lexed tokens: %s\", [seg.raw for seg in tokens] if tokens else None\n            )\n        except SQLLexError as err:\n            linter_logger.info(\"LEXING FAILED! (%s): %s\", templated_file.fname, err)\n            violations.append(err)\n            return None, violations, config\n\n        if not tokens:  # pragma: no cover TODO?\n            return None, violations, config\n\n        # Check that we've got sensible indentation from the lexer.\n        # We might need to suppress if it's a complicated file.\n        templating_blocks_indent = config.get(\"template_blocks_indent\", \"indentation\")\n        if isinstance(templating_blocks_indent, str):\n            force_block_indent = templating_blocks_indent.lower().strip() == \"force\"\n        else:\n            force_block_indent = False\n        templating_blocks_indent = bool(templating_blocks_indent)\n        # If we're forcing it through we don't check.\n        if templating_blocks_indent and not force_block_indent:\n            indent_balance = sum(\n                getattr(elem, \"indent_val\", 0)\n                for elem in cast(Tuple[BaseSegment, ...], tokens)\n            )\n            if indent_balance != 0:\n                linter_logger.debug(\n                    \"Indent balance test failed for %r. Template indents will not be linted for this file.\",\n                    templated_file.fname,\n                )\n                # Don't enable the templating blocks.\n                templating_blocks_indent = False\n\n        # The file will have been lexed without config, so check all indents\n        # are enabled.\n        new_tokens = []\n        for token in cast(Tuple[BaseSegment, ...], tokens):\n            if token.is_meta:\n                token = cast(MetaSegment, token)\n                if token.indent_val != 0:\n                    # Don't allow it if we're not linting templating block indents.\n                    if not templating_blocks_indent:\n                        continue\n            new_tokens.append(token)\n        # Return new buffer\n        return new_tokens, violations, config\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "_lex_templated_file", "templated_file", "templatedfile", "config", "fluffconfig", "tuple", "optional", "sequence", "basesegment", "list", "sqllexerror", "fluffconfig", "lex", "a", "templated", "file", "note", "this", "potentially", "mutates", "the", "config", "so", "make", "sure", "to", "use", "the", "returned", "one", "violations", "linter_logger", "info", "lexing", "raw", "s", "templated_file", "fname", "get", "the", "lexer", "lexer", "lexer", "config", "config", "lex", "the", "file", "and", "log", "any", "problems", "try", "tokens", "lex_vs", "lexer", "lex", "templated_file", "we", "might", "just", "get", "the", "violations", "as", "a", "list", "violations", "lex_vs", "linter_logger", "info", "lexed", "tokens", "s", "seg", "raw", "for", "seg", "in", "tokens", "if", "tokens", "else", "none", "except", "sqllexerror", "as", "err", "linter_logger", "info", "lexing", "failed", "s", "s", "templated_file", "fname", "err", "violations", "append", "err", "return", "none", "violations", "config", "if", "not", "tokens", "pragma", "no", "cover", "todo", "return", "none", "violations", "config", "check", "that", "we", "ve", "got", "sensible", "indentation", "from", "the", "lexer", "we", "might", "need", "to", "suppress", "if", "it", "s", "a", "complicated", "file", "templating_blocks_indent", "config", "get", "template_blocks_indent", "indentation", "if", "isinstance", "templating_blocks_indent", "str", "force_block_indent", "templating_blocks_indent", "lower", "strip", "force", "else", "force_block_indent", "false", "templating_blocks_indent", "bool", "templating_blocks_indent", "if", "we", "re", "forcing", "it", "through", "we", "don", "t", "check", "if", "templating_blocks_indent", "and", "not", "force_block_indent", "indent_balance", "sum", "getattr", "elem", "indent_val", "0", "for", "elem", "in", "cast", "tuple", "basesegment", "tokens", "if", "indent_balance", "0", "linter_logger", "debug", "indent", "balance", "test", "failed", "for", "r", "template", "indents", "will", "not", "be", "linted", "for", "this", "file", "templated_file", "fname", "don", "t", "enable", "the", "templating", "blocks", "templating_blocks_indent", "false", "the", "file", "will", "have", "been", "lexed", "without", "config", "so", "check", "all", "indents", "are", "enabled", "new_tokens", "for", "token", "in", "cast", "tuple", "basesegment", "tokens", "if", "token", "is_meta", "token", "cast", "metasegment", "token", "if", "token", "indent_val", "0", "don", "t", "allow", "it", "if", "we", "re", "not", "linting", "templating", "block", "indents", "if", "not", "templating_blocks_indent", "continue", "new_tokens", "append", "token", "return", "new", "buffer", "return", "new_tokens", "violations", "config"], "doc_len": 281}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter._parse_tokens", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "_parse_tokens", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def _parse_tokens(\n        tokens: Sequence[BaseSegment],\n        config: FluffConfig,\n        recurse: bool = True,\n        fname: Optional[str] = None,\n    ) -> Tuple[Optional[BaseSegment], List[SQLParseError]]:\n        parser = Parser(config=config)\n        violations = []\n        # Parse the file and log any problems\n        try:\n            parsed: Optional[BaseSegment] = parser.parse(\n                tokens, recurse=recurse, fname=fname\n            )\n        except SQLParseError as err:\n            linter_logger.info(\"PARSING FAILED! : %s\", err)\n            violations.append(err)\n            return None, violations\n\n        if parsed:\n            linter_logger.info(\"\\n###\\n#\\n# {}\\n#\\n###\".format(\"Parsed Tree:\"))\n            linter_logger.info(\"\\n\" + parsed.stringify())\n            # We may succeed parsing, but still have unparsable segments. Extract them here.\n            for unparsable in parsed.iter_unparsables():\n                # No exception has been raised explicitly, but we still create one here\n                # so that we can use the common interface\n                violations.append(\n                    SQLParseError(\n                        \"Line {0[0]}, Position {0[1]}: Found unparsable section: {1!r}\".format(\n                            unparsable.pos_marker.working_loc,\n                            unparsable.raw\n                            if len(unparsable.raw) < 40\n                            else unparsable.raw[:40] + \"...\",\n                        ),\n                        segment=unparsable,\n                    )\n                )\n                linter_logger.info(\"Found unparsable segment...\")\n                linter_logger.info(unparsable.stringify())\n        return parsed, violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "_parse_tokens", "tokens", "sequence", "basesegment", "config", "fluffconfig", "recurse", "bool", "true", "fname", "optional", "str", "none", "tuple", "optional", "basesegment", "list", "sqlparseerror", "parser", "parser", "config", "config", "violations", "parse", "the", "file", "and", "log", "any", "problems", "try", "parsed", "optional", "basesegment", "parser", "parse", "tokens", "recurse", "recurse", "fname", "fname", "except", "sqlparseerror", "as", "err", "linter_logger", "info", "parsing", "failed", "s", "err", "violations", "append", "err", "return", "none", "violations", "if", "parsed", "linter_logger", "info", "n", "n", "n", "n", "n", "format", "parsed", "tree", "linter_logger", "info", "n", "parsed", "stringify", "we", "may", "succeed", "parsing", "but", "still", "have", "unparsable", "segments", "extract", "them", "here", "for", "unparsable", "in", "parsed", "iter_unparsables", "no", "exception", "has", "been", "raised", "explicitly", "but", "we", "still", "create", "one", "here", "so", "that", "we", "can", "use", "the", "common", "interface", "violations", "append", "sqlparseerror", "line", "0", "0", "position", "0", "1", "found", "unparsable", "section", "1", "r", "format", "unparsable", "pos_marker", "working_loc", "unparsable", "raw", "if", "len", "unparsable", "raw", "40", "else", "unparsable", "raw", "40", "segment", "unparsable", "linter_logger", "info", "found", "unparsable", "segment", "linter_logger", "info", "unparsable", "stringify", "return", "parsed", "violations"], "doc_len": 162}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.parse_noqa", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "parse_noqa", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def parse_noqa(comment: str, line_no: int):\n        \"\"\"Extract ignore mask entries from a comment string.\"\"\"\n        # Also trim any whitespace afterward\n        if comment.startswith(\"noqa\"):\n            # This is an ignore identifier\n            comment_remainder = comment[4:]\n            if comment_remainder:\n                if not comment_remainder.startswith(\":\"):\n                    return SQLParseError(\n                        \"Malformed 'noqa' section. Expected 'noqa: <rule>[,...]\",\n                        line_no=line_no,\n                    )\n                comment_remainder = comment_remainder[1:].strip()\n                if comment_remainder:\n                    action: Optional[str]\n                    if \"=\" in comment_remainder:\n                        action, rule_part = comment_remainder.split(\"=\", 1)\n                        if action not in {\"disable\", \"enable\"}:  # pragma: no cover\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    else:\n                        action = None\n                        rule_part = comment_remainder\n                        if rule_part in {\"disable\", \"enable\"}:\n                            return SQLParseError(\n                                \"Malformed 'noqa' section. \"\n                                \"Expected 'noqa: enable=<rule>[,...] | all' \"\n                                \"or 'noqa: disable=<rule>[,...] | all\",\n                                line_no=line_no,\n                            )\n                    rules: Optional[Tuple[str, ...]]\n                    if rule_part != \"all\":\n                        rules = tuple(r.strip() for r in rule_part.split(\",\"))\n                    else:\n                        rules = None\n                    return NoQaDirective(line_no, rules, action)\n            return NoQaDirective(line_no, None, None)\n        return None\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "parse_noqa", "comment", "str", "line_no", "int", "extract", "ignore", "mask", "entries", "from", "a", "comment", "string", "also", "trim", "any", "whitespace", "afterward", "if", "comment", "startswith", "noqa", "this", "is", "an", "ignore", "identifier", "comment_remainder", "comment", "4", "if", "comment_remainder", "if", "not", "comment_remainder", "startswith", "return", "sqlparseerror", "malformed", "noqa", "section", "expected", "noqa", "rule", "line_no", "line_no", "comment_remainder", "comment_remainder", "1", "strip", "if", "comment_remainder", "action", "optional", "str", "if", "in", "comment_remainder", "action", "rule_part", "comment_remainder", "split", "1", "if", "action", "not", "in", "disable", "enable", "pragma", "no", "cover", "return", "sqlparseerror", "malformed", "noqa", "section", "expected", "noqa", "enable", "rule", "all", "or", "noqa", "disable", "rule", "all", "line_no", "line_no", "else", "action", "none", "rule_part", "comment_remainder", "if", "rule_part", "in", "disable", "enable", "return", "sqlparseerror", "malformed", "noqa", "section", "expected", "noqa", "enable", "rule", "all", "or", "noqa", "disable", "rule", "all", "line_no", "line_no", "rules", "optional", "tuple", "str", "if", "rule_part", "all", "rules", "tuple", "r", "strip", "for", "r", "in", "rule_part", "split", "else", "rules", "none", "return", "noqadirective", "line_no", "rules", "action", "return", "noqadirective", "line_no", "none", "none", "return", "none"], "doc_len": 155}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.remove_templated_errors", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "remove_templated_errors", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def remove_templated_errors(\n        linting_errors: List[SQLBaseError],\n    ) -> List[SQLBaseError]:\n        \"\"\"Filter a list of lint errors, removing those which only occur in templated slices.\"\"\"\n        # Filter out any linting errors in templated sections if relevant.\n        result: List[SQLBaseError] = []\n        for e in linting_errors:\n            if isinstance(e, SQLLintError):\n                if (\n                    # Is it in a literal section?\n                    e.segment.pos_marker.is_literal()\n                    # Is it a rule that is designed to work on templated sections?\n                    or e.rule.targets_templated\n                ):\n                    result.append(e)\n            else:\n                # If it's another type, just keep it. (E.g. SQLParseError from\n                # malformed \"noqa\" comment).\n                result.append(e)\n        return result\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "remove_templated_errors", "linting_errors", "list", "sqlbaseerror", "list", "sqlbaseerror", "filter", "a", "list", "of", "lint", "errors", "removing", "those", "which", "only", "occur", "in", "templated", "slices", "filter", "out", "any", "linting", "errors", "in", "templated", "sections", "if", "relevant", "result", "list", "sqlbaseerror", "for", "e", "in", "linting_errors", "if", "isinstance", "e", "sqllinterror", "if", "is", "it", "in", "a", "literal", "section", "e", "segment", "pos_marker", "is_literal", "is", "it", "a", "rule", "that", "is", "designed", "to", "work", "on", "templated", "sections", "or", "e", "rule", "targets_templated", "result", "append", "e", "else", "if", "it", "s", "another", "type", "just", "keep", "it", "e", "g", "sqlparseerror", "from", "malformed", "noqa", "comment", "result", "append", "e", "return", "result"], "doc_len": 100}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter._warn_unfixable", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "_warn_unfixable", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def _warn_unfixable(code: str):\n        linter_logger.warning(\n            f\"One fix for {code} not applied, it would re-cause the same error.\"\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "_warn_unfixable", "code", "str", "linter_logger", "warning", "f", "one", "fix", "for", "code", "not", "applied", "it", "would", "re", "cause", "the", "same", "error"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.parse_rendered", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "parse_rendered", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def parse_rendered(cls, rendered: RenderedFile, recurse: bool = True):\n        \"\"\"Parse a rendered file.\"\"\"\n        t0 = time.monotonic()\n        violations = cast(List[SQLBaseError], rendered.templater_violations)\n        tokens: Optional[Sequence[BaseSegment]]\n        if rendered.templated_file:\n            tokens, lvs, config = cls._lex_templated_file(\n                rendered.templated_file, rendered.config\n            )\n            violations += lvs\n        else:\n            tokens = None\n\n        t1 = time.monotonic()\n        linter_logger.info(\"PARSING (%s)\", rendered.fname)\n\n        if tokens:\n            parsed, pvs = cls._parse_tokens(\n                tokens, rendered.config, recurse=recurse, fname=rendered.fname\n            )\n            violations += pvs\n        else:\n            parsed = None\n\n        time_dict = {\n            **rendered.time_dict,\n            \"lexing\": t1 - t0,\n            \"parsing\": time.monotonic() - t1,\n        }\n        return ParsedString(\n            parsed,\n            violations,\n            time_dict,\n            rendered.templated_file,\n            rendered.config,\n            rendered.fname,\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "parse_rendered", "cls", "rendered", "renderedfile", "recurse", "bool", "true", "parse", "a", "rendered", "file", "t0", "time", "monotonic", "violations", "cast", "list", "sqlbaseerror", "rendered", "templater_violations", "tokens", "optional", "sequence", "basesegment", "if", "rendered", "templated_file", "tokens", "lvs", "config", "cls", "_lex_templated_file", "rendered", "templated_file", "rendered", "config", "violations", "lvs", "else", "tokens", "none", "t1", "time", "monotonic", "linter_logger", "info", "parsing", "s", "rendered", "fname", "if", "tokens", "parsed", "pvs", "cls", "_parse_tokens", "tokens", "rendered", "config", "recurse", "recurse", "fname", "rendered", "fname", "violations", "pvs", "else", "parsed", "none", "time_dict", "rendered", "time_dict", "lexing", "t1", "t0", "parsing", "time", "monotonic", "t1", "return", "parsedstring", "parsed", "violations", "time_dict", "rendered", "templated_file", "rendered", "config", "rendered", "fname"], "doc_len": 98}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.extract_ignore_from_comment", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "extract_ignore_from_comment", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def extract_ignore_from_comment(cls, comment: RawSegment):\n        \"\"\"Extract ignore mask entries from a comment segment.\"\"\"\n        # Also trim any whitespace afterward\n        comment_content = comment.raw_trimmed().strip()\n        comment_line, _ = comment.pos_marker.source_position()\n        result = cls.parse_noqa(comment_content, comment_line)\n        if isinstance(result, SQLParseError):\n            result.segment = comment\n        return result\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "extract_ignore_from_comment", "cls", "comment", "rawsegment", "extract", "ignore", "mask", "entries", "from", "a", "comment", "segment", "also", "trim", "any", "whitespace", "afterward", "comment_content", "comment", "raw_trimmed", "strip", "comment_line", "_", "comment", "pos_marker", "source_position", "result", "cls", "parse_noqa", "comment_content", "comment_line", "if", "isinstance", "result", "sqlparseerror", "result", "segment", "comment", "return", "result"], "doc_len": 48}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.extract_ignore_mask", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "extract_ignore_mask", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def extract_ignore_mask(\n        cls, tree: BaseSegment\n    ) -> Tuple[List[NoQaDirective], List[SQLBaseError]]:\n        \"\"\"Look for inline ignore comments and return NoQaDirectives.\"\"\"\n        ignore_buff: List[NoQaDirective] = []\n        violations: List[SQLBaseError] = []\n        for comment in tree.recursive_crawl(\"comment\"):\n            if comment.name == \"inline_comment\":\n                ignore_entry = cls.extract_ignore_from_comment(comment)\n                if isinstance(ignore_entry, SQLParseError):\n                    violations.append(ignore_entry)\n                elif ignore_entry:\n                    ignore_buff.append(ignore_entry)\n        if ignore_buff:\n            linter_logger.info(\"Parsed noqa directives from file: %r\", ignore_buff)\n        return ignore_buff, violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "extract_ignore_mask", "cls", "tree", "basesegment", "tuple", "list", "noqadirective", "list", "sqlbaseerror", "look", "for", "inline", "ignore", "comments", "and", "return", "noqadirectives", "ignore_buff", "list", "noqadirective", "violations", "list", "sqlbaseerror", "for", "comment", "in", "tree", "recursive_crawl", "comment", "if", "comment", "name", "inline_comment", "ignore_entry", "cls", "extract_ignore_from_comment", "comment", "if", "isinstance", "ignore_entry", "sqlparseerror", "violations", "append", "ignore_entry", "elif", "ignore_entry", "ignore_buff", "append", "ignore_entry", "if", "ignore_buff", "linter_logger", "info", "parsed", "noqa", "directives", "from", "file", "r", "ignore_buff", "return", "ignore_buff", "violations"], "doc_len": 71}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_fix_parsed", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_fix_parsed", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_fix_parsed(\n        cls,\n        tree: BaseSegment,\n        config: FluffConfig,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n        formatter: Any = None,\n    ) -> Tuple[BaseSegment, List[SQLBaseError], List[NoQaDirective]]:\n        \"\"\"Lint and optionally fix a tree object.\"\"\"\n        # Keep track of the linting errors\n        all_linting_errors = []\n        # A placeholder for the fixes we had on the previous loop\n        last_fixes = None\n        # Keep a set of previous versions to catch infinite loops.\n        previous_versions = {tree.raw}\n\n        # If we are fixing then we want to loop up to the runaway_limit, otherwise just once for linting.\n        loop_limit = config.get(\"runaway_limit\") if fix else 1\n\n        # Dispatch the output for the lint header\n        if formatter:\n            formatter.dispatch_lint_header(fname)\n\n        # Look for comment segments which might indicate lines to ignore.\n        ignore_buff, ivs = cls.extract_ignore_mask(tree)\n        all_linting_errors += ivs\n\n        for loop in range(loop_limit):\n            changed = False\n            for crawler in rule_set:\n                # fixes should be a dict {} with keys edit, delete, create\n                # delete is just a list of segments to delete\n                # edit and create are list of tuples. The first element is the\n                # \"anchor\", the segment to look for either to edit or to insert BEFORE.\n                # The second is the element to insert or create.\n                linting_errors, _, fixes, _ = crawler.crawl(\n                    tree,\n                    ignore_mask=ignore_buff,\n                    dialect=config.get(\"dialect_obj\"),\n                    fname=fname,\n                    templated_file=templated_file,\n                )\n                all_linting_errors += linting_errors\n\n                if fix and fixes:\n                    linter_logger.info(f\"Applying Fixes [{crawler.code}]: {fixes}\")\n                    # Do some sanity checks on the fixes before applying.\n                    if fixes == last_fixes:  # pragma: no cover\n                        cls._warn_unfixable(crawler.code)\n                    else:\n                        last_fixes = fixes\n                        new_tree, _ = tree.apply_fixes(fixes)\n                        # Check for infinite loops\n                        if new_tree.raw not in previous_versions:\n                            # We've not seen this version of the file so far. Continue.\n                            tree = new_tree\n                            previous_versions.add(tree.raw)\n                            changed = True\n                            continue\n                        else:\n                            # Applying these fixes took us back to a state which we've\n                            # seen before. Abort.\n                            cls._warn_unfixable(crawler.code)\n\n            if loop == 0:\n                # Keep track of initial errors for reporting.\n                initial_linting_errors = all_linting_errors.copy()\n\n            if fix and not changed:\n                # We did not change the file. Either the file is clean (no fixes), or\n                # any fixes which are present will take us back to a previous state.\n                linter_logger.info(\n                    f\"Fix loop complete. Stability achieved after {loop}/{loop_limit} loops.\"\n                )\n                break\n        if fix and loop + 1 == loop_limit:\n            linter_logger.warning(f\"Loop limit on fixes reached [{loop_limit}].\")\n\n        if config.get(\"ignore_templated_areas\", default=True):\n            initial_linting_errors = cls.remove_templated_errors(initial_linting_errors)\n\n        return tree, initial_linting_errors, ignore_buff\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_fix_parsed", "cls", "tree", "basesegment", "config", "fluffconfig", "rule_set", "list", "baserule", "fix", "bool", "false", "fname", "optional", "str", "none", "templated_file", "optional", "templatedfile", "none", "formatter", "any", "none", "tuple", "basesegment", "list", "sqlbaseerror", "list", "noqadirective", "lint", "and", "optionally", "fix", "a", "tree", "object", "keep", "track", "of", "the", "linting", "errors", "all_linting_errors", "a", "placeholder", "for", "the", "fixes", "we", "had", "on", "the", "previous", "loop", "last_fixes", "none", "keep", "a", "set", "of", "previous", "versions", "to", "catch", "infinite", "loops", "previous_versions", "tree", "raw", "if", "we", "are", "fixing", "then", "we", "want", "to", "loop", "up", "to", "the", "runaway_limit", "otherwise", "just", "once", "for", "linting", "loop_limit", "config", "get", "runaway_limit", "if", "fix", "else", "1", "dispatch", "the", "output", "for", "the", "lint", "header", "if", "formatter", "formatter", "dispatch_lint_header", "fname", "look", "for", "comment", "segments", "which", "might", "indicate", "lines", "to", "ignore", "ignore_buff", "ivs", "cls", "extract_ignore_mask", "tree", "all_linting_errors", "ivs", "for", "loop", "in", "range", "loop_limit", "changed", "false", "for", "crawler", "in", "rule_set", "fixes", "should", "be", "a", "dict", "with", "keys", "edit", "delete", "create", "delete", "is", "just", "a", "list", "of", "segments", "to", "delete", "edit", "and", "create", "are", "list", "of", "tuples", "the", "first", "element", "is", "the", "anchor", "the", "segment", "to", "look", "for", "either", "to", "edit", "or", "to", "insert", "before", "the", "second", "is", "the", "element", "to", "insert", "or", "create", "linting_errors", "_", "fixes", "_", "crawler", "crawl", "tree", "ignore_mask", "ignore_buff", "dialect", "config", "get", "dialect_obj", "fname", "fname", "templated_file", "templated_file", "all_linting_errors", "linting_errors", "if", "fix", "and", "fixes", "linter_logger", "info", "f", "applying", "fixes", "crawler", "code", "fixes", "do", "some", "sanity", "checks", "on", "the", "fixes", "before", "applying", "if", "fixes", "last_fixes", "pragma", "no", "cover", "cls", "_warn_unfixable", "crawler", "code", "else", "last_fixes", "fixes", "new_tree", "_", "tree", "apply_fixes", "fixes", "check", "for", "infinite", "loops", "if", "new_tree", "raw", "not", "in", "previous_versions", "we", "ve", "not", "seen", "this", "version", "of", "the", "file", "so", "far", "continue", "tree", "new_tree", "previous_versions", "add", "tree", "raw", "changed", "true", "continue", "else", "applying", "these", "fixes", "took", "us", "back", "to", "a", "state", "which", "we", "ve", "seen", "before", "abort", "cls", "_warn_unfixable", "crawler", "code", "if", "loop", "0", "keep", "track", "of", "initial", "errors", "for", "reporting", "initial_linting_errors", "all_linting_errors", "copy", "if", "fix", "and", "not", "changed", "we", "did", "not", "change", "the", "file", "either", "the", "file", "is", "clean", "no", "fixes", "or", "any", "fixes", "which", "are", "present", "will", "take", "us", "back", "to", "a", "previous", "state", "linter_logger", "info", "f", "fix", "loop", "complete", "stability", "achieved", "after", "loop", "loop_limit", "loops", "break", "if", "fix", "and", "loop", "1", "loop_limit", "linter_logger", "warning", "f", "loop", "limit", "on", "fixes", "reached", "loop_limit", "if", "config", "get", "ignore_templated_areas", "default", "true", "initial_linting_errors", "cls", "remove_templated_errors", "initial_linting_errors", "return", "tree", "initial_linting_errors", "ignore_buff"], "doc_len": 392}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_parsed", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_parsed", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_parsed(\n        cls,\n        parsed: ParsedString,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        formatter: Any = None,\n        encoding: str = \"utf8\",\n    ):\n        \"\"\"Lint a ParsedString and return a LintedFile.\"\"\"\n        violations = parsed.violations\n        time_dict = parsed.time_dict\n        tree: Optional[BaseSegment]\n        if parsed.tree:\n            t0 = time.monotonic()\n            linter_logger.info(\"LINTING (%s)\", parsed.fname)\n            tree, initial_linting_errors, ignore_buff = cls.lint_fix_parsed(\n                parsed.tree,\n                config=parsed.config,\n                rule_set=rule_set,\n                fix=fix,\n                fname=parsed.fname,\n                templated_file=parsed.templated_file,\n                formatter=formatter,\n            )\n            # Update the timing dict\n            time_dict[\"linting\"] = time.monotonic() - t0\n\n            # We're only going to return the *initial* errors, rather\n            # than any generated during the fixing cycle.\n            violations += initial_linting_errors\n        else:\n            # If no parsed tree, set to None\n            tree = None\n            ignore_buff = []\n\n        # We process the ignore config here if appropriate\n        for violation in violations:\n            violation.ignore_if_in(parsed.config.get(\"ignore\"))\n\n        linted_file = LintedFile(\n            parsed.fname,\n            violations,\n            time_dict,\n            tree,\n            ignore_mask=ignore_buff,\n            templated_file=parsed.templated_file,\n            encoding=encoding,\n        )\n\n        # This is the main command line output from linting.\n        if formatter:\n            formatter.dispatch_file_violations(\n                parsed.fname, linted_file, only_fixable=fix\n            )\n\n        # Safety flag for unset dialects\n        if parsed.config.get(\"dialect\") == \"ansi\" and linted_file.get_violations(\n            fixable=True if fix else None, types=SQLParseError\n        ):\n            if formatter:  # pragma: no cover TODO?\n                formatter.dispatch_dialect_warning()\n\n        return linted_file\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_parsed", "cls", "parsed", "parsedstring", "rule_set", "list", "baserule", "fix", "bool", "false", "formatter", "any", "none", "encoding", "str", "utf8", "lint", "a", "parsedstring", "and", "return", "a", "lintedfile", "violations", "parsed", "violations", "time_dict", "parsed", "time_dict", "tree", "optional", "basesegment", "if", "parsed", "tree", "t0", "time", "monotonic", "linter_logger", "info", "linting", "s", "parsed", "fname", "tree", "initial_linting_errors", "ignore_buff", "cls", "lint_fix_parsed", "parsed", "tree", "config", "parsed", "config", "rule_set", "rule_set", "fix", "fix", "fname", "parsed", "fname", "templated_file", "parsed", "templated_file", "formatter", "formatter", "update", "the", "timing", "dict", "time_dict", "linting", "time", "monotonic", "t0", "we", "re", "only", "going", "to", "return", "the", "initial", "errors", "rather", "than", "any", "generated", "during", "the", "fixing", "cycle", "violations", "initial_linting_errors", "else", "if", "no", "parsed", "tree", "set", "to", "none", "tree", "none", "ignore_buff", "we", "process", "the", "ignore", "config", "here", "if", "appropriate", "for", "violation", "in", "violations", "violation", "ignore_if_in", "parsed", "config", "get", "ignore", "linted_file", "lintedfile", "parsed", "fname", "violations", "time_dict", "tree", "ignore_mask", "ignore_buff", "templated_file", "parsed", "templated_file", "encoding", "encoding", "this", "is", "the", "main", "command", "line", "output", "from", "linting", "if", "formatter", "formatter", "dispatch_file_violations", "parsed", "fname", "linted_file", "only_fixable", "fix", "safety", "flag", "for", "unset", "dialects", "if", "parsed", "config", "get", "dialect", "ansi", "and", "linted_file", "get_violations", "fixable", "true", "if", "fix", "else", "none", "types", "sqlparseerror", "if", "formatter", "pragma", "no", "cover", "todo", "formatter", "dispatch_dialect_warning", "return", "linted_file"], "doc_len": 195}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_rendered", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_rendered", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_rendered(\n        cls,\n        rendered: RenderedFile,\n        rule_set: List[BaseRule],\n        fix: bool = False,\n        formatter: Any = None,\n    ) -> LintedFile:\n        \"\"\"Take a RenderedFile and return a LintedFile.\"\"\"\n        parsed = cls.parse_rendered(rendered)\n        return cls.lint_parsed(\n            parsed,\n            rule_set=rule_set,\n            fix=fix,\n            formatter=formatter,\n            encoding=rendered.encoding,\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_rendered", "cls", "rendered", "renderedfile", "rule_set", "list", "baserule", "fix", "bool", "false", "formatter", "any", "none", "lintedfile", "take", "a", "renderedfile", "and", "return", "a", "lintedfile", "parsed", "cls", "parse_rendered", "rendered", "return", "cls", "lint_parsed", "parsed", "rule_set", "rule_set", "fix", "fix", "formatter", "formatter", "encoding", "rendered", "encoding"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.render_string", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "render_string", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def render_string(\n        self, in_str: str, fname: str, config: FluffConfig, encoding: str\n    ) -> RenderedFile:\n        \"\"\"Template the file.\"\"\"\n        linter_logger.info(\"TEMPLATING RAW [%s] (%s)\", self.templater.name, fname)\n\n        # Start the templating timer\n        t0 = time.monotonic()\n\n        if not config.get(\"templater_obj\") == self.templater:\n            linter_logger.warning(\n                (\n                    f\"Attempt to set templater to {config.get('templater_obj').name} failed. Using {self.templater.name} \"\n                    \"templater. Templater cannot be set in a .sqlfluff file in a subdirectory of the current working \"\n                    \"directory. It can be set in a .sqlfluff in the current working directory. See Nesting section of the \"\n                    \"docs for more details.\"\n                )\n            )\n        try:\n            templated_file, templater_violations = self.templater.process(\n                in_str=in_str, fname=fname, config=config, formatter=self.formatter\n            )\n        except SQLTemplaterSkipFile as s:  # pragma: no cover\n            linter_logger.warning(str(s))\n            templated_file = None\n            templater_violations = []\n\n        if not templated_file:\n            linter_logger.info(\"TEMPLATING FAILED: %s\", templater_violations)\n\n        # Record time\n        time_dict = {\"templating\": time.monotonic() - t0}\n\n        return RenderedFile(\n            templated_file, templater_violations, config, time_dict, fname, encoding\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "render_string", "self", "in_str", "str", "fname", "str", "config", "fluffconfig", "encoding", "str", "renderedfile", "template", "the", "file", "linter_logger", "info", "templating", "raw", "s", "s", "self", "templater", "name", "fname", "start", "the", "templating", "timer", "t0", "time", "monotonic", "if", "not", "config", "get", "templater_obj", "self", "templater", "linter_logger", "warning", "f", "attempt", "to", "set", "templater", "to", "config", "get", "templater_obj", "name", "failed", "using", "self", "templater", "name", "templater", "templater", "cannot", "be", "set", "in", "a", "sqlfluff", "file", "in", "a", "subdirectory", "of", "the", "current", "working", "directory", "it", "can", "be", "set", "in", "a", "sqlfluff", "in", "the", "current", "working", "directory", "see", "nesting", "section", "of", "the", "docs", "for", "more", "details", "try", "templated_file", "templater_violations", "self", "templater", "process", "in_str", "in_str", "fname", "fname", "config", "config", "formatter", "self", "formatter", "except", "sqltemplaterskipfile", "as", "s", "pragma", "no", "cover", "linter_logger", "warning", "str", "s", "templated_file", "none", "templater_violations", "if", "not", "templated_file", "linter_logger", "info", "templating", "failed", "s", "templater_violations", "record", "time", "time_dict", "templating", "time", "monotonic", "t0", "return", "renderedfile", "templated_file", "templater_violations", "config", "time_dict", "fname", "encoding"], "doc_len": 154}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.render_file", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "render_file", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def render_file(self, fname: str, root_config: FluffConfig) -> RenderedFile:\n        \"\"\"Load and render a file with relevant config.\"\"\"\n        # Load the raw file.\n        raw_file, config, encoding = self._load_raw_file_and_config(fname, root_config)\n        # Render the file\n        return self.render_string(raw_file, fname, config, encoding)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "render_file", "self", "fname", "str", "root_config", "fluffconfig", "renderedfile", "load", "and", "render", "a", "file", "with", "relevant", "config", "load", "the", "raw", "file", "raw_file", "config", "encoding", "self", "_load_raw_file_and_config", "fname", "root_config", "render", "the", "file", "return", "self", "render_string", "raw_file", "fname", "config", "encoding"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.parse_string", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "parse_string", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def parse_string(\n        self,\n        in_str: str,\n        fname: str = \"<string>\",\n        recurse: bool = True,\n        config: Optional[FluffConfig] = None,\n        encoding: str = \"utf-8\",\n    ) -> ParsedString:\n        \"\"\"Parse a string.\"\"\"\n        violations: List[SQLBaseError] = []\n\n        # Dispatch the output for the template header (including the config diff)\n        if self.formatter:\n            self.formatter.dispatch_template_header(fname, self.config, config)\n\n        # Just use the local config from here:\n        config = config or self.config\n\n        # Scan the raw file for config commands.\n        config.process_raw_file_for_config(in_str)\n        rendered = self.render_string(in_str, fname, config, encoding)\n        violations += rendered.templater_violations\n\n        # Dispatch the output for the parse header\n        if self.formatter:\n            self.formatter.dispatch_parse_header(fname)\n\n        return self.parse_rendered(rendered, recurse=recurse)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "parse_string", "self", "in_str", "str", "fname", "str", "string", "recurse", "bool", "true", "config", "optional", "fluffconfig", "none", "encoding", "str", "utf", "8", "parsedstring", "parse", "a", "string", "violations", "list", "sqlbaseerror", "dispatch", "the", "output", "for", "the", "template", "header", "including", "the", "config", "diff", "if", "self", "formatter", "self", "formatter", "dispatch_template_header", "fname", "self", "config", "config", "just", "use", "the", "local", "config", "from", "here", "config", "config", "or", "self", "config", "scan", "the", "raw", "file", "for", "config", "commands", "config", "process_raw_file_for_config", "in_str", "rendered", "self", "render_string", "in_str", "fname", "config", "encoding", "violations", "rendered", "templater_violations", "dispatch", "the", "output", "for", "the", "parse", "header", "if", "self", "formatter", "self", "formatter", "dispatch_parse_header", "fname", "return", "self", "parse_rendered", "rendered", "recurse", "recurse"], "doc_len": 106}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.fix", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "fix", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def fix(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> Tuple[BaseSegment, List[SQLBaseError]]:\n        \"\"\"Return the fixed tree and violations from lintfix when we're fixing.\"\"\"\n        config = config or self.config\n        rule_set = self.get_ruleset(config=config)\n        fixed_tree, violations, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_set,\n            fix=True,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return fixed_tree, violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "fix", "self", "tree", "basesegment", "config", "optional", "fluffconfig", "none", "fname", "optional", "str", "none", "templated_file", "optional", "templatedfile", "none", "tuple", "basesegment", "list", "sqlbaseerror", "return", "the", "fixed", "tree", "and", "violations", "from", "lintfix", "when", "we", "re", "fixing", "config", "config", "or", "self", "config", "rule_set", "self", "get_ruleset", "config", "config", "fixed_tree", "violations", "_", "self", "lint_fix_parsed", "tree", "config", "rule_set", "fix", "true", "fname", "fname", "templated_file", "templated_file", "formatter", "self", "formatter", "return", "fixed_tree", "violations"], "doc_len": 70}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint(\n        self,\n        tree: BaseSegment,\n        config: Optional[FluffConfig] = None,\n        fname: Optional[str] = None,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> List[SQLBaseError]:\n        \"\"\"Return just the violations from lintfix when we're only linting.\"\"\"\n        config = config or self.config\n        rule_set = self.get_ruleset(config=config)\n        _, violations, _ = self.lint_fix_parsed(\n            tree,\n            config,\n            rule_set,\n            fix=False,\n            fname=fname,\n            templated_file=templated_file,\n            formatter=self.formatter,\n        )\n        return violations\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint", "self", "tree", "basesegment", "config", "optional", "fluffconfig", "none", "fname", "optional", "str", "none", "templated_file", "optional", "templatedfile", "none", "list", "sqlbaseerror", "return", "just", "the", "violations", "from", "lintfix", "when", "we", "re", "only", "linting", "config", "config", "or", "self", "config", "rule_set", "self", "get_ruleset", "config", "config", "_", "violations", "_", "self", "lint_fix_parsed", "tree", "config", "rule_set", "fix", "false", "fname", "fname", "templated_file", "templated_file", "formatter", "self", "formatter", "return", "violations"], "doc_len": 66}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_string", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_string", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_string(\n        self,\n        in_str: str = \"\",\n        fname: str = \"<string input>\",\n        fix: bool = False,\n        config: Optional[FluffConfig] = None,\n        encoding: str = \"utf8\",\n    ) -> LintedFile:\n        \"\"\"Lint a string.\n\n        Returns:\n            :obj:`LintedFile`: an object representing that linted file.\n\n        \"\"\"\n        # Sort out config, defaulting to the built in config if no override\n        config = config or self.config\n        # Parse the string.\n        parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\n        # Get rules as appropriate\n        rule_set = self.get_ruleset(config=config)\n        # Lint the file and return the LintedFile\n        return self.lint_parsed(\n            parsed, rule_set, fix=fix, formatter=self.formatter, encoding=encoding\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_string", "self", "in_str", "str", "fname", "str", "string", "input", "fix", "bool", "false", "config", "optional", "fluffconfig", "none", "encoding", "str", "utf8", "lintedfile", "lint", "a", "string", "returns", "obj", "lintedfile", "an", "object", "representing", "that", "linted", "file", "sort", "out", "config", "defaulting", "to", "the", "built", "in", "config", "if", "no", "override", "config", "config", "or", "self", "config", "parse", "the", "string", "parsed", "self", "parse_string", "in_str", "in_str", "fname", "fname", "config", "config", "get", "rules", "as", "appropriate", "rule_set", "self", "get_ruleset", "config", "config", "lint", "the", "file", "and", "return", "the", "lintedfile", "return", "self", "lint_parsed", "parsed", "rule_set", "fix", "fix", "formatter", "self", "formatter", "encoding", "encoding"], "doc_len": 96}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.paths_from_path", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "paths_from_path", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def paths_from_path(\n        self,\n        path: str,\n        ignore_file_name: str = \".sqlfluffignore\",\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        working_path: str = os.getcwd(),\n    ) -> List[str]:\n        \"\"\"Return a set of sql file paths from a potentially more ambiguous path string.\n\n        Here we also deal with the .sqlfluffignore file if present.\n\n        When a path to a file to be linted is explicitly passed\n        we look for ignore files in all directories that are parents of the file,\n        up to the current directory.\n\n        If the current directory is not a parent of the file we only\n        look for an ignore file in the direct parent of the file.\n\n        \"\"\"\n        if not os.path.exists(path):\n            if ignore_non_existent_files:\n                return []\n            else:\n                raise OSError(\"Specified path does not exist\")\n\n        # Files referred to exactly are also ignored if\n        # matched, but we warn the users when that happens\n        is_exact_file = os.path.isfile(path)\n\n        if is_exact_file:\n            # When the exact file to lint is passed, we\n            # fill path_walk with an input that follows\n            # the structure of `os.walk`:\n            #   (root, directories, files)\n            dirpath = os.path.dirname(path)\n            files = [os.path.basename(path)]\n            ignore_file_paths = ConfigLoader.find_ignore_config_files(\n                path=path, working_path=working_path, ignore_file_name=ignore_file_name\n            )\n            # Add paths that could contain \"ignore files\"\n            # to the path_walk list\n            path_walk_ignore_file = [\n                (\n                    os.path.dirname(ignore_file_path),\n                    None,\n                    # Only one possible file, since we only\n                    # have one \"ignore file name\"\n                    [os.path.basename(ignore_file_path)],\n                )\n                for ignore_file_path in ignore_file_paths\n            ]\n            path_walk: WalkableType = [(dirpath, None, files)] + path_walk_ignore_file\n        else:\n            path_walk = os.walk(path)\n\n        # If it's a directory then expand the path!\n        buffer = []\n        ignores = {}\n        for dirpath, _, filenames in path_walk:\n            for fname in filenames:\n                fpath = os.path.join(dirpath, fname)\n                # Handle potential .sqlfluffignore files\n                if ignore_files and fname == ignore_file_name:\n                    with open(fpath) as fh:\n                        spec = pathspec.PathSpec.from_lines(\"gitwildmatch\", fh)\n                        ignores[dirpath] = spec\n                    # We don't need to process the ignore file any futher\n                    continue\n\n                # We won't purge files *here* because there's an edge case\n                # that the ignore file is processed after the sql file.\n\n                # Scan for remaining files\n                for ext in self.config.get(\"sql_file_exts\", default=\".sql\").split(\",\"):\n                    # is it a sql file?\n                    if fname.endswith(ext):\n                        buffer.append(fpath)\n\n        if not ignore_files:\n            return sorted(buffer)\n\n        # Check the buffer for ignore items and normalise the rest.\n        filtered_buffer = []\n\n        for fpath in buffer:\n            abs_fpath = os.path.abspath(fpath)\n            for ignore_base, ignore_spec in ignores.items():\n                abs_ignore_base = os.path.abspath(ignore_base)\n                if abs_fpath.startswith(\n                    abs_ignore_base + os.sep\n                ) and ignore_spec.match_file(\n                    os.path.relpath(abs_fpath, abs_ignore_base)\n                ):\n                    # This file is ignored, skip it.\n                    if is_exact_file:\n                        linter_logger.warning(\n                            \"Exact file path %s was given but \"\n                            \"it was ignored by a %s pattern in %s, \"\n                            \"re-run with `--disregard-sqlfluffignores` to \"\n                            \"skip %s\"\n                            % (\n                                path,\n                                ignore_file_name,\n                                ignore_base,\n                                ignore_file_name,\n                            )\n                        )\n                    break\n            else:\n                filtered_buffer.append(os.path.normpath(fpath))\n\n        # Return\n        return sorted(filtered_buffer)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "paths_from_path", "self", "path", "str", "ignore_file_name", "str", "sqlfluffignore", "ignore_non_existent_files", "bool", "false", "ignore_files", "bool", "true", "working_path", "str", "os", "getcwd", "list", "str", "return", "a", "set", "of", "sql", "file", "paths", "from", "a", "potentially", "more", "ambiguous", "path", "string", "here", "we", "also", "deal", "with", "the", "sqlfluffignore", "file", "if", "present", "when", "a", "path", "to", "a", "file", "to", "be", "linted", "is", "explicitly", "passed", "we", "look", "for", "ignore", "files", "in", "all", "directories", "that", "are", "parents", "of", "the", "file", "up", "to", "the", "current", "directory", "if", "the", "current", "directory", "is", "not", "a", "parent", "of", "the", "file", "we", "only", "look", "for", "an", "ignore", "file", "in", "the", "direct", "parent", "of", "the", "file", "if", "not", "os", "path", "exists", "path", "if", "ignore_non_existent_files", "return", "else", "raise", "oserror", "specified", "path", "does", "not", "exist", "files", "referred", "to", "exactly", "are", "also", "ignored", "if", "matched", "but", "we", "warn", "the", "users", "when", "that", "happens", "is_exact_file", "os", "path", "isfile", "path", "if", "is_exact_file", "when", "the", "exact", "file", "to", "lint", "is", "passed", "we", "fill", "path_walk", "with", "an", "input", "that", "follows", "the", "structure", "of", "os", "walk", "root", "directories", "files", "dirpath", "os", "path", "dirname", "path", "files", "os", "path", "basename", "path", "ignore_file_paths", "configloader", "find_ignore_config_files", "path", "path", "working_path", "working_path", "ignore_file_name", "ignore_file_name", "add", "paths", "that", "could", "contain", "ignore", "files", "to", "the", "path_walk", "list", "path_walk_ignore_file", "os", "path", "dirname", "ignore_file_path", "none", "only", "one", "possible", "file", "since", "we", "only", "have", "one", "ignore", "file", "name", "os", "path", "basename", "ignore_file_path", "for", "ignore_file_path", "in", "ignore_file_paths", "path_walk", "walkabletype", "dirpath", "none", "files", "path_walk_ignore_file", "else", "path_walk", "os", "walk", "path", "if", "it", "s", "a", "directory", "then", "expand", "the", "path", "buffer", "ignores", "for", "dirpath", "_", "filenames", "in", "path_walk", "for", "fname", "in", "filenames", "fpath", "os", "path", "join", "dirpath", "fname", "handle", "potential", "sqlfluffignore", "files", "if", "ignore_files", "and", "fname", "ignore_file_name", "with", "open", "fpath", "as", "fh", "spec", "pathspec", "pathspec", "from_lines", "gitwildmatch", "fh", "ignores", "dirpath", "spec", "we", "don", "t", "need", "to", "process", "the", "ignore", "file", "any", "futher", "continue", "we", "won", "t", "purge", "files", "here", "because", "there", "s", "an", "edge", "case", "that", "the", "ignore", "file", "is", "processed", "after", "the", "sql", "file", "scan", "for", "remaining", "files", "for", "ext", "in", "self", "config", "get", "sql_file_exts", "default", "sql", "split", "is", "it", "a", "sql", "file", "if", "fname", "endswith", "ext", "buffer", "append", "fpath", "if", "not", "ignore_files", "return", "sorted", "buffer", "check", "the", "buffer", "for", "ignore", "items", "and", "normalise", "the", "rest", "filtered_buffer", "for", "fpath", "in", "buffer", "abs_fpath", "os", "path", "abspath", "fpath", "for", "ignore_base", "ignore_spec", "in", "ignores", "items", "abs_ignore_base", "os", "path", "abspath", "ignore_base", "if", "abs_fpath", "startswith", "abs_ignore_base", "os", "sep", "and", "ignore_spec", "match_file", "os", "path", "relpath", "abs_fpath", "abs_ignore_base", "this", "file", "is", "ignored", "skip", "it", "if", "is_exact_file", "linter_logger", "warning", "exact", "file", "path", "s", "was", "given", "but", "it", "was", "ignored", "by", "a", "s", "pattern", "in", "s", "re", "run", "with", "disregard", "sqlfluffignores", "to", "skip", "s", "path", "ignore_file_name", "ignore_base", "ignore_file_name", "break", "else", "filtered_buffer", "append", "os", "path", "normpath", "fpath", "return", "return", "sorted", "filtered_buffer"], "doc_len": 450}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_string_wrapped", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_string_wrapped", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_string_wrapped(\n        self, string: str, fname: str = \"<string input>\", fix: bool = False\n    ) -> LintingResult:\n        \"\"\"Lint strings directly.\"\"\"\n        result = LintingResult()\n        linted_path = LintedDir(fname)\n        linted_path.add(self.lint_string(string, fname=fname, fix=fix))\n        result.add(linted_path)\n        result.stop_timer()\n        return result\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_string_wrapped", "self", "string", "str", "fname", "str", "string", "input", "fix", "bool", "false", "lintingresult", "lint", "strings", "directly", "result", "lintingresult", "linted_path", "linteddir", "fname", "linted_path", "add", "self", "lint_string", "string", "fname", "fname", "fix", "fix", "result", "add", "linted_path", "result", "stop_timer", "return", "result"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_path", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_path", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_path(\n        self,\n        path: str,\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: int = 1,\n    ) -> LintedDir:\n        \"\"\"Lint a path.\"\"\"\n        linted_path = LintedDir(path)\n        if self.formatter:\n            self.formatter.dispatch_path(path)\n        fnames = list(\n            self.paths_from_path(\n                path,\n                ignore_non_existent_files=ignore_non_existent_files,\n                ignore_files=ignore_files,\n            )\n        )\n        runner = get_runner(\n            self,\n            self.config,\n            processes=processes,\n            allow_process_parallelism=self.allow_process_parallelism,\n        )\n        for linted_file in runner.run(fnames, fix):\n            linted_path.add(linted_file)\n            # If any fatal errors, then stop iteration.\n            if any(v.fatal for v in linted_file.violations):  # pragma: no cover\n                linter_logger.error(\"Fatal linting error. Halting further linting.\")\n                break\n        return linted_path\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_path", "self", "path", "str", "fix", "bool", "false", "ignore_non_existent_files", "bool", "false", "ignore_files", "bool", "true", "processes", "int", "1", "linteddir", "lint", "a", "path", "linted_path", "linteddir", "path", "if", "self", "formatter", "self", "formatter", "dispatch_path", "path", "fnames", "list", "self", "paths_from_path", "path", "ignore_non_existent_files", "ignore_non_existent_files", "ignore_files", "ignore_files", "runner", "get_runner", "self", "self", "config", "processes", "processes", "allow_process_parallelism", "self", "allow_process_parallelism", "for", "linted_file", "in", "runner", "run", "fnames", "fix", "linted_path", "add", "linted_file", "if", "any", "fatal", "errors", "then", "stop", "iteration", "if", "any", "v", "fatal", "for", "v", "in", "linted_file", "violations", "pragma", "no", "cover", "linter_logger", "error", "fatal", "linting", "error", "halting", "further", "linting", "break", "return", "linted_path"], "doc_len": 97}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.lint_paths", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "lint_paths", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def lint_paths(\n        self,\n        paths: Tuple[str, ...],\n        fix: bool = False,\n        ignore_non_existent_files: bool = False,\n        ignore_files: bool = True,\n        processes: int = 1,\n    ) -> LintingResult:\n        \"\"\"Lint an iterable of paths.\"\"\"\n        # If no paths specified - assume local\n        if len(paths) == 0:  # pragma: no cover\n            paths = (os.getcwd(),)\n        # Set up the result to hold what we get back\n        result = LintingResult()\n        for path in paths:\n            # Iterate through files recursively in the specified directory (if it's a directory)\n            # or read the file directly if it's not\n            result.add(\n                self.lint_path(\n                    path,\n                    fix=fix,\n                    ignore_non_existent_files=ignore_non_existent_files,\n                    ignore_files=ignore_files,\n                    processes=processes,\n                )\n            )\n        result.stop_timer()\n        return result\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "lint_paths", "self", "paths", "tuple", "str", "fix", "bool", "false", "ignore_non_existent_files", "bool", "false", "ignore_files", "bool", "true", "processes", "int", "1", "lintingresult", "lint", "an", "iterable", "of", "paths", "if", "no", "paths", "specified", "assume", "local", "if", "len", "paths", "0", "pragma", "no", "cover", "paths", "os", "getcwd", "set", "up", "the", "result", "to", "hold", "what", "we", "get", "back", "result", "lintingresult", "for", "path", "in", "paths", "iterate", "through", "files", "recursively", "in", "the", "specified", "directory", "if", "it", "s", "a", "directory", "or", "read", "the", "file", "directly", "if", "it", "s", "not", "result", "add", "self", "lint_path", "path", "fix", "fix", "ignore_non_existent_files", "ignore_non_existent_files", "ignore_files", "ignore_files", "processes", "processes", "result", "stop_timer", "return", "result"], "doc_len": 102}
{"doc_id": "src/sqlfluff/core/linter/linter.py::Linter.parse_path", "file_path": "src/sqlfluff/core/linter/linter.py", "class_name": "Linter", "func_name": "parse_path", "text": "文件路径: src/sqlfluff/core/linter/linter.py, 类名: Linter\n    def parse_path(self, path: str, recurse: bool = True) -> Iterator[ParsedString]:\n        \"\"\"Parse a path of sql files.\n\n        NB: This a generator which will yield the result of each file\n        within the path iteratively.\n        \"\"\"\n        for fname in self.paths_from_path(path):\n            if self.formatter:\n                self.formatter.dispatch_path(path)\n            # Load the file with the config and yield the result.\n            raw_file, config, encoding = self._load_raw_file_and_config(\n                fname, self.config\n            )\n            yield self.parse_string(\n                raw_file, fname=fname, recurse=recurse, config=config, encoding=encoding\n            )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linter", "py", "linter", "def", "parse_path", "self", "path", "str", "recurse", "bool", "true", "iterator", "parsedstring", "parse", "a", "path", "of", "sql", "files", "nb", "this", "a", "generator", "which", "will", "yield", "the", "result", "of", "each", "file", "within", "the", "path", "iteratively", "for", "fname", "in", "self", "paths_from_path", "path", "if", "self", "formatter", "self", "formatter", "dispatch_path", "path", "load", "the", "file", "with", "the", "config", "and", "yield", "the", "result", "raw_file", "config", "encoding", "self", "_load_raw_file_and_config", "fname", "self", "config", "yield", "self", "parse_string", "raw_file", "fname", "fname", "recurse", "recurse", "config", "config", "encoding", "encoding"], "doc_len": 82}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.__init__", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def __init__(self) -> None:\n        self.paths: List[LintedDir] = []\n        self._start_time: float = time.monotonic()\n        self.total_time: float = 0.0\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "__init__", "self", "none", "self", "paths", "list", "linteddir", "self", "_start_time", "float", "time", "monotonic", "self", "total_time", "float", "0", "0"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.sum_dicts", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "sum_dicts", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def sum_dicts(d1: Dict[str, Any], d2: Dict[str, Any]) -> Dict[str, Any]:\n        \"\"\"Take the keys of two dictionaries and add them.\"\"\"\n        keys = set(d1.keys()) | set(d2.keys())\n        return {key: d1.get(key, 0) + d2.get(key, 0) for key in keys}\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "sum_dicts", "d1", "dict", "str", "any", "d2", "dict", "str", "any", "dict", "str", "any", "take", "the", "keys", "of", "two", "dictionaries", "and", "add", "them", "keys", "set", "d1", "keys", "set", "d2", "keys", "return", "key", "d1", "get", "key", "0", "d2", "get", "key", "0", "for", "key", "in", "keys"], "doc_len": 50}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.combine_dicts", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "combine_dicts", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def combine_dicts(*d: dict) -> dict:\n        \"\"\"Take any set of dictionaries and combine them.\"\"\"\n        dict_buffer: dict = {}\n        for dct in d:\n            dict_buffer.update(dct)\n        return dict_buffer\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "combine_dicts", "d", "dict", "dict", "take", "any", "set", "of", "dictionaries", "and", "combine", "them", "dict_buffer", "dict", "for", "dct", "in", "d", "dict_buffer", "update", "dct", "return", "dict_buffer"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.add", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "add", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def add(self, path: LintedDir) -> None:\n        \"\"\"Add a new `LintedDir` to this result.\"\"\"\n        self.paths.append(path)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "add", "self", "path", "linteddir", "none", "add", "a", "new", "linteddir", "to", "this", "result", "self", "paths", "append", "path"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.stop_timer", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "stop_timer", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def stop_timer(self):\n        \"\"\"Stop the linting timer.\"\"\"\n        self.total_time = time.monotonic() - self._start_time\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "stop_timer", "self", "stop", "the", "linting", "timer", "self", "total_time", "time", "monotonic", "self", "_start_time"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.check_tuples", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def check_tuples(\n        self, by_path: Literal[False]\n    ) -> List[CheckTuple]:  # pragma: no cover\n        \"\"\"Return a List of CheckTuples when by_path is False.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "check_tuples", "self", "by_path", "literal", "false", "list", "checktuple", "pragma", "no", "cover", "return", "a", "list", "of", "checktuples", "when", "by_path", "is", "false"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.check_tuples", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def check_tuples(\n        self, by_path: Literal[True]\n    ) -> Dict[LintedDir, List[CheckTuple]]:  # pragma: no cover\n        \"\"\"Return a Dict of LintedDir and CheckTuples when by_path is True.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "check_tuples", "self", "by_path", "literal", "true", "dict", "linteddir", "list", "checktuple", "pragma", "no", "cover", "return", "a", "dict", "of", "linteddir", "and", "checktuples", "when", "by_path", "is", "true"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.check_tuples", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def check_tuples(self, by_path: bool = False):  # pragma: no cover\n        \"\"\"Default overload method.\"\"\"\n        ...\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "check_tuples", "self", "by_path", "bool", "false", "pragma", "no", "cover", "default", "overload", "method"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.check_tuples", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "check_tuples", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def check_tuples(self, by_path=False):\n        \"\"\"Fetch all check_tuples from all contained `LintedDir` objects.\n\n        Args:\n            by_path (:obj:`bool`, optional): When False, all the check_tuples\n                are aggregated into one flat list. When True, we return a `dict`\n                of paths, each with its own list of check_tuples. Defaults to False.\n\n        \"\"\"\n        if by_path:\n            buff: Dict[LintedDir, List[CheckTuple]] = {}\n            for path in self.paths:\n                buff.update(path.check_tuples(by_path=by_path))\n            return buff\n        else:\n            tuple_buffer: List[CheckTuple] = []\n            for path in self.paths:\n                tuple_buffer += path.check_tuples()\n            return tuple_buffer\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "check_tuples", "self", "by_path", "false", "fetch", "all", "check_tuples", "from", "all", "contained", "linteddir", "objects", "args", "by_path", "obj", "bool", "optional", "when", "false", "all", "the", "check_tuples", "are", "aggregated", "into", "one", "flat", "list", "when", "true", "we", "return", "a", "dict", "of", "paths", "each", "with", "its", "own", "list", "of", "check_tuples", "defaults", "to", "false", "if", "by_path", "buff", "dict", "linteddir", "list", "checktuple", "for", "path", "in", "self", "paths", "buff", "update", "path", "check_tuples", "by_path", "by_path", "return", "buff", "else", "tuple_buffer", "list", "checktuple", "for", "path", "in", "self", "paths", "tuple_buffer", "path", "check_tuples", "return", "tuple_buffer"], "doc_len": 88}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.num_violations", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "num_violations", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def num_violations(self, **kwargs) -> int:\n        \"\"\"Count the number of violations in the result.\"\"\"\n        return sum(path.num_violations(**kwargs) for path in self.paths)\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "num_violations", "self", "kwargs", "int", "count", "the", "number", "of", "violations", "in", "the", "result", "return", "sum", "path", "num_violations", "kwargs", "for", "path", "in", "self", "paths"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.get_violations", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "get_violations", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def get_violations(self, **kwargs):\n        \"\"\"Return a list of violations in the result.\"\"\"\n        buff = []\n        for path in self.paths:\n            buff += path.get_violations(**kwargs)\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "get_violations", "self", "kwargs", "return", "a", "list", "of", "violations", "in", "the", "result", "buff", "for", "path", "in", "self", "paths", "buff", "path", "get_violations", "kwargs", "return", "buff"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.violation_dict", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "violation_dict", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def violation_dict(self, **kwargs):\n        \"\"\"Return a dict of paths and violations.\"\"\"\n        return self.combine_dicts(\n            path.violation_dict(**kwargs) for path in self.paths\n        )  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "violation_dict", "self", "kwargs", "return", "a", "dict", "of", "paths", "and", "violations", "return", "self", "combine_dicts", "path", "violation_dict", "kwargs", "for", "path", "in", "self", "paths", "pragma", "no", "cover", "todo"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.stats", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "stats", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def stats(self) -> Dict[str, Any]:\n        \"\"\"Return a stats dictionary of this result.\"\"\"\n        all_stats: Dict[str, Any] = dict(files=0, clean=0, unclean=0, violations=0)\n        for path in self.paths:\n            all_stats = self.sum_dicts(path.stats(), all_stats)\n        if all_stats[\"files\"] > 0:\n            all_stats[\"avg per file\"] = (\n                all_stats[\"violations\"] * 1.0 / all_stats[\"files\"]\n            )\n            all_stats[\"unclean rate\"] = all_stats[\"unclean\"] * 1.0 / all_stats[\"files\"]\n        else:\n            all_stats[\"avg per file\"] = 0\n            all_stats[\"unclean rate\"] = 0\n        all_stats[\"clean files\"] = all_stats[\"clean\"]\n        all_stats[\"unclean files\"] = all_stats[\"unclean\"]\n        all_stats[\"exit code\"] = 65 if all_stats[\"violations\"] > 0 else 0\n        all_stats[\"status\"] = \"FAIL\" if all_stats[\"violations\"] > 0 else \"PASS\"\n        return all_stats\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "stats", "self", "dict", "str", "any", "return", "a", "stats", "dictionary", "of", "this", "result", "all_stats", "dict", "str", "any", "dict", "files", "0", "clean", "0", "unclean", "0", "violations", "0", "for", "path", "in", "self", "paths", "all_stats", "self", "sum_dicts", "path", "stats", "all_stats", "if", "all_stats", "files", "0", "all_stats", "avg", "per", "file", "all_stats", "violations", "1", "0", "all_stats", "files", "all_stats", "unclean", "rate", "all_stats", "unclean", "1", "0", "all_stats", "files", "else", "all_stats", "avg", "per", "file", "0", "all_stats", "unclean", "rate", "0", "all_stats", "clean", "files", "all_stats", "clean", "all_stats", "unclean", "files", "all_stats", "unclean", "all_stats", "exit", "code", "65", "if", "all_stats", "violations", "0", "else", "0", "all_stats", "status", "fail", "if", "all_stats", "violations", "0", "else", "pass", "return", "all_stats"], "doc_len": 108}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.timing_summary", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "timing_summary", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def timing_summary(self) -> Dict[str, Dict[str, float]]:\n        \"\"\"Return a timing summary.\"\"\"\n        timing = TimingSummary()\n        for dir in self.paths:\n            for file in dir.files:\n                timing.add(file.time_dict)\n        return timing.summary()\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "timing_summary", "self", "dict", "str", "dict", "str", "float", "return", "a", "timing", "summary", "timing", "timingsummary", "for", "dir", "in", "self", "paths", "for", "file", "in", "dir", "files", "timing", "add", "file", "time_dict", "return", "timing", "summary"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.as_records", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "as_records", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def as_records(self) -> List[dict]:\n        \"\"\"Return the result as a list of dictionaries.\n\n        Each record contains a key specifying the filepath, and a list of violations. This\n        method is useful for serialization as all objects will be builtin python types\n        (ints, strs).\n        \"\"\"\n        return [\n            {\n                \"filepath\": path,\n                \"violations\": sorted(\n                    # Sort violations by line and then position\n                    (v.get_info_dict() for v in violations),\n                    # The tuple allows sorting by line number, then position, then code\n                    key=lambda v: (v[\"line_no\"], v[\"line_pos\"], v[\"code\"]),\n                ),\n            }\n            for LintedDir in self.paths\n            for path, violations in LintedDir.violation_dict().items()\n            if violations\n        ]\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "as_records", "self", "list", "dict", "return", "the", "result", "as", "a", "list", "of", "dictionaries", "each", "record", "contains", "a", "key", "specifying", "the", "filepath", "and", "a", "list", "of", "violations", "this", "method", "is", "useful", "for", "serialization", "as", "all", "objects", "will", "be", "builtin", "python", "types", "ints", "strs", "return", "filepath", "path", "violations", "sorted", "sort", "violations", "by", "line", "and", "then", "position", "v", "get_info_dict", "for", "v", "in", "violations", "the", "tuple", "allows", "sorting", "by", "line", "number", "then", "position", "then", "code", "key", "lambda", "v", "v", "line_no", "v", "line_pos", "v", "code", "for", "linteddir", "in", "self", "paths", "for", "path", "violations", "in", "linteddir", "violation_dict", "items", "if", "violations"], "doc_len": 101}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.persist_changes", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "persist_changes", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def persist_changes(self, formatter, **kwargs) -> dict:\n        \"\"\"Run all the fixes for all the files and return a dict.\"\"\"\n        return self.combine_dicts(\n            *(\n                path.persist_changes(formatter=formatter, **kwargs)\n                for path in self.paths\n            )\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "persist_changes", "self", "formatter", "kwargs", "dict", "run", "all", "the", "fixes", "for", "all", "the", "files", "and", "return", "a", "dict", "return", "self", "combine_dicts", "path", "persist_changes", "formatter", "formatter", "kwargs", "for", "path", "in", "self", "paths"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/linter/linting_result.py::LintingResult.tree", "file_path": "src/sqlfluff/core/linter/linting_result.py", "class_name": "LintingResult", "func_name": "tree", "text": "文件路径: src/sqlfluff/core/linter/linting_result.py, 类名: LintingResult\n    def tree(self) -> Optional[BaseSegment]:  # pragma: no cover\n        \"\"\"A convenience method for when there is only one file and we want the tree.\"\"\"\n        if len(self.paths) > 1:\n            raise ValueError(\n                \".tree() cannot be called when a LintingResult contains more than one path.\"\n            )\n        return self.paths[0].tree\n", "tokens": ["src", "sqlfluff", "core", "linter", "linting_result", "py", "lintingresult", "def", "tree", "self", "optional", "basesegment", "pragma", "no", "cover", "a", "convenience", "method", "for", "when", "there", "is", "only", "one", "file", "and", "we", "want", "the", "tree", "if", "len", "self", "paths", "1", "raise", "valueerror", "tree", "cannot", "be", "called", "when", "a", "lintingresult", "contains", "more", "than", "one", "path", "return", "self", "paths", "0", "tree"], "doc_len": 54}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner.__init__", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def __init__(\n        self,\n        linter,\n        config,\n    ):\n        self.linter = linter\n        self.config = config\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "__init__", "self", "linter", "config", "self", "linter", "linter", "self", "config", "config"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner.iter_rendered", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "iter_rendered", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def iter_rendered(self, fnames):\n        \"\"\"Iterate through rendered files ready for linting.\"\"\"\n        for fname in self.linter.templater.sequence_files(\n            fnames, config=self.config, formatter=self.linter.formatter\n        ):\n            yield fname, self.linter.render_file(fname, self.config)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "iter_rendered", "self", "fnames", "iterate", "through", "rendered", "files", "ready", "for", "linting", "for", "fname", "in", "self", "linter", "templater", "sequence_files", "fnames", "config", "self", "config", "formatter", "self", "linter", "formatter", "yield", "fname", "self", "linter", "render_file", "fname", "self", "config"], "doc_len": 41}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner.iter_partials", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "iter_partials", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def iter_partials(self, fnames, fix: bool = False):\n        \"\"\"Iterate through partials for linted files.\n\n        Generates filenames and objects which return LintedFiles.\n        \"\"\"\n        for fname, rendered in self.iter_rendered(fnames):\n            # Generate a fresh ruleset\n            rule_set = self.linter.get_ruleset(config=rendered.config)\n            yield (\n                fname,\n                functools.partial(\n                    self.linter.lint_rendered,\n                    rendered,\n                    rule_set,\n                    fix,\n                    # Formatters may or may not be passed. They don't pickle\n                    # nicely so aren't appropriate in a multiprocessing world.\n                    self.linter.formatter if self.pass_formatter else None,\n                ),\n            )\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "iter_partials", "self", "fnames", "fix", "bool", "false", "iterate", "through", "partials", "for", "linted", "files", "generates", "filenames", "and", "objects", "which", "return", "lintedfiles", "for", "fname", "rendered", "in", "self", "iter_rendered", "fnames", "generate", "a", "fresh", "ruleset", "rule_set", "self", "linter", "get_ruleset", "config", "rendered", "config", "yield", "fname", "functools", "partial", "self", "linter", "lint_rendered", "rendered", "rule_set", "fix", "formatters", "may", "or", "may", "not", "be", "passed", "they", "don", "t", "pickle", "nicely", "so", "aren", "t", "appropriate", "in", "a", "multiprocessing", "world", "self", "linter", "formatter", "if", "self", "pass_formatter", "else", "none"], "doc_len": 83}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner.run", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "run", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def run(self, fnames: List[str], fix: bool):\n        \"\"\"Run linting on the specified list of files.\"\"\"\n        raise NotImplementedError  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "run", "self", "fnames", "list", "str", "fix", "bool", "run", "linting", "on", "the", "specified", "list", "of", "files", "raise", "notimplementederror", "pragma", "no", "cover"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner._init_global", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "_init_global", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def _init_global(cls, config):\n        \"\"\"Initializes any global state.\n\n        May be overridden by subclasses to apply global configuration, initialize\n        logger state in child processes, etc.\n        \"\"\"\n        pass\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "_init_global", "cls", "config", "initializes", "any", "global", "state", "may", "be", "overridden", "by", "subclasses", "to", "apply", "global", "configuration", "initialize", "logger", "state", "in", "child", "processes", "etc", "pass"], "doc_len": 32}
{"doc_id": "src/sqlfluff/core/linter/runner.py::BaseRunner._handle_lint_path_exception", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "BaseRunner", "func_name": "_handle_lint_path_exception", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: BaseRunner\n    def _handle_lint_path_exception(fname, e):\n        if isinstance(e, IOError):\n            # IOErrors are caught in commands.py, so propagate it\n            raise (e)  # pragma: no cover\n        linter_logger.warning(\n            f\"\"\"Unable to lint {fname} due to an internal error. \\\nPlease report this as an issue with your query's contents and stacktrace below!\nTo hide this warning, add the failing file to .sqlfluffignore\n{traceback.format_exc()}\"\"\",\n        )\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "baserunner", "def", "_handle_lint_path_exception", "fname", "e", "if", "isinstance", "e", "ioerror", "ioerrors", "are", "caught", "in", "commands", "py", "so", "propagate", "it", "raise", "e", "pragma", "no", "cover", "linter_logger", "warning", "f", "unable", "to", "lint", "fname", "due", "to", "an", "internal", "error", "please", "report", "this", "as", "an", "issue", "with", "your", "query", "s", "contents", "and", "stacktrace", "below", "to", "hide", "this", "warning", "add", "the", "failing", "file", "to", "sqlfluffignore", "traceback", "format_exc"], "doc_len": 67}
{"doc_id": "src/sqlfluff/core/linter/runner.py::SequentialRunner.run", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "SequentialRunner", "func_name": "run", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: SequentialRunner\n    def run(self, fnames, fix):\n        \"\"\"Sequential implementation.\"\"\"\n        for fname, partial in self.iter_partials(fnames, fix=fix):\n            try:\n                yield partial()\n            except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n                raise\n            except Exception as e:\n                self._handle_lint_path_exception(fname, e)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "sequentialrunner", "def", "run", "self", "fnames", "fix", "sequential", "implementation", "for", "fname", "partial", "in", "self", "iter_partials", "fnames", "fix", "fix", "try", "yield", "partial", "except", "bdb", "bdbquit", "keyboardinterrupt", "pragma", "no", "cover", "raise", "except", "exception", "as", "e", "self", "_handle_lint_path_exception", "fname", "e"], "doc_len": 42}
{"doc_id": "src/sqlfluff/core/linter/runner.py::ParallelRunner.__init__", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "ParallelRunner", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: ParallelRunner\n    def __init__(self, linter, config, processes):\n        super().__init__(linter, config)\n        self.processes = processes\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "parallelrunner", "def", "__init__", "self", "linter", "config", "processes", "super", "__init__", "linter", "config", "self", "processes", "processes"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/linter/runner.py::ParallelRunner.run", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "ParallelRunner", "func_name": "run", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: ParallelRunner\n    def run(self, fnames, fix):\n        \"\"\"Parallel implementation.\n\n        Note that the partials are generated one at a time then\n        passed directly into the pool as they're ready. This means\n        the main thread can do the IO work while passing the parsing\n        and linting work out to the threads.\n        \"\"\"\n        with self._create_pool(\n            self.processes,\n            self._init_global,\n            (self.config,),\n        ) as pool:\n            try:\n                for lint_result in self._map(\n                    pool, self._apply, self.iter_partials(fnames, fix=fix)\n                ):\n                    if isinstance(lint_result, DelayedException):\n                        try:\n                            lint_result.reraise()\n                        except Exception as e:\n                            self._handle_lint_path_exception(lint_result.fname, e)\n                    else:\n                        # It's a LintedDir.\n                        if self.linter.formatter:\n                            self.linter.formatter.dispatch_file_violations(\n                                lint_result.path, lint_result, only_fixable=fix\n                            )\n                        yield lint_result\n            except KeyboardInterrupt:  # pragma: no cover\n                # On keyboard interrupt (Ctrl-C), terminate the workers.\n                # Notify the user we've received the signal and are cleaning up,\n                # in case it takes awhile.\n                print(\"Received keyboard interrupt. Cleaning up and shutting down...\")\n                pool.terminate()\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "parallelrunner", "def", "run", "self", "fnames", "fix", "parallel", "implementation", "note", "that", "the", "partials", "are", "generated", "one", "at", "a", "time", "then", "passed", "directly", "into", "the", "pool", "as", "they", "re", "ready", "this", "means", "the", "main", "thread", "can", "do", "the", "io", "work", "while", "passing", "the", "parsing", "and", "linting", "work", "out", "to", "the", "threads", "with", "self", "_create_pool", "self", "processes", "self", "_init_global", "self", "config", "as", "pool", "try", "for", "lint_result", "in", "self", "_map", "pool", "self", "_apply", "self", "iter_partials", "fnames", "fix", "fix", "if", "isinstance", "lint_result", "delayedexception", "try", "lint_result", "reraise", "except", "exception", "as", "e", "self", "_handle_lint_path_exception", "lint_result", "fname", "e", "else", "it", "s", "a", "linteddir", "if", "self", "linter", "formatter", "self", "linter", "formatter", "dispatch_file_violations", "lint_result", "path", "lint_result", "only_fixable", "fix", "yield", "lint_result", "except", "keyboardinterrupt", "pragma", "no", "cover", "on", "keyboard", "interrupt", "ctrl", "c", "terminate", "the", "workers", "notify", "the", "user", "we", "ve", "received", "the", "signal", "and", "are", "cleaning", "up", "in", "case", "it", "takes", "awhile", "print", "received", "keyboard", "interrupt", "cleaning", "up", "and", "shutting", "down", "pool", "terminate"], "doc_len": 157}
{"doc_id": "src/sqlfluff/core/linter/runner.py::ParallelRunner._apply", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "ParallelRunner", "func_name": "_apply", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: ParallelRunner\n    def _apply(partial_tuple):\n        \"\"\"Shim function used in parallel mode.\"\"\"\n        # Unpack the tuple and ditch the filename in this case.\n        fname, partial = partial_tuple\n        try:\n            return partial()\n        # Capture any exceptions and return as delayed exception to handle\n        # in the main thread.\n        except Exception as e:\n            return DelayedException(e, fname=fname)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "parallelrunner", "def", "_apply", "partial_tuple", "shim", "function", "used", "in", "parallel", "mode", "unpack", "the", "tuple", "and", "ditch", "the", "filename", "in", "this", "case", "fname", "partial", "partial_tuple", "try", "return", "partial", "capture", "any", "exceptions", "and", "return", "as", "delayed", "exception", "to", "handle", "in", "the", "main", "thread", "except", "exception", "as", "e", "return", "delayedexception", "e", "fname", "fname"], "doc_len": 55}
{"doc_id": "src/sqlfluff/core/linter/runner.py::ParallelRunner._create_pool", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "ParallelRunner", "func_name": "_create_pool", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: ParallelRunner\n    def _create_pool(cls, *args, **kwargs):\n        return cls.POOL_TYPE(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "parallelrunner", "def", "_create_pool", "cls", "args", "kwargs", "return", "cls", "pool_type", "args", "kwargs"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/linter/runner.py::ParallelRunner._map", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "ParallelRunner", "func_name": "_map", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: ParallelRunner\n    def _map(cls, pool, *args, **kwargs):\n        \"\"\"Runs a class-appropriate version of the general map() function.\"\"\"\n        return getattr(pool, cls.MAP_FUNCTION_NAME)(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "parallelrunner", "def", "_map", "cls", "pool", "args", "kwargs", "runs", "a", "class", "appropriate", "version", "of", "the", "general", "map", "function", "return", "getattr", "pool", "cls", "map_function_name", "args", "kwargs"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/linter/runner.py::MultiProcessRunner._init_global", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "MultiProcessRunner", "func_name": "_init_global", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: MultiProcessRunner\n    def _init_global(cls, config):  # pragma: no cover\n        super()._init_global(config)\n\n        # Disable signal handling in the child processes to let the parent\n        # control all KeyboardInterrupt handling (Control C). This is\n        # necessary in order for keyboard interrupts to exit quickly and\n        # cleanly. Adapted from this post:\n        # https://stackoverflow.com/questions/11312525/catch-ctrlc-sigint-and-exit-multiprocesses-gracefully-in-python\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "multiprocessrunner", "def", "_init_global", "cls", "config", "pragma", "no", "cover", "super", "_init_global", "config", "disable", "signal", "handling", "in", "the", "child", "processes", "to", "let", "the", "parent", "control", "all", "keyboardinterrupt", "handling", "control", "c", "this", "is", "necessary", "in", "order", "for", "keyboard", "interrupts", "to", "exit", "quickly", "and", "cleanly", "adapted", "from", "this", "post", "https", "stackoverflow", "com", "questions", "11312525", "catch", "ctrlc", "sigint", "and", "exit", "multiprocesses", "gracefully", "in", "python", "signal", "signal", "signal", "sigint", "signal", "sig_ign"], "doc_len": 71}
{"doc_id": "src/sqlfluff/core/linter/runner.py::DelayedException.__init__", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "DelayedException", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: DelayedException\n    def __init__(self, ee, fname=None):\n        self.ee = ee\n        __, __, self.tb = sys.exc_info()\n        self.fname = None\n        super().__init__(str(ee))\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "delayedexception", "def", "__init__", "self", "ee", "fname", "none", "self", "ee", "ee", "__", "__", "self", "tb", "sys", "exc_info", "self", "fname", "none", "super", "__init__", "str", "ee"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/linter/runner.py::DelayedException.reraise", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": "DelayedException", "func_name": "reraise", "text": "文件路径: src/sqlfluff/core/linter/runner.py, 类名: DelayedException\n    def reraise(self):\n        \"\"\"Reraise the encapsulated exception.\"\"\"\n        raise self.ee.with_traceback(self.tb)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "delayedexception", "def", "reraise", "self", "reraise", "the", "encapsulated", "exception", "raise", "self", "ee", "with_traceback", "self", "tb"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/linter/runner.py::get_runner", "file_path": "src/sqlfluff/core/linter/runner.py", "class_name": null, "func_name": "get_runner", "text": "文件路径: src/sqlfluff/core/linter/runner.py\ndef get_runner(\n    linter,\n    config,\n    processes: int,\n    allow_process_parallelism: bool = True,\n) -> BaseRunner:\n    \"\"\"Generate a runner instance based on parallel and sytem configuration.\"\"\"\n    # Python multiprocessing isn't supported in 3.6 and before.\n    # The library exists but we get pickling errors with LintedFile.\n    if processes > 1 and sys.version_info >= (3, 7):\n        # Process parallelism isn't really supported during testing\n        # so this flag allows us to fall back to a threaded runner\n        # in those cases.\n        if allow_process_parallelism:\n            return MultiProcessRunner(linter, config, processes=processes)\n        else:\n            return MultiThreadRunner(linter, config, processes=processes)\n    else:\n        if processes > 1:\n            linter_logger.warning(\n                \"Parallel linting is not supported in Python %s.%s.\",\n                sys.version_info.major,\n                sys.version_info.minor,\n            )\n        return SequentialRunner(linter, config)\n", "tokens": ["src", "sqlfluff", "core", "linter", "runner", "py", "def", "get_runner", "linter", "config", "processes", "int", "allow_process_parallelism", "bool", "true", "baserunner", "generate", "a", "runner", "instance", "based", "on", "parallel", "and", "sytem", "configuration", "python", "multiprocessing", "isn", "t", "supported", "in", "3", "6", "and", "before", "the", "library", "exists", "but", "we", "get", "pickling", "errors", "with", "lintedfile", "if", "processes", "1", "and", "sys", "version_info", "3", "7", "process", "parallelism", "isn", "t", "really", "supported", "during", "testing", "so", "this", "flag", "allows", "us", "to", "fall", "back", "to", "a", "threaded", "runner", "in", "those", "cases", "if", "allow_process_parallelism", "return", "multiprocessrunner", "linter", "config", "processes", "processes", "else", "return", "multithreadrunner", "linter", "config", "processes", "processes", "else", "if", "processes", "1", "linter_logger", "warning", "parallel", "linting", "is", "not", "supported", "in", "python", "s", "s", "sys", "version_info", "major", "sys", "version_info", "minor", "return", "sequentialrunner", "linter", "config"], "doc_len": 117}
{"doc_id": "src/sqlfluff/core/parser/context.py::RootParseContext.__init__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "RootParseContext", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: RootParseContext\n    def __init__(self, dialect, indentation_config=None, recurse=True):\n        \"\"\"Store persistent config objects.\"\"\"\n        self.dialect = dialect\n        self.recurse = recurse\n        # Indentation config is used by Indent and Dedent and used to control\n        # the intended indentation of certain features. Specifically it is\n        # used in the Conditional grammar.\n        self.indentation_config = indentation_config or {}\n        # Initialise the blacklist\n        self.blacklist = ParseBlacklist()\n        # This is the logger that child objects will latch onto.\n        self.logger = parser_logger\n        # A uuid for this parse context to enable cache invalidation\n        self.uuid = uuid.uuid4()\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "rootparsecontext", "def", "__init__", "self", "dialect", "indentation_config", "none", "recurse", "true", "store", "persistent", "config", "objects", "self", "dialect", "dialect", "self", "recurse", "recurse", "indentation", "config", "is", "used", "by", "indent", "and", "dedent", "and", "used", "to", "control", "the", "intended", "indentation", "of", "certain", "features", "specifically", "it", "is", "used", "in", "the", "conditional", "grammar", "self", "indentation_config", "indentation_config", "or", "initialise", "the", "blacklist", "self", "blacklist", "parseblacklist", "this", "is", "the", "logger", "that", "child", "objects", "will", "latch", "onto", "self", "logger", "parser_logger", "a", "uuid", "for", "this", "parse", "context", "to", "enable", "cache", "invalidation", "self", "uuid", "uuid", "uuid4"], "doc_len": 88}
{"doc_id": "src/sqlfluff/core/parser/context.py::RootParseContext.from_config", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "RootParseContext", "func_name": "from_config", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: RootParseContext\n    def from_config(cls, config, **overrides):\n        \"\"\"Construct a `RootParseContext` from a `FluffConfig`.\"\"\"\n        indentation_config = config.get_section(\"indentation\") or {}\n        try:\n            indentation_config = {k: bool(v) for k, v in indentation_config.items()}\n        except TypeError:  # pragma: no cover\n            raise TypeError(\n                \"One of the configuration keys in the `indentation` section is not True or False: {!r}\".format(\n                    indentation_config\n                )\n            )\n        ctx = cls(\n            dialect=config.get(\"dialect_obj\"),\n            recurse=config.get(\"recurse\"),\n            indentation_config=indentation_config,\n        )\n        # Set any overrides in the creation\n        for key in overrides:\n            if overrides[key] is not None:\n                setattr(ctx, key, overrides[key])\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "rootparsecontext", "def", "from_config", "cls", "config", "overrides", "construct", "a", "rootparsecontext", "from", "a", "fluffconfig", "indentation_config", "config", "get_section", "indentation", "or", "try", "indentation_config", "k", "bool", "v", "for", "k", "v", "in", "indentation_config", "items", "except", "typeerror", "pragma", "no", "cover", "raise", "typeerror", "one", "of", "the", "configuration", "keys", "in", "the", "indentation", "section", "is", "not", "true", "or", "false", "r", "format", "indentation_config", "ctx", "cls", "dialect", "config", "get", "dialect_obj", "recurse", "config", "get", "recurse", "indentation_config", "indentation_config", "set", "any", "overrides", "in", "the", "creation", "for", "key", "in", "overrides", "if", "overrides", "key", "is", "not", "none", "setattr", "ctx", "key", "overrides", "key", "return", "ctx"], "doc_len": 93}
{"doc_id": "src/sqlfluff/core/parser/context.py::RootParseContext.__enter__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "RootParseContext", "func_name": "__enter__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: RootParseContext\n    def __enter__(self):\n        \"\"\"Enter into the context.\n\n        Here we return a basic ParseContext with initial values,\n        initialising just the recurse value.\n\n        Note: The RootParseContext is usually entered at the beginning\n        of the parse operation as follows::\n\n            with RootParseContext.from_config(...) as ctx:\n                parsed = file_segment.parse(parse_context=ctx)\n        \"\"\"\n        return ParseContext(root_ctx=self, recurse=self.recurse)\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "rootparsecontext", "def", "__enter__", "self", "enter", "into", "the", "context", "here", "we", "return", "a", "basic", "parsecontext", "with", "initial", "values", "initialising", "just", "the", "recurse", "value", "note", "the", "rootparsecontext", "is", "usually", "entered", "at", "the", "beginning", "of", "the", "parse", "operation", "as", "follows", "with", "rootparsecontext", "from_config", "as", "ctx", "parsed", "file_segment", "parse", "parse_context", "ctx", "return", "parsecontext", "root_ctx", "self", "recurse", "self", "recurse"], "doc_len": 60}
{"doc_id": "src/sqlfluff/core/parser/context.py::RootParseContext.__exit__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "RootParseContext", "func_name": "__exit__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: RootParseContext\n    def __exit__(self, type, value, traceback):\n        \"\"\"Clear up the context.\"\"\"\n        pass\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "rootparsecontext", "def", "__exit__", "self", "type", "value", "traceback", "clear", "up", "the", "context", "pass"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.__init__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def __init__(self, root_ctx, recurse=True):\n        self._root_ctx = root_ctx\n        self.recurse = recurse\n        # The following attributes are only accessible via a copy\n        # and not in the init method.\n        self.match_segment = None\n        self.match_depth = 0\n        self.parse_depth = 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "__init__", "self", "root_ctx", "recurse", "true", "self", "_root_ctx", "root_ctx", "self", "recurse", "recurse", "the", "following", "attributes", "are", "only", "accessible", "via", "a", "copy", "and", "not", "in", "the", "init", "method", "self", "match_segment", "none", "self", "match_depth", "0", "self", "parse_depth", "0"], "doc_len": 43}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.__getattr__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "__getattr__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def __getattr__(self, name):\n        \"\"\"If the attribute doesn't exist on this, revert to the root.\"\"\"\n        try:\n            return getattr(self._root_ctx, name)\n        except AttributeError:  # pragma: no cover\n            raise AttributeError(\n                \"Attribute {!r} not found in {!r} or {!r}\".format(\n                    name, type(self).__name__, type(self._root_ctx).__name__\n                )\n            )\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "__getattr__", "self", "name", "if", "the", "attribute", "doesn", "t", "exist", "on", "this", "revert", "to", "the", "root", "try", "return", "getattr", "self", "_root_ctx", "name", "except", "attributeerror", "pragma", "no", "cover", "raise", "attributeerror", "attribute", "r", "not", "found", "in", "r", "or", "r", "format", "name", "type", "self", "__name__", "type", "self", "_root_ctx", "__name__"], "doc_len": 53}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext._copy", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "_copy", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def _copy(self):\n        \"\"\"Mimic the copy.copy() method but restrict only to local vars.\"\"\"\n        ctx = self.__class__(root_ctx=self._root_ctx)\n        for key in self.__slots__:\n            setattr(ctx, key, getattr(self, key))\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "_copy", "self", "mimic", "the", "copy", "copy", "method", "but", "restrict", "only", "to", "local", "vars", "ctx", "self", "__class__", "root_ctx", "self", "_root_ctx", "for", "key", "in", "self", "__slots__", "setattr", "ctx", "key", "getattr", "self", "key", "return", "ctx"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.__enter__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "__enter__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def __enter__(self):\n        \"\"\"Enter into the context.\n\n        For the ParseContext, this just returns itself, because\n        we already have the right kind of object.\n        \"\"\"\n        return self\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "__enter__", "self", "enter", "into", "the", "context", "for", "the", "parsecontext", "this", "just", "returns", "itself", "because", "we", "already", "have", "the", "right", "kind", "of", "object", "return", "self"], "doc_len": 32}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.__exit__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "__exit__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def __exit__(self, type, value, traceback):\n        \"\"\"Clear up the context.\"\"\"\n        pass\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "__exit__", "self", "type", "value", "traceback", "clear", "up", "the", "context", "pass"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.deeper_match", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "deeper_match", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def deeper_match(self):\n        \"\"\"Return a copy with an incremented match depth.\"\"\"\n        ctx = self._copy()\n        ctx.match_depth += 1\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "deeper_match", "self", "return", "a", "copy", "with", "an", "incremented", "match", "depth", "ctx", "self", "_copy", "ctx", "match_depth", "1", "return", "ctx"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.deeper_parse", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "deeper_parse", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def deeper_parse(self):\n        \"\"\"Return a copy with an incremented parse depth.\"\"\"\n        ctx = self._copy()\n        if not isinstance(ctx.recurse, bool):  # pragma: no cover TODO?\n            ctx.recurse -= 1\n        ctx.parse_depth += 1\n        ctx.match_depth = 0\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "deeper_parse", "self", "return", "a", "copy", "with", "an", "incremented", "parse", "depth", "ctx", "self", "_copy", "if", "not", "isinstance", "ctx", "recurse", "bool", "pragma", "no", "cover", "todo", "ctx", "recurse", "1", "ctx", "parse_depth", "1", "ctx", "match_depth", "0", "return", "ctx"], "doc_len": 42}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.may_recurse", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "may_recurse", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def may_recurse(self):\n        \"\"\"Return True if allowed to recurse.\"\"\"\n        return self.recurse > 1 or self.recurse is True\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "may_recurse", "self", "return", "true", "if", "allowed", "to", "recurse", "return", "self", "recurse", "1", "or", "self", "recurse", "is", "true"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseContext.matching_segment", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseContext", "func_name": "matching_segment", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseContext\n    def matching_segment(self, name):\n        \"\"\"Set the name of the current matching segment.\n\n        NB: We don't reset the match depth here.\n        \"\"\"\n        ctx = self._copy()\n        ctx.match_segment = name\n        return ctx\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parsecontext", "def", "matching_segment", "self", "name", "set", "the", "name", "of", "the", "current", "matching", "segment", "nb", "we", "don", "t", "reset", "the", "match", "depth", "here", "ctx", "self", "_copy", "ctx", "match_segment", "name", "return", "ctx"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseBlacklist.__init__", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseBlacklist", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseBlacklist\n    def __init__(self):\n        self._blacklist_struct = {}\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parseblacklist", "def", "__init__", "self", "self", "_blacklist_struct"], "doc_len": 12}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseBlacklist._hashed_version", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseBlacklist", "func_name": "_hashed_version", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseBlacklist\n    def _hashed_version(self):  # pragma: no cover TODO?\n        return {\n            k: {hash(e) for e in self._blacklist_struct[k]}\n            for k in self._blacklist_struct\n        }\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parseblacklist", "def", "_hashed_version", "self", "pragma", "no", "cover", "todo", "return", "k", "hash", "e", "for", "e", "in", "self", "_blacklist_struct", "k", "for", "k", "in", "self", "_blacklist_struct"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseBlacklist.check", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseBlacklist", "func_name": "check", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseBlacklist\n    def check(self, seg_name, seg_tuple):\n        \"\"\"Check this seg_tuple against this seg_name.\n\n        Has this seg_tuple already been matched\n        unsuccessfully against this segment name.\n        \"\"\"\n        if seg_name in self._blacklist_struct:  # pragma: no cover TODO?\n            if seg_tuple in self._blacklist_struct[seg_name]:\n                return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parseblacklist", "def", "check", "self", "seg_name", "seg_tuple", "check", "this", "seg_tuple", "against", "this", "seg_name", "has", "this", "seg_tuple", "already", "been", "matched", "unsuccessfully", "against", "this", "segment", "name", "if", "seg_name", "in", "self", "_blacklist_struct", "pragma", "no", "cover", "todo", "if", "seg_tuple", "in", "self", "_blacklist_struct", "seg_name", "return", "true", "return", "false"], "doc_len": 48}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseBlacklist.mark", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseBlacklist", "func_name": "mark", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseBlacklist\n    def mark(self, seg_name, seg_tuple):\n        \"\"\"Mark this seg_tuple as not a match with this seg_name.\"\"\"\n        if seg_name in self._blacklist_struct:\n            self._blacklist_struct[seg_name].add(seg_tuple)\n        else:\n            self._blacklist_struct[seg_name] = {seg_tuple}\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parseblacklist", "def", "mark", "self", "seg_name", "seg_tuple", "mark", "this", "seg_tuple", "as", "not", "a", "match", "with", "this", "seg_name", "if", "seg_name", "in", "self", "_blacklist_struct", "self", "_blacklist_struct", "seg_name", "add", "seg_tuple", "else", "self", "_blacklist_struct", "seg_name", "seg_tuple"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/context.py::ParseBlacklist.clear", "file_path": "src/sqlfluff/core/parser/context.py", "class_name": "ParseBlacklist", "func_name": "clear", "text": "文件路径: src/sqlfluff/core/parser/context.py, 类名: ParseBlacklist\n    def clear(self):\n        \"\"\"Clear the blacklist struct.\"\"\"\n        self._blacklist_struct = {}\n", "tokens": ["src", "sqlfluff", "core", "parser", "context", "py", "parseblacklist", "def", "clear", "self", "clear", "the", "blacklist", "struct", "self", "_blacklist_struct"], "doc_len": 16}
{"doc_id": "src/sqlfluff/core/parser/helpers.py::join_segments_raw", "file_path": "src/sqlfluff/core/parser/helpers.py", "class_name": null, "func_name": "join_segments_raw", "text": "文件路径: src/sqlfluff/core/parser/helpers.py\ndef join_segments_raw(segments: Tuple[\"BaseSegment\", ...]) -> str:\n    \"\"\"Make a string from the joined `raw` attributes of an iterable of segments.\"\"\"\n    return \"\".join(s.raw for s in segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "helpers", "py", "def", "join_segments_raw", "segments", "tuple", "basesegment", "str", "make", "a", "string", "from", "the", "joined", "raw", "attributes", "of", "an", "iterable", "of", "segments", "return", "join", "s", "raw", "for", "s", "in", "segments"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/helpers.py::join_segments_raw_curtailed", "file_path": "src/sqlfluff/core/parser/helpers.py", "class_name": null, "func_name": "join_segments_raw_curtailed", "text": "文件路径: src/sqlfluff/core/parser/helpers.py\ndef join_segments_raw_curtailed(segments: Tuple[\"BaseSegment\", ...], length=20) -> str:\n    \"\"\"Make a string up to a certain length from an iterable of segments.\"\"\"\n    return curtail_string(join_segments_raw(segments), length=length)\n", "tokens": ["src", "sqlfluff", "core", "parser", "helpers", "py", "def", "join_segments_raw_curtailed", "segments", "tuple", "basesegment", "length", "20", "str", "make", "a", "string", "up", "to", "a", "certain", "length", "from", "an", "iterable", "of", "segments", "return", "curtail_string", "join_segments_raw", "segments", "length", "length"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/helpers.py::check_still_complete", "file_path": "src/sqlfluff/core/parser/helpers.py", "class_name": null, "func_name": "check_still_complete", "text": "文件路径: src/sqlfluff/core/parser/helpers.py\ndef check_still_complete(\n    segments_in: Tuple[\"BaseSegment\", ...],\n    matched_segments: Tuple[\"BaseSegment\", ...],\n    unmatched_segments: Tuple[\"BaseSegment\", ...],\n) -> bool:\n    \"\"\"Check that the segments in are the same as the segments out.\"\"\"\n    initial_str = join_segments_raw(segments_in)\n    current_str = join_segments_raw(matched_segments + unmatched_segments)\n\n    if initial_str != current_str:\n        raise SQLParseError(\n            f\"Could not parse: {current_str}\",\n            segment=unmatched_segments[0],\n        )\n    return True\n", "tokens": ["src", "sqlfluff", "core", "parser", "helpers", "py", "def", "check_still_complete", "segments_in", "tuple", "basesegment", "matched_segments", "tuple", "basesegment", "unmatched_segments", "tuple", "basesegment", "bool", "check", "that", "the", "segments", "in", "are", "the", "same", "as", "the", "segments", "out", "initial_str", "join_segments_raw", "segments_in", "current_str", "join_segments_raw", "matched_segments", "unmatched_segments", "if", "initial_str", "current_str", "raise", "sqlparseerror", "f", "could", "not", "parse", "current_str", "segment", "unmatched_segments", "0", "return", "true"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/parser/helpers.py::trim_non_code_segments", "file_path": "src/sqlfluff/core/parser/helpers.py", "class_name": null, "func_name": "trim_non_code_segments", "text": "文件路径: src/sqlfluff/core/parser/helpers.py\ndef trim_non_code_segments(\n    segments: Tuple[\"BaseSegment\", ...]\n) -> Tuple[\n    Tuple[\"BaseSegment\", ...], Tuple[\"BaseSegment\", ...], Tuple[\"BaseSegment\", ...]\n]:\n    \"\"\"Take segments and split off surrounding non-code segments as appropriate.\n\n    We use slices to avoid creating too many unnecessary tuples.\n    \"\"\"\n    pre_idx = 0\n    seg_len = len(segments)\n    post_idx = seg_len\n\n    if segments:\n        seg_len = len(segments)\n\n        # Trim the start\n        while pre_idx < seg_len and not segments[pre_idx].is_code:\n            pre_idx += 1\n\n        # Trim the end\n        while post_idx > pre_idx and not segments[post_idx - 1].is_code:\n            post_idx -= 1\n\n    return segments[:pre_idx], segments[pre_idx:post_idx], segments[post_idx:]\n", "tokens": ["src", "sqlfluff", "core", "parser", "helpers", "py", "def", "trim_non_code_segments", "segments", "tuple", "basesegment", "tuple", "tuple", "basesegment", "tuple", "basesegment", "tuple", "basesegment", "take", "segments", "and", "split", "off", "surrounding", "non", "code", "segments", "as", "appropriate", "we", "use", "slices", "to", "avoid", "creating", "too", "many", "unnecessary", "tuples", "pre_idx", "0", "seg_len", "len", "segments", "post_idx", "seg_len", "if", "segments", "seg_len", "len", "segments", "trim", "the", "start", "while", "pre_idx", "seg_len", "and", "not", "segments", "pre_idx", "is_code", "pre_idx", "1", "trim", "the", "end", "while", "post_idx", "pre_idx", "and", "not", "segments", "post_idx", "1", "is_code", "post_idx", "1", "return", "segments", "pre_idx", "segments", "pre_idx", "post_idx", "segments", "post_idx"], "doc_len": 86}
{"doc_id": "src/sqlfluff/core/parser/helpers.py::iter_indices", "file_path": "src/sqlfluff/core/parser/helpers.py", "class_name": null, "func_name": "iter_indices", "text": "文件路径: src/sqlfluff/core/parser/helpers.py\ndef iter_indices(seq: List, val: Any) -> Iterator[int]:\n    \"\"\"Iterate all indices in a list that val occurs at.\n\n    Args:\n        seq (list): A list to look for indices in.\n        val: What to look for.\n\n    Yields:\n        int: The index of val in seq.\n\n    Examples:\n        The function works like str.index() but iterates all\n        the results rather than returning the first.\n\n        >>> print([i for i in iter_indices([1, 0, 2, 3, 2], 2)])\n        [2, 4]\n    \"\"\"\n    for idx, el in enumerate(seq):\n        if el == val:\n            yield idx\n", "tokens": ["src", "sqlfluff", "core", "parser", "helpers", "py", "def", "iter_indices", "seq", "list", "val", "any", "iterator", "int", "iterate", "all", "indices", "in", "a", "list", "that", "val", "occurs", "at", "args", "seq", "list", "a", "list", "to", "look", "for", "indices", "in", "val", "what", "to", "look", "for", "yields", "int", "the", "index", "of", "val", "in", "seq", "examples", "the", "function", "works", "like", "str", "index", "but", "iterates", "all", "the", "results", "rather", "than", "returning", "the", "first", "print", "i", "for", "i", "in", "iter_indices", "1", "0", "2", "3", "2", "2", "2", "4", "for", "idx", "el", "in", "enumerate", "seq", "if", "el", "val", "yield", "idx"], "doc_len": 89}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::TemplateElement.from_element", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "TemplateElement", "func_name": "from_element", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: TemplateElement\n    def from_element(cls, element: LexedElement, template_slice: slice):\n        \"\"\"Make a TemplateElement from a LexedElement.\"\"\"\n        return cls(\n            raw=element.raw, template_slice=template_slice, matcher=element.matcher\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "templateelement", "def", "from_element", "cls", "element", "lexedelement", "template_slice", "slice", "make", "a", "templateelement", "from", "a", "lexedelement", "return", "cls", "raw", "element", "raw", "template_slice", "template_slice", "matcher", "element", "matcher"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::TemplateElement.to_segment", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "TemplateElement", "func_name": "to_segment", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: TemplateElement\n    def to_segment(self, pos_marker):\n        \"\"\"Create a segment from this lexed element.\"\"\"\n        return self.matcher.construct_segment(self.raw, pos_marker=pos_marker)\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "templateelement", "def", "to_segment", "self", "pos_marker", "create", "a", "segment", "from", "this", "lexed", "element", "return", "self", "matcher", "construct_segment", "self", "raw", "pos_marker", "pos_marker"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::LexMatch.__bool__", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "LexMatch", "func_name": "__bool__", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: LexMatch\n    def __bool__(self):\n        \"\"\"A LexMatch is truthy if it contains a non-zero number of matched elements.\"\"\"\n        return len(self.elements) > 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexmatch", "def", "__bool__", "self", "a", "lexmatch", "is", "truthy", "if", "it", "contains", "a", "non", "zero", "number", "of", "matched", "elements", "return", "len", "self", "elements", "0"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer.__init__", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def __init__(\n        self,\n        name,\n        template,\n        segment_class,\n        subdivider=None,\n        trim_post_subdivide=None,\n        segment_kwargs=None,\n    ):\n        self.name = name\n        self.template = template\n        self.segment_class = segment_class\n        self.subdivider = subdivider\n        self.trim_post_subdivide = trim_post_subdivide\n        self.segment_kwargs = segment_kwargs or {}\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "__init__", "self", "name", "template", "segment_class", "subdivider", "none", "trim_post_subdivide", "none", "segment_kwargs", "none", "self", "name", "name", "self", "template", "template", "self", "segment_class", "segment_class", "self", "subdivider", "subdivider", "self", "trim_post_subdivide", "trim_post_subdivide", "self", "segment_kwargs", "segment_kwargs", "or"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer.__repr__", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}: {self.name}>\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "__repr__", "self", "return", "f", "self", "__class__", "__name__", "self", "name"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer._match", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "_match", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def _match(self, forward_string: str) -> Optional[LexedElement]:\n        \"\"\"The private match function. Just look for a literal string.\"\"\"\n        if forward_string.startswith(self.template):\n            return LexedElement(self.template, self)\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "_match", "self", "forward_string", "str", "optional", "lexedelement", "the", "private", "match", "function", "just", "look", "for", "a", "literal", "string", "if", "forward_string", "startswith", "self", "template", "return", "lexedelement", "self", "template", "self", "else", "return", "none"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer.search", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "search", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def search(self, forward_string: str) -> Optional[Tuple[int, int]]:\n        \"\"\"Use string methods to find a substring.\"\"\"\n        loc = forward_string.find(self.template)\n        if loc >= 0:\n            return loc, loc + len(self.template)\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "search", "self", "forward_string", "str", "optional", "tuple", "int", "int", "use", "string", "methods", "to", "find", "a", "substring", "loc", "forward_string", "find", "self", "template", "if", "loc", "0", "return", "loc", "loc", "len", "self", "template", "else", "return", "none"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer._trim_match", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "_trim_match", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def _trim_match(self, matched_str: str) -> List[LexedElement]:\n        \"\"\"Given a string, trim if we are allowed to.\n\n        Returns:\n            :obj:`tuple` of LexedElement\n\n        \"\"\"\n        elem_buff: List[LexedElement] = []\n        content_buff = \"\"\n        str_buff = matched_str\n\n        if self.trim_post_subdivide:\n            while str_buff:\n                # Iterate through subdividing as appropriate\n                trim_pos = self.trim_post_subdivide.search(str_buff)\n                # No match? Break\n                if not trim_pos:\n                    break\n                # Start match?\n                elif trim_pos[0] == 0:\n                    elem_buff.append(\n                        LexedElement(\n                            str_buff[: trim_pos[1]],\n                            self.trim_post_subdivide,\n                        )\n                    )\n                    str_buff = str_buff[trim_pos[1] :]\n                # End Match?\n                elif trim_pos[1] == len(str_buff):\n                    elem_buff += [\n                        LexedElement(\n                            content_buff + str_buff[: trim_pos[0]],\n                            self,\n                        ),\n                        LexedElement(\n                            str_buff[trim_pos[0] : trim_pos[1]],\n                            self.trim_post_subdivide,\n                        ),\n                    ]\n                    content_buff, str_buff = \"\", \"\"\n                # Mid Match? (carry on)\n                else:\n                    content_buff += str_buff[: trim_pos[1]]\n                    str_buff = str_buff[trim_pos[1] :]\n\n        # Do we have anything left? (or did nothing happen)\n        if content_buff + str_buff:\n            elem_buff.append(\n                LexedElement(content_buff + str_buff, self),\n            )\n        return elem_buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "_trim_match", "self", "matched_str", "str", "list", "lexedelement", "given", "a", "string", "trim", "if", "we", "are", "allowed", "to", "returns", "obj", "tuple", "of", "lexedelement", "elem_buff", "list", "lexedelement", "content_buff", "str_buff", "matched_str", "if", "self", "trim_post_subdivide", "while", "str_buff", "iterate", "through", "subdividing", "as", "appropriate", "trim_pos", "self", "trim_post_subdivide", "search", "str_buff", "no", "match", "break", "if", "not", "trim_pos", "break", "start", "match", "elif", "trim_pos", "0", "0", "elem_buff", "append", "lexedelement", "str_buff", "trim_pos", "1", "self", "trim_post_subdivide", "str_buff", "str_buff", "trim_pos", "1", "end", "match", "elif", "trim_pos", "1", "len", "str_buff", "elem_buff", "lexedelement", "content_buff", "str_buff", "trim_pos", "0", "self", "lexedelement", "str_buff", "trim_pos", "0", "trim_pos", "1", "self", "trim_post_subdivide", "content_buff", "str_buff", "mid", "match", "carry", "on", "else", "content_buff", "str_buff", "trim_pos", "1", "str_buff", "str_buff", "trim_pos", "1", "do", "we", "have", "anything", "left", "or", "did", "nothing", "happen", "if", "content_buff", "str_buff", "elem_buff", "append", "lexedelement", "content_buff", "str_buff", "self", "return", "elem_buff"], "doc_len": 131}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer._subdivide", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "_subdivide", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def _subdivide(self, matched: LexedElement) -> List[LexedElement]:\n        \"\"\"Given a string, subdivide if we area allowed to.\n\n        Returns:\n            :obj:`tuple` of segments\n\n        \"\"\"\n        # Can we have to subdivide?\n        if self.subdivider:\n            # Yes subdivision\n            elem_buff: List[LexedElement] = []\n            str_buff = matched.raw\n            while str_buff:\n                # Iterate through subdividing as appropriate\n                div_pos = self.subdivider.search(str_buff)\n                if div_pos:\n                    # Found a division\n                    trimmed_elems = self._trim_match(str_buff[: div_pos[0]])\n                    div_elem = LexedElement(\n                        str_buff[div_pos[0] : div_pos[1]], self.subdivider\n                    )\n                    elem_buff += trimmed_elems + [div_elem]\n                    str_buff = str_buff[div_pos[1] :]\n                else:\n                    # No more division matches. Trim?\n                    trimmed_elems = self._trim_match(str_buff)\n                    elem_buff += trimmed_elems\n                    break\n            return elem_buff\n        else:\n            return [matched]\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "_subdivide", "self", "matched", "lexedelement", "list", "lexedelement", "given", "a", "string", "subdivide", "if", "we", "area", "allowed", "to", "returns", "obj", "tuple", "of", "segments", "can", "we", "have", "to", "subdivide", "if", "self", "subdivider", "yes", "subdivision", "elem_buff", "list", "lexedelement", "str_buff", "matched", "raw", "while", "str_buff", "iterate", "through", "subdividing", "as", "appropriate", "div_pos", "self", "subdivider", "search", "str_buff", "if", "div_pos", "found", "a", "division", "trimmed_elems", "self", "_trim_match", "str_buff", "div_pos", "0", "div_elem", "lexedelement", "str_buff", "div_pos", "0", "div_pos", "1", "self", "subdivider", "elem_buff", "trimmed_elems", "div_elem", "str_buff", "str_buff", "div_pos", "1", "else", "no", "more", "division", "matches", "trim", "trimmed_elems", "self", "_trim_match", "str_buff", "elem_buff", "trimmed_elems", "break", "return", "elem_buff", "else", "return", "matched"], "doc_len": 101}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer.match", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def match(self, forward_string: str) -> LexMatch:\n        \"\"\"Given a string, match what we can and return the rest.\n\n        Returns:\n            :obj:`LexMatch`\n\n        \"\"\"\n        if len(forward_string) == 0:  # pragma: no cover\n            raise ValueError(\"Unexpected empty string!\")\n        matched = self._match(forward_string)\n\n        if matched:\n            # Handle potential subdivision elsewhere.\n            new_elements = self._subdivide(matched)\n\n            return LexMatch(\n                forward_string[len(matched.raw) :],\n                new_elements,\n            )\n        else:\n            return LexMatch(forward_string, [])\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "match", "self", "forward_string", "str", "lexmatch", "given", "a", "string", "match", "what", "we", "can", "and", "return", "the", "rest", "returns", "obj", "lexmatch", "if", "len", "forward_string", "0", "pragma", "no", "cover", "raise", "valueerror", "unexpected", "empty", "string", "matched", "self", "_match", "forward_string", "if", "matched", "handle", "potential", "subdivision", "elsewhere", "new_elements", "self", "_subdivide", "matched", "return", "lexmatch", "forward_string", "len", "matched", "raw", "new_elements", "else", "return", "lexmatch", "forward_string"], "doc_len": 64}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::StringLexer.construct_segment", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "StringLexer", "func_name": "construct_segment", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: StringLexer\n    def construct_segment(self, raw, pos_marker):\n        \"\"\"Construct a segment using the given class a properties.\"\"\"\n        return self.segment_class(\n            raw=raw, pos_marker=pos_marker, name=self.name, **self.segment_kwargs\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "stringlexer", "def", "construct_segment", "self", "raw", "pos_marker", "construct", "a", "segment", "using", "the", "given", "class", "a", "properties", "return", "self", "segment_class", "raw", "raw", "pos_marker", "pos_marker", "name", "self", "name", "self", "segment_kwargs"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::RegexLexer.__init__", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "RegexLexer", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: RegexLexer\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        # We might want to configure this at some point, but for now, newlines\n        # do get matched by .\n        flags = re.DOTALL\n        self._compiled_regex = re.compile(self.template, flags)\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "regexlexer", "def", "__init__", "self", "args", "kwargs", "super", "__init__", "args", "kwargs", "we", "might", "want", "to", "configure", "this", "at", "some", "point", "but", "for", "now", "newlines", "do", "get", "matched", "by", "flags", "re", "dotall", "self", "_compiled_regex", "re", "compile", "self", "template", "flags"], "doc_len": 43}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::RegexLexer._match", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "RegexLexer", "func_name": "_match", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: RegexLexer\n    def _match(self, forward_string: str) -> Optional[LexedElement]:\n        \"\"\"Use regexes to match chunks.\"\"\"\n        match = self._compiled_regex.match(forward_string)\n        if match:\n            # We can only match strings with length\n            match_str = match.group(0)\n            if match_str:\n                return LexedElement(match_str, self)\n            else:\n                lexer_logger.warning(\n                    f\"Zero length Lex item returned from {self.name!r}. Report this as a bug.\"\n                )\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "regexlexer", "def", "_match", "self", "forward_string", "str", "optional", "lexedelement", "use", "regexes", "to", "match", "chunks", "match", "self", "_compiled_regex", "match", "forward_string", "if", "match", "we", "can", "only", "match", "strings", "with", "length", "match_str", "match", "group", "0", "if", "match_str", "return", "lexedelement", "match_str", "self", "else", "lexer_logger", "warning", "f", "zero", "length", "lex", "item", "returned", "from", "self", "name", "r", "report", "this", "as", "a", "bug", "return", "none"], "doc_len": 63}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::RegexLexer.search", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "RegexLexer", "func_name": "search", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: RegexLexer\n    def search(self, forward_string: str) -> Optional[Tuple[int, int]]:\n        \"\"\"Use regex to find a substring.\"\"\"\n        match = self._compiled_regex.search(forward_string)\n        if match:\n            # We can only match strings with length\n            if match.group(0):\n                return match.span()\n            else:  # pragma: no cover\n                lexer_logger.warning(\n                    f\"Zero length Lex item returned from {self.name!r}. Report this as a bug.\"\n                )\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "regexlexer", "def", "search", "self", "forward_string", "str", "optional", "tuple", "int", "int", "use", "regex", "to", "find", "a", "substring", "match", "self", "_compiled_regex", "search", "forward_string", "if", "match", "we", "can", "only", "match", "strings", "with", "length", "if", "match", "group", "0", "return", "match", "span", "else", "pragma", "no", "cover", "lexer_logger", "warning", "f", "zero", "length", "lex", "item", "returned", "from", "self", "name", "r", "report", "this", "as", "a", "bug", "return", "none"], "doc_len": 66}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.__init__", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def __init__(\n        self,\n        config: Optional[FluffConfig] = None,\n        last_resort_lexer: Optional[StringLexer] = None,\n        dialect: Optional[str] = None,\n    ):\n        # Allow optional config and dialect\n        self.config = FluffConfig.from_kwargs(config=config, dialect=dialect)\n        # Store the matchers\n        self.lexer_matchers = self.config.get(\"dialect_obj\").get_lexer_matchers()\n\n        self.last_resort_lexer = last_resort_lexer or RegexLexer(\n            \"<unlexable>\",\n            r\"[^\\t\\n\\,\\.\\ \\-\\+\\*\\\\\\/\\'\\\"\\;\\:\\[\\]\\(\\)\\|]*\",\n            UnlexableSegment,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "__init__", "self", "config", "optional", "fluffconfig", "none", "last_resort_lexer", "optional", "stringlexer", "none", "dialect", "optional", "str", "none", "allow", "optional", "config", "and", "dialect", "self", "config", "fluffconfig", "from_kwargs", "config", "config", "dialect", "dialect", "store", "the", "matchers", "self", "lexer_matchers", "self", "config", "get", "dialect_obj", "get_lexer_matchers", "self", "last_resort_lexer", "last_resort_lexer", "or", "regexlexer", "unlexable", "r", "t", "n", "unlexablesegment"], "doc_len": 55}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.lex", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "lex", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def lex(\n        self, raw: Union[str, TemplatedFile]\n    ) -> Tuple[Tuple[BaseSegment, ...], List[SQLLexError]]:\n        \"\"\"Take a string or TemplatedFile and return segments.\n\n        If we fail to match the *whole* string, then we must have\n        found something that we cannot lex. If that happens we should\n        package it up as unlexable and keep track of the exceptions.\n        \"\"\"\n        # Make sure we've got a string buffer and a template\n        # regardless of what was passed in.\n        if isinstance(raw, str):\n            template = TemplatedFile.from_string(raw)\n            str_buff = raw\n        else:\n            template = raw\n            str_buff = str(template)\n\n        # Lex the string to get a tuple of LexedElement\n        element_buffer: List[LexedElement] = []\n        while True:\n            res = self.lex_match(str_buff, self.lexer_matchers)\n            element_buffer += res.elements\n            if res.forward_string:\n                resort_res = self.last_resort_lexer.match(res.forward_string)\n                if not resort_res:\n                    # If we STILL can't match, then just panic out.\n                    raise SQLLexError(\n                        f\"Fatal. Unable to lex characters: {0!r}\".format(\n                            res.forward_string[:10] + \"...\"\n                            if len(res.forward_string) > 9\n                            else res.forward_string\n                        )\n                    )\n                str_buff = resort_res.forward_string\n                element_buffer += resort_res.elements\n            else:  # pragma: no cover TODO?\n                break\n\n        # Map tuple LexedElement to list of TemplateElement.\n        # This adds the template_slice to the object.\n        templated_buffer = self.map_template_slices(element_buffer, template)\n\n        # Turn lexed elements into segments.\n        segments: Tuple[RawSegment, ...] = self.elements_to_segments(\n            templated_buffer, template\n        )\n\n        # Generate any violations\n        violations: List[SQLLexError] = self.violations_from_segments(segments)\n\n        return segments, violations\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "lex", "self", "raw", "union", "str", "templatedfile", "tuple", "tuple", "basesegment", "list", "sqllexerror", "take", "a", "string", "or", "templatedfile", "and", "return", "segments", "if", "we", "fail", "to", "match", "the", "whole", "string", "then", "we", "must", "have", "found", "something", "that", "we", "cannot", "lex", "if", "that", "happens", "we", "should", "package", "it", "up", "as", "unlexable", "and", "keep", "track", "of", "the", "exceptions", "make", "sure", "we", "ve", "got", "a", "string", "buffer", "and", "a", "template", "regardless", "of", "what", "was", "passed", "in", "if", "isinstance", "raw", "str", "template", "templatedfile", "from_string", "raw", "str_buff", "raw", "else", "template", "raw", "str_buff", "str", "template", "lex", "the", "string", "to", "get", "a", "tuple", "of", "lexedelement", "element_buffer", "list", "lexedelement", "while", "true", "res", "self", "lex_match", "str_buff", "self", "lexer_matchers", "element_buffer", "res", "elements", "if", "res", "forward_string", "resort_res", "self", "last_resort_lexer", "match", "res", "forward_string", "if", "not", "resort_res", "if", "we", "still", "can", "t", "match", "then", "just", "panic", "out", "raise", "sqllexerror", "f", "fatal", "unable", "to", "lex", "characters", "0", "r", "format", "res", "forward_string", "10", "if", "len", "res", "forward_string", "9", "else", "res", "forward_string", "str_buff", "resort_res", "forward_string", "element_buffer", "resort_res", "elements", "else", "pragma", "no", "cover", "todo", "break", "map", "tuple", "lexedelement", "to", "list", "of", "templateelement", "this", "adds", "the", "template_slice", "to", "the", "object", "templated_buffer", "self", "map_template_slices", "element_buffer", "template", "turn", "lexed", "elements", "into", "segments", "segments", "tuple", "rawsegment", "self", "elements_to_segments", "templated_buffer", "template", "generate", "any", "violations", "violations", "list", "sqllexerror", "self", "violations_from_segments", "segments", "return", "segments", "violations"], "doc_len": 216}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.elements_to_segments", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "elements_to_segments", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def elements_to_segments(\n        self, elements: List[TemplateElement], templated_file: TemplatedFile\n    ) -> Tuple[RawSegment, ...]:\n        \"\"\"Convert a tuple of lexed elements into a tuple of segments.\"\"\"\n        # Working buffer to build up segments\n        segment_buffer: List[RawSegment] = []\n\n        lexer_logger.info(\"Elements to Segments.\")\n        # Get the templated slices to re-insert tokens for them\n        source_only_slices = templated_file.source_only_slices()\n        lexer_logger.info(\"Source-only slices: %s\", source_only_slices)\n        stash_source_slice, last_source_slice = None, None\n\n        # Now work out source slices, and add in template placeholders.\n        for idx, element in enumerate(elements):\n            # Calculate Source Slice\n            if idx != 0:\n                last_source_slice = stash_source_slice\n            source_slice = templated_file.templated_slice_to_source_slice(\n                element.template_slice\n            )\n            stash_source_slice = source_slice\n            # Output the slice as we lex.\n            lexer_logger.debug(\n                \"  %s, %s, %s, %r\",\n                idx,\n                element,\n                source_slice,\n                templated_file.templated_str[element.template_slice],\n            )\n\n            # The calculated source slice will include any source only slices.\n            # We should consider all of them in turn to see whether we can\n            # insert them.\n            so_slices = []\n            # Only look for source only slices if we've got a new source slice to\n            # avoid unnecessary duplication.\n            if last_source_slice != source_slice:\n                for source_only_slice in source_only_slices:\n                    # If it's later in the source, stop looking. Any later\n                    # ones *also* won't match.\n                    if source_only_slice.source_idx >= source_slice.stop:\n                        break\n                    elif source_only_slice.source_idx >= source_slice.start:\n                        so_slices.append(source_only_slice)\n\n            if so_slices:\n                lexer_logger.debug(\"    Collected Source Only Slices\")\n                for so_slice in so_slices:\n                    lexer_logger.debug(\"       %s\", so_slice)\n\n                # Calculate some things which will be useful\n                templ_str = templated_file.templated_str[element.template_slice]\n                source_str = templated_file.source_str[source_slice]\n\n                # For reasons which aren't entirely clear right now, if there is\n                # an included literal, it will always be at the end. Let's see if it's\n                # there.\n                if source_str.endswith(templ_str):\n                    existing_len = len(templ_str)\n                else:\n                    existing_len = 0\n\n                # Calculate slices\n                placeholder_slice = slice(\n                    source_slice.start, source_slice.stop - existing_len\n                )\n                placeholder_str = source_str[:-existing_len]\n                source_slice = slice(\n                    source_slice.stop - existing_len, source_slice.stop\n                )\n                # If it doesn't manage to extract a placeholder string from the source\n                # just concatenate the source only strings. There is almost always\n                # only one of them.\n                if not placeholder_str:\n                    placeholder_str = \"\".join(s.raw for s in so_slices)\n                lexer_logger.debug(\n                    \"    Overlap Length: %s. PS: %s, LS: %s, p_str: %r, templ_str: %r\",\n                    existing_len,\n                    placeholder_slice,\n                    source_slice,\n                    placeholder_str,\n                    templ_str,\n                )\n\n                # Caluculate potential indent/dedent\n                block_slices = sum(s.slice_type.startswith(\"block_\") for s in so_slices)\n                block_balance = sum(\n                    s.slice_type == \"block_start\" for s in so_slices\n                ) - sum(s.slice_type == \"block_end\" for s in so_slices)\n                lead_dedent = so_slices[0].slice_type in (\"block_end\", \"block_mid\")\n                trail_indent = so_slices[-1].slice_type in (\"block_start\", \"block_mid\")\n                add_indents = self.config.get(\"template_blocks_indent\", \"indentation\")\n                lexer_logger.debug(\n                    \"    Block Slices: %s. Block Balance: %s. Lead: %s, Trail: %s, Add: %s\",\n                    block_slices,\n                    block_balance,\n                    lead_dedent,\n                    trail_indent,\n                    add_indents,\n                )\n\n                # Add a dedent if appropriate.\n                if lead_dedent and add_indents:\n                    lexer_logger.debug(\"      DEDENT\")\n                    segment_buffer.append(\n                        Dedent(\n                            pos_marker=PositionMarker.from_point(\n                                placeholder_slice.start,\n                                element.template_slice.start,\n                                templated_file,\n                            )\n                        )\n                    )\n\n                # Always add a placeholder\n                segment_buffer.append(\n                    TemplateSegment(\n                        pos_marker=PositionMarker(\n                            placeholder_slice,\n                            slice(\n                                element.template_slice.start,\n                                element.template_slice.start,\n                            ),\n                            templated_file,\n                        ),\n                        source_str=placeholder_str,\n                        block_type=so_slices[0].slice_type\n                        if len(so_slices) == 1\n                        else \"compound\",\n                    )\n                )\n                lexer_logger.debug(\n                    \"      Placholder: %s, %r\", segment_buffer[-1], placeholder_str\n                )\n\n                # Add a dedent if appropriate.\n                if trail_indent and add_indents:\n                    lexer_logger.debug(\"      INDENT\")\n                    segment_buffer.append(\n                        Indent(\n                            pos_marker=PositionMarker.from_point(\n                                placeholder_slice.stop,\n                                element.template_slice.start,\n                                templated_file,\n                            )\n                        )\n                    )\n\n            # Add the actual segment\n            segment_buffer.append(\n                element.to_segment(\n                    pos_marker=PositionMarker(\n                        source_slice,\n                        element.template_slice,\n                        templated_file,\n                    ),\n                )\n            )\n\n        # Convert to tuple before return\n        return tuple(segment_buffer)\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "elements_to_segments", "self", "elements", "list", "templateelement", "templated_file", "templatedfile", "tuple", "rawsegment", "convert", "a", "tuple", "of", "lexed", "elements", "into", "a", "tuple", "of", "segments", "working", "buffer", "to", "build", "up", "segments", "segment_buffer", "list", "rawsegment", "lexer_logger", "info", "elements", "to", "segments", "get", "the", "templated", "slices", "to", "re", "insert", "tokens", "for", "them", "source_only_slices", "templated_file", "source_only_slices", "lexer_logger", "info", "source", "only", "slices", "s", "source_only_slices", "stash_source_slice", "last_source_slice", "none", "none", "now", "work", "out", "source", "slices", "and", "add", "in", "template", "placeholders", "for", "idx", "element", "in", "enumerate", "elements", "calculate", "source", "slice", "if", "idx", "0", "last_source_slice", "stash_source_slice", "source_slice", "templated_file", "templated_slice_to_source_slice", "element", "template_slice", "stash_source_slice", "source_slice", "output", "the", "slice", "as", "we", "lex", "lexer_logger", "debug", "s", "s", "s", "r", "idx", "element", "source_slice", "templated_file", "templated_str", "element", "template_slice", "the", "calculated", "source", "slice", "will", "include", "any", "source", "only", "slices", "we", "should", "consider", "all", "of", "them", "in", "turn", "to", "see", "whether", "we", "can", "insert", "them", "so_slices", "only", "look", "for", "source", "only", "slices", "if", "we", "ve", "got", "a", "new", "source", "slice", "to", "avoid", "unnecessary", "duplication", "if", "last_source_slice", "source_slice", "for", "source_only_slice", "in", "source_only_slices", "if", "it", "s", "later", "in", "the", "source", "stop", "looking", "any", "later", "ones", "also", "won", "t", "match", "if", "source_only_slice", "source_idx", "source_slice", "stop", "break", "elif", "source_only_slice", "source_idx", "source_slice", "start", "so_slices", "append", "source_only_slice", "if", "so_slices", "lexer_logger", "debug", "collected", "source", "only", "slices", "for", "so_slice", "in", "so_slices", "lexer_logger", "debug", "s", "so_slice", "calculate", "some", "things", "which", "will", "be", "useful", "templ_str", "templated_file", "templated_str", "element", "template_slice", "source_str", "templated_file", "source_str", "source_slice", "for", "reasons", "which", "aren", "t", "entirely", "clear", "right", "now", "if", "there", "is", "an", "included", "literal", "it", "will", "always", "be", "at", "the", "end", "let", "s", "see", "if", "it", "s", "there", "if", "source_str", "endswith", "templ_str", "existing_len", "len", "templ_str", "else", "existing_len", "0", "calculate", "slices", "placeholder_slice", "slice", "source_slice", "start", "source_slice", "stop", "existing_len", "placeholder_str", "source_str", "existing_len", "source_slice", "slice", "source_slice", "stop", "existing_len", "source_slice", "stop", "if", "it", "doesn", "t", "manage", "to", "extract", "a", "placeholder", "string", "from", "the", "source", "just", "concatenate", "the", "source", "only", "strings", "there", "is", "almost", "always", "only", "one", "of", "them", "if", "not", "placeholder_str", "placeholder_str", "join", "s", "raw", "for", "s", "in", "so_slices", "lexer_logger", "debug", "overlap", "length", "s", "ps", "s", "ls", "s", "p_str", "r", "templ_str", "r", "existing_len", "placeholder_slice", "source_slice", "placeholder_str", "templ_str", "caluculate", "potential", "indent", "dedent", "block_slices", "sum", "s", "slice_type", "startswith", "block_", "for", "s", "in", "so_slices", "block_balance", "sum", "s", "slice_type", "block_start", "for", "s", "in", "so_slices", "sum", "s", "slice_type", "block_end", "for", "s", "in", "so_slices", "lead_dedent", "so_slices", "0", "slice_type", "in", "block_end", "block_mid", "trail_indent", "so_slices", "1", "slice_type", "in", "block_start", "block_mid", "add_indents", "self", "config", "get", "template_blocks_indent", "indentation", "lexer_logger", "debug", "block", "slices", "s", "block", "balance", "s", "lead", "s", "trail", "s", "add", "s", "block_slices", "block_balance", "lead_dedent", "trail_indent", "add_indents", "add", "a", "dedent", "if", "appropriate", "if", "lead_dedent", "and", "add_indents", "lexer_logger", "debug", "dedent", "segment_buffer", "append", "dedent", "pos_marker", "positionmarker", "from_point", "placeholder_slice", "start", "element", "template_slice", "start", "templated_file", "always", "add", "a", "placeholder", "segment_buffer", "append", "templatesegment", "pos_marker", "positionmarker", "placeholder_slice", "slice", "element", "template_slice", "start", "element", "template_slice", "start", "templated_file", "source_str", "placeholder_str", "block_type", "so_slices", "0", "slice_type", "if", "len", "so_slices", "1", "else", "compound", "lexer_logger", "debug", "placholder", "s", "r", "segment_buffer", "1", "placeholder_str", "add", "a", "dedent", "if", "appropriate", "if", "trail_indent", "and", "add_indents", "lexer_logger", "debug", "indent", "segment_buffer", "append", "indent", "pos_marker", "positionmarker", "from_point", "placeholder_slice", "stop", "element", "template_slice", "start", "templated_file", "add", "the", "actual", "segment", "segment_buffer", "append", "element", "to_segment", "pos_marker", "positionmarker", "source_slice", "element", "template_slice", "templated_file", "convert", "to", "tuple", "before", "return", "return", "tuple", "segment_buffer"], "doc_len": 521}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.violations_from_segments", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "violations_from_segments", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def violations_from_segments(segments: Tuple[RawSegment, ...]) -> List[SQLLexError]:\n        \"\"\"Generate any lexing errors for any unlexables.\"\"\"\n        violations = []\n        for segment in segments:\n            if segment.is_type(\"unlexable\"):\n                violations.append(\n                    SQLLexError(\n                        \"Unable to lex characters: {!r}\".format(\n                            segment.raw[:10] + \"...\"\n                            if len(segment.raw) > 9\n                            else segment.raw\n                        ),\n                        pos=segment.pos_marker,\n                    )\n                )\n        return violations\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "violations_from_segments", "segments", "tuple", "rawsegment", "list", "sqllexerror", "generate", "any", "lexing", "errors", "for", "any", "unlexables", "violations", "for", "segment", "in", "segments", "if", "segment", "is_type", "unlexable", "violations", "append", "sqllexerror", "unable", "to", "lex", "characters", "r", "format", "segment", "raw", "10", "if", "len", "segment", "raw", "9", "else", "segment", "raw", "pos", "segment", "pos_marker", "return", "violations"], "doc_len": 55}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.lex_match", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "lex_match", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def lex_match(forward_string: str, lexer_matchers: List[StringLexer]) -> LexMatch:\n        \"\"\"Iteratively match strings using the selection of submatchers.\"\"\"\n        elem_buff: List[LexedElement] = []\n        while True:\n            if len(forward_string) == 0:\n                return LexMatch(forward_string, elem_buff)\n            for matcher in lexer_matchers:\n                res = matcher.match(forward_string)\n                if res.elements:\n                    # If we have new segments then whoop!\n                    elem_buff += res.elements\n                    forward_string = res.forward_string\n                    # Cycle back around again and start with the top\n                    # matcher again.\n                    break\n            else:\n                # We've got so far, but now can't match. Return\n                return LexMatch(forward_string, elem_buff)\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "lex_match", "forward_string", "str", "lexer_matchers", "list", "stringlexer", "lexmatch", "iteratively", "match", "strings", "using", "the", "selection", "of", "submatchers", "elem_buff", "list", "lexedelement", "while", "true", "if", "len", "forward_string", "0", "return", "lexmatch", "forward_string", "elem_buff", "for", "matcher", "in", "lexer_matchers", "res", "matcher", "match", "forward_string", "if", "res", "elements", "if", "we", "have", "new", "segments", "then", "whoop", "elem_buff", "res", "elements", "forward_string", "res", "forward_string", "cycle", "back", "around", "again", "and", "start", "with", "the", "top", "matcher", "again", "break", "else", "we", "ve", "got", "so", "far", "but", "now", "can", "t", "match", "return", "return", "lexmatch", "forward_string", "elem_buff"], "doc_len": 88}
{"doc_id": "src/sqlfluff/core/parser/lexer.py::Lexer.map_template_slices", "file_path": "src/sqlfluff/core/parser/lexer.py", "class_name": "Lexer", "func_name": "map_template_slices", "text": "文件路径: src/sqlfluff/core/parser/lexer.py, 类名: Lexer\n    def map_template_slices(\n        elements: List[LexedElement], template: TemplatedFile\n    ) -> List[TemplateElement]:\n        \"\"\"Create a tuple of TemplateElement from a tuple of LexedElement.\n\n        This adds slices in the templated file to the original lexed\n        elements. We'll need this to work out the position in the source\n        file.\n        \"\"\"\n        idx = 0\n        templated_buff: List[TemplateElement] = []\n        for element in elements:\n            template_slice = slice(idx, idx + len(element.raw))\n            idx += len(element.raw)\n            templated_buff.append(TemplateElement.from_element(element, template_slice))\n            if (\n                template.templated_str[template_slice] != element.raw\n            ):  # pragma: no cover\n                raise ValueError(\n                    \"Template and lexed elements do not match. This should never \"\n                    f\"happen {element.raw!r} != {template.templated_str[template_slice]!r}\"\n                )\n        return templated_buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "lexer", "py", "lexer", "def", "map_template_slices", "elements", "list", "lexedelement", "template", "templatedfile", "list", "templateelement", "create", "a", "tuple", "of", "templateelement", "from", "a", "tuple", "of", "lexedelement", "this", "adds", "slices", "in", "the", "templated", "file", "to", "the", "original", "lexed", "elements", "we", "ll", "need", "this", "to", "work", "out", "the", "position", "in", "the", "source", "file", "idx", "0", "templated_buff", "list", "templateelement", "for", "element", "in", "elements", "template_slice", "slice", "idx", "idx", "len", "element", "raw", "idx", "len", "element", "raw", "templated_buff", "append", "templateelement", "from_element", "element", "template_slice", "if", "template", "templated_str", "template_slice", "element", "raw", "pragma", "no", "cover", "raise", "valueerror", "template", "and", "lexed", "elements", "do", "not", "match", "this", "should", "never", "f", "happen", "element", "raw", "r", "template", "templated_str", "template_slice", "r", "return", "templated_buff"], "doc_len": 109}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__post_init__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__post_init__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __post_init__(self):\n        # If the working position has not been explicitly set\n        # then infer it from the position in the templated file.\n        # This is accurate up until the point that any fixes have\n        # been applied.\n        if self.working_line_no == -1 or self.working_line_pos == -1:\n            line_no, line_pos = self.templated_position()\n            # Use the base method because we're working with a frozen class\n            object.__setattr__(self, \"working_line_no\", line_no)\n            object.__setattr__(self, \"working_line_pos\", line_pos)\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__post_init__", "self", "if", "the", "working", "position", "has", "not", "been", "explicitly", "set", "then", "infer", "it", "from", "the", "position", "in", "the", "templated", "file", "this", "is", "accurate", "up", "until", "the", "point", "that", "any", "fixes", "have", "been", "applied", "if", "self", "working_line_no", "1", "or", "self", "working_line_pos", "1", "line_no", "line_pos", "self", "templated_position", "use", "the", "base", "method", "because", "we", "re", "working", "with", "a", "frozen", "class", "object", "__setattr__", "self", "working_line_no", "line_no", "object", "__setattr__", "self", "working_line_pos", "line_pos"], "doc_len": 76}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__str__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __str__(self):\n        return self.to_source_string()\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__str__", "self", "return", "self", "to_source_string"], "doc_len": 13}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__gt__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__gt__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __gt__(self, other):\n        return self.working_loc > other.working_loc  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__gt__", "self", "other", "return", "self", "working_loc", "other", "working_loc", "pragma", "no", "cover", "todo"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__lt__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__lt__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __lt__(self, other):\n        return self.working_loc < other.working_loc  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__lt__", "self", "other", "return", "self", "working_loc", "other", "working_loc", "pragma", "no", "cover", "todo"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__ge__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__ge__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __ge__(self, other):\n        return self.working_loc >= other.working_loc  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__ge__", "self", "other", "return", "self", "working_loc", "other", "working_loc", "pragma", "no", "cover", "todo"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.__le__", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "__le__", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def __le__(self, other):\n        return self.working_loc <= other.working_loc  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "__le__", "self", "other", "return", "self", "working_loc", "other", "working_loc", "pragma", "no", "cover", "todo"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.working_loc", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "working_loc", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def working_loc(self) -> Tuple[int, int]:\n        \"\"\"Location tuple for the working position.\"\"\"\n        return self.working_line_no, self.working_line_pos\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "working_loc", "self", "tuple", "int", "int", "location", "tuple", "for", "the", "working", "position", "return", "self", "working_line_no", "self", "working_line_pos"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.working_loc_after", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "working_loc_after", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def working_loc_after(self, raw: str) -> Tuple[int, int]:\n        \"\"\"Location tuple for the working position.\"\"\"\n        return self.infer_next_position(\n            raw,\n            self.working_line_no,\n            self.working_line_pos,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "working_loc_after", "self", "raw", "str", "tuple", "int", "int", "location", "tuple", "for", "the", "working", "position", "return", "self", "infer_next_position", "raw", "self", "working_line_no", "self", "working_line_pos"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.from_point", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "from_point", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def from_point(\n        cls,\n        source_point: int,\n        templated_point: int,\n        templated_file: \"TemplatedFile\",\n        **kwargs,\n    ):\n        \"\"\"Convenience method for creating point markers.\"\"\"\n        return cls(\n            slice(source_point, source_point),\n            slice(templated_point, templated_point),\n            templated_file,\n            **kwargs,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "from_point", "cls", "source_point", "int", "templated_point", "int", "templated_file", "templatedfile", "kwargs", "convenience", "method", "for", "creating", "point", "markers", "return", "cls", "slice", "source_point", "source_point", "slice", "templated_point", "templated_point", "templated_file", "kwargs"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.from_child_markers", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "from_child_markers", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def from_child_markers(cls, *markers):\n        \"\"\"Create a parent marker from it's children.\"\"\"\n        source_slice = slice(\n            min(m.source_slice.start for m in markers),\n            max(m.source_slice.stop for m in markers),\n        )\n        templated_slice = slice(\n            min(m.templated_slice.start for m in markers),\n            max(m.templated_slice.stop for m in markers),\n        )\n        templated_files = {m.templated_file for m in markers}\n        if len(templated_files) != 1:  # pragma: no cover\n            raise ValueError(\"Attempted to make a parent marker from multiple files.\")\n        templated_file = templated_files.pop()\n        return cls(source_slice, templated_slice, templated_file)\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "from_child_markers", "cls", "markers", "create", "a", "parent", "marker", "from", "it", "s", "children", "source_slice", "slice", "min", "m", "source_slice", "start", "for", "m", "in", "markers", "max", "m", "source_slice", "stop", "for", "m", "in", "markers", "templated_slice", "slice", "min", "m", "templated_slice", "start", "for", "m", "in", "markers", "max", "m", "templated_slice", "stop", "for", "m", "in", "markers", "templated_files", "m", "templated_file", "for", "m", "in", "markers", "if", "len", "templated_files", "1", "pragma", "no", "cover", "raise", "valueerror", "attempted", "to", "make", "a", "parent", "marker", "from", "multiple", "files", "templated_file", "templated_files", "pop", "return", "cls", "source_slice", "templated_slice", "templated_file"], "doc_len": 88}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.source_position", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "source_position", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def source_position(self) -> Tuple[int, int]:\n        \"\"\"Return the line and position of this marker in the source.\"\"\"\n        return self.templated_file.get_line_pos_of_char_pos(\n            self.source_slice.start, source=True\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "source_position", "self", "tuple", "int", "int", "return", "the", "line", "and", "position", "of", "this", "marker", "in", "the", "source", "return", "self", "templated_file", "get_line_pos_of_char_pos", "self", "source_slice", "start", "source", "true"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.templated_position", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "templated_position", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def templated_position(self) -> Tuple[int, int]:\n        \"\"\"Return the line and position of this marker in the source.\"\"\"\n        return self.templated_file.get_line_pos_of_char_pos(\n            self.templated_slice.start, source=False\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "templated_position", "self", "tuple", "int", "int", "return", "the", "line", "and", "position", "of", "this", "marker", "in", "the", "source", "return", "self", "templated_file", "get_line_pos_of_char_pos", "self", "templated_slice", "start", "source", "false"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.line_no", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "line_no", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def line_no(self) -> int:\n        \"\"\"Return the line number in the source.\"\"\"\n        return self.source_position()[0]\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "line_no", "self", "int", "return", "the", "line", "number", "in", "the", "source", "return", "self", "source_position", "0"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.line_pos", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "line_pos", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def line_pos(self) -> int:\n        \"\"\"Return the line position in the source.\"\"\"\n        return self.source_position()[1]\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "line_pos", "self", "int", "return", "the", "line", "position", "in", "the", "source", "return", "self", "source_position", "1"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.to_source_string", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "to_source_string", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def to_source_string(self) -> str:\n        \"\"\"Make a formatted string of this position.\"\"\"\n        line, pos = self.source_position()\n        return f\"[L:{line:3d}, P:{pos:3d}]\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "to_source_string", "self", "str", "make", "a", "formatted", "string", "of", "this", "position", "line", "pos", "self", "source_position", "return", "f", "l", "line", "3d", "p", "pos", "3d"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.start_point_marker", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "start_point_marker", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def start_point_marker(self) -> \"PositionMarker\":\n        \"\"\"Get a point marker from the start.\"\"\"\n        return self.__class__.from_point(\n            self.source_slice.start,\n            self.templated_slice.start,\n            templated_file=self.templated_file,\n            # Start points also pass on the working position.\n            working_line_no=self.working_line_no,\n            working_line_pos=self.working_line_pos,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "start_point_marker", "self", "positionmarker", "get", "a", "point", "marker", "from", "the", "start", "return", "self", "__class__", "from_point", "self", "source_slice", "start", "self", "templated_slice", "start", "templated_file", "self", "templated_file", "start", "points", "also", "pass", "on", "the", "working", "position", "working_line_no", "self", "working_line_no", "working_line_pos", "self", "working_line_pos"], "doc_len": 45}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.end_point_marker", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "end_point_marker", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def end_point_marker(self) -> \"PositionMarker\":\n        \"\"\"Get a point marker from the end.\"\"\"\n        return self.__class__.from_point(\n            self.source_slice.stop,\n            self.templated_slice.stop,\n            templated_file=self.templated_file,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "end_point_marker", "self", "positionmarker", "get", "a", "point", "marker", "from", "the", "end", "return", "self", "__class__", "from_point", "self", "source_slice", "stop", "self", "templated_slice", "stop", "templated_file", "self", "templated_file"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.slice_is_point", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "slice_is_point", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def slice_is_point(test_slice):\n        \"\"\"Is this slice a point.\"\"\"\n        return test_slice.start == test_slice.stop\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "slice_is_point", "test_slice", "is", "this", "slice", "a", "point", "return", "test_slice", "start", "test_slice", "stop"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.is_point", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "is_point", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def is_point(self) -> bool:\n        \"\"\"A marker is a point if it has zero length in templated and source file.\"\"\"\n        return self.slice_is_point(self.source_slice) and self.slice_is_point(\n            self.templated_slice\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "is_point", "self", "bool", "a", "marker", "is", "a", "point", "if", "it", "has", "zero", "length", "in", "templated", "and", "source", "file", "return", "self", "slice_is_point", "self", "source_slice", "and", "self", "slice_is_point", "self", "templated_slice"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.infer_next_position", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "infer_next_position", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def infer_next_position(raw: str, line_no: int, line_pos: int) -> Tuple[int, int]:\n        \"\"\"Using the raw string provided to infer the position of the next.\n\n        NB: Line position in 1-indexed.\n        \"\"\"\n        # No content?\n        if not raw:\n            return line_no, line_pos\n        split = raw.split(\"\\n\")\n        return (\n            line_no + len(split) - 1,\n            line_pos + len(raw) if len(split) == 1 else len(split[-1]) + 1,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "infer_next_position", "raw", "str", "line_no", "int", "line_pos", "int", "tuple", "int", "int", "using", "the", "raw", "string", "provided", "to", "infer", "the", "position", "of", "the", "next", "nb", "line", "position", "in", "1", "indexed", "no", "content", "if", "not", "raw", "return", "line_no", "line_pos", "split", "raw", "split", "n", "return", "line_no", "len", "split", "1", "line_pos", "len", "raw", "if", "len", "split", "1", "else", "len", "split", "1", "1"], "doc_len": 65}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.with_working_position", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "with_working_position", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def with_working_position(self, line_no: int, line_pos: int):\n        \"\"\"Copy this position and replace the working position.\"\"\"\n        return self.__class__(\n            source_slice=self.source_slice,\n            templated_slice=self.templated_slice,\n            templated_file=self.templated_file,\n            working_line_no=line_no,\n            working_line_pos=line_pos,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "with_working_position", "self", "line_no", "int", "line_pos", "int", "copy", "this", "position", "and", "replace", "the", "working", "position", "return", "self", "__class__", "source_slice", "self", "source_slice", "templated_slice", "self", "templated_slice", "templated_file", "self", "templated_file", "working_line_no", "line_no", "working_line_pos", "line_pos"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.is_literal", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "is_literal", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def is_literal(self) -> bool:\n        \"\"\"Infer literalness from context.\n\n        is_literal should return True if a fix can be applied across this area\n        in the templated file while being confident that the fix is still\n        appropriate in the source file. This obviously applies to any slices\n        which are the same in the source and the templated files. Slices which\n        are zero-length in the source are also \"literal\" because they can't be\n        \"broken\" by any fixes, because they don't exist in the source. This\n        includes meta segments and any segments added during the fixing process.\n\n        This value is used for:\n        - Ignoring linting errors in templated sections.\n        - Whether `iter_patches` can return without recursing.\n        - Whether certain rules (such as L046) are triggered.\n        \"\"\"\n        return self.templated_file.is_source_slice_literal(self.source_slice)\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "is_literal", "self", "bool", "infer", "literalness", "from", "context", "is_literal", "should", "return", "true", "if", "a", "fix", "can", "be", "applied", "across", "this", "area", "in", "the", "templated", "file", "while", "being", "confident", "that", "the", "fix", "is", "still", "appropriate", "in", "the", "source", "file", "this", "obviously", "applies", "to", "any", "slices", "which", "are", "the", "same", "in", "the", "source", "and", "the", "templated", "files", "slices", "which", "are", "zero", "length", "in", "the", "source", "are", "also", "literal", "because", "they", "can", "t", "be", "broken", "by", "any", "fixes", "because", "they", "don", "t", "exist", "in", "the", "source", "this", "includes", "meta", "segments", "and", "any", "segments", "added", "during", "the", "fixing", "process", "this", "value", "is", "used", "for", "ignoring", "linting", "errors", "in", "templated", "sections", "whether", "iter_patches", "can", "return", "without", "recursing", "whether", "certain", "rules", "such", "as", "l046", "are", "triggered", "return", "self", "templated_file", "is_source_slice_literal", "self", "source_slice"], "doc_len": 133}
{"doc_id": "src/sqlfluff/core/parser/markers.py::PositionMarker.source_str", "file_path": "src/sqlfluff/core/parser/markers.py", "class_name": "PositionMarker", "func_name": "source_str", "text": "文件路径: src/sqlfluff/core/parser/markers.py, 类名: PositionMarker\n    def source_str(self) -> str:\n        \"\"\"Returns the string in the source at this position.\"\"\"\n        return self.templated_file.source_str[self.source_slice]\n", "tokens": ["src", "sqlfluff", "core", "parser", "markers", "py", "positionmarker", "def", "source_str", "self", "str", "returns", "the", "string", "in", "the", "source", "at", "this", "position", "return", "self", "templated_file", "source_str", "self", "source_slice"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/matchable.py::Matchable.is_optional", "file_path": "src/sqlfluff/core/parser/matchable.py", "class_name": "Matchable", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/matchable.py, 类名: Matchable\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "matchable", "py", "matchable", "def", "is_optional", "self", "bool", "return", "whether", "this", "element", "is", "optional"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/parser/matchable.py::Matchable.simple", "file_path": "src/sqlfluff/core/parser/matchable.py", "class_name": "Matchable", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/matchable.py, 类名: Matchable\n    def simple(self, parse_context: \"ParseContext\") -> Optional[List[str]]:\n        \"\"\"Try to obtain a simple response from the matcher.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "matchable", "py", "matchable", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "try", "to", "obtain", "a", "simple", "response", "from", "the", "matcher"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/parser/matchable.py::Matchable.match", "file_path": "src/sqlfluff/core/parser/matchable.py", "class_name": "Matchable", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/matchable.py, 类名: Matchable\n    def match(self, segments: tuple, parse_context: \"ParseContext\") -> \"MatchResult\":\n        \"\"\"Match against this matcher.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "matchable", "py", "matchable", "def", "match", "self", "segments", "tuple", "parse_context", "parsecontext", "matchresult", "match", "against", "this", "matcher"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/parser/matchable.py::Matchable.copy", "file_path": "src/sqlfluff/core/parser/matchable.py", "class_name": "Matchable", "func_name": "copy", "text": "文件路径: src/sqlfluff/core/parser/matchable.py, 类名: Matchable\n    def copy(self, **kwargs) -> \"Matchable\":  # pragma: no cover TODO?\n        \"\"\"Copy this Matchable.\"\"\"\n        return copy.copy(self)\n", "tokens": ["src", "sqlfluff", "core", "parser", "matchable", "py", "matchable", "def", "copy", "self", "kwargs", "matchable", "pragma", "no", "cover", "todo", "copy", "this", "matchable", "return", "copy", "copy", "self"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::LateLoggingObject.__init__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "LateLoggingObject", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: LateLoggingObject\n    def __init__(self, logger, msg, v_level=3):\n        self.v_level = v_level\n        self.logger = logger\n        self.msg = msg\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "lateloggingobject", "def", "__init__", "self", "logger", "msg", "v_level", "3", "self", "v_level", "v_level", "self", "logger", "logger", "self", "msg", "msg"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::LateLoggingObject.__str__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "LateLoggingObject", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: LateLoggingObject\n    def __str__(self):  # pragma: no cover TODO?\n        \"\"\"Actually materialise the string.\"\"\"\n        return self.msg\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "lateloggingobject", "def", "__str__", "self", "pragma", "no", "cover", "todo", "actually", "materialise", "the", "string", "return", "self", "msg"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::LateLoggingObject.log", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "LateLoggingObject", "func_name": "log", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: LateLoggingObject\n    def log(self):\n        \"\"\"Actually log this object.\"\"\"\n        # Otherwise carry on...\n        if self.v_level == 3:\n            self.logger.info(self)\n        elif self.v_level == 4:\n            self.logger.debug(self)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "lateloggingobject", "def", "log", "self", "actually", "log", "this", "object", "otherwise", "carry", "on", "if", "self", "v_level", "3", "self", "logger", "info", "self", "elif", "self", "v_level", "4", "self", "logger", "debug", "self"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::ParseMatchLogObject.__init__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "ParseMatchLogObject", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: ParseMatchLogObject\n    def __init__(self, parse_context, grammar, func, msg, v_level=3, **kwargs):\n        super().__init__(v_level=v_level, logger=parse_context.logger, msg=msg)\n        self.context = parse_context\n        self.grammar = grammar\n        self.func = func\n        self.kwargs = kwargs\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "parsematchlogobject", "def", "__init__", "self", "parse_context", "grammar", "func", "msg", "v_level", "3", "kwargs", "super", "__init__", "v_level", "v_level", "logger", "parse_context", "logger", "msg", "msg", "self", "context", "parse_context", "self", "grammar", "grammar", "self", "func", "func", "self", "kwargs", "kwargs"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::ParseMatchLogObject.__str__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "ParseMatchLogObject", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: ParseMatchLogObject\n    def __str__(self):\n        \"\"\"Actually materialise the string.\"\"\"\n        symbol = self.kwargs.pop(\"symbol\", \"\")\n        s = \"[PD:{:<2} MD:{:<2}]\\t{:<50}\\t{:<20}\\t{:<4}\".format(\n            self.context.parse_depth,\n            self.context.match_depth,\n            (\".\" * self.context.match_depth) + str(self.context.match_segment),\n            f\"{self.grammar:.5}.{self.func} {self.msg}\",\n            symbol,\n        )\n        if self.kwargs:\n            s += \"\\t[{}]\".format(\n                \", \".join(\n                    f\"{k}={repr(v) if isinstance(v, str) else str(v)}\"\n                    for k, v in self.kwargs.items()\n                )\n            )\n        return s\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "parsematchlogobject", "def", "__str__", "self", "actually", "materialise", "the", "string", "symbol", "self", "kwargs", "pop", "symbol", "s", "pd", "2", "md", "2", "t", "50", "t", "20", "t", "4", "format", "self", "context", "parse_depth", "self", "context", "match_depth", "self", "context", "match_depth", "str", "self", "context", "match_segment", "f", "self", "grammar", "5", "self", "func", "self", "msg", "symbol", "if", "self", "kwargs", "s", "t", "format", "join", "f", "k", "repr", "v", "if", "isinstance", "v", "str", "else", "str", "v", "for", "k", "v", "in", "self", "kwargs", "items", "return", "s"], "doc_len": 80}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::parse_match_logging", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": null, "func_name": "parse_match_logging", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py\ndef parse_match_logging(grammar, func, msg, parse_context, v_level=3, **kwargs):\n    \"\"\"Log in a particular consistent format for use while matching.\"\"\"\n    # Make a late bound log object so we only do the string manipulation when we need to.\n    ParseMatchLogObject(\n        parse_context, grammar, func, msg, v_level=v_level, **kwargs\n    ).log()\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "def", "parse_match_logging", "grammar", "func", "msg", "parse_context", "v_level", "3", "kwargs", "log", "in", "a", "particular", "consistent", "format", "for", "use", "while", "matching", "make", "a", "late", "bound", "log", "object", "so", "we", "only", "do", "the", "string", "manipulation", "when", "we", "need", "to", "parsematchlogobject", "parse_context", "grammar", "func", "msg", "v_level", "v_level", "kwargs", "log"], "doc_len": 51}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::LateBoundJoinSegmentsCurtailed.__init__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "LateBoundJoinSegmentsCurtailed", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: LateBoundJoinSegmentsCurtailed\n    def __init__(self, segments):\n        self.segments = segments\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "lateboundjoinsegmentscurtailed", "def", "__init__", "self", "segments", "self", "segments", "segments"], "doc_len": 14}
{"doc_id": "src/sqlfluff/core/parser/match_logging.py::LateBoundJoinSegmentsCurtailed.__str__", "file_path": "src/sqlfluff/core/parser/match_logging.py", "class_name": "LateBoundJoinSegmentsCurtailed", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/match_logging.py, 类名: LateBoundJoinSegmentsCurtailed\n    def __str__(self):\n        return repr(join_segments_raw_curtailed(self.segments))\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_logging", "py", "lateboundjoinsegmentscurtailed", "def", "__str__", "self", "return", "repr", "join_segments_raw_curtailed", "self", "segments"], "doc_len": 15}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.matched_length", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "matched_length", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def matched_length(self) -> int:\n        \"\"\"Return the length of the match in characters.\"\"\"\n        return sum(seg.matched_length for seg in self.matched_segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "matched_length", "self", "int", "return", "the", "length", "of", "the", "match", "in", "characters", "return", "sum", "seg", "matched_length", "for", "seg", "in", "self", "matched_segments"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.all_segments", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "all_segments", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def all_segments(self) -> Tuple[\"BaseSegment\", ...]:\n        \"\"\"Return a tuple of all the segments, matched or otherwise.\"\"\"\n        return self.matched_segments + self.unmatched_segments\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "all_segments", "self", "tuple", "basesegment", "return", "a", "tuple", "of", "all", "the", "segments", "matched", "or", "otherwise", "return", "self", "matched_segments", "self", "unmatched_segments"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.__len__", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "__len__", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def __len__(self) -> int:\n        return len(self.matched_segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "__len__", "self", "int", "return", "len", "self", "matched_segments"], "doc_len": 15}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.is_complete", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "is_complete", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def is_complete(self) -> bool:\n        \"\"\"Return true if everything has matched.\n\n        Note: An empty match is not a match so will return False.\n        \"\"\"\n        return len(self.unmatched_segments) == 0 and len(self.matched_segments) > 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "is_complete", "self", "bool", "return", "true", "if", "everything", "has", "matched", "note", "an", "empty", "match", "is", "not", "a", "match", "so", "will", "return", "false", "return", "len", "self", "unmatched_segments", "0", "and", "len", "self", "matched_segments", "0"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.has_match", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "has_match", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def has_match(self) -> bool:\n        \"\"\"Return true if *anything* has matched.\"\"\"\n        return len(self) > 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "has_match", "self", "bool", "return", "true", "if", "anything", "has", "matched", "return", "len", "self", "0"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.__bool__", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "__bool__", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def __bool__(self) -> bool:\n        return self.has_match()\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "__bool__", "self", "bool", "return", "self", "has_match"], "doc_len": 14}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.raw_matched", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "raw_matched", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def raw_matched(self) -> str:\n        \"\"\"Make a string from the raw matched segments.\"\"\"\n        return join_segments_raw(self.matched_segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "raw_matched", "self", "str", "make", "a", "string", "from", "the", "raw", "matched", "segments", "return", "join_segments_raw", "self", "matched_segments"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.__str__", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def __str__(self) -> str:\n        content = self.raw_matched()\n        # Clip the content if it's long.\n        # The ends are the important bits.\n        if len(content) > 32:\n            content = content[:15] + \"...\" + content[-15:]\n        return \"<MatchResult {}/{}: {!r}>\".format(\n            len(self.matched_segments),\n            len(self.matched_segments) + len(self.unmatched_segments),\n            content,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "__str__", "self", "str", "content", "self", "raw_matched", "clip", "the", "content", "if", "it", "s", "long", "the", "ends", "are", "the", "important", "bits", "if", "len", "content", "32", "content", "content", "15", "content", "15", "return", "matchresult", "r", "format", "len", "self", "matched_segments", "len", "self", "matched_segments", "len", "self", "unmatched_segments", "content"], "doc_len": 50}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.__eq__", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "__eq__", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def __eq__(self, other):\n        \"\"\"Equals function override.\n\n        This allows comparison to tuples for testing.\n        \"\"\"\n        if isinstance(other, MatchResult):  # pragma: no cover TODO?\n            return (\n                self.matched_segments == other.matched_segments\n                and self.unmatched_segments == other.unmatched_segments\n            )\n        elif isinstance(other, tuple):  # pragma: no cover TODO?\n            return self.matched_segments == other\n        else:  # pragma: no cover\n            raise TypeError(f\"Unexpected equality comparison: type: {type(other)}\")\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "__eq__", "self", "other", "equals", "function", "override", "this", "allows", "comparison", "to", "tuples", "for", "testing", "if", "isinstance", "other", "matchresult", "pragma", "no", "cover", "todo", "return", "self", "matched_segments", "other", "matched_segments", "and", "self", "unmatched_segments", "other", "unmatched_segments", "elif", "isinstance", "other", "tuple", "pragma", "no", "cover", "todo", "return", "self", "matched_segments", "other", "else", "pragma", "no", "cover", "raise", "typeerror", "f", "unexpected", "equality", "comparison", "type", "type", "other"], "doc_len": 64}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.seg_to_tuple", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "seg_to_tuple", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def seg_to_tuple(segs) -> Tuple[\"BaseSegment\", ...]:\n        \"\"\"Munge types to a tuple.\"\"\"\n        # Is other iterable?\n        try:\n            iterator = iter(segs)\n        except TypeError:  # pragma: no cover\n            is_iterable = False\n        else:\n            is_iterable = True\n\n        if is_iterable:\n            return tuple(iterator)\n        else:  # pragma: no cover TODO?\n            # Blindly make into tuple here.\n            return (segs,)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "seg_to_tuple", "segs", "tuple", "basesegment", "munge", "types", "to", "a", "tuple", "is", "other", "iterable", "try", "iterator", "iter", "segs", "except", "typeerror", "pragma", "no", "cover", "is_iterable", "false", "else", "is_iterable", "true", "if", "is_iterable", "return", "tuple", "iterator", "else", "pragma", "no", "cover", "todo", "blindly", "make", "into", "tuple", "here", "return", "segs"], "doc_len": 51}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.from_unmatched", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "from_unmatched", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def from_unmatched(cls, unmatched: Tuple[\"BaseSegment\", ...]) -> \"MatchResult\":\n        \"\"\"Construct a `MatchResult` from just unmatched segments.\"\"\"\n        return cls(matched_segments=(), unmatched_segments=cls.seg_to_tuple(unmatched))\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "from_unmatched", "cls", "unmatched", "tuple", "basesegment", "matchresult", "construct", "a", "matchresult", "from", "just", "unmatched", "segments", "return", "cls", "matched_segments", "unmatched_segments", "cls", "seg_to_tuple", "unmatched"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.from_matched", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "from_matched", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def from_matched(cls, matched: Tuple[\"BaseSegment\", ...]) -> \"MatchResult\":\n        \"\"\"Construct a `MatchResult` from just matched segments.\"\"\"\n        return cls(unmatched_segments=(), matched_segments=cls.seg_to_tuple(matched))\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "from_matched", "cls", "matched", "tuple", "basesegment", "matchresult", "construct", "a", "matchresult", "from", "just", "matched", "segments", "return", "cls", "unmatched_segments", "matched_segments", "cls", "seg_to_tuple", "matched"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.from_empty", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "from_empty", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def from_empty(cls) -> \"MatchResult\":\n        \"\"\"Construct an empty `MatchResult`.\"\"\"\n        return cls(unmatched_segments=(), matched_segments=())\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "from_empty", "cls", "matchresult", "construct", "an", "empty", "matchresult", "return", "cls", "unmatched_segments", "matched_segments"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/parser/match_result.py::MatchResult.__add__", "file_path": "src/sqlfluff/core/parser/match_result.py", "class_name": "MatchResult", "func_name": "__add__", "text": "文件路径: src/sqlfluff/core/parser/match_result.py, 类名: MatchResult\n    def __add__(self, other) -> \"MatchResult\":\n        \"\"\"Override add for concatenating things onto this match.\"\"\"\n        if isinstance(other, MatchResult):\n            return self.__class__(\n                matched_segments=self.matched_segments + other.matched_segments,\n                unmatched_segments=self.unmatched_segments,\n            )  # pragma: no cover TODO?\n        else:\n            return self.__class__(\n                matched_segments=self.matched_segments + self.seg_to_tuple(other),\n                unmatched_segments=self.unmatched_segments,\n            )\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_result", "py", "matchresult", "def", "__add__", "self", "other", "matchresult", "override", "add", "for", "concatenating", "things", "onto", "this", "match", "if", "isinstance", "other", "matchresult", "return", "self", "__class__", "matched_segments", "self", "matched_segments", "other", "matched_segments", "unmatched_segments", "self", "unmatched_segments", "pragma", "no", "cover", "todo", "else", "return", "self", "__class__", "matched_segments", "self", "matched_segments", "self", "seg_to_tuple", "other", "unmatched_segments", "self", "unmatched_segments"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/parser/match_wrapper.py::WrapParseMatchLogObject.__init__", "file_path": "src/sqlfluff/core/parser/match_wrapper.py", "class_name": "WrapParseMatchLogObject", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/match_wrapper.py, 类名: WrapParseMatchLogObject\n    def __init__(self, match, segments, **kwargs):\n        self.match = match\n        self.segments = segments\n        super().__init__(msg=\"OUT\", match=match, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_wrapper", "py", "wrapparsematchlogobject", "def", "__init__", "self", "match", "segments", "kwargs", "self", "match", "match", "self", "segments", "segments", "super", "__init__", "msg", "out", "match", "match", "kwargs"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/match_wrapper.py::WrapParseMatchLogObject.__str__", "file_path": "src/sqlfluff/core/parser/match_wrapper.py", "class_name": "WrapParseMatchLogObject", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/match_wrapper.py, 类名: WrapParseMatchLogObject\n    def __str__(self):\n        if self.match.is_complete():\n            self.kwargs[\"symbol\"] = \"++\"\n        elif self.match:\n            self.kwargs[\"symbol\"] = \"+\"\n        self.kwargs[\"seg\"] = repr(join_segments_raw_curtailed(self.segments))\n        return super().__str__()\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_wrapper", "py", "wrapparsematchlogobject", "def", "__str__", "self", "if", "self", "match", "is_complete", "self", "kwargs", "symbol", "elif", "self", "match", "self", "kwargs", "symbol", "self", "kwargs", "seg", "repr", "join_segments_raw_curtailed", "self", "segments", "return", "super", "__str__"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/match_wrapper.py::match_wrapper", "file_path": "src/sqlfluff/core/parser/match_wrapper.py", "class_name": null, "func_name": "match_wrapper", "text": "文件路径: src/sqlfluff/core/parser/match_wrapper.py\ndef match_wrapper(v_level=3):\n    \"\"\"Wraps a .match() method to add validation and logging.\n\n    This is designed to be used as follows:\n\n        class SomeMatchableObject(object):\n            @match_wrapper()\n            def match(self, segments, parse_context):\n                ...\n                return m\n\n    This applies a common logging framework to both Grammar and\n    Segment based match routines.\n    \"\"\"\n\n    def inner_match_wrapper(func):\n        \"\"\"Decorate a match function.\"\"\"\n\n        def wrapped_match_method(self_cls, segments: tuple, parse_context):\n            \"\"\"A wrapper on the match function to do some basic validation.\"\"\"\n            # Do the match\n            m = func(self_cls, segments, parse_context=parse_context)\n\n            name = getattr(self_cls, \"__name__\", self_cls.__class__.__name__)\n\n            # Validate result\n            if not isinstance(m, MatchResult):  # pragma: no cover\n                parse_context.logger.warning(\n                    f\"{name}.match, returned {type(m)} rather than MatchResult\"\n                )\n\n            # Log the result.\n            WrapParseMatchLogObject(\n                grammar=name,\n                func=\"match\",\n                match=m,\n                parse_context=parse_context,\n                segments=segments,\n                v_level=v_level,\n            ).log()\n\n            # Basic Validation, skipped here because it still happens in the parse commands.\n            return m\n\n        return wrapped_match_method\n\n    return inner_match_wrapper\n", "tokens": ["src", "sqlfluff", "core", "parser", "match_wrapper", "py", "def", "match_wrapper", "v_level", "3", "wraps", "a", "match", "method", "to", "add", "validation", "and", "logging", "this", "is", "designed", "to", "be", "used", "as", "follows", "class", "somematchableobject", "object", "match_wrapper", "def", "match", "self", "segments", "parse_context", "return", "m", "this", "applies", "a", "common", "logging", "framework", "to", "both", "grammar", "and", "segment", "based", "match", "routines", "def", "inner_match_wrapper", "func", "decorate", "a", "match", "function", "def", "wrapped_match_method", "self_cls", "segments", "tuple", "parse_context", "a", "wrapper", "on", "the", "match", "function", "to", "do", "some", "basic", "validation", "do", "the", "match", "m", "func", "self_cls", "segments", "parse_context", "parse_context", "name", "getattr", "self_cls", "__name__", "self_cls", "__class__", "__name__", "validate", "result", "if", "not", "isinstance", "m", "matchresult", "pragma", "no", "cover", "parse_context", "logger", "warning", "f", "name", "match", "returned", "type", "m", "rather", "than", "matchresult", "log", "the", "result", "wrapparsematchlogobject", "grammar", "name", "func", "match", "match", "m", "parse_context", "parse_context", "segments", "segments", "v_level", "v_level", "log", "basic", "validation", "skipped", "here", "because", "it", "still", "happens", "in", "the", "parse", "commands", "return", "m", "return", "wrapped_match_method", "return", "inner_match_wrapper"], "doc_len": 149}
{"doc_id": "src/sqlfluff/core/parser/parser.py::Parser.__init__", "file_path": "src/sqlfluff/core/parser/parser.py", "class_name": "Parser", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/parser.py, 类名: Parser\n    def __init__(\n        self, config: Optional[FluffConfig] = None, dialect: Optional[str] = None\n    ):\n        # Allow optional config and dialect\n        self.config = FluffConfig.from_kwargs(config=config, dialect=dialect)\n        self.RootSegment = self.config.get(\"dialect_obj\").get_root_segment()\n", "tokens": ["src", "sqlfluff", "core", "parser", "parser", "py", "parser", "def", "__init__", "self", "config", "optional", "fluffconfig", "none", "dialect", "optional", "str", "none", "allow", "optional", "config", "and", "dialect", "self", "config", "fluffconfig", "from_kwargs", "config", "config", "dialect", "dialect", "self", "rootsegment", "self", "config", "get", "dialect_obj", "get_root_segment"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/parser.py::Parser.parse", "file_path": "src/sqlfluff/core/parser/parser.py", "class_name": "Parser", "func_name": "parse", "text": "文件路径: src/sqlfluff/core/parser/parser.py, 类名: Parser\n    def parse(\n        self, segments: Sequence[\"BaseSegment\"], recurse=True, fname: str = None\n    ) -> Optional[\"BaseSegment\"]:\n        \"\"\"Parse a series of lexed tokens using the current dialect.\"\"\"\n        if not segments:\n            return None\n        # Instantiate the root segment\n        root_segment = self.RootSegment(segments=segments, fname=fname)\n        # Call .parse() on that segment\n        with RootParseContext.from_config(config=self.config, recurse=recurse) as ctx:\n            parsed = root_segment.parse(parse_context=ctx)\n        return parsed\n", "tokens": ["src", "sqlfluff", "core", "parser", "parser", "py", "parser", "def", "parse", "self", "segments", "sequence", "basesegment", "recurse", "true", "fname", "str", "none", "optional", "basesegment", "parse", "a", "series", "of", "lexed", "tokens", "using", "the", "current", "dialect", "if", "not", "segments", "return", "none", "instantiate", "the", "root", "segment", "root_segment", "self", "rootsegment", "segments", "segments", "fname", "fname", "call", "parse", "on", "that", "segment", "with", "rootparsecontext", "from_config", "config", "self", "config", "recurse", "recurse", "as", "ctx", "parsed", "root_segment", "parse", "parse_context", "ctx", "return", "parsed"], "doc_len": 68}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser.__init__", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def __init__(\n        self,\n        template: str,\n        raw_class: Type[RawSegment],\n        name: Optional[str] = None,\n        type: Optional[str] = None,\n        optional: bool = False,\n        **segment_kwargs,\n    ):\n        # String matchers are not case sensitive, so we make the template\n        # uppercase on creation. If any SQL dialect is found to be case\n        # sensitive for keywords, this could be extended to allow\n        # case sensitivity.\n        self.template = template.upper()\n        self.raw_class = raw_class\n        self.name = name\n        self.type = type\n        self.optional = optional\n        self.segment_kwargs = segment_kwargs or {}\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "__init__", "self", "template", "str", "raw_class", "type", "rawsegment", "name", "optional", "str", "none", "type", "optional", "str", "none", "optional", "bool", "false", "segment_kwargs", "string", "matchers", "are", "not", "case", "sensitive", "so", "we", "make", "the", "template", "uppercase", "on", "creation", "if", "any", "sql", "dialect", "is", "found", "to", "be", "case", "sensitive", "for", "keywords", "this", "could", "be", "extended", "to", "allow", "case", "sensitivity", "self", "template", "template", "upper", "self", "raw_class", "raw_class", "self", "name", "name", "self", "type", "type", "self", "optional", "optional", "self", "segment_kwargs", "segment_kwargs", "or"], "doc_len": 81}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser.is_optional", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\"\"\"\n        return self.optional\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "is_optional", "self", "bool", "return", "whether", "this", "element", "is", "optional", "return", "self", "optional"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser.simple", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def simple(self, parse_context: \"ParseContext\") -> Optional[List[str]]:\n        \"\"\"Return simple options for this matcher.\n\n        Because string matchers are not case sensitive we can\n        just return the template here.\n        \"\"\"\n        return [self.template]\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "return", "simple", "options", "for", "this", "matcher", "because", "string", "matchers", "are", "not", "case", "sensitive", "we", "can", "just", "return", "the", "template", "here", "return", "self", "template"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser._is_first_match", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "_is_first_match", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def _is_first_match(self, segment: BaseSegment):\n        \"\"\"Does the segment provided match according to the current rules.\"\"\"\n        # Is the target a match and IS IT CODE.\n        # The latter stops us accidentally matching comments.\n        if self.template == segment.raw.upper() and segment.is_code:\n            return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "_is_first_match", "self", "segment", "basesegment", "does", "the", "segment", "provided", "match", "according", "to", "the", "current", "rules", "is", "the", "target", "a", "match", "and", "is", "it", "code", "the", "latter", "stops", "us", "accidentally", "matching", "comments", "if", "self", "template", "segment", "raw", "upper", "and", "segment", "is_code", "return", "true", "return", "false"], "doc_len": 51}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser._make_match_from_first_result", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "_make_match_from_first_result", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def _make_match_from_first_result(self, segments: Tuple[BaseSegment, ...]):\n        \"\"\"Make a MatchResult from the first segment in the given list.\n\n        This is a helper function for reuse by other parsers.\n        \"\"\"\n        # Construct the segment object\n        new_seg = self.raw_class(\n            raw=segments[0].raw,\n            pos_marker=segments[0].pos_marker,\n            type=self.type,\n            name=self.name,\n            **self.segment_kwargs,\n        )\n        # Return as a tuple\n        return MatchResult((new_seg,), segments[1:])\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "_make_match_from_first_result", "self", "segments", "tuple", "basesegment", "make", "a", "matchresult", "from", "the", "first", "segment", "in", "the", "given", "list", "this", "is", "a", "helper", "function", "for", "reuse", "by", "other", "parsers", "construct", "the", "segment", "object", "new_seg", "self", "raw_class", "raw", "segments", "0", "raw", "pos_marker", "segments", "0", "pos_marker", "type", "self", "type", "name", "self", "name", "self", "segment_kwargs", "return", "as", "a", "tuple", "return", "matchresult", "new_seg", "segments", "1"], "doc_len": 66}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::StringParser.match", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "StringParser", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: StringParser\n    def match(\n        self,\n        segments: Union[BaseSegment, Tuple[BaseSegment, ...]],\n        parse_context: \"ParseContext\",\n    ) -> MatchResult:\n        \"\"\"Compare input segments for a match, return a `MatchResult`.\n\n        Note: For matching here, we only consider the *first* element,\n        because we assume that a keyword can only span one raw segment.\n        \"\"\"\n        # If we've been passed the singular, make it a tuple\n        if isinstance(segments, BaseSegment):\n            segments = (segments,)\n\n        # We're only going to match against the first element\n        if len(segments) >= 1:\n            # Is the first one already of this type?\n            if (\n                isinstance(segments[0], self.raw_class)\n                and segments[0].name == self.name\n                and segments[0].is_type(self.type)\n            ):\n                return MatchResult((segments[0],), segments[1:])\n            # Does it match?\n            elif self._is_first_match(segments[0]):\n                return self._make_match_from_first_result(segments)\n        return MatchResult.from_unmatched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "stringparser", "def", "match", "self", "segments", "union", "basesegment", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "compare", "input", "segments", "for", "a", "match", "return", "a", "matchresult", "note", "for", "matching", "here", "we", "only", "consider", "the", "first", "element", "because", "we", "assume", "that", "a", "keyword", "can", "only", "span", "one", "raw", "segment", "if", "we", "ve", "been", "passed", "the", "singular", "make", "it", "a", "tuple", "if", "isinstance", "segments", "basesegment", "segments", "segments", "we", "re", "only", "going", "to", "match", "against", "the", "first", "element", "if", "len", "segments", "1", "is", "the", "first", "one", "already", "of", "this", "type", "if", "isinstance", "segments", "0", "self", "raw_class", "and", "segments", "0", "name", "self", "name", "and", "segments", "0", "is_type", "self", "type", "return", "matchresult", "segments", "0", "segments", "1", "does", "it", "match", "elif", "self", "_is_first_match", "segments", "0", "return", "self", "_make_match_from_first_result", "segments", "return", "matchresult", "from_unmatched", "segments"], "doc_len": 128}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::NamedParser.simple", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "NamedParser", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: NamedParser\n    def simple(cls, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        NamedParser segment does NOT for now. We might need to later for efficiency.\n\n        There is a way that this *could* be enabled, by allowing *another*\n        shortcut route, to look ahead at the names of upcoming segments,\n        rather than their content.\n        \"\"\"\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "namedparser", "def", "simple", "cls", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "namedparser", "segment", "does", "not", "for", "now", "we", "might", "need", "to", "later", "for", "efficiency", "there", "is", "a", "way", "that", "this", "could", "be", "enabled", "by", "allowing", "another", "shortcut", "route", "to", "look", "ahead", "at", "the", "names", "of", "upcoming", "segments", "rather", "than", "their", "content", "return", "none"], "doc_len": 66}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::NamedParser._is_first_match", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "NamedParser", "func_name": "_is_first_match", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: NamedParser\n    def _is_first_match(self, segment: BaseSegment):\n        \"\"\"Does the segment provided match according to the current rules.\n\n        NamedParser implements its own matching function where\n        we assume that ._template is the `name` of a segment.\n        \"\"\"\n        # Case sensitivity is not supported. Names are all\n        # lowercase by convention.\n        if self.template.lower() == segment.name.lower():\n            return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "namedparser", "def", "_is_first_match", "self", "segment", "basesegment", "does", "the", "segment", "provided", "match", "according", "to", "the", "current", "rules", "namedparser", "implements", "its", "own", "matching", "function", "where", "we", "assume", "that", "_template", "is", "the", "name", "of", "a", "segment", "case", "sensitivity", "is", "not", "supported", "names", "are", "all", "lowercase", "by", "convention", "if", "self", "template", "lower", "segment", "name", "lower", "return", "true", "return", "false"], "doc_len": 61}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::RegexParser.__init__", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "RegexParser", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: RegexParser\n    def __init__(\n        self,\n        template: str,\n        raw_class: Type[RawSegment],\n        name: Optional[str] = None,\n        type: Optional[str] = None,\n        optional: bool = False,\n        anti_template: Optional[str] = None,\n        **segment_kwargs,\n    ):\n        # Store the optional anti-template\n        self.anti_template = anti_template\n        super().__init__(\n            template=template,\n            raw_class=raw_class,\n            name=name,\n            type=type,\n            optional=optional,\n            **segment_kwargs,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "regexparser", "def", "__init__", "self", "template", "str", "raw_class", "type", "rawsegment", "name", "optional", "str", "none", "type", "optional", "str", "none", "optional", "bool", "false", "anti_template", "optional", "str", "none", "segment_kwargs", "store", "the", "optional", "anti", "template", "self", "anti_template", "anti_template", "super", "__init__", "template", "template", "raw_class", "raw_class", "name", "name", "type", "type", "optional", "optional", "segment_kwargs"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::RegexParser.simple", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "RegexParser", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: RegexParser\n    def simple(cls, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        Regex segment does NOT for now. We might need to later for efficiency.\n        \"\"\"\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "regexparser", "def", "simple", "cls", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "regex", "segment", "does", "not", "for", "now", "we", "might", "need", "to", "later", "for", "efficiency", "return", "none"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/parsers.py::RegexParser._is_first_match", "file_path": "src/sqlfluff/core/parser/parsers.py", "class_name": "RegexParser", "func_name": "_is_first_match", "text": "文件路径: src/sqlfluff/core/parser/parsers.py, 类名: RegexParser\n    def _is_first_match(self, segment: BaseSegment):\n        \"\"\"Does the segment provided match according to the current rules.\n\n        RegexParser implements its own matching function where\n        we assume that ._template is a r\"\" string, and is formatted\n        for use directly as a regex. This only matches on a single segment.\n        \"\"\"\n        if len(segment.raw) == 0:  # pragma: no cover TODO?\n            # If it's of zero length it's probably a meta segment.\n            # In any case, it won't match here.\n            return False\n        # Try the regex. Case sensitivity is not supported.\n        result = re.match(self.template, segment.raw_upper)\n        if result:\n            result_string = result.group(0)\n            # Check that we've fully matched\n            if result_string == segment.raw_upper:\n                # Check that the anti_template (if set) hasn't also matched\n                if self.anti_template and re.match(\n                    self.anti_template, segment.raw_upper\n                ):\n                    return False\n                else:\n                    return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "parsers", "py", "regexparser", "def", "_is_first_match", "self", "segment", "basesegment", "does", "the", "segment", "provided", "match", "according", "to", "the", "current", "rules", "regexparser", "implements", "its", "own", "matching", "function", "where", "we", "assume", "that", "_template", "is", "a", "r", "string", "and", "is", "formatted", "for", "use", "directly", "as", "a", "regex", "this", "only", "matches", "on", "a", "single", "segment", "if", "len", "segment", "raw", "0", "pragma", "no", "cover", "todo", "if", "it", "s", "of", "zero", "length", "it", "s", "probably", "a", "meta", "segment", "in", "any", "case", "it", "won", "t", "match", "here", "return", "false", "try", "the", "regex", "case", "sensitivity", "is", "not", "supported", "result", "re", "match", "self", "template", "segment", "raw_upper", "if", "result", "result_string", "result", "group", "0", "check", "that", "we", "ve", "fully", "matched", "if", "result_string", "segment", "raw_upper", "check", "that", "the", "anti_template", "if", "set", "hasn", "t", "also", "matched", "if", "self", "anti_template", "and", "re", "match", "self", "anti_template", "segment", "raw_upper", "return", "false", "else", "return", "true", "return", "false"], "doc_len": 142}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf.__init__", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def __init__(self, *args, **kwargs):\n        self.max_times = kwargs.pop(\"max_times\", None)\n        self.min_times = kwargs.pop(\"min_times\", 0)\n        # Any patterns to _prevent_ a match.\n        self.exclude = kwargs.pop(\"exclude\", None)\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "__init__", "self", "args", "kwargs", "self", "max_times", "kwargs", "pop", "max_times", "none", "self", "min_times", "kwargs", "pop", "min_times", "0", "any", "patterns", "to", "_prevent_", "a", "match", "self", "exclude", "kwargs", "pop", "exclude", "none", "super", "__init__", "args", "kwargs"], "doc_len": 41}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf.simple", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        AnyNumberOf does provide this, as long as *all* the elements *also* do.\n        \"\"\"\n        simple_buff = [\n            opt.simple(parse_context=parse_context) for opt in self._elements\n        ]\n        if any(elem is None for elem in simple_buff):\n            return None\n        # Flatten the list\n        return [inner for outer in simple_buff for inner in outer]\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "anynumberof", "does", "provide", "this", "as", "long", "as", "all", "the", "elements", "also", "do", "simple_buff", "opt", "simple", "parse_context", "parse_context", "for", "opt", "in", "self", "_elements", "if", "any", "elem", "is", "none", "for", "elem", "in", "simple_buff", "return", "none", "flatten", "the", "list", "return", "inner", "for", "outer", "in", "simple_buff", "for", "inner", "in", "outer"], "doc_len": 71}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf.is_optional", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def is_optional(self) -> bool:\n        \"\"\"Return whether this element is optional.\n\n        This is mostly set in the init method, but also in this\n        case, if min_times is zero then this is also optional.\n        \"\"\"\n        return self.optional or self.min_times == 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "is_optional", "self", "bool", "return", "whether", "this", "element", "is", "optional", "this", "is", "mostly", "set", "in", "the", "init", "method", "but", "also", "in", "this", "case", "if", "min_times", "is", "zero", "then", "this", "is", "also", "optional", "return", "self", "optional", "or", "self", "min_times", "0"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf._prune_options", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "_prune_options", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def _prune_options(\n        self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> Tuple[List[MatchableType], List[str]]:\n        \"\"\"Use the simple matchers to prune which options to match on.\"\"\"\n        str_buff = [segment.raw_upper for segment in self._iter_raw_segs(segments)]\n\n        available_options = []\n        simple_opts = []\n        prune_buff = []\n        non_simple = 0\n        pruned_simple = 0\n        matched_simple = 0\n\n        # Find the first code element to match against.\n        first_elem = None\n        for elem in str_buff:\n            if elem.strip():\n                first_elem = elem\n                break\n\n        for opt in self._elements:\n            simple = opt.simple(parse_context=parse_context)\n            if simple is None:\n                # This element is not simple, we have to do a\n                # full match with it...\n                available_options.append(opt)\n                non_simple += 1\n                continue\n            # Otherwise we have a simple option, so let's use\n            # it for pruning.\n            for simple_opt in simple:\n                # Check it's not a whitespace option\n                if not simple_opt.strip():  # pragma: no cover\n                    raise NotImplementedError(\n                        \"_prune_options not supported for whitespace matching.\"\n                    )\n                # We want to know if the first meaningful element of the str_buff\n                # matches the option.\n                if simple_opt in str_buff:\n                    # match the FIRST non-whitespace element of the list.\n                    if first_elem != simple_opt:\n                        # No match, carry on.\n                        continue\n                    # If we get here, it's matched the FIRST element of the string buffer.\n                    available_options.append(opt)\n                    simple_opts.append(simple_opt)\n                    matched_simple += 1\n                    break\n            else:\n                # Ditch this option, the simple match has failed\n                prune_buff.append(opt)\n                pruned_simple += 1\n                continue\n\n        parse_match_logging(\n            self.__class__.__name__,\n            \"match\",\n            \"PRN\",\n            parse_context=parse_context,\n            v_level=3,\n            ns=non_simple,\n            ps=pruned_simple,\n            ms=matched_simple,\n            pruned=prune_buff,\n            opts=available_options or \"ALL\",\n        )\n\n        return available_options, simple_opts\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "_prune_options", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "tuple", "list", "matchabletype", "list", "str", "use", "the", "simple", "matchers", "to", "prune", "which", "options", "to", "match", "on", "str_buff", "segment", "raw_upper", "for", "segment", "in", "self", "_iter_raw_segs", "segments", "available_options", "simple_opts", "prune_buff", "non_simple", "0", "pruned_simple", "0", "matched_simple", "0", "find", "the", "first", "code", "element", "to", "match", "against", "first_elem", "none", "for", "elem", "in", "str_buff", "if", "elem", "strip", "first_elem", "elem", "break", "for", "opt", "in", "self", "_elements", "simple", "opt", "simple", "parse_context", "parse_context", "if", "simple", "is", "none", "this", "element", "is", "not", "simple", "we", "have", "to", "do", "a", "full", "match", "with", "it", "available_options", "append", "opt", "non_simple", "1", "continue", "otherwise", "we", "have", "a", "simple", "option", "so", "let", "s", "use", "it", "for", "pruning", "for", "simple_opt", "in", "simple", "check", "it", "s", "not", "a", "whitespace", "option", "if", "not", "simple_opt", "strip", "pragma", "no", "cover", "raise", "notimplementederror", "_prune_options", "not", "supported", "for", "whitespace", "matching", "we", "want", "to", "know", "if", "the", "first", "meaningful", "element", "of", "the", "str_buff", "matches", "the", "option", "if", "simple_opt", "in", "str_buff", "match", "the", "first", "non", "whitespace", "element", "of", "the", "list", "if", "first_elem", "simple_opt", "no", "match", "carry", "on", "continue", "if", "we", "get", "here", "it", "s", "matched", "the", "first", "element", "of", "the", "string", "buffer", "available_options", "append", "opt", "simple_opts", "append", "simple_opt", "matched_simple", "1", "break", "else", "ditch", "this", "option", "the", "simple", "match", "has", "failed", "prune_buff", "append", "opt", "pruned_simple", "1", "continue", "parse_match_logging", "self", "__class__", "__name__", "match", "prn", "parse_context", "parse_context", "v_level", "3", "ns", "non_simple", "ps", "pruned_simple", "ms", "matched_simple", "pruned", "prune_buff", "opts", "available_options", "or", "all", "return", "available_options", "simple_opts"], "doc_len": 242}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf._match_once", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "_match_once", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def _match_once(\n        self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match the forward segments against the available elements once.\n\n        This serves as the main body of OneOf, but also a building block\n        for AnyNumberOf.\n        \"\"\"\n        # For efficiency, we'll be pruning options if we can\n        # based on their simpleness. this provides a short cut\n        # to return earlier if we can.\n        # `segments` may already be nested so we need to break out\n        # the raw segments within it.\n        available_options, _ = self._prune_options(\n            segments, parse_context=parse_context\n        )\n\n        # If we've pruned all the options, return unmatched (with some logging).\n        if not available_options:\n            return MatchResult.from_unmatched(segments)\n\n        with parse_context.deeper_match() as ctx:\n            match, _ = self._longest_trimmed_match(\n                segments,\n                available_options,\n                parse_context=ctx,\n                trim_noncode=False,\n            )\n\n        return match\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "_match_once", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "match", "the", "forward", "segments", "against", "the", "available", "elements", "once", "this", "serves", "as", "the", "main", "body", "of", "oneof", "but", "also", "a", "building", "block", "for", "anynumberof", "for", "efficiency", "we", "ll", "be", "pruning", "options", "if", "we", "can", "based", "on", "their", "simpleness", "this", "provides", "a", "short", "cut", "to", "return", "earlier", "if", "we", "can", "segments", "may", "already", "be", "nested", "so", "we", "need", "to", "break", "out", "the", "raw", "segments", "within", "it", "available_options", "_", "self", "_prune_options", "segments", "parse_context", "parse_context", "if", "we", "ve", "pruned", "all", "the", "options", "return", "unmatched", "with", "some", "logging", "if", "not", "available_options", "return", "matchresult", "from_unmatched", "segments", "with", "parse_context", "deeper_match", "as", "ctx", "match", "_", "self", "_longest_trimmed_match", "segments", "available_options", "parse_context", "ctx", "trim_noncode", "false", "return", "match"], "doc_len": 125}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::AnyNumberOf.match", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "AnyNumberOf", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: AnyNumberOf\n    def match(\n        self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match against any of the elements a relevant number of times.\n\n        If it matches multiple, it returns the longest, and if any are the same\n        length it returns the first (unless we explicitly just match first).\n        \"\"\"\n        # First if we have an *exclude* option, we should check that\n        # which would prevent the rest of this grammar from matching.\n        if self.exclude:\n            with parse_context.deeper_match() as ctx:\n                if self.exclude.match(segments, parse_context=ctx):\n                    return MatchResult.from_unmatched(segments)\n\n        # Match on each of the options\n        matched_segments: MatchResult = MatchResult.from_empty()\n        unmatched_segments: Tuple[BaseSegment, ...] = segments\n        n_matches = 0\n        while True:\n            if self.max_times and n_matches >= self.max_times:\n                # We've matched as many times as we can\n                return MatchResult(\n                    matched_segments.matched_segments, unmatched_segments\n                )\n\n            # Is there anything left to match?\n            if len(unmatched_segments) == 0:\n                # No...\n                if n_matches >= self.min_times:\n                    return MatchResult(\n                        matched_segments.matched_segments, unmatched_segments\n                    )\n                else:  # pragma: no cover TODO?\n                    # We didn't meet the hurdle\n                    return MatchResult.from_unmatched(unmatched_segments)\n\n            # If we've already matched once...\n            if n_matches > 0 and self.allow_gaps:\n                # Consume any non-code if there is any\n                pre_seg, mid_seg, post_seg = trim_non_code_segments(unmatched_segments)\n                unmatched_segments = mid_seg + post_seg\n            else:\n                pre_seg = ()  # empty tuple\n\n            match = self._match_once(unmatched_segments, parse_context=parse_context)\n            if match:\n                matched_segments += pre_seg + match.matched_segments\n                unmatched_segments = match.unmatched_segments\n                n_matches += 1\n            else:\n                # If we get here, then we've not managed to match. And the next\n                # unmatched segments are meaningful, i.e. they're not what we're\n                # looking for.\n                if n_matches >= self.min_times:\n                    return MatchResult(\n                        matched_segments.matched_segments, pre_seg + unmatched_segments\n                    )\n                else:\n                    # We didn't meet the hurdle\n                    return MatchResult.from_unmatched(unmatched_segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "anynumberof", "def", "match", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "match", "against", "any", "of", "the", "elements", "a", "relevant", "number", "of", "times", "if", "it", "matches", "multiple", "it", "returns", "the", "longest", "and", "if", "any", "are", "the", "same", "length", "it", "returns", "the", "first", "unless", "we", "explicitly", "just", "match", "first", "first", "if", "we", "have", "an", "exclude", "option", "we", "should", "check", "that", "which", "would", "prevent", "the", "rest", "of", "this", "grammar", "from", "matching", "if", "self", "exclude", "with", "parse_context", "deeper_match", "as", "ctx", "if", "self", "exclude", "match", "segments", "parse_context", "ctx", "return", "matchresult", "from_unmatched", "segments", "match", "on", "each", "of", "the", "options", "matched_segments", "matchresult", "matchresult", "from_empty", "unmatched_segments", "tuple", "basesegment", "segments", "n_matches", "0", "while", "true", "if", "self", "max_times", "and", "n_matches", "self", "max_times", "we", "ve", "matched", "as", "many", "times", "as", "we", "can", "return", "matchresult", "matched_segments", "matched_segments", "unmatched_segments", "is", "there", "anything", "left", "to", "match", "if", "len", "unmatched_segments", "0", "no", "if", "n_matches", "self", "min_times", "return", "matchresult", "matched_segments", "matched_segments", "unmatched_segments", "else", "pragma", "no", "cover", "todo", "we", "didn", "t", "meet", "the", "hurdle", "return", "matchresult", "from_unmatched", "unmatched_segments", "if", "we", "ve", "already", "matched", "once", "if", "n_matches", "0", "and", "self", "allow_gaps", "consume", "any", "non", "code", "if", "there", "is", "any", "pre_seg", "mid_seg", "post_seg", "trim_non_code_segments", "unmatched_segments", "unmatched_segments", "mid_seg", "post_seg", "else", "pre_seg", "empty", "tuple", "match", "self", "_match_once", "unmatched_segments", "parse_context", "parse_context", "if", "match", "matched_segments", "pre_seg", "match", "matched_segments", "unmatched_segments", "match", "unmatched_segments", "n_matches", "1", "else", "if", "we", "get", "here", "then", "we", "ve", "not", "managed", "to", "match", "and", "the", "next", "unmatched", "segments", "are", "meaningful", "i", "e", "they", "re", "not", "what", "we", "re", "looking", "for", "if", "n_matches", "self", "min_times", "return", "matchresult", "matched_segments", "matched_segments", "pre_seg", "unmatched_segments", "else", "we", "didn", "t", "meet", "the", "hurdle", "return", "matchresult", "from_unmatched", "unmatched_segments"], "doc_len": 266}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::OneOf.__init__", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "OneOf", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: OneOf\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, max_times=1, min_times=1, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "oneof", "def", "__init__", "self", "args", "kwargs", "super", "__init__", "args", "max_times", "1", "min_times", "1", "kwargs"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/grammar/anyof.py::OptionallyBracketed.__init__", "file_path": "src/sqlfluff/core/parser/grammar/anyof.py", "class_name": "OptionallyBracketed", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/anyof.py, 类名: OptionallyBracketed\n    def __init__(self, *args, **kwargs):\n        super().__init__(\n            Bracketed(*args),\n            # In the case that there is only one argument, no sequence is required.\n            args[0] if len(args) == 1 else Sequence(*args),\n            **kwargs,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "anyof", "py", "optionallybracketed", "def", "__init__", "self", "args", "kwargs", "super", "__init__", "bracketed", "args", "in", "the", "case", "that", "there", "is", "only", "one", "argument", "no", "sequence", "is", "required", "args", "0", "if", "len", "args", "1", "else", "sequence", "args", "kwargs"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BracketInfo.to_segment", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BracketInfo", "func_name": "to_segment", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BracketInfo\n    def to_segment(self, end_bracket):\n        \"\"\"Turn the contained segments into a bracketed segment.\"\"\"\n        return BracketedSegment(\n            segments=self.segments,\n            start_bracket=(self.bracket,),\n            end_bracket=end_bracket,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "bracketinfo", "def", "to_segment", "self", "end_bracket", "turn", "the", "contained", "segments", "into", "a", "bracketed", "segment", "return", "bracketedsegment", "segments", "self", "segments", "start_bracket", "self", "bracket", "end_bracket", "end_bracket"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::cached_method_for_parse_context", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": null, "func_name": "cached_method_for_parse_context", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py\ndef cached_method_for_parse_context(func):\n    \"\"\"A decorator to cache the output of this method for a given parse context.\n\n    This cache automatically invalidates if the uuid\n    of the parse context changes. The value is store\n    in the __dict__ attribute of the class against a\n    key unique to that function.\n    \"\"\"\n    cache_key = \"__cache_\" + func.__name__\n\n    def wrapped_method(self, parse_context: ParseContext):\n        \"\"\"Cache the output of the method against a given parse context.\"\"\"\n        cache_tuple: Tuple = self.__dict__.get(cache_key, (None, None))\n        # Do we currently have a cached value?\n        if cache_tuple[0] == parse_context.uuid:\n            return cache_tuple[1]\n        # Generate a new value, cache it and return\n        result = func(self, parse_context=parse_context)\n        self.__dict__[cache_key] = (parse_context.uuid, result)\n        return result\n\n    return wrapped_method\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "def", "cached_method_for_parse_context", "func", "a", "decorator", "to", "cache", "the", "output", "of", "this", "method", "for", "a", "given", "parse", "context", "this", "cache", "automatically", "invalidates", "if", "the", "uuid", "of", "the", "parse", "context", "changes", "the", "value", "is", "store", "in", "the", "__dict__", "attribute", "of", "the", "class", "against", "a", "key", "unique", "to", "that", "function", "cache_key", "__cache_", "func", "__name__", "def", "wrapped_method", "self", "parse_context", "parsecontext", "cache", "the", "output", "of", "the", "method", "against", "a", "given", "parse", "context", "cache_tuple", "tuple", "self", "__dict__", "get", "cache_key", "none", "none", "do", "we", "currently", "have", "a", "cached", "value", "if", "cache_tuple", "0", "parse_context", "uuid", "return", "cache_tuple", "1", "generate", "a", "new", "value", "cache", "it", "and", "return", "result", "func", "self", "parse_context", "parse_context", "self", "__dict__", "cache_key", "parse_context", "uuid", "result", "return", "result", "return", "wrapped_method"], "doc_len": 120}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar._resolve_ref", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "_resolve_ref", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def _resolve_ref(elem):\n        \"\"\"Resolve potential string references to things we can match against.\"\"\"\n        initialisers = [\n            # t: instance / f: class, ref, func\n            (True, str, Ref.keyword),\n            (True, BaseGrammar, lambda x: x),\n            (True, StringParser, lambda x: x),\n            (False, BaseSegment, lambda x: x),\n        ]\n        # Get-out clause for None\n        if elem is None:\n            return None\n\n        for instance, init_type, init_func in initialisers:\n            if (instance and isinstance(elem, init_type)) or (\n                not instance and issubclass(elem, init_type)\n            ):\n                return init_func(elem)\n        raise TypeError(\n            \"Grammar element [{!r}] was found of unexpected type [{}] was found.\".format(\n                elem, type(elem)\n            )  # pragma: no cover\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "_resolve_ref", "elem", "resolve", "potential", "string", "references", "to", "things", "we", "can", "match", "against", "initialisers", "t", "instance", "f", "class", "ref", "func", "true", "str", "ref", "keyword", "true", "basegrammar", "lambda", "x", "x", "true", "stringparser", "lambda", "x", "x", "false", "basesegment", "lambda", "x", "x", "get", "out", "clause", "for", "none", "if", "elem", "is", "none", "return", "none", "for", "instance", "init_type", "init_func", "in", "initialisers", "if", "instance", "and", "isinstance", "elem", "init_type", "or", "not", "instance", "and", "issubclass", "elem", "init_type", "return", "init_func", "elem", "raise", "typeerror", "grammar", "element", "r", "was", "found", "of", "unexpected", "type", "was", "found", "format", "elem", "type", "elem", "pragma", "no", "cover"], "doc_len": 99}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.__init__", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def __init__(\n        self,\n        *args,\n        allow_gaps=True,\n        optional=False,\n        ephemeral_name=None,\n    ):\n        \"\"\"Deal with kwargs common to all grammars.\n\n        Args:\n            *args: Any number of elements which because the subjects\n                of this grammar.\n            allow_gaps (:obj:`bool`, optional): Does this instance of the\n                grammar allow gaps between the elements it matches? This\n                may be exhibited slightly differently in each grammar. See\n                that grammar for details. Defaults `True`.\n            optional (:obj:`bool`, optional): In the context of a sequence,\n                is this grammar *optional*, i.e. can it be skipped if no\n                match is found. Outside of a Sequence, this option does nothing.\n                Defaults `False`.\n            ephemeral_name (:obj:`str`, optional): If specified this allows\n                the grammar to match anything, and create an EphemeralSegment\n                with the given name in its place. The content of this grammar\n                is passed to the segment, and will become the parse grammar\n                for it. If used widely this is an excellent way of breaking\n                up the parse process and also signposting the name of a given\n                chunk of code that might be parsed separately.\n        \"\"\"\n        # We provide a common interface for any grammar that allows positional elements.\n        # If *any* for the elements are a string and not a grammar, then this is a shortcut\n        # to the Ref.keyword grammar by default.\n        if self.allow_keyword_string_refs:\n            self._elements = []\n            for elem in args:\n                self._elements.append(self._resolve_ref(elem))\n        else:\n            self._elements = list(args)\n\n        # Now we deal with the standard kwargs\n        self.allow_gaps = allow_gaps\n        self.optional = optional\n        # ephemeral_name is a flag to indicate whether we need to make an\n        # EphemeralSegment class. This is effectively syntactic sugar\n        # to allow us to avoid specifying a EphemeralSegment directly in a dialect.\n        # If this is the case, the actual segment construction happens in the\n        # match_wrapper.\n        self.ephemeral_name = ephemeral_name\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "__init__", "self", "args", "allow_gaps", "true", "optional", "false", "ephemeral_name", "none", "deal", "with", "kwargs", "common", "to", "all", "grammars", "args", "args", "any", "number", "of", "elements", "which", "because", "the", "subjects", "of", "this", "grammar", "allow_gaps", "obj", "bool", "optional", "does", "this", "instance", "of", "the", "grammar", "allow", "gaps", "between", "the", "elements", "it", "matches", "this", "may", "be", "exhibited", "slightly", "differently", "in", "each", "grammar", "see", "that", "grammar", "for", "details", "defaults", "true", "optional", "obj", "bool", "optional", "in", "the", "context", "of", "a", "sequence", "is", "this", "grammar", "optional", "i", "e", "can", "it", "be", "skipped", "if", "no", "match", "is", "found", "outside", "of", "a", "sequence", "this", "option", "does", "nothing", "defaults", "false", "ephemeral_name", "obj", "str", "optional", "if", "specified", "this", "allows", "the", "grammar", "to", "match", "anything", "and", "create", "an", "ephemeralsegment", "with", "the", "given", "name", "in", "its", "place", "the", "content", "of", "this", "grammar", "is", "passed", "to", "the", "segment", "and", "will", "become", "the", "parse", "grammar", "for", "it", "if", "used", "widely", "this", "is", "an", "excellent", "way", "of", "breaking", "up", "the", "parse", "process", "and", "also", "signposting", "the", "name", "of", "a", "given", "chunk", "of", "code", "that", "might", "be", "parsed", "separately", "we", "provide", "a", "common", "interface", "for", "any", "grammar", "that", "allows", "positional", "elements", "if", "any", "for", "the", "elements", "are", "a", "string", "and", "not", "a", "grammar", "then", "this", "is", "a", "shortcut", "to", "the", "ref", "keyword", "grammar", "by", "default", "if", "self", "allow_keyword_string_refs", "self", "_elements", "for", "elem", "in", "args", "self", "_elements", "append", "self", "_resolve_ref", "elem", "else", "self", "_elements", "list", "args", "now", "we", "deal", "with", "the", "standard", "kwargs", "self", "allow_gaps", "allow_gaps", "self", "optional", "optional", "ephemeral_name", "is", "a", "flag", "to", "indicate", "whether", "we", "need", "to", "make", "an", "ephemeralsegment", "class", "this", "is", "effectively", "syntactic", "sugar", "to", "allow", "us", "to", "avoid", "specifying", "a", "ephemeralsegment", "directly", "in", "a", "dialect", "if", "this", "is", "the", "case", "the", "actual", "segment", "construction", "happens", "in", "the", "match_wrapper", "self", "ephemeral_name", "ephemeral_name"], "doc_len": 294}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.is_optional", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def is_optional(self):\n        \"\"\"Return whether this segment is optional.\n\n        The optional attribute is set in the __init__ method.\n        \"\"\"\n        return self.optional\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "is_optional", "self", "return", "whether", "this", "segment", "is", "optional", "the", "optional", "attribute", "is", "set", "in", "the", "__init__", "method", "return", "self", "optional"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def match(self, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext):\n        \"\"\"Match a list of segments against this segment.\n\n        Matching can be done from either the raw or the segments.\n        This raw function can be overridden, or a grammar defined\n        on the underlying class.\n        \"\"\"\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} has no match function implemented\"\n        )  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "match", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "match", "a", "list", "of", "segments", "against", "this", "segment", "matching", "can", "be", "done", "from", "either", "the", "raw", "or", "the", "segments", "this", "raw", "function", "can", "be", "overridden", "or", "a", "grammar", "defined", "on", "the", "underlying", "class", "raise", "notimplementederror", "f", "self", "__class__", "__name__", "has", "no", "match", "function", "implemented", "pragma", "no", "cover"], "doc_len": 63}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.simple", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a lowercase hash matching route?\"\"\"\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "lowercase", "hash", "matching", "route", "return", "none"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar._iter_raw_segs", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "_iter_raw_segs", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def _iter_raw_segs(segments):\n        for segment in segments:\n            yield from segment.iter_raw_seg()\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "_iter_raw_segs", "segments", "for", "segment", "in", "segments", "yield", "from", "segment", "iter_raw_seg"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar._longest_trimmed_match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "_longest_trimmed_match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def _longest_trimmed_match(\n        cls,\n        segments: Tuple[\"BaseSegment\", ...],\n        matchers: List[\"MatchableType\"],\n        parse_context: ParseContext,\n        trim_noncode=True,\n    ) -> Tuple[MatchResult, Optional[\"MatchableType\"]]:\n        \"\"\"Return longest match from a selection of matchers.\n\n        Prioritise the first match, and if multiple match at the same point the longest.\n        If two matches of the same length match at the same time, then it's the first in\n        the iterable of matchers.\n\n        Returns:\n            `tuple` of (match_object, matcher).\n\n        \"\"\"\n        # Have we been passed an empty list?\n        if len(segments) == 0:\n            return MatchResult.from_empty(), None\n\n        # If gaps are allowed, trim the ends.\n        if trim_noncode:\n            pre_nc, segments, post_nc = trim_non_code_segments(segments)\n\n        best_match_length = 0\n        # iterate at this position across all the matchers\n        for matcher in matchers:\n            # MyPy seems to require a type hint here. Not quite sure why.\n            res_match: MatchResult = matcher.match(\n                segments, parse_context=parse_context\n            )\n            if res_match.is_complete():\n                # Just return it! (WITH THE RIGHT OTHER STUFF)\n                if trim_noncode:\n                    return (\n                        MatchResult.from_matched(\n                            pre_nc + res_match.matched_segments + post_nc\n                        ),\n                        matcher,\n                    )\n                else:\n                    return res_match, matcher\n            elif res_match:\n                # We've got an incomplete match, if it's the best so far keep it.\n                if res_match.matched_length > best_match_length:\n                    best_match = res_match, matcher\n                    best_match_length = res_match.matched_length\n            # We could stash segments here, but given we might have some successful\n            # matches here, we shouldn't, because they'll be mutated in the wrong way.\n            # Eventually there might be a performance gain from doing that sensibly here.\n\n        # If we get here, then there wasn't a complete match. If we\n        # has a best_match, return that.\n        if best_match_length > 0:\n            if trim_noncode:\n                return (\n                    MatchResult(\n                        pre_nc + best_match[0].matched_segments,\n                        best_match[0].unmatched_segments + post_nc,\n                    ),\n                    best_match[1],\n                )\n            else:\n                return best_match\n        # If no match at all, return nothing\n        return MatchResult.from_unmatched(segments), None\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "_longest_trimmed_match", "cls", "segments", "tuple", "basesegment", "matchers", "list", "matchabletype", "parse_context", "parsecontext", "trim_noncode", "true", "tuple", "matchresult", "optional", "matchabletype", "return", "longest", "match", "from", "a", "selection", "of", "matchers", "prioritise", "the", "first", "match", "and", "if", "multiple", "match", "at", "the", "same", "point", "the", "longest", "if", "two", "matches", "of", "the", "same", "length", "match", "at", "the", "same", "time", "then", "it", "s", "the", "first", "in", "the", "iterable", "of", "matchers", "returns", "tuple", "of", "match_object", "matcher", "have", "we", "been", "passed", "an", "empty", "list", "if", "len", "segments", "0", "return", "matchresult", "from_empty", "none", "if", "gaps", "are", "allowed", "trim", "the", "ends", "if", "trim_noncode", "pre_nc", "segments", "post_nc", "trim_non_code_segments", "segments", "best_match_length", "0", "iterate", "at", "this", "position", "across", "all", "the", "matchers", "for", "matcher", "in", "matchers", "mypy", "seems", "to", "require", "a", "type", "hint", "here", "not", "quite", "sure", "why", "res_match", "matchresult", "matcher", "match", "segments", "parse_context", "parse_context", "if", "res_match", "is_complete", "just", "return", "it", "with", "the", "right", "other", "stuff", "if", "trim_noncode", "return", "matchresult", "from_matched", "pre_nc", "res_match", "matched_segments", "post_nc", "matcher", "else", "return", "res_match", "matcher", "elif", "res_match", "we", "ve", "got", "an", "incomplete", "match", "if", "it", "s", "the", "best", "so", "far", "keep", "it", "if", "res_match", "matched_length", "best_match_length", "best_match", "res_match", "matcher", "best_match_length", "res_match", "matched_length", "we", "could", "stash", "segments", "here", "but", "given", "we", "might", "have", "some", "successful", "matches", "here", "we", "shouldn", "t", "because", "they", "ll", "be", "mutated", "in", "the", "wrong", "way", "eventually", "there", "might", "be", "a", "performance", "gain", "from", "doing", "that", "sensibly", "here", "if", "we", "get", "here", "then", "there", "wasn", "t", "a", "complete", "match", "if", "we", "has", "a", "best_match", "return", "that", "if", "best_match_length", "0", "if", "trim_noncode", "return", "matchresult", "pre_nc", "best_match", "0", "matched_segments", "best_match", "0", "unmatched_segments", "post_nc", "best_match", "1", "else", "return", "best_match", "if", "no", "match", "at", "all", "return", "nothing", "return", "matchresult", "from_unmatched", "segments", "none"], "doc_len": 276}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar._look_ahead_match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "_look_ahead_match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def _look_ahead_match(cls, segments, matchers, parse_context):\n        \"\"\"Look ahead for matches beyond the first element of the segments list.\n\n        This function also contains the performance improved hash-matching approach to\n        searching for matches, which should significantly improve performance.\n\n        Prioritise the first match, and if multiple match at the same point the longest.\n        If two matches of the same length match at the same time, then it's the first in\n        the iterable of matchers.\n\n        Returns:\n            `tuple` of (unmatched_segments, match_object, matcher).\n\n        \"\"\"\n        parse_match_logging(\n            cls.__name__,\n            \"_look_ahead_match\",\n            \"IN\",\n            parse_context=parse_context,\n            v_level=4,\n            ls=len(segments),\n            seg=LateBoundJoinSegmentsCurtailed(segments),\n        )\n\n        # Do some type munging\n        matchers = list(matchers)\n        if isinstance(segments, BaseSegment):  # pragma: no cover TODO?\n            segments = [segments]\n\n        # Have we been passed an empty list?\n        if len(segments) == 0:  # pragma: no cover TODO?\n            return ((), MatchResult.from_empty(), None)\n\n        # Here we enable a performance optimisation. Most of the time in this cycle\n        # happens in loops looking for simple matchers which we should\n        # be able to find a shortcut for.\n        # First: Assess the matchers passed in, if any are\n        # \"simple\", then we effectively use a hash lookup across the\n        # content of segments to quickly evaluate if the segment is present.\n        # Matchers which aren't \"simple\" still take a slower route.\n        _matchers = [\n            (matcher, matcher.simple(parse_context=parse_context))\n            for matcher in matchers\n        ]\n        simple_matchers = [matcher for matcher in _matchers if matcher[1]]\n        non_simple_matchers = [matcher[0] for matcher in _matchers if not matcher[1]]\n        best_simple_match = None\n        if simple_matchers:\n            # If they're all simple we can use a hash match to identify the first one.\n            # Build a buffer of all the upper case raw segments ahead of us.\n            str_buff = []\n            # For existing compound segments, we should assume that within\n            # that segment, things are internally consistent, that means\n            # rather than enumerating all the individual segments of a longer\n            # one we just dump out the whole segment, but splitting off the\n            # first element seperated by whitespace. This is a) faster and\n            # also b) prevents some really horrible bugs with bracket matching.\n            # See https://github.com/sqlfluff/sqlfluff/issues/433\n\n            def _trim_elem(seg):\n                s = seg.raw_upper.split(maxsplit=1)\n                return s[0] if s else \"\"\n\n            str_buff = [_trim_elem(seg) for seg in segments]\n            match_queue = []\n\n            for matcher, simple in simple_matchers:\n                # Simple will be a tuple of options\n                for simple_option in simple:\n                    # NOTE: We use iter_indices to make sure we capture\n                    # all instances of potential matches if there are many.\n                    # This is important for bracket counting.\n                    for buff_pos in iter_indices(str_buff, simple_option):\n                        match_queue.append((matcher, buff_pos, simple_option))\n\n            # Sort the match queue. First to process AT THE END.\n            # That means we pop from the end.\n            match_queue = sorted(match_queue, key=lambda x: x[1])\n\n            parse_match_logging(\n                cls.__name__,\n                \"_look_ahead_match\",\n                \"SI\",\n                parse_context=parse_context,\n                v_level=4,\n                mq=match_queue,\n                sb=str_buff,\n            )\n\n            while match_queue:\n                # We've managed to match. We can shortcut home.\n                # NB: We may still need to deal with whitespace.\n                queued_matcher, queued_buff_pos, queued_option = match_queue.pop()\n                # Here we do the actual transform to the new segment.\n                match = queued_matcher.match(segments[queued_buff_pos:], parse_context)\n                if not match:\n                    # We've had something match in simple matching, but then later excluded.\n                    # Log but then move on to the next item on the list.\n                    parse_match_logging(\n                        cls.__name__,\n                        \"_look_ahead_match\",\n                        \"NM\",\n                        parse_context=parse_context,\n                        v_level=4,\n                        _so=queued_option,\n                    )\n                    continue\n                # Ok we have a match. Because we sorted the list, we'll take it!\n                best_simple_match = (segments[:queued_buff_pos], match, queued_matcher)\n\n        if not non_simple_matchers:\n            # There are no other matchers, we can just shortcut now.\n\n            parse_match_logging(\n                cls.__name__,\n                \"_look_ahead_match\",\n                \"SC\",\n                parse_context=parse_context,\n                v_level=4,\n                bsm=None\n                if not best_simple_match\n                else (\n                    len(best_simple_match[0]),\n                    len(best_simple_match[1]),\n                    best_simple_match[2],\n                ),\n            )\n\n            if best_simple_match:\n                return best_simple_match\n            else:\n                return ((), MatchResult.from_unmatched(segments), None)\n\n        # Make some buffers\n        seg_buff = segments\n        pre_seg_buff = ()  # NB: Tuple\n\n        # Loop\n        while True:\n            # Do we have anything left to match on?\n            if seg_buff:\n                # Great, carry on.\n                pass\n            else:\n                # We've got to the end without a match, return empty\n                return ((), MatchResult.from_unmatched(segments), None)\n\n            # We only check the NON-simple ones here for brevity.\n            mat, m = cls._longest_trimmed_match(\n                seg_buff,\n                non_simple_matchers,\n                parse_context=parse_context,\n                trim_noncode=False,\n            )\n\n            if mat and not best_simple_match:\n                return (pre_seg_buff, mat, m)\n            elif mat:\n                # It will be earlier than the simple one if we've even checked,\n                # but there's a chance that this might be *longer*, or just FIRST.\n                pre_lengths = (len(pre_seg_buff), len(best_simple_match[0]))\n                mat_lengths = (len(mat), len(best_simple_match[1]))\n                mat_indexes = (matchers.index(m), matchers.index(best_simple_match[2]))\n                if (\n                    (pre_lengths[0] < pre_lengths[1])\n                    or (\n                        pre_lengths[0] == pre_lengths[1]\n                        and mat_lengths[0] > mat_lengths[1]\n                    )\n                    or (\n                        pre_lengths[0] == pre_lengths[1]\n                        and mat_lengths[0] == mat_lengths[1]\n                        and mat_indexes[0] < mat_indexes[1]\n                    )\n                ):\n                    return (pre_seg_buff, mat, m)\n                else:\n                    return best_simple_match\n            else:\n                # If there aren't any matches, then advance the buffer and try again.\n                # Two improvements:\n                # 1) if we get as far as the first simple match, then return that.\n                # 2) be eager in consuming non-code segments if allowed\n                if best_simple_match and len(pre_seg_buff) >= len(best_simple_match[0]):\n                    return best_simple_match\n\n                pre_seg_buff += (seg_buff[0],)\n                seg_buff = seg_buff[1:]\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "_look_ahead_match", "cls", "segments", "matchers", "parse_context", "look", "ahead", "for", "matches", "beyond", "the", "first", "element", "of", "the", "segments", "list", "this", "function", "also", "contains", "the", "performance", "improved", "hash", "matching", "approach", "to", "searching", "for", "matches", "which", "should", "significantly", "improve", "performance", "prioritise", "the", "first", "match", "and", "if", "multiple", "match", "at", "the", "same", "point", "the", "longest", "if", "two", "matches", "of", "the", "same", "length", "match", "at", "the", "same", "time", "then", "it", "s", "the", "first", "in", "the", "iterable", "of", "matchers", "returns", "tuple", "of", "unmatched_segments", "match_object", "matcher", "parse_match_logging", "cls", "__name__", "_look_ahead_match", "in", "parse_context", "parse_context", "v_level", "4", "ls", "len", "segments", "seg", "lateboundjoinsegmentscurtailed", "segments", "do", "some", "type", "munging", "matchers", "list", "matchers", "if", "isinstance", "segments", "basesegment", "pragma", "no", "cover", "todo", "segments", "segments", "have", "we", "been", "passed", "an", "empty", "list", "if", "len", "segments", "0", "pragma", "no", "cover", "todo", "return", "matchresult", "from_empty", "none", "here", "we", "enable", "a", "performance", "optimisation", "most", "of", "the", "time", "in", "this", "cycle", "happens", "in", "loops", "looking", "for", "simple", "matchers", "which", "we", "should", "be", "able", "to", "find", "a", "shortcut", "for", "first", "assess", "the", "matchers", "passed", "in", "if", "any", "are", "simple", "then", "we", "effectively", "use", "a", "hash", "lookup", "across", "the", "content", "of", "segments", "to", "quickly", "evaluate", "if", "the", "segment", "is", "present", "matchers", "which", "aren", "t", "simple", "still", "take", "a", "slower", "route", "_matchers", "matcher", "matcher", "simple", "parse_context", "parse_context", "for", "matcher", "in", "matchers", "simple_matchers", "matcher", "for", "matcher", "in", "_matchers", "if", "matcher", "1", "non_simple_matchers", "matcher", "0", "for", "matcher", "in", "_matchers", "if", "not", "matcher", "1", "best_simple_match", "none", "if", "simple_matchers", "if", "they", "re", "all", "simple", "we", "can", "use", "a", "hash", "match", "to", "identify", "the", "first", "one", "build", "a", "buffer", "of", "all", "the", "upper", "case", "raw", "segments", "ahead", "of", "us", "str_buff", "for", "existing", "compound", "segments", "we", "should", "assume", "that", "within", "that", "segment", "things", "are", "internally", "consistent", "that", "means", "rather", "than", "enumerating", "all", "the", "individual", "segments", "of", "a", "longer", "one", "we", "just", "dump", "out", "the", "whole", "segment", "but", "splitting", "off", "the", "first", "element", "seperated", "by", "whitespace", "this", "is", "a", "faster", "and", "also", "b", "prevents", "some", "really", "horrible", "bugs", "with", "bracket", "matching", "see", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "433", "def", "_trim_elem", "seg", "s", "seg", "raw_upper", "split", "maxsplit", "1", "return", "s", "0", "if", "s", "else", "str_buff", "_trim_elem", "seg", "for", "seg", "in", "segments", "match_queue", "for", "matcher", "simple", "in", "simple_matchers", "simple", "will", "be", "a", "tuple", "of", "options", "for", "simple_option", "in", "simple", "note", "we", "use", "iter_indices", "to", "make", "sure", "we", "capture", "all", "instances", "of", "potential", "matches", "if", "there", "are", "many", "this", "is", "important", "for", "bracket", "counting", "for", "buff_pos", "in", "iter_indices", "str_buff", "simple_option", "match_queue", "append", "matcher", "buff_pos", "simple_option", "sort", "the", "match", "queue", "first", "to", "process", "at", "the", "end", "that", "means", "we", "pop", "from", "the", "end", "match_queue", "sorted", "match_queue", "key", "lambda", "x", "x", "1", "parse_match_logging", "cls", "__name__", "_look_ahead_match", "si", "parse_context", "parse_context", "v_level", "4", "mq", "match_queue", "sb", "str_buff", "while", "match_queue", "we", "ve", "managed", "to", "match", "we", "can", "shortcut", "home", "nb", "we", "may", "still", "need", "to", "deal", "with", "whitespace", "queued_matcher", "queued_buff_pos", "queued_option", "match_queue", "pop", "here", "we", "do", "the", "actual", "transform", "to", "the", "new", "segment", "match", "queued_matcher", "match", "segments", "queued_buff_pos", "parse_context", "if", "not", "match", "we", "ve", "had", "something", "match", "in", "simple", "matching", "but", "then", "later", "excluded", "log", "but", "then", "move", "on", "to", "the", "next", "item", "on", "the", "list", "parse_match_logging", "cls", "__name__", "_look_ahead_match", "nm", "parse_context", "parse_context", "v_level", "4", "_so", "queued_option", "continue", "ok", "we", "have", "a", "match", "because", "we", "sorted", "the", "list", "we", "ll", "take", "it", "best_simple_match", "segments", "queued_buff_pos", "match", "queued_matcher", "if", "not", "non_simple_matchers", "there", "are", "no", "other", "matchers", "we", "can", "just", "shortcut", "now", "parse_match_logging", "cls", "__name__", "_look_ahead_match", "sc", "parse_context", "parse_context", "v_level", "4", "bsm", "none", "if", "not", "best_simple_match", "else", "len", "best_simple_match", "0", "len", "best_simple_match", "1", "best_simple_match", "2", "if", "best_simple_match", "return", "best_simple_match", "else", "return", "matchresult", "from_unmatched", "segments", "none", "make", "some", "buffers", "seg_buff", "segments", "pre_seg_buff", "nb", "tuple", "loop", "while", "true", "do", "we", "have", "anything", "left", "to", "match", "on", "if", "seg_buff", "great", "carry", "on", "pass", "else", "we", "ve", "got", "to", "the", "end", "without", "a", "match", "return", "empty", "return", "matchresult", "from_unmatched", "segments", "none", "we", "only", "check", "the", "non", "simple", "ones", "here", "for", "brevity", "mat", "m", "cls", "_longest_trimmed_match", "seg_buff", "non_simple_matchers", "parse_context", "parse_context", "trim_noncode", "false", "if", "mat", "and", "not", "best_simple_match", "return", "pre_seg_buff", "mat", "m", "elif", "mat", "it", "will", "be", "earlier", "than", "the", "simple", "one", "if", "we", "ve", "even", "checked", "but", "there", "s", "a", "chance", "that", "this", "might", "be", "longer", "or", "just", "first", "pre_lengths", "len", "pre_seg_buff", "len", "best_simple_match", "0", "mat_lengths", "len", "mat", "len", "best_simple_match", "1", "mat_indexes", "matchers", "index", "m", "matchers", "index", "best_simple_match", "2", "if", "pre_lengths", "0", "pre_lengths", "1", "or", "pre_lengths", "0", "pre_lengths", "1", "and", "mat_lengths", "0", "mat_lengths", "1", "or", "pre_lengths", "0", "pre_lengths", "1", "and", "mat_lengths", "0", "mat_lengths", "1", "and", "mat_indexes", "0", "mat_indexes", "1", "return", "pre_seg_buff", "mat", "m", "else", "return", "best_simple_match", "else", "if", "there", "aren", "t", "any", "matches", "then", "advance", "the", "buffer", "and", "try", "again", "two", "improvements", "1", "if", "we", "get", "as", "far", "as", "the", "first", "simple", "match", "then", "return", "that", "2", "be", "eager", "in", "consuming", "non", "code", "segments", "if", "allowed", "if", "best_simple_match", "and", "len", "pre_seg_buff", "len", "best_simple_match", "0", "return", "best_simple_match", "pre_seg_buff", "seg_buff", "0", "seg_buff", "seg_buff", "1"], "doc_len": 808}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar._bracket_sensitive_look_ahead_match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "_bracket_sensitive_look_ahead_match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def _bracket_sensitive_look_ahead_match(\n        cls,\n        segments,\n        matchers,\n        parse_context,\n        start_bracket=None,\n        end_bracket=None,\n        bracket_pairs_set=\"bracket_pairs\",\n    ) -> Tuple[Tuple[BaseSegment, ...], MatchResult, Optional[MatchableType]]:\n        \"\"\"Same as `_look_ahead_match` but with bracket counting.\n\n        NB: Given we depend on `_look_ahead_match` we can also utilise\n        the same performance optimisations which are implemented there.\n\n        bracket_pairs_set: Allows specific segments to override the available\n            bracket pairs. See the definition of \"angle_bracket_pairs\" in the\n            BigQuery dialect for additional context on why this exists.\n\n        Returns:\n            `tuple` of (unmatched_segments, match_object, matcher).\n\n        \"\"\"\n        # Type munging\n        matchers = list(matchers)\n        if isinstance(segments, BaseSegment):  # pragma: no cover TODO?\n            segments = [segments]\n\n        # Have we been passed an empty list?\n        if len(segments) == 0:\n            return ((), MatchResult.from_unmatched(segments), None)\n\n        # Get hold of the bracket matchers from the dialect, and append them\n        # to the list of matchers. We get them from the relevant set on the\n        # dialect. We use zip twice to \"unzip\" them. We ignore the first\n        # argument because that's just the name.\n        _, start_bracket_refs, end_bracket_refs, persists = zip(\n            *parse_context.dialect.sets(bracket_pairs_set)\n        )\n        # These are matchables, probably StringParsers.\n        start_brackets = [\n            parse_context.dialect.ref(seg_ref) for seg_ref in start_bracket_refs\n        ]\n        end_brackets = [\n            parse_context.dialect.ref(seg_ref) for seg_ref in end_bracket_refs\n        ]\n        # Add any bracket-like things passed as arguments\n        if start_bracket:\n            start_brackets += [start_bracket]\n        if end_bracket:\n            end_brackets += [end_bracket]\n        bracket_matchers = start_brackets + end_brackets\n\n        # Make some buffers\n        seg_buff: Tuple[BaseSegment, ...] = segments\n        pre_seg_buff: Tuple[BaseSegment, ...] = ()\n        bracket_stack: List[BracketInfo] = []\n\n        # Iterate\n        while True:\n            # Do we have anything left to match on?\n            if seg_buff:\n                # Yes we have buffer left to work with.\n                # Are we already in a bracket stack?\n                if bracket_stack:\n                    # Yes, we're just looking for the closing bracket, or\n                    # another opening bracket.\n                    pre, match, matcher = cls._look_ahead_match(\n                        seg_buff,\n                        bracket_matchers,\n                        parse_context=parse_context,\n                    )\n\n                    if match:\n                        # NB: We can only consider this as a nested bracket if the start\n                        # and end tokens are not the same. If a matcher is both a start and\n                        # end token we cannot deepen the bracket stack. In general, quoted\n                        # strings are a typical example where the start and end tokens are\n                        # the same. Currently, though, quoted strings are handled elsewhere\n                        # in the parser, and there are no cases where *this* code has to\n                        # handle identical start and end brackets. For now, consider this\n                        # a small, speculative investment in a possible future requirement.\n                        if matcher in start_brackets and matcher not in end_brackets:\n                            # Add any segments leading up to this to the previous bracket.\n                            bracket_stack[-1].segments += pre\n                            # Add a bracket to the stack and add the matches from the segment.\n                            bracket_stack.append(\n                                BracketInfo(\n                                    bracket=match.matched_segments[0],\n                                    segments=match.matched_segments,\n                                )\n                            )\n                            seg_buff = match.unmatched_segments\n                            continue\n                        elif matcher in end_brackets:\n                            # Found an end bracket. Does its type match that of\n                            # the innermost start bracket? E.g. \")\" matches \"(\",\n                            # \"]\" matches \"[\".\n                            # For the start bracket we don't have the matcher\n                            # but we can work out the name, so we use that for\n                            # the lookup.\n                            start_index = [\n                                bracket.name for bracket in start_brackets\n                            ].index(bracket_stack[-1].bracket.name)\n                            # For the end index, we can just look for the matcher\n                            end_index = end_brackets.index(matcher)\n                            bracket_types_match = start_index == end_index\n                            if bracket_types_match:\n                                # Yes, the types match. So we've found a\n                                # matching end bracket. Pop the stack, construct\n                                # a bracketed segment and carry\n                                # on.\n\n                                # Complete the bracketed info\n                                bracket_stack[-1].segments += (\n                                    pre + match.matched_segments\n                                )\n                                # Construct a bracketed segment (as a tuple) if allowed.\n                                persist_bracket = persists[end_brackets.index(matcher)]\n                                if persist_bracket:\n                                    new_segments: Tuple[BaseSegment, ...] = (\n                                        bracket_stack[-1].to_segment(\n                                            end_bracket=match.matched_segments\n                                        ),\n                                    )\n                                else:\n                                    new_segments = bracket_stack[-1].segments\n                                # Remove the bracket set from the stack\n                                bracket_stack.pop()\n                                # If we're still in a bracket, add the new segments to that bracket\n                                # Otherwise add them to the buffer\n                                if bracket_stack:\n                                    bracket_stack[-1].segments += new_segments\n                                else:\n                                    pre_seg_buff += new_segments\n                                seg_buff = match.unmatched_segments\n                                continue\n                            else:\n                                # The types don't match. Error.\n                                raise SQLParseError(\n                                    f\"Found unexpected end bracket!, \"\n                                    f\"was expecting \"\n                                    f\"{end_brackets[start_index]}, \"\n                                    f\"but got {matcher}\",\n                                    segment=match.matched_segments[0],\n                                )\n\n                        else:  # pragma: no cover\n                            raise RuntimeError(\"I don't know how we get here?!\")\n                    else:  # pragma: no cover\n                        # No match, we're in a bracket stack. Error.\n                        raise SQLParseError(\n                            \"Couldn't find closing bracket for opening bracket.\",\n                            segment=bracket_stack[-1].bracket,\n                        )\n                else:\n                    # No, we're open to more opening brackets or the thing(s)\n                    # that we're otherwise looking for.\n                    pre, match, matcher = cls._look_ahead_match(\n                        seg_buff,\n                        matchers + bracket_matchers,\n                        parse_context=parse_context,\n                    )\n\n                    if match:\n                        if matcher in matchers:\n                            # It's one of the things we were looking for!\n                            # Return.\n                            return (pre_seg_buff + pre, match, matcher)\n                        elif matcher in start_brackets:\n                            # We've found the start of a bracket segment.\n                            # NB: It might not *actually* be the bracket itself,\n                            # but could be some non-code element preceding it.\n                            # That's actually ok.\n\n                            # Add the bracket to the stack.\n                            bracket_stack.append(\n                                BracketInfo(\n                                    bracket=match.matched_segments[0],\n                                    segments=match.matched_segments,\n                                )\n                            )\n                            # The matched element has already been added to the bracket.\n                            # Add anything before it to the pre segment buffer.\n                            # Reset the working buffer.\n                            pre_seg_buff += pre\n                            seg_buff = match.unmatched_segments\n                            continue\n                        elif matcher in end_brackets:\n                            # We've found an unexpected end bracket! This is likely\n                            # because we're matching a section which should have ended.\n                            # If we had a match, it would have matched by now, so this\n                            # means no match.\n                            parse_match_logging(\n                                cls.__name__,\n                                \"_bracket_sensitive_look_ahead_match\",\n                                \"UEXB\",\n                                parse_context=parse_context,\n                                v_level=3,\n                                got=matcher,\n                            )\n                            # From here we'll drop out to the happy unmatched exit.\n                        else:  # pragma: no cover\n                            # This shouldn't happen!?\n                            raise NotImplementedError(\n                                \"This shouldn't happen. Panic in _bracket_sensitive_look_ahead_match.\"\n                            )\n                    # Not in a bracket stack, but no match.\n                    # From here we'll drop out to the happy unmatched exit.\n            else:\n                # No we're at the end:\n                # Now check have we closed all our brackets?\n                if bracket_stack:  # pragma: no cover\n                    # No we haven't.\n                    raise SQLParseError(\n                        f\"Couldn't find closing bracket for opened brackets: `{bracket_stack}`.\",\n                        segment=bracket_stack[-1].bracket,\n                    )\n\n            # This is the happy unmatched path. This occurs when:\n            # - We reached the end with no open brackets.\n            # - No match while outside a bracket stack.\n            # - We found an unexpected end bracket before matching something interesting.\n            # We return with the mutated segments so we can\n            # reuse any bracket matching.\n            return ((), MatchResult.from_unmatched(pre_seg_buff + seg_buff), None)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "_bracket_sensitive_look_ahead_match", "cls", "segments", "matchers", "parse_context", "start_bracket", "none", "end_bracket", "none", "bracket_pairs_set", "bracket_pairs", "tuple", "tuple", "basesegment", "matchresult", "optional", "matchabletype", "same", "as", "_look_ahead_match", "but", "with", "bracket", "counting", "nb", "given", "we", "depend", "on", "_look_ahead_match", "we", "can", "also", "utilise", "the", "same", "performance", "optimisations", "which", "are", "implemented", "there", "bracket_pairs_set", "allows", "specific", "segments", "to", "override", "the", "available", "bracket", "pairs", "see", "the", "definition", "of", "angle_bracket_pairs", "in", "the", "bigquery", "dialect", "for", "additional", "context", "on", "why", "this", "exists", "returns", "tuple", "of", "unmatched_segments", "match_object", "matcher", "type", "munging", "matchers", "list", "matchers", "if", "isinstance", "segments", "basesegment", "pragma", "no", "cover", "todo", "segments", "segments", "have", "we", "been", "passed", "an", "empty", "list", "if", "len", "segments", "0", "return", "matchresult", "from_unmatched", "segments", "none", "get", "hold", "of", "the", "bracket", "matchers", "from", "the", "dialect", "and", "append", "them", "to", "the", "list", "of", "matchers", "we", "get", "them", "from", "the", "relevant", "set", "on", "the", "dialect", "we", "use", "zip", "twice", "to", "unzip", "them", "we", "ignore", "the", "first", "argument", "because", "that", "s", "just", "the", "name", "_", "start_bracket_refs", "end_bracket_refs", "persists", "zip", "parse_context", "dialect", "sets", "bracket_pairs_set", "these", "are", "matchables", "probably", "stringparsers", "start_brackets", "parse_context", "dialect", "ref", "seg_ref", "for", "seg_ref", "in", "start_bracket_refs", "end_brackets", "parse_context", "dialect", "ref", "seg_ref", "for", "seg_ref", "in", "end_bracket_refs", "add", "any", "bracket", "like", "things", "passed", "as", "arguments", "if", "start_bracket", "start_brackets", "start_bracket", "if", "end_bracket", "end_brackets", "end_bracket", "bracket_matchers", "start_brackets", "end_brackets", "make", "some", "buffers", "seg_buff", "tuple", "basesegment", "segments", "pre_seg_buff", "tuple", "basesegment", "bracket_stack", "list", "bracketinfo", "iterate", "while", "true", "do", "we", "have", "anything", "left", "to", "match", "on", "if", "seg_buff", "yes", "we", "have", "buffer", "left", "to", "work", "with", "are", "we", "already", "in", "a", "bracket", "stack", "if", "bracket_stack", "yes", "we", "re", "just", "looking", "for", "the", "closing", "bracket", "or", "another", "opening", "bracket", "pre", "match", "matcher", "cls", "_look_ahead_match", "seg_buff", "bracket_matchers", "parse_context", "parse_context", "if", "match", "nb", "we", "can", "only", "consider", "this", "as", "a", "nested", "bracket", "if", "the", "start", "and", "end", "tokens", "are", "not", "the", "same", "if", "a", "matcher", "is", "both", "a", "start", "and", "end", "token", "we", "cannot", "deepen", "the", "bracket", "stack", "in", "general", "quoted", "strings", "are", "a", "typical", "example", "where", "the", "start", "and", "end", "tokens", "are", "the", "same", "currently", "though", "quoted", "strings", "are", "handled", "elsewhere", "in", "the", "parser", "and", "there", "are", "no", "cases", "where", "this", "code", "has", "to", "handle", "identical", "start", "and", "end", "brackets", "for", "now", "consider", "this", "a", "small", "speculative", "investment", "in", "a", "possible", "future", "requirement", "if", "matcher", "in", "start_brackets", "and", "matcher", "not", "in", "end_brackets", "add", "any", "segments", "leading", "up", "to", "this", "to", "the", "previous", "bracket", "bracket_stack", "1", "segments", "pre", "add", "a", "bracket", "to", "the", "stack", "and", "add", "the", "matches", "from", "the", "segment", "bracket_stack", "append", "bracketinfo", "bracket", "match", "matched_segments", "0", "segments", "match", "matched_segments", "seg_buff", "match", "unmatched_segments", "continue", "elif", "matcher", "in", "end_brackets", "found", "an", "end", "bracket", "does", "its", "type", "match", "that", "of", "the", "innermost", "start", "bracket", "e", "g", "matches", "matches", "for", "the", "start", "bracket", "we", "don", "t", "have", "the", "matcher", "but", "we", "can", "work", "out", "the", "name", "so", "we", "use", "that", "for", "the", "lookup", "start_index", "bracket", "name", "for", "bracket", "in", "start_brackets", "index", "bracket_stack", "1", "bracket", "name", "for", "the", "end", "index", "we", "can", "just", "look", "for", "the", "matcher", "end_index", "end_brackets", "index", "matcher", "bracket_types_match", "start_index", "end_index", "if", "bracket_types_match", "yes", "the", "types", "match", "so", "we", "ve", "found", "a", "matching", "end", "bracket", "pop", "the", "stack", "construct", "a", "bracketed", "segment", "and", "carry", "on", "complete", "the", "bracketed", "info", "bracket_stack", "1", "segments", "pre", "match", "matched_segments", "construct", "a", "bracketed", "segment", "as", "a", "tuple", "if", "allowed", "persist_bracket", "persists", "end_brackets", "index", "matcher", "if", "persist_bracket", "new_segments", "tuple", "basesegment", "bracket_stack", "1", "to_segment", "end_bracket", "match", "matched_segments", "else", "new_segments", "bracket_stack", "1", "segments", "remove", "the", "bracket", "set", "from", "the", "stack", "bracket_stack", "pop", "if", "we", "re", "still", "in", "a", "bracket", "add", "the", "new", "segments", "to", "that", "bracket", "otherwise", "add", "them", "to", "the", "buffer", "if", "bracket_stack", "bracket_stack", "1", "segments", "new_segments", "else", "pre_seg_buff", "new_segments", "seg_buff", "match", "unmatched_segments", "continue", "else", "the", "types", "don", "t", "match", "error", "raise", "sqlparseerror", "f", "found", "unexpected", "end", "bracket", "f", "was", "expecting", "f", "end_brackets", "start_index", "f", "but", "got", "matcher", "segment", "match", "matched_segments", "0", "else", "pragma", "no", "cover", "raise", "runtimeerror", "i", "don", "t", "know", "how", "we", "get", "here", "else", "pragma", "no", "cover", "no", "match", "we", "re", "in", "a", "bracket", "stack", "error", "raise", "sqlparseerror", "couldn", "t", "find", "closing", "bracket", "for", "opening", "bracket", "segment", "bracket_stack", "1", "bracket", "else", "no", "we", "re", "open", "to", "more", "opening", "brackets", "or", "the", "thing", "s", "that", "we", "re", "otherwise", "looking", "for", "pre", "match", "matcher", "cls", "_look_ahead_match", "seg_buff", "matchers", "bracket_matchers", "parse_context", "parse_context", "if", "match", "if", "matcher", "in", "matchers", "it", "s", "one", "of", "the", "things", "we", "were", "looking", "for", "return", "return", "pre_seg_buff", "pre", "match", "matcher", "elif", "matcher", "in", "start_brackets", "we", "ve", "found", "the", "start", "of", "a", "bracket", "segment", "nb", "it", "might", "not", "actually", "be", "the", "bracket", "itself", "but", "could", "be", "some", "non", "code", "element", "preceding", "it", "that", "s", "actually", "ok", "add", "the", "bracket", "to", "the", "stack", "bracket_stack", "append", "bracketinfo", "bracket", "match", "matched_segments", "0", "segments", "match", "matched_segments", "the", "matched", "element", "has", "already", "been", "added", "to", "the", "bracket", "add", "anything", "before", "it", "to", "the", "pre", "segment", "buffer", "reset", "the", "working", "buffer", "pre_seg_buff", "pre", "seg_buff", "match", "unmatched_segments", "continue", "elif", "matcher", "in", "end_brackets", "we", "ve", "found", "an", "unexpected", "end", "bracket", "this", "is", "likely", "because", "we", "re", "matching", "a", "section", "which", "should", "have", "ended", "if", "we", "had", "a", "match", "it", "would", "have", "matched", "by", "now", "so", "this", "means", "no", "match", "parse_match_logging", "cls", "__name__", "_bracket_sensitive_look_ahead_match", "uexb", "parse_context", "parse_context", "v_level", "3", "got", "matcher", "from", "here", "we", "ll", "drop", "out", "to", "the", "happy", "unmatched", "exit", "else", "pragma", "no", "cover", "this", "shouldn", "t", "happen", "raise", "notimplementederror", "this", "shouldn", "t", "happen", "panic", "in", "_bracket_sensitive_look_ahead_match", "not", "in", "a", "bracket", "stack", "but", "no", "match", "from", "here", "we", "ll", "drop", "out", "to", "the", "happy", "unmatched", "exit", "else", "no", "we", "re", "at", "the", "end", "now", "check", "have", "we", "closed", "all", "our", "brackets", "if", "bracket_stack", "pragma", "no", "cover", "no", "we", "haven", "t", "raise", "sqlparseerror", "f", "couldn", "t", "find", "closing", "bracket", "for", "opened", "brackets", "bracket_stack", "segment", "bracket_stack", "1", "bracket", "this", "is", "the", "happy", "unmatched", "path", "this", "occurs", "when", "we", "reached", "the", "end", "with", "no", "open", "brackets", "no", "match", "while", "outside", "a", "bracket", "stack", "we", "found", "an", "unexpected", "end", "bracket", "before", "matching", "something", "interesting", "we", "return", "with", "the", "mutated", "segments", "so", "we", "can", "reuse", "any", "bracket", "matching", "return", "matchresult", "from_unmatched", "pre_seg_buff", "seg_buff", "none"], "doc_len": 993}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.__str__", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def __str__(self):  # pragma: no cover TODO?\n        return repr(self)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "__str__", "self", "pragma", "no", "cover", "todo", "return", "repr", "self"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.__repr__", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def __repr__(self):\n        return \"<{}: [{}]>\".format(\n            self.__class__.__name__,\n            curtail_string(\n                \", \".join(curtail_string(repr(elem), 40) for elem in self._elements),\n                100,\n            ),\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "__repr__", "self", "return", "format", "self", "__class__", "__name__", "curtail_string", "join", "curtail_string", "repr", "elem", "40", "for", "elem", "in", "self", "_elements", "100"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.__eq__", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "__eq__", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def __eq__(self, other):\n        \"\"\"Two grammars are equal if their elements and types are equal.\n\n        NOTE: This could potentially mean that two grammars with\n        the same elements but _different configuration_ will be\n        classed as the same. If this matters for your use case,\n        consider extending this function.\n\n        e.g. `OneOf(foo) == OneOf(foo, optional=True)`\n        \"\"\"\n        return type(self) is type(other) and self._elements == other._elements\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "__eq__", "self", "other", "two", "grammars", "are", "equal", "if", "their", "elements", "and", "types", "are", "equal", "note", "this", "could", "potentially", "mean", "that", "two", "grammars", "with", "the", "same", "elements", "but", "_different", "configuration_", "will", "be", "classed", "as", "the", "same", "if", "this", "matters", "for", "your", "use", "case", "consider", "extending", "this", "function", "e", "g", "oneof", "foo", "oneof", "foo", "optional", "true", "return", "type", "self", "is", "type", "other", "and", "self", "_elements", "other", "_elements"], "doc_len": 74}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::BaseGrammar.copy", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "BaseGrammar", "func_name": "copy", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: BaseGrammar\n    def copy(\n        self,\n        insert: Optional[list] = None,\n        at: Optional[int] = None,\n        before: Optional[Any] = None,\n        remove: Optional[list] = None,\n        **kwargs,\n    ):\n        \"\"\"Create a copy of this grammar, optionally with differences.\n\n        This is mainly used in dialect inheritance.\n\n\n        Args:\n            insert (:obj:`list`, optional): Matchable elements to\n                insert. This is inserted pre-expansion so can include\n                unexpanded elements as normal.\n            at (:obj:`int`, optional): The position in the elements\n                to insert the item. Defaults to `None` which means\n                insert at the end of the elements.\n            before (optional): An alternative to _at_ to determine the\n                position of an insertion. Using this inserts the elements\n                immediately before the position of this element.\n                Note that this is not an _index_ but an element to look\n                for (i.e. a Segment or Grammar which will be compared\n                with other elements for equality).\n            remove (:obj:`list`, optional): A list of individual\n                elements to remove from a grammar. Removal is done\n                *after* insertion so that order is preserved.\n                Elements are searched for individually.\n\n        \"\"\"\n        # Copy only the *grammar* elements. The rest comes through\n        # as is because they should just be classes rather than\n        # instances.\n        new_elems = [\n            elem.copy() if isinstance(elem, BaseGrammar) else elem\n            for elem in self._elements\n        ]\n        if insert:\n            if at is not None and before is not None:  # pragma: no cover\n                raise ValueError(\n                    \"Cannot specify `at` and `before` in BaseGrammar.copy().\"\n                )\n            if before is not None:\n                try:\n                    idx = new_elems.index(before)\n                except ValueError:  # pragma: no cover\n                    raise ValueError(\n                        \"Could not insert {} in copy of {}. {} not Found.\".format(\n                            insert, self, before\n                        )\n                    )\n                new_elems = new_elems[:idx] + insert + new_elems[idx:]\n            elif at is None:\n                new_elems = new_elems + insert\n            else:\n                new_elems = new_elems[:at] + insert + new_elems[at:]\n        if remove:\n            for elem in remove:\n                try:\n                    new_elems.remove(elem)\n                except ValueError:  # pragma: no cover\n                    raise ValueError(\n                        \"Could not remove {} from copy of {}. Not Found.\".format(\n                            elem, self\n                        )\n                    )\n        new_seg = copy.copy(self)\n        new_seg._elements = new_elems\n        return new_seg\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "basegrammar", "def", "copy", "self", "insert", "optional", "list", "none", "at", "optional", "int", "none", "before", "optional", "any", "none", "remove", "optional", "list", "none", "kwargs", "create", "a", "copy", "of", "this", "grammar", "optionally", "with", "differences", "this", "is", "mainly", "used", "in", "dialect", "inheritance", "args", "insert", "obj", "list", "optional", "matchable", "elements", "to", "insert", "this", "is", "inserted", "pre", "expansion", "so", "can", "include", "unexpanded", "elements", "as", "normal", "at", "obj", "int", "optional", "the", "position", "in", "the", "elements", "to", "insert", "the", "item", "defaults", "to", "none", "which", "means", "insert", "at", "the", "end", "of", "the", "elements", "before", "optional", "an", "alternative", "to", "_at_", "to", "determine", "the", "position", "of", "an", "insertion", "using", "this", "inserts", "the", "elements", "immediately", "before", "the", "position", "of", "this", "element", "note", "that", "this", "is", "not", "an", "_index_", "but", "an", "element", "to", "look", "for", "i", "e", "a", "segment", "or", "grammar", "which", "will", "be", "compared", "with", "other", "elements", "for", "equality", "remove", "obj", "list", "optional", "a", "list", "of", "individual", "elements", "to", "remove", "from", "a", "grammar", "removal", "is", "done", "after", "insertion", "so", "that", "order", "is", "preserved", "elements", "are", "searched", "for", "individually", "copy", "only", "the", "grammar", "elements", "the", "rest", "comes", "through", "as", "is", "because", "they", "should", "just", "be", "classes", "rather", "than", "instances", "new_elems", "elem", "copy", "if", "isinstance", "elem", "basegrammar", "else", "elem", "for", "elem", "in", "self", "_elements", "if", "insert", "if", "at", "is", "not", "none", "and", "before", "is", "not", "none", "pragma", "no", "cover", "raise", "valueerror", "cannot", "specify", "at", "and", "before", "in", "basegrammar", "copy", "if", "before", "is", "not", "none", "try", "idx", "new_elems", "index", "before", "except", "valueerror", "pragma", "no", "cover", "raise", "valueerror", "could", "not", "insert", "in", "copy", "of", "not", "found", "format", "insert", "self", "before", "new_elems", "new_elems", "idx", "insert", "new_elems", "idx", "elif", "at", "is", "none", "new_elems", "new_elems", "insert", "else", "new_elems", "new_elems", "at", "insert", "new_elems", "at", "if", "remove", "for", "elem", "in", "remove", "try", "new_elems", "remove", "elem", "except", "valueerror", "pragma", "no", "cover", "raise", "valueerror", "could", "not", "remove", "from", "copy", "of", "not", "found", "format", "elem", "self", "new_seg", "copy", "copy", "self", "new_seg", "_elements", "new_elems", "return", "new_seg"], "doc_len": 317}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref.simple", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        A ref is simple, if the thing it references is simple.\n        \"\"\"\n        return self._get_elem(dialect=parse_context.dialect).simple(\n            parse_context=parse_context\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "a", "ref", "is", "simple", "if", "the", "thing", "it", "references", "is", "simple", "return", "self", "_get_elem", "dialect", "parse_context", "dialect", "simple", "parse_context", "parse_context"], "doc_len": 45}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref._get_ref", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "_get_ref", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def _get_ref(self):\n        \"\"\"Get the name of the thing we're referencing.\"\"\"\n        # Unusually for a grammar we expect _elements to be a list of strings.\n        # Notable ONE string for now.\n        if len(self._elements) == 1:\n            # We're good on length. Get the name of the reference\n            return self._elements[0]\n        else:  # pragma: no cover\n            raise ValueError(\n                \"Ref grammar can only deal with precisely one element for now. Instead found {!r}\".format(\n                    self._elements\n                )\n            )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "_get_ref", "self", "get", "the", "name", "of", "the", "thing", "we", "re", "referencing", "unusually", "for", "a", "grammar", "we", "expect", "_elements", "to", "be", "a", "list", "of", "strings", "notable", "one", "string", "for", "now", "if", "len", "self", "_elements", "1", "we", "re", "good", "on", "length", "get", "the", "name", "of", "the", "reference", "return", "self", "_elements", "0", "else", "pragma", "no", "cover", "raise", "valueerror", "ref", "grammar", "can", "only", "deal", "with", "precisely", "one", "element", "for", "now", "instead", "found", "r", "format", "self", "_elements"], "doc_len": 81}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref._get_elem", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "_get_elem", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def _get_elem(self, dialect):\n        \"\"\"Get the actual object we're referencing.\"\"\"\n        if dialect:\n            # Use the dialect to retrieve the grammar it refers to.\n            return dialect.ref(self._get_ref())\n        else:  # pragma: no cover\n            raise ReferenceError(\"No Dialect has been provided to Ref grammar!\")\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "_get_elem", "self", "dialect", "get", "the", "actual", "object", "we", "re", "referencing", "if", "dialect", "use", "the", "dialect", "to", "retrieve", "the", "grammar", "it", "refers", "to", "return", "dialect", "ref", "self", "_get_ref", "else", "pragma", "no", "cover", "raise", "referenceerror", "no", "dialect", "has", "been", "provided", "to", "ref", "grammar"], "doc_len": 50}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref.__repr__", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def __repr__(self):\n        return \"<Ref: {}{}>\".format(\n            \", \".join(self._elements), \" [opt]\" if self.is_optional() else \"\"\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "__repr__", "self", "return", "ref", "format", "join", "self", "_elements", "opt", "if", "self", "is_optional", "else"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref.match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def match(self, segments, parse_context):\n        \"\"\"Match a list of segments against this segment.\n\n        Matching can be done from either the raw or the segments.\n        This raw function can be overridden, or a grammar defined\n        on the underlying class.\n\n        The match element of Ref, also implements the caching\n        using the parse_context `blacklist` methods.\n        \"\"\"\n        elem = self._get_elem(dialect=parse_context.dialect)\n\n        if not elem:  # pragma: no cover\n            raise ValueError(f\"Null Element returned! _elements: {self._elements!r}\")\n\n        # First check against the efficiency Cache.\n        # We rely on segments not being mutated within a given\n        # match cycle and so the ids should continue to refer to unchanged\n        # objects.\n        seg_tuple = (id(seg) for seg in segments)\n        self_name = self._get_ref()\n        if parse_context.blacklist.check(\n            self_name, seg_tuple\n        ):  # pragma: no cover TODO?\n            # This has been tried before.\n            parse_match_logging(\n                self.__class__.__name__,\n                \"match\",\n                \"SKIP\",\n                parse_context=parse_context,\n                v_level=3,\n                self_name=self_name,\n            )\n            return MatchResult.from_unmatched(segments)\n\n        # Match against that. NB We're not incrementing the match_depth here.\n        # References shouldn't really count as a depth of match.\n        with parse_context.matching_segment(self._get_ref()) as ctx:\n            resp = elem.match(segments=segments, parse_context=ctx)\n        if not resp:\n            parse_context.blacklist.mark(self_name, seg_tuple)\n        return resp\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "match", "self", "segments", "parse_context", "match", "a", "list", "of", "segments", "against", "this", "segment", "matching", "can", "be", "done", "from", "either", "the", "raw", "or", "the", "segments", "this", "raw", "function", "can", "be", "overridden", "or", "a", "grammar", "defined", "on", "the", "underlying", "class", "the", "match", "element", "of", "ref", "also", "implements", "the", "caching", "using", "the", "parse_context", "blacklist", "methods", "elem", "self", "_get_elem", "dialect", "parse_context", "dialect", "if", "not", "elem", "pragma", "no", "cover", "raise", "valueerror", "f", "null", "element", "returned", "_elements", "self", "_elements", "r", "first", "check", "against", "the", "efficiency", "cache", "we", "rely", "on", "segments", "not", "being", "mutated", "within", "a", "given", "match", "cycle", "and", "so", "the", "ids", "should", "continue", "to", "refer", "to", "unchanged", "objects", "seg_tuple", "id", "seg", "for", "seg", "in", "segments", "self_name", "self", "_get_ref", "if", "parse_context", "blacklist", "check", "self_name", "seg_tuple", "pragma", "no", "cover", "todo", "this", "has", "been", "tried", "before", "parse_match_logging", "self", "__class__", "__name__", "match", "skip", "parse_context", "parse_context", "v_level", "3", "self_name", "self_name", "return", "matchresult", "from_unmatched", "segments", "match", "against", "that", "nb", "we", "re", "not", "incrementing", "the", "match_depth", "here", "references", "shouldn", "t", "really", "count", "as", "a", "depth", "of", "match", "with", "parse_context", "matching_segment", "self", "_get_ref", "as", "ctx", "resp", "elem", "match", "segments", "segments", "parse_context", "ctx", "if", "not", "resp", "parse_context", "blacklist", "mark", "self_name", "seg_tuple", "return", "resp"], "doc_len": 197}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Ref.keyword", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Ref", "func_name": "keyword", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Ref\n    def keyword(cls, keyword, **kwargs):\n        \"\"\"Generate a reference to a keyword by name.\n\n        This function is entirely syntactic sugar, and designed\n        for more readable dialects.\n\n        Ref.keyword('select') == Ref('SelectKeywordSegment')\n\n        \"\"\"\n        name = keyword.capitalize() + \"KeywordSegment\"\n        return cls(name, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "ref", "def", "keyword", "cls", "keyword", "kwargs", "generate", "a", "reference", "to", "a", "keyword", "by", "name", "this", "function", "is", "entirely", "syntactic", "sugar", "and", "designed", "for", "more", "readable", "dialects", "ref", "keyword", "select", "ref", "selectkeywordsegment", "name", "keyword", "capitalize", "keywordsegment", "return", "cls", "name", "kwargs"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Anything.match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Anything", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Anything\n    def match(self, segments, parse_context):\n        \"\"\"Matches... Anything.\n\n        Most useful in match grammars, where a later parse grammar\n        will work out what's inside.\n        \"\"\"\n        return MatchResult.from_matched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "anything", "def", "match", "self", "segments", "parse_context", "matches", "anything", "most", "useful", "in", "match", "grammars", "where", "a", "later", "parse", "grammar", "will", "work", "out", "what", "s", "inside", "return", "matchresult", "from_matched", "segments"], "doc_len": 35}
{"doc_id": "src/sqlfluff/core/parser/grammar/base.py::Nothing.match", "file_path": "src/sqlfluff/core/parser/grammar/base.py", "class_name": "Nothing", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/base.py, 类名: Nothing\n    def match(self, segments, parse_context):\n        \"\"\"Matches... nothing.\n\n        Useful for placeholders which might be overwritten by other\n        dialects.\n        \"\"\"\n        return MatchResult.from_unmatched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "base", "py", "nothing", "def", "match", "self", "segments", "parse_context", "matches", "nothing", "useful", "for", "placeholders", "which", "might", "be", "overwritten", "by", "other", "dialects", "return", "matchresult", "from_unmatched", "segments"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/grammar/conditional.py::Conditional.__init__", "file_path": "src/sqlfluff/core/parser/grammar/conditional.py", "class_name": "Conditional", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/conditional.py, 类名: Conditional\n    def __init__(self, *args, config_type: str = \"indentation\", **rules):\n        if not all(issubclass(arg, Indent) for arg in args):  # pragma: no cover\n            raise ValueError(\n                \"Conditional is only designed to work with Indent segments.\"\n            )\n        if len(args) != 1:  # pragma: no cover\n            raise ValueError(\n                \"Conditional is only designed to work with a single element.\"\n            )\n        if not config_type:  # pragma: no cover\n            raise ValueError(\"Conditional config_type must be set.\")\n        elif config_type not in (\"indentation\"):  # pragma: no cover\n            raise ValueError(\n                \"Only 'indentation' is supported as a Conditional config_type.\"\n            )\n        if not rules:  # pragma: no cover\n            raise ValueError(\"Conditional requires rules to be set.\")\n        self._config_type = config_type\n        self._config_rules = rules\n        super().__init__(*args)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "conditional", "py", "conditional", "def", "__init__", "self", "args", "config_type", "str", "indentation", "rules", "if", "not", "all", "issubclass", "arg", "indent", "for", "arg", "in", "args", "pragma", "no", "cover", "raise", "valueerror", "conditional", "is", "only", "designed", "to", "work", "with", "indent", "segments", "if", "len", "args", "1", "pragma", "no", "cover", "raise", "valueerror", "conditional", "is", "only", "designed", "to", "work", "with", "a", "single", "element", "if", "not", "config_type", "pragma", "no", "cover", "raise", "valueerror", "conditional", "config_type", "must", "be", "set", "elif", "config_type", "not", "in", "indentation", "pragma", "no", "cover", "raise", "valueerror", "only", "indentation", "is", "supported", "as", "a", "conditional", "config_type", "if", "not", "rules", "pragma", "no", "cover", "raise", "valueerror", "conditional", "requires", "rules", "to", "be", "set", "self", "_config_type", "config_type", "self", "_config_rules", "rules", "super", "__init__", "args"], "doc_len": 113}
{"doc_id": "src/sqlfluff/core/parser/grammar/conditional.py::Conditional.is_enabled", "file_path": "src/sqlfluff/core/parser/grammar/conditional.py", "class_name": "Conditional", "func_name": "is_enabled", "text": "文件路径: src/sqlfluff/core/parser/grammar/conditional.py, 类名: Conditional\n    def is_enabled(self, parse_context):\n        \"\"\"Evaluate conditionals and return whether enabled.\"\"\"\n        # NOTE: Because only \"indentation\" is the only current config_type\n        # supported, this code is much simpler that would be required in\n        # future if multiple options are available.\n        if self._config_type != \"indentation\":  # pragma: no cover\n            raise ValueError(\n                \"Only 'indentation' is supported as a Conditional config_type.\"\n            )\n        config_section = parse_context.indentation_config\n        # If any rules fail, return no match.\n        for rule, val in self._config_rules.items():\n            # Assume False if not set.\n            conf_val = config_section.get(rule, False)\n            # Coerce to boolean.\n            if val != bool(conf_val):\n                return False\n        return True\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "conditional", "py", "conditional", "def", "is_enabled", "self", "parse_context", "evaluate", "conditionals", "and", "return", "whether", "enabled", "note", "because", "only", "indentation", "is", "the", "only", "current", "config_type", "supported", "this", "code", "is", "much", "simpler", "that", "would", "be", "required", "in", "future", "if", "multiple", "options", "are", "available", "if", "self", "_config_type", "indentation", "pragma", "no", "cover", "raise", "valueerror", "only", "indentation", "is", "supported", "as", "a", "conditional", "config_type", "config_section", "parse_context", "indentation_config", "if", "any", "rules", "fail", "return", "no", "match", "for", "rule", "val", "in", "self", "_config_rules", "items", "assume", "false", "if", "not", "set", "conf_val", "config_section", "get", "rule", "false", "coerce", "to", "boolean", "if", "val", "bool", "conf_val", "return", "false", "return", "true"], "doc_len": 99}
{"doc_id": "src/sqlfluff/core/parser/grammar/conditional.py::Conditional.match", "file_path": "src/sqlfluff/core/parser/grammar/conditional.py", "class_name": "Conditional", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/conditional.py, 类名: Conditional\n    def match(self, segments, parse_context):\n        \"\"\"Evaluate conditionals and return content.\"\"\"\n        if not self.is_enabled(parse_context):  # pragma: no cover TODO?\n            return MatchResult.from_unmatched(segments)\n\n        # Instantiate the new element and return\n        new_seg = self._elements[0]()\n        return MatchResult((new_seg,), segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "conditional", "py", "conditional", "def", "match", "self", "segments", "parse_context", "evaluate", "conditionals", "and", "return", "content", "if", "not", "self", "is_enabled", "parse_context", "pragma", "no", "cover", "todo", "return", "matchresult", "from_unmatched", "segments", "instantiate", "the", "new", "element", "and", "return", "new_seg", "self", "_elements", "0", "return", "matchresult", "new_seg", "segments"], "doc_len": 45}
{"doc_id": "src/sqlfluff/core/parser/grammar/delimited.py::Delimited.__init__", "file_path": "src/sqlfluff/core/parser/grammar/delimited.py", "class_name": "Delimited", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/delimited.py, 类名: Delimited\n    def __init__(\n        self,\n        *args,\n        delimiter=Ref(\"CommaSegment\"),\n        allow_trailing=False,\n        terminator=None,\n        min_delimiters=None,\n        **kwargs,\n    ):\n        if delimiter is None:  # pragma: no cover\n            raise ValueError(\"Delimited grammars require a `delimiter`\")\n        self.bracket_pairs_set = kwargs.pop(\"bracket_pairs_set\", \"bracket_pairs\")\n        self.delimiter = self._resolve_ref(delimiter)\n        self.allow_trailing = allow_trailing\n        self.terminator = self._resolve_ref(terminator)\n        # Setting min delimiters means we have to match at least this number\n        self.min_delimiters = min_delimiters\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "delimited", "py", "delimited", "def", "__init__", "self", "args", "delimiter", "ref", "commasegment", "allow_trailing", "false", "terminator", "none", "min_delimiters", "none", "kwargs", "if", "delimiter", "is", "none", "pragma", "no", "cover", "raise", "valueerror", "delimited", "grammars", "require", "a", "delimiter", "self", "bracket_pairs_set", "kwargs", "pop", "bracket_pairs_set", "bracket_pairs", "self", "delimiter", "self", "_resolve_ref", "delimiter", "self", "allow_trailing", "allow_trailing", "self", "terminator", "self", "_resolve_ref", "terminator", "setting", "min", "delimiters", "means", "we", "have", "to", "match", "at", "least", "this", "number", "self", "min_delimiters", "min_delimiters", "super", "__init__", "args", "kwargs"], "doc_len": 74}
{"doc_id": "src/sqlfluff/core/parser/grammar/delimited.py::Delimited.match", "file_path": "src/sqlfluff/core/parser/grammar/delimited.py", "class_name": "Delimited", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/delimited.py, 类名: Delimited\n    def match(\n        self, segments: Tuple[BaseSegment, ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match an arbitrary number of elements separated by a delimiter.\n\n        Note that if there are multiple elements passed in that they will be treated\n        as different options of what can be delimited, rather than a sequence.\n        \"\"\"\n        # Have we been passed an empty list?\n        if len(segments) == 0:\n            return MatchResult.from_empty()\n\n        # Make some buffers\n        seg_buff = segments\n        matched_segments = MatchResult.from_empty()\n        # delimiters is a list of tuples containing delimiter segments as we find them.\n        delimiters: List[BaseSegment] = []\n\n        # First iterate through all the segments, looking for the delimiter.\n        # Second, split the list on each of the delimiters, and ensure that\n        # each sublist in turn matches one of the elements.\n\n        # In more detail, match against delimiter, if we match, put a slice\n        # up to that point onto a list of slices. Carry on.\n        while True:\n            # Check to see whether we've exhausted the buffer, either by iterating through it,\n            # or by consuming all the non-code segments already.\n            # NB: If we're here then we've already tried matching the remaining segments against\n            # the content, so we must be in a trailing case.\n            if len(seg_buff) == 0:\n                # Append the remaining buffer in case we're in the not is_code case.\n                matched_segments += seg_buff\n                # Nothing left, this is potentially a trailing case?\n                if self.allow_trailing and (\n                    self.min_delimiters is None\n                    or len(delimiters) >= self.min_delimiters\n                ):  # pragma: no cover TODO?\n                    # It is! (nothing left so no unmatched segments to append)\n                    return MatchResult.from_matched(matched_segments.matched_segments)\n                else:  # pragma: no cover TODO?\n                    return MatchResult.from_unmatched(segments)\n\n            # We rely on _bracket_sensitive_look_ahead_match to do the bracket counting\n            # element of this now. We look ahead to find a delimiter or terminator.\n            matchers = [self.delimiter]\n            if self.terminator:\n                matchers.append(self.terminator)\n            # If gaps aren't allowed, a gap (or non-code segment), acts like a terminator.\n            if not self.allow_gaps:\n                matchers.append(NonCodeMatcher())\n\n            with parse_context.deeper_match() as ctx:\n                (\n                    pre_content,\n                    delimiter_match,\n                    delimiter_matcher,\n                ) = self._bracket_sensitive_look_ahead_match(\n                    seg_buff,\n                    matchers,\n                    parse_context=ctx,\n                    bracket_pairs_set=self.bracket_pairs_set,\n                )\n\n            # Store the mutated segments to reuse.\n            mutated_segments = pre_content + delimiter_match.all_segments()\n\n            # Have we found a delimiter or terminator looking forward?\n            if delimiter_match:\n                if delimiter_matcher is self.delimiter:\n                    # Yes. Store it and then match the contents up to now.\n                    delimiters.append(delimiter_match.matched_segments)\n\n                # We now test the intervening section as to whether it matches one\n                # of the things we're looking for. NB: If it's of zero length then\n                # we return without trying it.\n                if len(pre_content) > 0:\n                    pre_non_code, pre_content, post_non_code = trim_non_code_segments(\n                        pre_content\n                    )\n                    # Check for whitespace gaps.\n                    # We do this explicitly here rather than relying on an\n                    # untrimmed match so we can handle _whitespace_ explicitly\n                    # compared to other non code segments like placeholders.\n                    if not self.allow_gaps and any(\n                        seg.is_whitespace for seg in pre_non_code + post_non_code\n                    ):\n                        return MatchResult.from_unmatched(\n                            mutated_segments\n                        )  # pragma: no cover TODO?\n\n                    with parse_context.deeper_match() as ctx:\n                        match, _ = self._longest_trimmed_match(\n                            segments=pre_content,\n                            matchers=self._elements,\n                            parse_context=ctx,\n                            # We've already trimmed\n                            trim_noncode=False,\n                        )\n                    # No match, or an incomplete match: Not allowed\n                    if not match or not match.is_complete():\n                        return MatchResult.from_unmatched(mutated_segments)\n\n                    # We have a complete match!\n\n                    # First add the segment up to the delimiter to the matched segments\n                    matched_segments += (\n                        pre_non_code + match.matched_segments + post_non_code\n                    )\n                    # Then it depends what we matched.\n                    # Delimiter\n                    if delimiter_matcher is self.delimiter:\n                        # Then add the delimiter to the matched segments\n                        matched_segments += delimiter_match.matched_segments\n                        # Break this for loop and move on, looking for the next delimiter\n                        seg_buff = delimiter_match.unmatched_segments\n                        # Still got some buffer left. Carry on.\n                        continue\n                    # Terminator (or the gap terminator).\n                    elif delimiter_matcher is self.terminator or isinstance(\n                        delimiter_matcher, NonCodeMatcher\n                    ):\n                        # We just return straight away here. We don't add the terminator to\n                        # this match, it should go with the unmatched parts.\n\n                        # First check we've had enough delimiters\n                        if (\n                            self.min_delimiters\n                            and len(delimiters) < self.min_delimiters\n                        ):\n                            return MatchResult.from_unmatched(mutated_segments)\n                        else:\n                            return MatchResult(\n                                matched_segments.matched_segments,\n                                delimiter_match.all_segments(),\n                            )\n                    else:  # pragma: no cover\n                        raise RuntimeError(\n                            (\n                                \"I don't know how I got here. Matched instead on {}, which \"\n                                \"doesn't appear to be delimiter or terminator\"\n                            ).format(delimiter_matcher)\n                        )\n                else:\n                    # Zero length section between delimiters, or zero code\n                    # elements if appropriate. Return unmatched.\n                    return MatchResult.from_unmatched(mutated_segments)\n            else:\n                # No match for a delimiter looking forward, this means we're\n                # at the end. In this case we look for a potential partial match\n                # looking forward. We know it's a non-zero length section because\n                # we checked that up front.\n\n                # First check we're had enough delimiters, because if we haven't then\n                # there's no sense to try matching\n                if self.min_delimiters and len(delimiters) < self.min_delimiters:\n                    return MatchResult.from_unmatched(mutated_segments)\n\n                # We use the whitespace padded match to hoover up whitespace if enabled,\n                # and default to the longest matcher. We don't care which one matches.\n                pre_non_code, trimmed_segments, post_non_code = trim_non_code_segments(\n                    mutated_segments\n                )\n                # Check for whitespace gaps.\n                # We do this explicitly here rather than relying on an\n                # untrimmed match so we can handle _whitespace_ explicitly\n                # compared to other non code segments like placeholders.\n                if not self.allow_gaps and any(\n                    seg.is_whitespace for seg in pre_non_code + post_non_code\n                ):\n                    return MatchResult.from_unmatched(\n                        mutated_segments\n                    )  # pragma: no cover TODO?\n\n                with parse_context.deeper_match() as ctx:\n                    mat, _ = self._longest_trimmed_match(\n                        trimmed_segments,\n                        self._elements,\n                        parse_context=ctx,\n                        # We've already trimmed\n                        trim_noncode=False,\n                    )\n\n                if mat:\n                    # We've got something at the end. Return!\n                    if mat.unmatched_segments:\n                        # We have something unmatched and so we should let it also have the trailing elements\n                        return MatchResult(\n                            matched_segments.matched_segments\n                            + pre_non_code\n                            + mat.matched_segments,\n                            mat.unmatched_segments + post_non_code,\n                        )\n                    else:\n                        # If there's nothing unmatched in the most recent match, then we can consume the trailing\n                        # non code segments\n                        return MatchResult.from_matched(\n                            matched_segments.matched_segments\n                            + pre_non_code\n                            + mat.matched_segments\n                            + post_non_code,\n                        )\n                else:\n                    # No match at the end, are we allowed to trail? If we are then return,\n                    # otherwise we fail because we can't match the last element.\n                    if self.allow_trailing:\n                        return MatchResult(matched_segments.matched_segments, seg_buff)\n                    else:\n                        return MatchResult.from_unmatched(mutated_segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "delimited", "py", "delimited", "def", "match", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "match", "an", "arbitrary", "number", "of", "elements", "separated", "by", "a", "delimiter", "note", "that", "if", "there", "are", "multiple", "elements", "passed", "in", "that", "they", "will", "be", "treated", "as", "different", "options", "of", "what", "can", "be", "delimited", "rather", "than", "a", "sequence", "have", "we", "been", "passed", "an", "empty", "list", "if", "len", "segments", "0", "return", "matchresult", "from_empty", "make", "some", "buffers", "seg_buff", "segments", "matched_segments", "matchresult", "from_empty", "delimiters", "is", "a", "list", "of", "tuples", "containing", "delimiter", "segments", "as", "we", "find", "them", "delimiters", "list", "basesegment", "first", "iterate", "through", "all", "the", "segments", "looking", "for", "the", "delimiter", "second", "split", "the", "list", "on", "each", "of", "the", "delimiters", "and", "ensure", "that", "each", "sublist", "in", "turn", "matches", "one", "of", "the", "elements", "in", "more", "detail", "match", "against", "delimiter", "if", "we", "match", "put", "a", "slice", "up", "to", "that", "point", "onto", "a", "list", "of", "slices", "carry", "on", "while", "true", "check", "to", "see", "whether", "we", "ve", "exhausted", "the", "buffer", "either", "by", "iterating", "through", "it", "or", "by", "consuming", "all", "the", "non", "code", "segments", "already", "nb", "if", "we", "re", "here", "then", "we", "ve", "already", "tried", "matching", "the", "remaining", "segments", "against", "the", "content", "so", "we", "must", "be", "in", "a", "trailing", "case", "if", "len", "seg_buff", "0", "append", "the", "remaining", "buffer", "in", "case", "we", "re", "in", "the", "not", "is_code", "case", "matched_segments", "seg_buff", "nothing", "left", "this", "is", "potentially", "a", "trailing", "case", "if", "self", "allow_trailing", "and", "self", "min_delimiters", "is", "none", "or", "len", "delimiters", "self", "min_delimiters", "pragma", "no", "cover", "todo", "it", "is", "nothing", "left", "so", "no", "unmatched", "segments", "to", "append", "return", "matchresult", "from_matched", "matched_segments", "matched_segments", "else", "pragma", "no", "cover", "todo", "return", "matchresult", "from_unmatched", "segments", "we", "rely", "on", "_bracket_sensitive_look_ahead_match", "to", "do", "the", "bracket", "counting", "element", "of", "this", "now", "we", "look", "ahead", "to", "find", "a", "delimiter", "or", "terminator", "matchers", "self", "delimiter", "if", "self", "terminator", "matchers", "append", "self", "terminator", "if", "gaps", "aren", "t", "allowed", "a", "gap", "or", "non", "code", "segment", "acts", "like", "a", "terminator", "if", "not", "self", "allow_gaps", "matchers", "append", "noncodematcher", "with", "parse_context", "deeper_match", "as", "ctx", "pre_content", "delimiter_match", "delimiter_matcher", "self", "_bracket_sensitive_look_ahead_match", "seg_buff", "matchers", "parse_context", "ctx", "bracket_pairs_set", "self", "bracket_pairs_set", "store", "the", "mutated", "segments", "to", "reuse", "mutated_segments", "pre_content", "delimiter_match", "all_segments", "have", "we", "found", "a", "delimiter", "or", "terminator", "looking", "forward", "if", "delimiter_match", "if", "delimiter_matcher", "is", "self", "delimiter", "yes", "store", "it", "and", "then", "match", "the", "contents", "up", "to", "now", "delimiters", "append", "delimiter_match", "matched_segments", "we", "now", "test", "the", "intervening", "section", "as", "to", "whether", "it", "matches", "one", "of", "the", "things", "we", "re", "looking", "for", "nb", "if", "it", "s", "of", "zero", "length", "then", "we", "return", "without", "trying", "it", "if", "len", "pre_content", "0", "pre_non_code", "pre_content", "post_non_code", "trim_non_code_segments", "pre_content", "check", "for", "whitespace", "gaps", "we", "do", "this", "explicitly", "here", "rather", "than", "relying", "on", "an", "untrimmed", "match", "so", "we", "can", "handle", "_whitespace_", "explicitly", "compared", "to", "other", "non", "code", "segments", "like", "placeholders", "if", "not", "self", "allow_gaps", "and", "any", "seg", "is_whitespace", "for", "seg", "in", "pre_non_code", "post_non_code", "return", "matchresult", "from_unmatched", "mutated_segments", "pragma", "no", "cover", "todo", "with", "parse_context", "deeper_match", "as", "ctx", "match", "_", "self", "_longest_trimmed_match", "segments", "pre_content", "matchers", "self", "_elements", "parse_context", "ctx", "we", "ve", "already", "trimmed", "trim_noncode", "false", "no", "match", "or", "an", "incomplete", "match", "not", "allowed", "if", "not", "match", "or", "not", "match", "is_complete", "return", "matchresult", "from_unmatched", "mutated_segments", "we", "have", "a", "complete", "match", "first", "add", "the", "segment", "up", "to", "the", "delimiter", "to", "the", "matched", "segments", "matched_segments", "pre_non_code", "match", "matched_segments", "post_non_code", "then", "it", "depends", "what", "we", "matched", "delimiter", "if", "delimiter_matcher", "is", "self", "delimiter", "then", "add", "the", "delimiter", "to", "the", "matched", "segments", "matched_segments", "delimiter_match", "matched_segments", "break", "this", "for", "loop", "and", "move", "on", "looking", "for", "the", "next", "delimiter", "seg_buff", "delimiter_match", "unmatched_segments", "still", "got", "some", "buffer", "left", "carry", "on", "continue", "terminator", "or", "the", "gap", "terminator", "elif", "delimiter_matcher", "is", "self", "terminator", "or", "isinstance", "delimiter_matcher", "noncodematcher", "we", "just", "return", "straight", "away", "here", "we", "don", "t", "add", "the", "terminator", "to", "this", "match", "it", "should", "go", "with", "the", "unmatched", "parts", "first", "check", "we", "ve", "had", "enough", "delimiters", "if", "self", "min_delimiters", "and", "len", "delimiters", "self", "min_delimiters", "return", "matchresult", "from_unmatched", "mutated_segments", "else", "return", "matchresult", "matched_segments", "matched_segments", "delimiter_match", "all_segments", "else", "pragma", "no", "cover", "raise", "runtimeerror", "i", "don", "t", "know", "how", "i", "got", "here", "matched", "instead", "on", "which", "doesn", "t", "appear", "to", "be", "delimiter", "or", "terminator", "format", "delimiter_matcher", "else", "zero", "length", "section", "between", "delimiters", "or", "zero", "code", "elements", "if", "appropriate", "return", "unmatched", "return", "matchresult", "from_unmatched", "mutated_segments", "else", "no", "match", "for", "a", "delimiter", "looking", "forward", "this", "means", "we", "re", "at", "the", "end", "in", "this", "case", "we", "look", "for", "a", "potential", "partial", "match", "looking", "forward", "we", "know", "it", "s", "a", "non", "zero", "length", "section", "because", "we", "checked", "that", "up", "front", "first", "check", "we", "re", "had", "enough", "delimiters", "because", "if", "we", "haven", "t", "then", "there", "s", "no", "sense", "to", "try", "matching", "if", "self", "min_delimiters", "and", "len", "delimiters", "self", "min_delimiters", "return", "matchresult", "from_unmatched", "mutated_segments", "we", "use", "the", "whitespace", "padded", "match", "to", "hoover", "up", "whitespace", "if", "enabled", "and", "default", "to", "the", "longest", "matcher", "we", "don", "t", "care", "which", "one", "matches", "pre_non_code", "trimmed_segments", "post_non_code", "trim_non_code_segments", "mutated_segments", "check", "for", "whitespace", "gaps", "we", "do", "this", "explicitly", "here", "rather", "than", "relying", "on", "an", "untrimmed", "match", "so", "we", "can", "handle", "_whitespace_", "explicitly", "compared", "to", "other", "non", "code", "segments", "like", "placeholders", "if", "not", "self", "allow_gaps", "and", "any", "seg", "is_whitespace", "for", "seg", "in", "pre_non_code", "post_non_code", "return", "matchresult", "from_unmatched", "mutated_segments", "pragma", "no", "cover", "todo", "with", "parse_context", "deeper_match", "as", "ctx", "mat", "_", "self", "_longest_trimmed_match", "trimmed_segments", "self", "_elements", "parse_context", "ctx", "we", "ve", "already", "trimmed", "trim_noncode", "false", "if", "mat", "we", "ve", "got", "something", "at", "the", "end", "return", "if", "mat", "unmatched_segments", "we", "have", "something", "unmatched", "and", "so", "we", "should", "let", "it", "also", "have", "the", "trailing", "elements", "return", "matchresult", "matched_segments", "matched_segments", "pre_non_code", "mat", "matched_segments", "mat", "unmatched_segments", "post_non_code", "else", "if", "there", "s", "nothing", "unmatched", "in", "the", "most", "recent", "match", "then", "we", "can", "consume", "the", "trailing", "non", "code", "segments", "return", "matchresult", "from_matched", "matched_segments", "matched_segments", "pre_non_code", "mat", "matched_segments", "post_non_code", "else", "no", "match", "at", "the", "end", "are", "we", "allowed", "to", "trail", "if", "we", "are", "then", "return", "otherwise", "we", "fail", "because", "we", "can", "t", "match", "the", "last", "element", "if", "self", "allow_trailing", "return", "matchresult", "matched_segments", "matched_segments", "seg_buff", "else", "return", "matchresult", "from_unmatched", "mutated_segments"], "doc_len": 966}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::GreedyUntil.__init__", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "GreedyUntil", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: GreedyUntil\n    def __init__(self, *args, **kwargs):\n        self.enforce_whitespace_preceding_terminator = kwargs.pop(\n            \"enforce_whitespace_preceding_terminator\", False\n        )\n        super().__init__(*args, **kwargs)\n        if not self.allow_gaps:  # pragma: no cover\n            raise NotImplementedError(\n                f\"{self.__class__} does not support allow_gaps=False.\"\n            )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "greedyuntil", "def", "__init__", "self", "args", "kwargs", "self", "enforce_whitespace_preceding_terminator", "kwargs", "pop", "enforce_whitespace_preceding_terminator", "false", "super", "__init__", "args", "kwargs", "if", "not", "self", "allow_gaps", "pragma", "no", "cover", "raise", "notimplementederror", "f", "self", "__class__", "does", "not", "support", "allow_gaps", "false"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::GreedyUntil.match", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "GreedyUntil", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: GreedyUntil\n    def match(self, segments, parse_context):\n        \"\"\"Matching for GreedyUntil works just how you'd expect.\"\"\"\n        return self.greedy_match(\n            segments,\n            parse_context,\n            matchers=self._elements,\n            enforce_whitespace_preceding_terminator=self.enforce_whitespace_preceding_terminator,\n            include_terminator=False,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "greedyuntil", "def", "match", "self", "segments", "parse_context", "matching", "for", "greedyuntil", "works", "just", "how", "you", "d", "expect", "return", "self", "greedy_match", "segments", "parse_context", "matchers", "self", "_elements", "enforce_whitespace_preceding_terminator", "self", "enforce_whitespace_preceding_terminator", "include_terminator", "false"], "doc_len": 35}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::GreedyUntil.greedy_match", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "GreedyUntil", "func_name": "greedy_match", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: GreedyUntil\n    def greedy_match(\n        cls,\n        segments,\n        parse_context,\n        matchers,\n        enforce_whitespace_preceding_terminator,\n        include_terminator=False,\n    ):\n        \"\"\"Matching for GreedyUntil works just how you'd expect.\"\"\"\n        seg_buff = segments\n        seg_bank = ()  # Empty tuple\n        # If no terminators then just return the whole thing.\n        if matchers == [None]:\n            return MatchResult.from_matched(segments)\n\n        while True:\n            with parse_context.deeper_match() as ctx:\n                pre, mat, matcher = cls._bracket_sensitive_look_ahead_match(\n                    seg_buff, matchers, parse_context=ctx\n                )\n\n            # Do we have a match?\n            if mat:\n                # Do we need to enforce whitespace preceding?\n                if enforce_whitespace_preceding_terminator:\n                    # Does the match include some whitespace already?\n                    # Work forward\n                    idx = 0\n                    while True:\n                        elem = mat.matched_segments[idx]\n                        if elem.is_meta:  # pragma: no cover TODO?\n                            idx += 1\n                            continue\n                        elif elem.is_type(\n                            \"whitespace\", \"newline\"\n                        ):  # pragma: no cover TODO?\n                            allowable_match = True\n                            break\n                        else:\n                            # No whitespace before. Not allowed.\n                            allowable_match = False\n                            break\n\n                    # If we're not ok yet, work backward to the preceding sections.\n                    if not allowable_match:\n                        idx = -1\n                        while True:\n                            if len(pre) < abs(idx):  # pragma: no cover TODO?\n                                # If we're at the start, it's ok\n                                allowable_match = True\n                                break\n                            if pre[idx].is_meta:  # pragma: no cover TODO?\n                                idx -= 1\n                                continue\n                            elif pre[idx].is_type(\"whitespace\", \"newline\"):\n                                allowable_match = True\n                                break\n                            else:\n                                # No whitespace before. Not allowed.\n                                allowable_match = False\n                                break\n\n                    # If this match isn't preceded by whitespace and that is\n                    # a requirement, then we can't use it. Carry on...\n                    if not allowable_match:\n                        # Update our buffers and continue onward\n                        seg_bank = pre + mat.matched_segments\n                        seg_buff = mat.unmatched_segments\n                        # Loop around, don't return yet\n                        continue\n\n                # Depending on whether we found a terminator or not we treat\n                # the result slightly differently. If no terminator was found,\n                # we just use the whole unmatched segment. If we did find one,\n                # we match up until (but not including [unless self.include_terminator\n                # is true]) that terminator.\n                if mat:\n                    # Return everything up to the match unless it's a gap matcher.\n                    if include_terminator:\n                        return MatchResult(\n                            seg_bank + pre + mat.matched_segments,\n                            mat.unmatched_segments,\n                        )\n\n                    # We can't claim any non-code segments, so we trim them off the end.\n                    leading_nc, pre_seg_mid, trailing_nc = trim_non_code_segments(\n                        seg_bank + pre\n                    )\n                    return MatchResult(\n                        leading_nc + pre_seg_mid,\n                        trailing_nc + mat.all_segments(),\n                    )\n                # No terminator, just return the whole thing.\n                return MatchResult.from_matched(\n                    mat.unmatched_segments\n                )  # pragma: no cover TODO?\n            else:\n                # Return everything\n                return MatchResult.from_matched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "greedyuntil", "def", "greedy_match", "cls", "segments", "parse_context", "matchers", "enforce_whitespace_preceding_terminator", "include_terminator", "false", "matching", "for", "greedyuntil", "works", "just", "how", "you", "d", "expect", "seg_buff", "segments", "seg_bank", "empty", "tuple", "if", "no", "terminators", "then", "just", "return", "the", "whole", "thing", "if", "matchers", "none", "return", "matchresult", "from_matched", "segments", "while", "true", "with", "parse_context", "deeper_match", "as", "ctx", "pre", "mat", "matcher", "cls", "_bracket_sensitive_look_ahead_match", "seg_buff", "matchers", "parse_context", "ctx", "do", "we", "have", "a", "match", "if", "mat", "do", "we", "need", "to", "enforce", "whitespace", "preceding", "if", "enforce_whitespace_preceding_terminator", "does", "the", "match", "include", "some", "whitespace", "already", "work", "forward", "idx", "0", "while", "true", "elem", "mat", "matched_segments", "idx", "if", "elem", "is_meta", "pragma", "no", "cover", "todo", "idx", "1", "continue", "elif", "elem", "is_type", "whitespace", "newline", "pragma", "no", "cover", "todo", "allowable_match", "true", "break", "else", "no", "whitespace", "before", "not", "allowed", "allowable_match", "false", "break", "if", "we", "re", "not", "ok", "yet", "work", "backward", "to", "the", "preceding", "sections", "if", "not", "allowable_match", "idx", "1", "while", "true", "if", "len", "pre", "abs", "idx", "pragma", "no", "cover", "todo", "if", "we", "re", "at", "the", "start", "it", "s", "ok", "allowable_match", "true", "break", "if", "pre", "idx", "is_meta", "pragma", "no", "cover", "todo", "idx", "1", "continue", "elif", "pre", "idx", "is_type", "whitespace", "newline", "allowable_match", "true", "break", "else", "no", "whitespace", "before", "not", "allowed", "allowable_match", "false", "break", "if", "this", "match", "isn", "t", "preceded", "by", "whitespace", "and", "that", "is", "a", "requirement", "then", "we", "can", "t", "use", "it", "carry", "on", "if", "not", "allowable_match", "update", "our", "buffers", "and", "continue", "onward", "seg_bank", "pre", "mat", "matched_segments", "seg_buff", "mat", "unmatched_segments", "loop", "around", "don", "t", "return", "yet", "continue", "depending", "on", "whether", "we", "found", "a", "terminator", "or", "not", "we", "treat", "the", "result", "slightly", "differently", "if", "no", "terminator", "was", "found", "we", "just", "use", "the", "whole", "unmatched", "segment", "if", "we", "did", "find", "one", "we", "match", "up", "until", "but", "not", "including", "unless", "self", "include_terminator", "is", "true", "that", "terminator", "if", "mat", "return", "everything", "up", "to", "the", "match", "unless", "it", "s", "a", "gap", "matcher", "if", "include_terminator", "return", "matchresult", "seg_bank", "pre", "mat", "matched_segments", "mat", "unmatched_segments", "we", "can", "t", "claim", "any", "non", "code", "segments", "so", "we", "trim", "them", "off", "the", "end", "leading_nc", "pre_seg_mid", "trailing_nc", "trim_non_code_segments", "seg_bank", "pre", "return", "matchresult", "leading_nc", "pre_seg_mid", "trailing_nc", "mat", "all_segments", "no", "terminator", "just", "return", "the", "whole", "thing", "return", "matchresult", "from_matched", "mat", "unmatched_segments", "pragma", "no", "cover", "todo", "else", "return", "everything", "return", "matchresult", "from_matched", "segments"], "doc_len": 361}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::StartsWith.__init__", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "StartsWith", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: StartsWith\n    def __init__(self, target, *args, **kwargs):\n        self.target = self._resolve_ref(target)\n        self.terminator = self._resolve_ref(kwargs.pop(\"terminator\", None))\n        self.include_terminator = kwargs.pop(\"include_terminator\", False)\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "startswith", "def", "__init__", "self", "target", "args", "kwargs", "self", "target", "self", "_resolve_ref", "target", "self", "terminator", "self", "_resolve_ref", "kwargs", "pop", "terminator", "none", "self", "include_terminator", "kwargs", "pop", "include_terminator", "false", "super", "__init__", "args", "kwargs"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::StartsWith.simple", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "StartsWith", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: StartsWith\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        `StartsWith` is simple, if the thing it starts with is also simple.\n        \"\"\"\n        return self.target.simple(parse_context=parse_context)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "startswith", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "startswith", "is", "simple", "if", "the", "thing", "it", "starts", "with", "is", "also", "simple", "return", "self", "target", "simple", "parse_context", "parse_context"], "doc_len": 43}
{"doc_id": "src/sqlfluff/core/parser/grammar/greedy.py::StartsWith.match", "file_path": "src/sqlfluff/core/parser/grammar/greedy.py", "class_name": "StartsWith", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/greedy.py, 类名: StartsWith\n    def match(self, segments, parse_context):\n        \"\"\"Match if this sequence starts with a match.\"\"\"\n        first_code_idx = None\n        # Work through to find the first code segment...\n        for idx, seg in enumerate(segments):\n            if seg.is_code:\n                first_code_idx = idx\n                break\n        else:\n            # We've trying to match on a sequence of segments which contain no code.\n            # That means this isn't a match.\n            return MatchResult.from_unmatched(segments)  # pragma: no cover TODO?\n        with parse_context.deeper_match() as ctx:\n            match = self.target.match(\n                segments=segments[first_code_idx:], parse_context=ctx\n            )\n\n        if not match:\n            return MatchResult.from_unmatched(segments)\n\n        # The match will probably have returned a mutated version rather\n        # that the raw segment sent for matching. We need to reinsert it\n        # back into the sequence in place of the raw one, but we can't\n        # just assign at the index because it's a tuple and not a list.\n        # to get around that we do this slightly more elaborate construction.\n\n        # NB: This match may be partial or full, either is cool. In the case\n        # of a partial match, given that we're only interested in what it STARTS\n        # with, then we can still used the unmatched parts on the end.\n        # We still need to deal with any non-code segments at the start.\n        greedy_match = self.greedy_match(\n            match.unmatched_segments,\n            parse_context,\n            matchers=[self.terminator],\n            enforce_whitespace_preceding_terminator=self.enforce_whitespace_preceding_terminator,\n            include_terminator=self.include_terminator,\n        )\n\n        # NB: If all we matched in the greedy match was non-code then we can't\n        # claim it.\n        if not any(seg.is_code for seg in greedy_match.matched_segments):\n            # So just return the original match.\n            return match\n\n        # Otherwise Combine the results.\n        return MatchResult(\n            match.matched_segments + greedy_match.matched_segments,\n            greedy_match.unmatched_segments,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "greedy", "py", "startswith", "def", "match", "self", "segments", "parse_context", "match", "if", "this", "sequence", "starts", "with", "a", "match", "first_code_idx", "none", "work", "through", "to", "find", "the", "first", "code", "segment", "for", "idx", "seg", "in", "enumerate", "segments", "if", "seg", "is_code", "first_code_idx", "idx", "break", "else", "we", "ve", "trying", "to", "match", "on", "a", "sequence", "of", "segments", "which", "contain", "no", "code", "that", "means", "this", "isn", "t", "a", "match", "return", "matchresult", "from_unmatched", "segments", "pragma", "no", "cover", "todo", "with", "parse_context", "deeper_match", "as", "ctx", "match", "self", "target", "match", "segments", "segments", "first_code_idx", "parse_context", "ctx", "if", "not", "match", "return", "matchresult", "from_unmatched", "segments", "the", "match", "will", "probably", "have", "returned", "a", "mutated", "version", "rather", "that", "the", "raw", "segment", "sent", "for", "matching", "we", "need", "to", "reinsert", "it", "back", "into", "the", "sequence", "in", "place", "of", "the", "raw", "one", "but", "we", "can", "t", "just", "assign", "at", "the", "index", "because", "it", "s", "a", "tuple", "and", "not", "a", "list", "to", "get", "around", "that", "we", "do", "this", "slightly", "more", "elaborate", "construction", "nb", "this", "match", "may", "be", "partial", "or", "full", "either", "is", "cool", "in", "the", "case", "of", "a", "partial", "match", "given", "that", "we", "re", "only", "interested", "in", "what", "it", "starts", "with", "then", "we", "can", "still", "used", "the", "unmatched", "parts", "on", "the", "end", "we", "still", "need", "to", "deal", "with", "any", "non", "code", "segments", "at", "the", "start", "greedy_match", "self", "greedy_match", "match", "unmatched_segments", "parse_context", "matchers", "self", "terminator", "enforce_whitespace_preceding_terminator", "self", "enforce_whitespace_preceding_terminator", "include_terminator", "self", "include_terminator", "nb", "if", "all", "we", "matched", "in", "the", "greedy", "match", "was", "non", "code", "then", "we", "can", "t", "claim", "it", "if", "not", "any", "seg", "is_code", "for", "seg", "in", "greedy_match", "matched_segments", "so", "just", "return", "the", "original", "match", "return", "match", "otherwise", "combine", "the", "results", "return", "matchresult", "match", "matched_segments", "greedy_match", "matched_segments", "greedy_match", "unmatched_segments"], "doc_len": 271}
{"doc_id": "src/sqlfluff/core/parser/grammar/noncode.py::NonCodeMatcher.simple", "file_path": "src/sqlfluff/core/parser/grammar/noncode.py", "class_name": "NonCodeMatcher", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/noncode.py, 类名: NonCodeMatcher\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"This element doesn't work with simple.\"\"\"\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "noncode", "py", "noncodematcher", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "this", "element", "doesn", "t", "work", "with", "simple", "return", "none"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/grammar/noncode.py::NonCodeMatcher.is_optional", "file_path": "src/sqlfluff/core/parser/grammar/noncode.py", "class_name": "NonCodeMatcher", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/grammar/noncode.py, 类名: NonCodeMatcher\n    def is_optional(self):  # pragma: no cover TODO?\n        \"\"\"Not optional.\"\"\"\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "noncode", "py", "noncodematcher", "def", "is_optional", "self", "pragma", "no", "cover", "todo", "not", "optional", "return", "false"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/parser/grammar/noncode.py::NonCodeMatcher.match", "file_path": "src/sqlfluff/core/parser/grammar/noncode.py", "class_name": "NonCodeMatcher", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/noncode.py, 类名: NonCodeMatcher\n    def match(self, segments, parse_context):\n        \"\"\"Match any starting non-code segments.\"\"\"\n        if not isinstance(segments, tuple):  # pragma: no cover\n            raise TypeError(\"NonCodeMatcher expects a tuple.\")\n        idx = 0\n        while idx < len(segments) and not segments[idx].is_code:\n            idx += 1\n        return MatchResult(segments[:idx], segments[idx:])\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "noncode", "py", "noncodematcher", "def", "match", "self", "segments", "parse_context", "match", "any", "starting", "non", "code", "segments", "if", "not", "isinstance", "segments", "tuple", "pragma", "no", "cover", "raise", "typeerror", "noncodematcher", "expects", "a", "tuple", "idx", "0", "while", "idx", "len", "segments", "and", "not", "segments", "idx", "is_code", "idx", "1", "return", "matchresult", "segments", "idx", "segments", "idx"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Sequence.simple", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Sequence", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Sequence\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        Sequence does provide this, as long as the *first* non-optional\n        element does, *AND* and optional elements which preceded it also do.\n        \"\"\"\n        simple_buff = []\n        for opt in self._elements:\n            simple = opt.simple(parse_context=parse_context)\n            if not simple:\n                return None\n            simple_buff += simple\n\n            if not opt.is_optional():\n                # We found our first non-optional element!\n                return simple_buff\n        # If *all* elements are optional AND simple, I guess it's also simple.\n        return simple_buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "sequence", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "sequence", "does", "provide", "this", "as", "long", "as", "the", "first", "non", "optional", "element", "does", "and", "and", "optional", "elements", "which", "preceded", "it", "also", "do", "simple_buff", "for", "opt", "in", "self", "_elements", "simple", "opt", "simple", "parse_context", "parse_context", "if", "not", "simple", "return", "none", "simple_buff", "simple", "if", "not", "opt", "is_optional", "we", "found", "our", "first", "non", "optional", "element", "return", "simple_buff", "if", "all", "elements", "are", "optional", "and", "simple", "i", "guess", "it", "s", "also", "simple", "return", "simple_buff"], "doc_len": 93}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Sequence.match", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Sequence", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Sequence\n    def match(self, segments, parse_context):\n        \"\"\"Match a specific sequence of elements.\"\"\"\n        if isinstance(segments, BaseSegment):\n            segments = tuple(segments)  # pragma: no cover TODO?\n\n        matched_segments = MatchResult.from_empty()\n        unmatched_segments = segments\n\n        # Buffers of uninstantiated meta segments.\n        meta_pre_nc = ()\n        meta_post_nc = ()\n        early_break = False\n\n        for idx, elem in enumerate(self._elements):\n            # Check for an early break.\n            if early_break:\n                break\n\n            while True:\n                # Consume non-code if appropriate\n                if self.allow_gaps:\n                    pre_nc, mid_seg, post_nc = trim_non_code_segments(\n                        unmatched_segments\n                    )\n                else:\n                    pre_nc = ()\n                    mid_seg = unmatched_segments\n                    post_nc = ()\n\n                # Is it an indent or dedent?\n                if elem.is_meta:\n                    # Elements with a negative indent value come AFTER\n                    # the whitespace. Positive or neutral come BEFORE.\n                    if elem.indent_val < 0:\n                        meta_post_nc += (elem(),)\n                    else:\n                        meta_pre_nc += (elem(),)\n                    break\n\n                # Is it a conditional? If so is it active\n                if isinstance(elem, Conditional) and not elem.is_enabled(parse_context):\n                    # If it's not active, skip it.\n                    break\n\n                if len(pre_nc + mid_seg + post_nc) == 0:\n                    # We've run our of sequence without matching everything.\n                    # Do only optional or meta elements remain?\n                    if all(\n                        e.is_optional() or e.is_meta or isinstance(e, Conditional)\n                        for e in self._elements[idx:]\n                    ):\n                        # then it's ok, and we can return what we've got so far.\n                        # No need to deal with anything left over because we're at the end,\n                        # unless it's a meta segment.\n\n                        # We'll add those meta segments after any existing ones. So\n                        # the go on the meta_post_nc stack.\n                        for e in self._elements[idx:]:\n                            # If it's meta, instantiate it.\n                            if e.is_meta:\n                                meta_post_nc += (e(),)  # pragma: no cover TODO?\n                            # If it's conditional and it's enabled, match it.\n                            if isinstance(e, Conditional) and e.is_enabled(\n                                parse_context\n                            ):\n                                meta_match = e.match(tuple(), parse_context)\n                                if meta_match:\n                                    meta_post_nc += meta_match.matched_segments\n\n                        # Early break to exit via the happy match path.\n                        early_break = True\n                        break\n                    else:\n                        # we've got to the end of the sequence without matching all\n                        # required elements.\n                        return MatchResult.from_unmatched(segments)\n                else:\n                    # We've already dealt with potential whitespace above, so carry on to matching\n                    with parse_context.deeper_match() as ctx:\n                        elem_match = elem.match(mid_seg, parse_context=ctx)\n\n                    if elem_match.has_match():\n                        # We're expecting mostly partial matches here, but complete\n                        # matches are possible. Don't be greedy with whitespace!\n                        matched_segments += (\n                            meta_pre_nc\n                            + pre_nc\n                            + meta_post_nc\n                            + elem_match.matched_segments\n                        )\n                        meta_pre_nc = ()\n                        meta_post_nc = ()\n                        unmatched_segments = elem_match.unmatched_segments + post_nc\n                        # Each time we do this, we do a sense check to make sure we haven't\n                        # dropped anything. (Because it's happened before!).\n                        check_still_complete(\n                            segments,\n                            matched_segments.matched_segments,\n                            unmatched_segments,\n                        )\n\n                        # Break out of the while loop and move to the next element.\n                        break\n                    else:\n                        # If we can't match an element, we should ascertain whether it's\n                        # required. If so then fine, move on, but otherwise we should crash\n                        # out without a match. We have not matched the sequence.\n                        if elem.is_optional():\n                            # This will crash us out of the while loop and move us\n                            # onto the next matching element\n                            break\n                        else:\n                            return MatchResult.from_unmatched(segments)\n\n        # If we get to here, we've matched all of the elements (or skipped them)\n        # but still have some segments left (or perhaps have precisely zero left).\n        # In either case, we're golden. Return successfully, with any leftovers as\n        # the unmatched elements. Meta all go at the end regardless of wny trailing\n        # whitespace.\n        return MatchResult(\n            BaseSegment._position_segments(\n                matched_segments.matched_segments + meta_pre_nc + meta_post_nc,\n            ),\n            unmatched_segments,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "sequence", "def", "match", "self", "segments", "parse_context", "match", "a", "specific", "sequence", "of", "elements", "if", "isinstance", "segments", "basesegment", "segments", "tuple", "segments", "pragma", "no", "cover", "todo", "matched_segments", "matchresult", "from_empty", "unmatched_segments", "segments", "buffers", "of", "uninstantiated", "meta", "segments", "meta_pre_nc", "meta_post_nc", "early_break", "false", "for", "idx", "elem", "in", "enumerate", "self", "_elements", "check", "for", "an", "early", "break", "if", "early_break", "break", "while", "true", "consume", "non", "code", "if", "appropriate", "if", "self", "allow_gaps", "pre_nc", "mid_seg", "post_nc", "trim_non_code_segments", "unmatched_segments", "else", "pre_nc", "mid_seg", "unmatched_segments", "post_nc", "is", "it", "an", "indent", "or", "dedent", "if", "elem", "is_meta", "elements", "with", "a", "negative", "indent", "value", "come", "after", "the", "whitespace", "positive", "or", "neutral", "come", "before", "if", "elem", "indent_val", "0", "meta_post_nc", "elem", "else", "meta_pre_nc", "elem", "break", "is", "it", "a", "conditional", "if", "so", "is", "it", "active", "if", "isinstance", "elem", "conditional", "and", "not", "elem", "is_enabled", "parse_context", "if", "it", "s", "not", "active", "skip", "it", "break", "if", "len", "pre_nc", "mid_seg", "post_nc", "0", "we", "ve", "run", "our", "of", "sequence", "without", "matching", "everything", "do", "only", "optional", "or", "meta", "elements", "remain", "if", "all", "e", "is_optional", "or", "e", "is_meta", "or", "isinstance", "e", "conditional", "for", "e", "in", "self", "_elements", "idx", "then", "it", "s", "ok", "and", "we", "can", "return", "what", "we", "ve", "got", "so", "far", "no", "need", "to", "deal", "with", "anything", "left", "over", "because", "we", "re", "at", "the", "end", "unless", "it", "s", "a", "meta", "segment", "we", "ll", "add", "those", "meta", "segments", "after", "any", "existing", "ones", "so", "the", "go", "on", "the", "meta_post_nc", "stack", "for", "e", "in", "self", "_elements", "idx", "if", "it", "s", "meta", "instantiate", "it", "if", "e", "is_meta", "meta_post_nc", "e", "pragma", "no", "cover", "todo", "if", "it", "s", "conditional", "and", "it", "s", "enabled", "match", "it", "if", "isinstance", "e", "conditional", "and", "e", "is_enabled", "parse_context", "meta_match", "e", "match", "tuple", "parse_context", "if", "meta_match", "meta_post_nc", "meta_match", "matched_segments", "early", "break", "to", "exit", "via", "the", "happy", "match", "path", "early_break", "true", "break", "else", "we", "ve", "got", "to", "the", "end", "of", "the", "sequence", "without", "matching", "all", "required", "elements", "return", "matchresult", "from_unmatched", "segments", "else", "we", "ve", "already", "dealt", "with", "potential", "whitespace", "above", "so", "carry", "on", "to", "matching", "with", "parse_context", "deeper_match", "as", "ctx", "elem_match", "elem", "match", "mid_seg", "parse_context", "ctx", "if", "elem_match", "has_match", "we", "re", "expecting", "mostly", "partial", "matches", "here", "but", "complete", "matches", "are", "possible", "don", "t", "be", "greedy", "with", "whitespace", "matched_segments", "meta_pre_nc", "pre_nc", "meta_post_nc", "elem_match", "matched_segments", "meta_pre_nc", "meta_post_nc", "unmatched_segments", "elem_match", "unmatched_segments", "post_nc", "each", "time", "we", "do", "this", "we", "do", "a", "sense", "check", "to", "make", "sure", "we", "haven", "t", "dropped", "anything", "because", "it", "s", "happened", "before", "check_still_complete", "segments", "matched_segments", "matched_segments", "unmatched_segments", "break", "out", "of", "the", "while", "loop", "and", "move", "to", "the", "next", "element", "break", "else", "if", "we", "can", "t", "match", "an", "element", "we", "should", "ascertain", "whether", "it", "s", "required", "if", "so", "then", "fine", "move", "on", "but", "otherwise", "we", "should", "crash", "out", "without", "a", "match", "we", "have", "not", "matched", "the", "sequence", "if", "elem", "is_optional", "this", "will", "crash", "us", "out", "of", "the", "while", "loop", "and", "move", "us", "onto", "the", "next", "matching", "element", "break", "else", "return", "matchresult", "from_unmatched", "segments", "if", "we", "get", "to", "here", "we", "ve", "matched", "all", "of", "the", "elements", "or", "skipped", "them", "but", "still", "have", "some", "segments", "left", "or", "perhaps", "have", "precisely", "zero", "left", "in", "either", "case", "we", "re", "golden", "return", "successfully", "with", "any", "leftovers", "as", "the", "unmatched", "elements", "meta", "all", "go", "at", "the", "end", "regardless", "of", "wny", "trailing", "whitespace", "return", "matchresult", "basesegment", "_position_segments", "matched_segments", "matched_segments", "meta_pre_nc", "meta_post_nc", "unmatched_segments"], "doc_len": 532}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Bracketed.__init__", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Bracketed", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Bracketed\n    def __init__(self, *args, **kwargs):\n        # Store the bracket type. NB: This is only\n        # hydrated into segments at runtime.\n        self.bracket_type = kwargs.pop(\"bracket_type\", \"round\")\n        self.bracket_pairs_set = kwargs.pop(\"bracket_pairs_set\", \"bracket_pairs\")\n        # Allow optional override for special bracket-like things\n        self.start_bracket = kwargs.pop(\"start_bracket\", None)\n        self.end_bracket = kwargs.pop(\"end_bracket\", None)\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "bracketed", "def", "__init__", "self", "args", "kwargs", "store", "the", "bracket", "type", "nb", "this", "is", "only", "hydrated", "into", "segments", "at", "runtime", "self", "bracket_type", "kwargs", "pop", "bracket_type", "round", "self", "bracket_pairs_set", "kwargs", "pop", "bracket_pairs_set", "bracket_pairs", "allow", "optional", "override", "for", "special", "bracket", "like", "things", "self", "start_bracket", "kwargs", "pop", "start_bracket", "none", "self", "end_bracket", "kwargs", "pop", "end_bracket", "none", "super", "__init__", "args", "kwargs"], "doc_len": 62}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Bracketed.simple", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Bracketed", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Bracketed\n    def simple(self, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support a uppercase hash matching route?\n\n        Bracketed does this easily, we just look for the bracket.\n        \"\"\"\n        start_bracket, _, _ = self.get_bracket_from_dialect(parse_context)\n        return start_bracket.simple(parse_context=parse_context)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "bracketed", "def", "simple", "self", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "a", "uppercase", "hash", "matching", "route", "bracketed", "does", "this", "easily", "we", "just", "look", "for", "the", "bracket", "start_bracket", "_", "_", "self", "get_bracket_from_dialect", "parse_context", "return", "start_bracket", "simple", "parse_context", "parse_context"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Bracketed.get_bracket_from_dialect", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Bracketed", "func_name": "get_bracket_from_dialect", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Bracketed\n    def get_bracket_from_dialect(self, parse_context):\n        \"\"\"Rehydrate the bracket segments in question.\"\"\"\n        for bracket_type, start_ref, end_ref, persists in parse_context.dialect.sets(\n            self.bracket_pairs_set\n        ):\n            if bracket_type == self.bracket_type:\n                start_bracket = parse_context.dialect.ref(start_ref)\n                end_bracket = parse_context.dialect.ref(end_ref)\n                break\n        else:  # pragma: no cover\n            raise ValueError(\n                \"bracket_type {!r} not found in bracket_pairs of {!r} dialect.\".format(\n                    self.bracket_type, parse_context.dialect.name\n                )\n            )\n        return start_bracket, end_bracket, persists\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "bracketed", "def", "get_bracket_from_dialect", "self", "parse_context", "rehydrate", "the", "bracket", "segments", "in", "question", "for", "bracket_type", "start_ref", "end_ref", "persists", "in", "parse_context", "dialect", "sets", "self", "bracket_pairs_set", "if", "bracket_type", "self", "bracket_type", "start_bracket", "parse_context", "dialect", "ref", "start_ref", "end_bracket", "parse_context", "dialect", "ref", "end_ref", "break", "else", "pragma", "no", "cover", "raise", "valueerror", "bracket_type", "r", "not", "found", "in", "bracket_pairs", "of", "r", "dialect", "format", "self", "bracket_type", "parse_context", "dialect", "name", "return", "start_bracket", "end_bracket", "persists"], "doc_len": 69}
{"doc_id": "src/sqlfluff/core/parser/grammar/sequence.py::Bracketed.match", "file_path": "src/sqlfluff/core/parser/grammar/sequence.py", "class_name": "Bracketed", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/grammar/sequence.py, 类名: Bracketed\n    def match(\n        self, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match if this is a bracketed sequence, with content that matches one of the elements.\n\n        1. work forwards to find the first bracket.\n           If we find something other that whitespace, then fail out.\n        2. Once we have the first bracket, we need to bracket count forward to find its partner.\n        3. Assuming we find its partner then we try and match what goes between them\n           using the match method of Sequence.\n           If we match, great. If not, then we return an empty match.\n           If we never find its partner then we return an empty match but should probably\n           log a parsing warning, or error?\n\n        \"\"\"\n        # Trim ends if allowed.\n        if self.allow_gaps:\n            pre_nc, seg_buff, post_nc = trim_non_code_segments(segments)\n        else:\n            seg_buff = segments  # pragma: no cover TODO?\n\n        # Rehydrate the bracket segments in question.\n        # bracket_persits controls whether we make a BracketedSegment or not.\n        start_bracket, end_bracket, bracket_persists = self.get_bracket_from_dialect(\n            parse_context\n        )\n        # Allow optional override for special bracket-like things\n        start_bracket = self.start_bracket or start_bracket\n        end_bracket = self.end_bracket or end_bracket\n\n        # Are we dealing with a pre-existing BracketSegment?\n        if seg_buff[0].is_type(\"bracketed\"):\n            seg: BracketedSegment = cast(BracketedSegment, seg_buff[0])\n            content_segs = seg.segments[len(seg.start_bracket) : -len(seg.end_bracket)]\n            bracket_segment = seg\n            trailing_segments = seg_buff[1:]\n        # Otherwise try and match the segments directly.\n        else:\n            # Look for the first bracket\n            with parse_context.deeper_match() as ctx:\n                start_match = start_bracket.match(seg_buff, parse_context=ctx)\n            if start_match:\n                seg_buff = start_match.unmatched_segments\n            else:\n                # Can't find the opening bracket. No Match.\n                return MatchResult.from_unmatched(segments)\n\n            # Look for the closing bracket\n            content_segs, end_match, _ = self._bracket_sensitive_look_ahead_match(\n                segments=seg_buff,\n                matchers=[end_bracket],\n                parse_context=parse_context,\n                start_bracket=start_bracket,\n                end_bracket=end_bracket,\n                bracket_pairs_set=self.bracket_pairs_set,\n            )\n            if not end_match:  # pragma: no cover\n                raise SQLParseError(\n                    \"Couldn't find closing bracket for opening bracket.\",\n                    segment=start_match.matched_segments[0],\n                )\n\n            # Construct a bracket segment\n            bracket_segment = BracketedSegment(\n                segments=(\n                    start_match.matched_segments\n                    + content_segs\n                    + end_match.matched_segments\n                ),\n                start_bracket=start_match.matched_segments,\n                end_bracket=end_match.matched_segments,\n            )\n            trailing_segments = end_match.unmatched_segments\n\n        # Then trim whitespace and deal with the case of non-code content e.g. \"(   )\"\n        if self.allow_gaps:\n            pre_segs, content_segs, post_segs = trim_non_code_segments(content_segs)\n        else:  # pragma: no cover TODO?\n            pre_segs = ()\n            post_segs = ()\n\n        # If we've got a case of empty brackets check whether that is allowed.\n        if not content_segs:\n            if not self._elements or (\n                all(e.is_optional() for e in self._elements)\n                and (self.allow_gaps or (not pre_segs and not post_segs))\n            ):\n                return MatchResult(\n                    (bracket_segment,)\n                    if bracket_persists\n                    else bracket_segment.segments,\n                    trailing_segments,\n                )\n            else:\n                return MatchResult.from_unmatched(segments)\n\n        # Match the content using super. Sequence will interpret the content of the elements.\n        with parse_context.deeper_match() as ctx:\n            content_match = super().match(content_segs, parse_context=ctx)\n\n        # We require a complete match for the content (hopefully for obvious reasons)\n        if content_match.is_complete():\n            # Reconstruct the bracket segment post match.\n            # We need to realign the meta segments so the pos markers are correct.\n            # Have we already got indents?\n            meta_idx = None\n            for idx, seg in enumerate(bracket_segment.segments):\n                if seg.is_meta and cast(MetaSegment, seg).indent_val > 0:\n                    meta_idx = idx\n                    break\n            # If we've already got indents, don't add more.\n            if meta_idx:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    bracket_segment.start_bracket\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + bracket_segment.end_bracket\n                )\n            # Append some indent and dedent tokens at the start and the end.\n            else:\n                bracket_segment.segments = BaseSegment._position_segments(\n                    # NB: The nc segments go *outside* the indents.\n                    bracket_segment.start_bracket\n                    + (Indent(),)  # Add a meta indent here\n                    + pre_segs\n                    + content_match.all_segments()\n                    + post_segs\n                    + (Dedent(),)  # Add a meta indent here\n                    + bracket_segment.end_bracket\n                )\n            return MatchResult(\n                (bracket_segment,) if bracket_persists else bracket_segment.segments,\n                trailing_segments,\n            )\n        # No complete match. Fail.\n        else:\n            return MatchResult.from_unmatched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "grammar", "sequence", "py", "bracketed", "def", "match", "self", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "match", "if", "this", "is", "a", "bracketed", "sequence", "with", "content", "that", "matches", "one", "of", "the", "elements", "1", "work", "forwards", "to", "find", "the", "first", "bracket", "if", "we", "find", "something", "other", "that", "whitespace", "then", "fail", "out", "2", "once", "we", "have", "the", "first", "bracket", "we", "need", "to", "bracket", "count", "forward", "to", "find", "its", "partner", "3", "assuming", "we", "find", "its", "partner", "then", "we", "try", "and", "match", "what", "goes", "between", "them", "using", "the", "match", "method", "of", "sequence", "if", "we", "match", "great", "if", "not", "then", "we", "return", "an", "empty", "match", "if", "we", "never", "find", "its", "partner", "then", "we", "return", "an", "empty", "match", "but", "should", "probably", "log", "a", "parsing", "warning", "or", "error", "trim", "ends", "if", "allowed", "if", "self", "allow_gaps", "pre_nc", "seg_buff", "post_nc", "trim_non_code_segments", "segments", "else", "seg_buff", "segments", "pragma", "no", "cover", "todo", "rehydrate", "the", "bracket", "segments", "in", "question", "bracket_persits", "controls", "whether", "we", "make", "a", "bracketedsegment", "or", "not", "start_bracket", "end_bracket", "bracket_persists", "self", "get_bracket_from_dialect", "parse_context", "allow", "optional", "override", "for", "special", "bracket", "like", "things", "start_bracket", "self", "start_bracket", "or", "start_bracket", "end_bracket", "self", "end_bracket", "or", "end_bracket", "are", "we", "dealing", "with", "a", "pre", "existing", "bracketsegment", "if", "seg_buff", "0", "is_type", "bracketed", "seg", "bracketedsegment", "cast", "bracketedsegment", "seg_buff", "0", "content_segs", "seg", "segments", "len", "seg", "start_bracket", "len", "seg", "end_bracket", "bracket_segment", "seg", "trailing_segments", "seg_buff", "1", "otherwise", "try", "and", "match", "the", "segments", "directly", "else", "look", "for", "the", "first", "bracket", "with", "parse_context", "deeper_match", "as", "ctx", "start_match", "start_bracket", "match", "seg_buff", "parse_context", "ctx", "if", "start_match", "seg_buff", "start_match", "unmatched_segments", "else", "can", "t", "find", "the", "opening", "bracket", "no", "match", "return", "matchresult", "from_unmatched", "segments", "look", "for", "the", "closing", "bracket", "content_segs", "end_match", "_", "self", "_bracket_sensitive_look_ahead_match", "segments", "seg_buff", "matchers", "end_bracket", "parse_context", "parse_context", "start_bracket", "start_bracket", "end_bracket", "end_bracket", "bracket_pairs_set", "self", "bracket_pairs_set", "if", "not", "end_match", "pragma", "no", "cover", "raise", "sqlparseerror", "couldn", "t", "find", "closing", "bracket", "for", "opening", "bracket", "segment", "start_match", "matched_segments", "0", "construct", "a", "bracket", "segment", "bracket_segment", "bracketedsegment", "segments", "start_match", "matched_segments", "content_segs", "end_match", "matched_segments", "start_bracket", "start_match", "matched_segments", "end_bracket", "end_match", "matched_segments", "trailing_segments", "end_match", "unmatched_segments", "then", "trim", "whitespace", "and", "deal", "with", "the", "case", "of", "non", "code", "content", "e", "g", "if", "self", "allow_gaps", "pre_segs", "content_segs", "post_segs", "trim_non_code_segments", "content_segs", "else", "pragma", "no", "cover", "todo", "pre_segs", "post_segs", "if", "we", "ve", "got", "a", "case", "of", "empty", "brackets", "check", "whether", "that", "is", "allowed", "if", "not", "content_segs", "if", "not", "self", "_elements", "or", "all", "e", "is_optional", "for", "e", "in", "self", "_elements", "and", "self", "allow_gaps", "or", "not", "pre_segs", "and", "not", "post_segs", "return", "matchresult", "bracket_segment", "if", "bracket_persists", "else", "bracket_segment", "segments", "trailing_segments", "else", "return", "matchresult", "from_unmatched", "segments", "match", "the", "content", "using", "super", "sequence", "will", "interpret", "the", "content", "of", "the", "elements", "with", "parse_context", "deeper_match", "as", "ctx", "content_match", "super", "match", "content_segs", "parse_context", "ctx", "we", "require", "a", "complete", "match", "for", "the", "content", "hopefully", "for", "obvious", "reasons", "if", "content_match", "is_complete", "reconstruct", "the", "bracket", "segment", "post", "match", "we", "need", "to", "realign", "the", "meta", "segments", "so", "the", "pos", "markers", "are", "correct", "have", "we", "already", "got", "indents", "meta_idx", "none", "for", "idx", "seg", "in", "enumerate", "bracket_segment", "segments", "if", "seg", "is_meta", "and", "cast", "metasegment", "seg", "indent_val", "0", "meta_idx", "idx", "break", "if", "we", "ve", "already", "got", "indents", "don", "t", "add", "more", "if", "meta_idx", "bracket_segment", "segments", "basesegment", "_position_segments", "bracket_segment", "start_bracket", "pre_segs", "content_match", "all_segments", "post_segs", "bracket_segment", "end_bracket", "append", "some", "indent", "and", "dedent", "tokens", "at", "the", "start", "and", "the", "end", "else", "bracket_segment", "segments", "basesegment", "_position_segments", "nb", "the", "nc", "segments", "go", "outside", "the", "indents", "bracket_segment", "start_bracket", "indent", "add", "a", "meta", "indent", "here", "pre_segs", "content_match", "all_segments", "post_segs", "dedent", "add", "a", "meta", "indent", "here", "bracket_segment", "end_bracket", "return", "matchresult", "bracket_segment", "if", "bracket_persists", "else", "bracket_segment", "segments", "trailing_segments", "no", "complete", "match", "fail", "else", "return", "matchresult", "from_unmatched", "segments"], "doc_len": 571}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def __init__(\n        self,\n        segments,\n        pos_marker=None,\n        name: Optional[str] = None,\n    ):\n        # A cache variable for expandable\n        self._is_expandable = None\n        # Surrogate name option.\n        self._surrogate_name = name\n\n        if len(segments) == 0:  # pragma: no cover\n            raise RuntimeError(\n                \"Setting {} with a zero length segment set. This shouldn't happen.\".format(\n                    self.__class__\n                )\n            )\n\n        if hasattr(segments, \"matched_segments\"):  # pragma: no cover TODO?\n            # Safely extract segments from a match\n            self.segments = segments.matched_segments\n        elif isinstance(segments, tuple):\n            self.segments = segments\n        elif isinstance(segments, list):\n            self.segments = tuple(segments)\n        else:  # pragma: no cover\n            raise TypeError(f\"Unexpected type passed to BaseSegment: {type(segments)}\")\n\n        if not pos_marker:\n            # If no pos given, it's the pos of the first segment.\n            if isinstance(segments, (tuple, list)):\n                pos_marker = PositionMarker.from_child_markers(\n                    *(seg.pos_marker for seg in segments)\n                )\n            else:  # pragma: no cover\n                raise TypeError(\n                    f\"Unexpected type passed to BaseSegment: {type(segments)}\"\n                )\n        self.pos_marker: PositionMarker = pos_marker\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "__init__", "self", "segments", "pos_marker", "none", "name", "optional", "str", "none", "a", "cache", "variable", "for", "expandable", "self", "_is_expandable", "none", "surrogate", "name", "option", "self", "_surrogate_name", "name", "if", "len", "segments", "0", "pragma", "no", "cover", "raise", "runtimeerror", "setting", "with", "a", "zero", "length", "segment", "set", "this", "shouldn", "t", "happen", "format", "self", "__class__", "if", "hasattr", "segments", "matched_segments", "pragma", "no", "cover", "todo", "safely", "extract", "segments", "from", "a", "match", "self", "segments", "segments", "matched_segments", "elif", "isinstance", "segments", "tuple", "self", "segments", "segments", "elif", "isinstance", "segments", "list", "self", "segments", "tuple", "segments", "else", "pragma", "no", "cover", "raise", "typeerror", "f", "unexpected", "type", "passed", "to", "basesegment", "type", "segments", "if", "not", "pos_marker", "if", "no", "pos", "given", "it", "s", "the", "pos", "of", "the", "first", "segment", "if", "isinstance", "segments", "tuple", "list", "pos_marker", "positionmarker", "from_child_markers", "seg", "pos_marker", "for", "seg", "in", "segments", "else", "pragma", "no", "cover", "raise", "typeerror", "f", "unexpected", "type", "passed", "to", "basesegment", "type", "segments", "self", "pos_marker", "positionmarker", "pos_marker"], "doc_len": 149}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.__eq__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "__eq__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def __eq__(self, other):\n        # NB: this should also work for RawSegment\n        return (\n            # Same class NAME. (could be constructed elsewhere)\n            self.__class__.__name__ == other.__class__.__name__\n            and (self.raw == other.raw)\n            # Both must have a non-null position marker to compare.\n            and self.pos_marker\n            and other.pos_marker\n            # We only match that the *start* is the same. This means we can\n            # still effectively construct searches look for segments.\n            # This is important for .apply_fixes().\n            and (\n                self.pos_marker.start_point_marker()\n                == other.pos_marker.start_point_marker()\n            )\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "__eq__", "self", "other", "nb", "this", "should", "also", "work", "for", "rawsegment", "return", "same", "class", "name", "could", "be", "constructed", "elsewhere", "self", "__class__", "__name__", "other", "__class__", "__name__", "and", "self", "raw", "other", "raw", "both", "must", "have", "a", "non", "null", "position", "marker", "to", "compare", "and", "self", "pos_marker", "and", "other", "pos_marker", "we", "only", "match", "that", "the", "start", "is", "the", "same", "this", "means", "we", "can", "still", "effectively", "construct", "searches", "look", "for", "segments", "this", "is", "important", "for", "apply_fixes", "and", "self", "pos_marker", "start_point_marker", "other", "pos_marker", "start_point_marker"], "doc_len": 86}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.__hash__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "__hash__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def __hash__(self):\n        return hash(\n            (self.__class__.__name__, self.raw, self.pos_marker.source_position())\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "__hash__", "self", "return", "hash", "self", "__class__", "__name__", "self", "raw", "self", "pos_marker", "source_position"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.__repr__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def __repr__(self):\n        return f\"<{self.__class__.__name__}: ({self.pos_marker})>\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "__repr__", "self", "return", "f", "self", "__class__", "__name__", "self", "pos_marker"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._comments", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_comments", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _comments(self):\n        \"\"\"Returns only the comment elements of this segment.\"\"\"\n        return [seg for seg in self.segments if seg.is_type(\"comment\")]\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_comments", "self", "returns", "only", "the", "comment", "elements", "of", "this", "segment", "return", "seg", "for", "seg", "in", "self", "segments", "if", "seg", "is_type", "comment"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._non_comments", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_non_comments", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _non_comments(self):  # pragma: no cover TODO?\n        \"\"\"Returns only the non-comment elements of this segment.\"\"\"\n        return [seg for seg in self.segments if not seg.is_type(\"comment\")]\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_non_comments", "self", "pragma", "no", "cover", "todo", "returns", "only", "the", "non", "comment", "elements", "of", "this", "segment", "return", "seg", "for", "seg", "in", "self", "segments", "if", "not", "seg", "is_type", "comment"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.name", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "name", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def name(self):\n        \"\"\"The name of this segment.\n\n        The reason for three routes for names is that some subclasses\n        might want to override the name rather than just getting\n        the class name. Instances may also override this with the\n        _surrogate_name.\n\n        Name should be specific to this kind of segment, while `type`\n        should be a higher level descriptor of the kind of segment.\n        For example, the name of `+` is 'plus' but the type might be\n        'binary_operator'.\n        \"\"\"\n        return self._surrogate_name or self._name or self.__class__.__name__\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "name", "self", "the", "name", "of", "this", "segment", "the", "reason", "for", "three", "routes", "for", "names", "is", "that", "some", "subclasses", "might", "want", "to", "override", "the", "name", "rather", "than", "just", "getting", "the", "class", "name", "instances", "may", "also", "override", "this", "with", "the", "_surrogate_name", "name", "should", "be", "specific", "to", "this", "kind", "of", "segment", "while", "type", "should", "be", "a", "higher", "level", "descriptor", "of", "the", "kind", "of", "segment", "for", "example", "the", "name", "of", "is", "plus", "but", "the", "type", "might", "be", "binary_operator", "return", "self", "_surrogate_name", "or", "self", "_name", "or", "self", "__class__", "__name__"], "doc_len": 93}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_expandable", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_expandable", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_expandable(self):\n        \"\"\"Return true if it is meaningful to call `expand` on this segment.\n\n        We need to do this recursively because even if *this* segment doesn't\n        need expanding, maybe one of its children does.\n\n        Once a segment is *not* expandable, it can never become so, which is\n        why the variable is cached.\n        \"\"\"\n        if self._is_expandable is False:\n            return self._is_expandable\n        elif self.parse_grammar:\n            return True\n        elif self.segments and any(s.is_expandable for s in self.segments):\n            return True\n        else:\n            # Cache the variable\n            self._is_expandable = False\n            return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_expandable", "self", "return", "true", "if", "it", "is", "meaningful", "to", "call", "expand", "on", "this", "segment", "we", "need", "to", "do", "this", "recursively", "because", "even", "if", "this", "segment", "doesn", "t", "need", "expanding", "maybe", "one", "of", "its", "children", "does", "once", "a", "segment", "is", "not", "expandable", "it", "can", "never", "become", "so", "which", "is", "why", "the", "variable", "is", "cached", "if", "self", "_is_expandable", "is", "false", "return", "self", "_is_expandable", "elif", "self", "parse_grammar", "return", "true", "elif", "self", "segments", "and", "any", "s", "is_expandable", "for", "s", "in", "self", "segments", "return", "true", "else", "cache", "the", "variable", "self", "_is_expandable", "false", "return", "false"], "doc_len": 98}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_code", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_code", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_code(self):\n        \"\"\"Return True if this segment contains any code.\"\"\"\n        return any(seg.is_code for seg in self.segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_code", "self", "return", "true", "if", "this", "segment", "contains", "any", "code", "return", "any", "seg", "is_code", "for", "seg", "in", "self", "segments"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_comment", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_comment", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_comment(self):  # pragma: no cover TODO?\n        \"\"\"Return True if this is entirely made of comments.\"\"\"\n        return all(seg.is_comment for seg in self.segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_comment", "self", "pragma", "no", "cover", "todo", "return", "true", "if", "this", "is", "entirely", "made", "of", "comments", "return", "all", "seg", "is_comment", "for", "seg", "in", "self", "segments"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_whitespace", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_whitespace", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_whitespace(self):\n        \"\"\"Return True if this segment is entirely whitespace.\"\"\"\n        return all(seg.is_whitespace for seg in self.segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_whitespace", "self", "return", "true", "if", "this", "segment", "is", "entirely", "whitespace", "return", "all", "seg", "is_whitespace", "for", "seg", "in", "self", "segments"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.raw", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "raw", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def raw(self):\n        \"\"\"Make a string from the segments of this segment.\"\"\"\n        return self._reconstruct()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "raw", "self", "make", "a", "string", "from", "the", "segments", "of", "this", "segment", "return", "self", "_reconstruct"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.raw_upper", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "raw_upper", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def raw_upper(self):\n        \"\"\"Make an uppercase string from the segments of this segment.\"\"\"\n        return self._reconstruct().upper()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "raw_upper", "self", "make", "an", "uppercase", "string", "from", "the", "segments", "of", "this", "segment", "return", "self", "_reconstruct", "upper"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.matched_length", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "matched_length", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def matched_length(self):\n        \"\"\"Return the length of the segment in characters.\"\"\"\n        return sum(seg.matched_length for seg in self.segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "matched_length", "self", "return", "the", "length", "of", "the", "segment", "in", "characters", "return", "sum", "seg", "matched_length", "for", "seg", "in", "self", "segments"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.segs_to_tuple", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "segs_to_tuple", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def segs_to_tuple(segs, **kwargs):  # pragma: no cover TODO?\n        \"\"\"Return a tuple structure from an iterable of segments.\"\"\"\n        return tuple(seg.to_tuple(**kwargs) for seg in segs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "segs_to_tuple", "segs", "kwargs", "pragma", "no", "cover", "todo", "return", "a", "tuple", "structure", "from", "an", "iterable", "of", "segments", "return", "tuple", "seg", "to_tuple", "kwargs", "for", "seg", "in", "segs"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._suffix", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_suffix", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _suffix():\n        \"\"\"Return any extra output required at the end when logging.\n\n        NB Override this for specific subclasses if we want extra output.\n        \"\"\"\n        return \"\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_suffix", "return", "any", "extra", "output", "required", "at", "the", "end", "when", "logging", "nb", "override", "this", "for", "specific", "subclasses", "if", "we", "want", "extra", "output", "return"], "doc_len": 32}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.expand", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "expand", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def expand(segments, parse_context):\n        \"\"\"Expand the list of child segments using their `parse` methods.\"\"\"\n        segs = ()\n        for stmt in segments:\n            try:\n                if not stmt.is_expandable:\n                    parse_context.logger.info(\n                        \"[PD:%s] Skipping expansion of %s...\",\n                        parse_context.parse_depth,\n                        stmt,\n                    )\n                    segs += (stmt,)\n                    continue\n            except Exception as err:  # pragma: no cover\n                parse_context.logger.error(\n                    \"%s has no attribute `is_expandable`. This segment appears poorly constructed.\",\n                    stmt,\n                )\n                raise err\n            if not hasattr(stmt, \"parse\"):  # pragma: no cover\n                raise ValueError(\n                    \"{} has no method `parse`. This segment appears poorly constructed.\".format(\n                        stmt\n                    )\n                )\n            parse_depth_msg = \"Parse Depth {}. Expanding: {}: {!r}\".format(\n                parse_context.parse_depth,\n                stmt.__class__.__name__,\n                curtail_string(stmt.raw, length=40),\n            )\n            parse_context.logger.info(frame_msg(parse_depth_msg))\n            res = stmt.parse(parse_context=parse_context)\n            if isinstance(res, BaseSegment):\n                segs += (res,)\n            else:\n                # We might get back an iterable of segments\n                segs += tuple(res)\n        # Basic Validation\n        check_still_complete(segments, segs, ())\n        return segs\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "expand", "segments", "parse_context", "expand", "the", "list", "of", "child", "segments", "using", "their", "parse", "methods", "segs", "for", "stmt", "in", "segments", "try", "if", "not", "stmt", "is_expandable", "parse_context", "logger", "info", "pd", "s", "skipping", "expansion", "of", "s", "parse_context", "parse_depth", "stmt", "segs", "stmt", "continue", "except", "exception", "as", "err", "pragma", "no", "cover", "parse_context", "logger", "error", "s", "has", "no", "attribute", "is_expandable", "this", "segment", "appears", "poorly", "constructed", "stmt", "raise", "err", "if", "not", "hasattr", "stmt", "parse", "pragma", "no", "cover", "raise", "valueerror", "has", "no", "method", "parse", "this", "segment", "appears", "poorly", "constructed", "format", "stmt", "parse_depth_msg", "parse", "depth", "expanding", "r", "format", "parse_context", "parse_depth", "stmt", "__class__", "__name__", "curtail_string", "stmt", "raw", "length", "40", "parse_context", "logger", "info", "frame_msg", "parse_depth_msg", "res", "stmt", "parse", "parse_context", "parse_context", "if", "isinstance", "res", "basesegment", "segs", "res", "else", "we", "might", "get", "back", "an", "iterable", "of", "segments", "segs", "tuple", "res", "basic", "validation", "check_still_complete", "segments", "segs", "return", "segs"], "doc_len": 142}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._position_segments", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_position_segments", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _position_segments(cls, segments, parent_pos=None):\n        \"\"\"Refresh positions of segments within a span.\n\n        This does two things:\n        - Assign positions to any segments without them.\n        - Updates the working line_no and line_pos for all\n          segments during fixing.\n\n        New segments are assumed to be metas or insertions\n        and so therefore have a zero-length position in the\n        source and templated file.\n        \"\"\"\n        # If there are no segments, there's no need to reposition.\n        if not segments:\n            return segments\n\n        # Work out our starting position for working through\n        if parent_pos:\n            line_no = parent_pos.working_line_no\n            line_pos = parent_pos.working_line_pos\n        # If we don't have it, infer it from the first position\n        # in this segment that does have a position.\n        else:\n            for fwd_seg in segments:\n                if fwd_seg.pos_marker:\n                    line_no = fwd_seg.pos_marker.working_line_no\n                    line_pos = fwd_seg.pos_marker.working_line_pos\n                    break\n            else:  # pragma: no cover\n                linter_logger.warning(\"SEG: %r, POS: %r\", segments, parent_pos)\n                raise ValueError(\"Unable to find working position.\")\n\n        # Use the index so that we can look forward\n        # and backward.\n        for idx, segment in enumerate(segments):\n            # Fill any that don't have a position.\n            if not segment.pos_marker:\n                # Can we get a position from the previous?\n                if idx > 0:\n                    segment.pos_marker = segments[idx - 1].pos_marker.end_point_marker()\n                # Can we get it from the parent?\n                elif parent_pos:\n                    segment.pos_marker = parent_pos.start_point_marker()\n                # Search forward for a following one, if we have to?\n                else:\n                    for fwd_seg in segments[idx + 1 :]:\n                        if fwd_seg.pos_marker:\n                            segments[\n                                idx\n                            ].pos_marker = fwd_seg.pos_marker.start_point_marker()\n                            break\n                    else:  # pragma: no cover\n                        raise ValueError(\"Unable to position new segment\")\n\n            # Update the working position.\n            segment.pos_marker = segment.pos_marker.with_working_position(\n                line_no,\n                line_pos,\n            )\n            line_no, line_pos = segment.pos_marker.infer_next_position(\n                segment.raw, line_no, line_pos\n            )\n\n            # If this segment has children, recurse and reposition them too.\n            if segment.segments:\n                segment.segments = cls._position_segments(\n                    segment.segments, parent_pos=segment.pos_marker\n                )\n\n        return segments\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_position_segments", "cls", "segments", "parent_pos", "none", "refresh", "positions", "of", "segments", "within", "a", "span", "this", "does", "two", "things", "assign", "positions", "to", "any", "segments", "without", "them", "updates", "the", "working", "line_no", "and", "line_pos", "for", "all", "segments", "during", "fixing", "new", "segments", "are", "assumed", "to", "be", "metas", "or", "insertions", "and", "so", "therefore", "have", "a", "zero", "length", "position", "in", "the", "source", "and", "templated", "file", "if", "there", "are", "no", "segments", "there", "s", "no", "need", "to", "reposition", "if", "not", "segments", "return", "segments", "work", "out", "our", "starting", "position", "for", "working", "through", "if", "parent_pos", "line_no", "parent_pos", "working_line_no", "line_pos", "parent_pos", "working_line_pos", "if", "we", "don", "t", "have", "it", "infer", "it", "from", "the", "first", "position", "in", "this", "segment", "that", "does", "have", "a", "position", "else", "for", "fwd_seg", "in", "segments", "if", "fwd_seg", "pos_marker", "line_no", "fwd_seg", "pos_marker", "working_line_no", "line_pos", "fwd_seg", "pos_marker", "working_line_pos", "break", "else", "pragma", "no", "cover", "linter_logger", "warning", "seg", "r", "pos", "r", "segments", "parent_pos", "raise", "valueerror", "unable", "to", "find", "working", "position", "use", "the", "index", "so", "that", "we", "can", "look", "forward", "and", "backward", "for", "idx", "segment", "in", "enumerate", "segments", "fill", "any", "that", "don", "t", "have", "a", "position", "if", "not", "segment", "pos_marker", "can", "we", "get", "a", "position", "from", "the", "previous", "if", "idx", "0", "segment", "pos_marker", "segments", "idx", "1", "pos_marker", "end_point_marker", "can", "we", "get", "it", "from", "the", "parent", "elif", "parent_pos", "segment", "pos_marker", "parent_pos", "start_point_marker", "search", "forward", "for", "a", "following", "one", "if", "we", "have", "to", "else", "for", "fwd_seg", "in", "segments", "idx", "1", "if", "fwd_seg", "pos_marker", "segments", "idx", "pos_marker", "fwd_seg", "pos_marker", "start_point_marker", "break", "else", "pragma", "no", "cover", "raise", "valueerror", "unable", "to", "position", "new", "segment", "update", "the", "working", "position", "segment", "pos_marker", "segment", "pos_marker", "with_working_position", "line_no", "line_pos", "line_no", "line_pos", "segment", "pos_marker", "infer_next_position", "segment", "raw", "line_no", "line_pos", "if", "this", "segment", "has", "children", "recurse", "and", "reposition", "them", "too", "if", "segment", "segments", "segment", "segments", "cls", "_position_segments", "segment", "segments", "parent_pos", "segment", "pos_marker", "return", "segments"], "doc_len": 296}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.simple", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def simple(cls, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support an uppercase hash matching route?\n\n        This should be true if the MATCH grammar is simple. Most more\n        complicated segments will be assumed to overwrite this method\n        if they wish to be considered simple.\n        \"\"\"\n        if cls.match_grammar:\n            return cls.match_grammar.simple(parse_context=parse_context)\n        else:  # pragma: no cover TODO?\n            # Other segments will either override this method, or aren't\n            # simple.\n            return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "simple", "cls", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "an", "uppercase", "hash", "matching", "route", "this", "should", "be", "true", "if", "the", "match", "grammar", "is", "simple", "most", "more", "complicated", "segments", "will", "be", "assumed", "to", "overwrite", "this", "method", "if", "they", "wish", "to", "be", "considered", "simple", "if", "cls", "match_grammar", "return", "cls", "match_grammar", "simple", "parse_context", "parse_context", "else", "pragma", "no", "cover", "todo", "other", "segments", "will", "either", "override", "this", "method", "or", "aren", "t", "simple", "return", "none"], "doc_len": 80}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_optional", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_optional", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_optional(cls):\n        \"\"\"Return True if this segment is optional.\n\n        This is used primarily in sequence matching, where optional\n        segments can be skipped.\n        \"\"\"\n        return cls.optional\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_optional", "cls", "return", "true", "if", "this", "segment", "is", "optional", "this", "is", "used", "primarily", "in", "sequence", "matching", "where", "optional", "segments", "can", "be", "skipped", "return", "cls", "optional"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.class_is_type", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "class_is_type", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def class_is_type(cls, *seg_type):\n        \"\"\"Is this segment class (or its parent) of the given type.\"\"\"\n        # Do we match on the type of _this_ class.\n        if cls.type in seg_type:\n            return True\n        # If not, check types of parents.\n        for base_class in cls.__bases__:\n            if base_class is object:\n                break\n            elif base_class.type in seg_type:\n                return True\n            elif base_class.type == \"base\":\n                break\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "class_is_type", "cls", "seg_type", "is", "this", "segment", "class", "or", "its", "parent", "of", "the", "given", "type", "do", "we", "match", "on", "the", "type", "of", "_this_", "class", "if", "cls", "type", "in", "seg_type", "return", "true", "if", "not", "check", "types", "of", "parents", "for", "base_class", "in", "cls", "__bases__", "if", "base_class", "is", "object", "break", "elif", "base_class", "type", "in", "seg_type", "return", "true", "elif", "base_class", "type", "base", "break", "return", "false"], "doc_len": 69}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.structural_simplify", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "structural_simplify", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def structural_simplify(cls, elem):\n        \"\"\"Simplify the structure recursively so it serializes nicely in json/yaml.\"\"\"\n        if len(elem) == 0:\n            return None\n        elif isinstance(elem, tuple):\n            # Does this look like an element?\n            if len(elem) == 2 and isinstance(elem[0], str):\n                # This looks like a single element, make a dict\n                elem = {elem[0]: cls.structural_simplify(elem[1])}\n            elif isinstance(elem[0], tuple):\n                # This looks like a list of elements.\n                keys = [e[0] for e in elem]\n                # Any duplicate elements?\n                if len(set(keys)) == len(keys):\n                    # No, we can use a mapping tuple\n                    elem = {e[0]: cls.structural_simplify(e[1]) for e in elem}\n                else:\n                    # Yes, this has to be a list :(\n                    elem = [cls.structural_simplify(e) for e in elem]\n        return elem\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "structural_simplify", "cls", "elem", "simplify", "the", "structure", "recursively", "so", "it", "serializes", "nicely", "in", "json", "yaml", "if", "len", "elem", "0", "return", "none", "elif", "isinstance", "elem", "tuple", "does", "this", "look", "like", "an", "element", "if", "len", "elem", "2", "and", "isinstance", "elem", "0", "str", "this", "looks", "like", "a", "single", "element", "make", "a", "dict", "elem", "elem", "0", "cls", "structural_simplify", "elem", "1", "elif", "isinstance", "elem", "0", "tuple", "this", "looks", "like", "a", "list", "of", "elements", "keys", "e", "0", "for", "e", "in", "elem", "any", "duplicate", "elements", "if", "len", "set", "keys", "len", "keys", "no", "we", "can", "use", "a", "mapping", "tuple", "elem", "e", "0", "cls", "structural_simplify", "e", "1", "for", "e", "in", "elem", "else", "yes", "this", "has", "to", "be", "a", "list", "elem", "cls", "structural_simplify", "e", "for", "e", "in", "elem", "return", "elem"], "doc_len": 128}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.match", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def match(\n        cls, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Match a list of segments against this segment.\n\n        Note: Match for segments is done in the ABSTRACT.\n        When dealing with concrete then we're always in parse.\n        Parse is what happens during expand.\n\n        Matching can be done from either the raw or the segments.\n        This raw function can be overridden, or a grammar defined\n        on the underlying class.\n        \"\"\"\n        # Edge case, but it's possible that we have *already matched* on\n        # a previous cycle. Do should first check whether this is a case\n        # of that.\n        if len(segments) == 1 and isinstance(segments[0], cls):\n            # This has already matched. Winner.\n            parse_match_logging(\n                cls.__name__,\n                \"_match\",\n                \"SELF\",\n                parse_context=parse_context,\n                v_level=3,\n                symbol=\"+++\",\n            )\n            return MatchResult.from_matched(segments)\n        elif len(segments) > 1 and isinstance(segments[0], cls):\n            parse_match_logging(\n                cls.__name__,\n                \"_match\",\n                \"SELF\",\n                parse_context=parse_context,\n                v_level=3,\n                symbol=\"+++\",\n            )\n            # This has already matched, but only partially.\n            return MatchResult((segments[0],), segments[1:])\n\n        if cls.match_grammar:\n            # Call the private method\n            with parse_context.deeper_match() as ctx:\n                m = cls.match_grammar.match(segments=segments, parse_context=ctx)\n\n            # Calling unify here, allows the MatchResult class to do all the type checking.\n            if not isinstance(m, MatchResult):  # pragma: no cover\n                raise TypeError(\n                    \"[PD:{} MD:{}] {}.match. Result is {}, not a MatchResult!\".format(\n                        parse_context.parse_depth,\n                        parse_context.match_depth,\n                        cls.__name__,\n                        type(m),\n                    )\n                )\n            # Once unified we can deal with it just as a MatchResult\n            if m.has_match():\n                return MatchResult(\n                    (cls(segments=m.matched_segments),), m.unmatched_segments\n                )\n            else:\n                return MatchResult.from_unmatched(segments)\n        else:  # pragma: no cover\n            raise NotImplementedError(\n                f\"{cls.__name__} has no match function implemented\"\n            )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "match", "cls", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "match", "a", "list", "of", "segments", "against", "this", "segment", "note", "match", "for", "segments", "is", "done", "in", "the", "abstract", "when", "dealing", "with", "concrete", "then", "we", "re", "always", "in", "parse", "parse", "is", "what", "happens", "during", "expand", "matching", "can", "be", "done", "from", "either", "the", "raw", "or", "the", "segments", "this", "raw", "function", "can", "be", "overridden", "or", "a", "grammar", "defined", "on", "the", "underlying", "class", "edge", "case", "but", "it", "s", "possible", "that", "we", "have", "already", "matched", "on", "a", "previous", "cycle", "do", "should", "first", "check", "whether", "this", "is", "a", "case", "of", "that", "if", "len", "segments", "1", "and", "isinstance", "segments", "0", "cls", "this", "has", "already", "matched", "winner", "parse_match_logging", "cls", "__name__", "_match", "self", "parse_context", "parse_context", "v_level", "3", "symbol", "return", "matchresult", "from_matched", "segments", "elif", "len", "segments", "1", "and", "isinstance", "segments", "0", "cls", "parse_match_logging", "cls", "__name__", "_match", "self", "parse_context", "parse_context", "v_level", "3", "symbol", "this", "has", "already", "matched", "but", "only", "partially", "return", "matchresult", "segments", "0", "segments", "1", "if", "cls", "match_grammar", "call", "the", "private", "method", "with", "parse_context", "deeper_match", "as", "ctx", "m", "cls", "match_grammar", "match", "segments", "segments", "parse_context", "ctx", "calling", "unify", "here", "allows", "the", "matchresult", "class", "to", "do", "all", "the", "type", "checking", "if", "not", "isinstance", "m", "matchresult", "pragma", "no", "cover", "raise", "typeerror", "pd", "md", "match", "result", "is", "not", "a", "matchresult", "format", "parse_context", "parse_depth", "parse_context", "match_depth", "cls", "__name__", "type", "m", "once", "unified", "we", "can", "deal", "with", "it", "just", "as", "a", "matchresult", "if", "m", "has_match", "return", "matchresult", "cls", "segments", "m", "matched_segments", "m", "unmatched_segments", "else", "return", "matchresult", "from_unmatched", "segments", "else", "pragma", "no", "cover", "raise", "notimplementederror", "f", "cls", "__name__", "has", "no", "match", "function", "implemented"], "doc_len": 262}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._reconstruct", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_reconstruct", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _reconstruct(self):\n        \"\"\"Make a string from the segments of this segment.\"\"\"\n        return \"\".join(seg.raw for seg in self.segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_reconstruct", "self", "make", "a", "string", "from", "the", "segments", "of", "this", "segment", "return", "join", "seg", "raw", "for", "seg", "in", "self", "segments"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment._preface", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "_preface", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def _preface(self, ident, tabsize):\n        \"\"\"Returns the preamble to any logging.\"\"\"\n        padded_type = \"{padding}{modifier}{type}\".format(\n            padding=\" \" * (ident * tabsize),\n            modifier=\"[META] \" if self.is_meta else \"\",\n            type=self.get_type() + \":\",\n        )\n        preface = \"{pos:20}|{padded_type:60}  {suffix}\".format(\n            pos=str(self.pos_marker) if self.pos_marker else \"-\",\n            padded_type=padded_type,\n            suffix=self._suffix() or \"\",\n        )\n        # Trim unnecessary whitespace before returning\n        return preface.rstrip()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "_preface", "self", "ident", "tabsize", "returns", "the", "preamble", "to", "any", "logging", "padded_type", "padding", "modifier", "type", "format", "padding", "ident", "tabsize", "modifier", "meta", "if", "self", "is_meta", "else", "type", "self", "get_type", "preface", "pos", "20", "padded_type", "60", "suffix", "format", "pos", "str", "self", "pos_marker", "if", "self", "pos_marker", "else", "padded_type", "padded_type", "suffix", "self", "_suffix", "or", "trim", "unnecessary", "whitespace", "before", "returning", "return", "preface", "rstrip"], "doc_len": 65}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_type", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_type", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_type(self):\n        \"\"\"Returns the type of this segment as a string.\"\"\"\n        return self.type\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_type", "self", "returns", "the", "type", "of", "this", "segment", "as", "a", "string", "return", "self", "type"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_type", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_type", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_type(self, *seg_type):\n        \"\"\"Is this segment (or its parent) of the given type.\"\"\"\n        return self.class_is_type(*seg_type)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_type", "self", "seg_type", "is", "this", "segment", "or", "its", "parent", "of", "the", "given", "type", "return", "self", "class_is_type", "seg_type"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.invalidate_caches", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "invalidate_caches", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def invalidate_caches(self):\n        \"\"\"Invalidate the cached properties.\n\n        This should be called whenever the segments within this\n        segment is mutated.\n        \"\"\"\n        for key in [\"is_code\", \"is_comment\", \"raw\", \"raw_upper\", \"matched_length\"]:\n            self.__dict__.pop(key, None)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "invalidate_caches", "self", "invalidate", "the", "cached", "properties", "this", "should", "be", "called", "whenever", "the", "segments", "within", "this", "segment", "is", "mutated", "for", "key", "in", "is_code", "is_comment", "raw", "raw_upper", "matched_length", "self", "__dict__", "pop", "key", "none"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_start_point_marker", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_start_point_marker", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_start_point_marker(self):\n        \"\"\"Get a point marker at the start of this segment.\"\"\"\n        return self.pos_marker.start_point_marker()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_start_point_marker", "self", "get", "a", "point", "marker", "at", "the", "start", "of", "this", "segment", "return", "self", "pos_marker", "start_point_marker"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_end_point_marker", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_end_point_marker", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_end_point_marker(self):\n        \"\"\"Get a point marker at the end of this segment.\"\"\"\n        return self.pos_marker.end_point_marker()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_end_point_marker", "self", "get", "a", "point", "marker", "at", "the", "end", "of", "this", "segment", "return", "self", "pos_marker", "end_point_marker"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_start_loc", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_start_loc", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_start_loc(self):\n        \"\"\"Get a location tuple at the start of this segment.\"\"\"\n        return self.pos_marker.working_loc\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_start_loc", "self", "get", "a", "location", "tuple", "at", "the", "start", "of", "this", "segment", "return", "self", "pos_marker", "working_loc"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_end_loc", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_end_loc", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_end_loc(self):\n        \"\"\"Get a location tuple at the end of this segment.\"\"\"\n        return self.pos_marker.working_loc_after(\n            self.raw,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_end_loc", "self", "get", "a", "location", "tuple", "at", "the", "end", "of", "this", "segment", "return", "self", "pos_marker", "working_loc_after", "self", "raw"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.stringify", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "stringify", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def stringify(self, ident=0, tabsize=4, code_only=False):\n        \"\"\"Use indentation to render this segment and its children as a string.\"\"\"\n        buff = StringIO()\n        preface = self._preface(ident=ident, tabsize=tabsize)\n        buff.write(preface + \"\\n\")\n        if not code_only and self.comment_separate and len(self._comments) > 0:\n            if self._comments:  # pragma: no cover TODO?\n                buff.write((\" \" * ((ident + 1) * tabsize)) + \"Comments:\" + \"\\n\")\n                for seg in self._comments:\n                    buff.write(\n                        seg.stringify(\n                            ident=ident + 2,\n                            tabsize=tabsize,\n                            code_only=code_only,\n                        )\n                    )\n            if self._non_comments:  # pragma: no cover TODO?\n                buff.write((\" \" * ((ident + 1) * tabsize)) + \"Code:\" + \"\\n\")\n                for seg in self._non_comments:\n                    buff.write(\n                        seg.stringify(\n                            ident=ident + 2,\n                            tabsize=tabsize,\n                            code_only=code_only,\n                        )\n                    )\n        else:\n            for seg in self.segments:\n                # If we're in code_only, only show the code segments, otherwise always true\n                if not code_only or seg.is_code:\n                    buff.write(\n                        seg.stringify(\n                            ident=ident + 1,\n                            tabsize=tabsize,\n                            code_only=code_only,\n                        )\n                    )\n        return buff.getvalue()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "stringify", "self", "ident", "0", "tabsize", "4", "code_only", "false", "use", "indentation", "to", "render", "this", "segment", "and", "its", "children", "as", "a", "string", "buff", "stringio", "preface", "self", "_preface", "ident", "ident", "tabsize", "tabsize", "buff", "write", "preface", "n", "if", "not", "code_only", "and", "self", "comment_separate", "and", "len", "self", "_comments", "0", "if", "self", "_comments", "pragma", "no", "cover", "todo", "buff", "write", "ident", "1", "tabsize", "comments", "n", "for", "seg", "in", "self", "_comments", "buff", "write", "seg", "stringify", "ident", "ident", "2", "tabsize", "tabsize", "code_only", "code_only", "if", "self", "_non_comments", "pragma", "no", "cover", "todo", "buff", "write", "ident", "1", "tabsize", "code", "n", "for", "seg", "in", "self", "_non_comments", "buff", "write", "seg", "stringify", "ident", "ident", "2", "tabsize", "tabsize", "code_only", "code_only", "else", "for", "seg", "in", "self", "segments", "if", "we", "re", "in", "code_only", "only", "show", "the", "code", "segments", "otherwise", "always", "true", "if", "not", "code_only", "or", "seg", "is_code", "buff", "write", "seg", "stringify", "ident", "ident", "1", "tabsize", "tabsize", "code_only", "code_only", "return", "buff", "getvalue"], "doc_len": 152}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.to_tuple", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "to_tuple", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def to_tuple(self, code_only=False, show_raw=False, include_meta=False):\n        \"\"\"Return a tuple structure from this segment.\"\"\"\n        # works for both base and raw\n\n        if show_raw and not self.segments:\n            result = (self.get_type(), self.raw)\n        elif code_only:\n            result = (\n                self.get_type(),\n                tuple(\n                    seg.to_tuple(\n                        code_only=code_only,\n                        show_raw=show_raw,\n                        include_meta=include_meta,\n                    )\n                    for seg in self.segments\n                    if seg.is_code and not seg.is_meta\n                ),\n            )\n        else:\n            result = (\n                self.get_type(),\n                tuple(\n                    seg.to_tuple(\n                        code_only=code_only,\n                        show_raw=show_raw,\n                        include_meta=include_meta,\n                    )\n                    for seg in self.segments\n                    if include_meta or not seg.is_meta\n                ),\n            )\n        return result\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "to_tuple", "self", "code_only", "false", "show_raw", "false", "include_meta", "false", "return", "a", "tuple", "structure", "from", "this", "segment", "works", "for", "both", "base", "and", "raw", "if", "show_raw", "and", "not", "self", "segments", "result", "self", "get_type", "self", "raw", "elif", "code_only", "result", "self", "get_type", "tuple", "seg", "to_tuple", "code_only", "code_only", "show_raw", "show_raw", "include_meta", "include_meta", "for", "seg", "in", "self", "segments", "if", "seg", "is_code", "and", "not", "seg", "is_meta", "else", "result", "self", "get_type", "tuple", "seg", "to_tuple", "code_only", "code_only", "show_raw", "show_raw", "include_meta", "include_meta", "for", "seg", "in", "self", "segments", "if", "include_meta", "or", "not", "seg", "is_meta", "return", "result"], "doc_len": 93}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.as_record", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "as_record", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def as_record(self, **kwargs):\n        \"\"\"Return the segment as a structurally simplified record.\n\n        This is useful for serialization to yaml or json.\n        kwargs passed to to_tuple\n        \"\"\"\n        return self.structural_simplify(self.to_tuple(**kwargs))\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "as_record", "self", "kwargs", "return", "the", "segment", "as", "a", "structurally", "simplified", "record", "this", "is", "useful", "for", "serialization", "to", "yaml", "or", "json", "kwargs", "passed", "to", "to_tuple", "return", "self", "structural_simplify", "self", "to_tuple", "kwargs"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.raw_list", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "raw_list", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def raw_list(self):  # pragma: no cover TODO?\n        \"\"\"Return a list of raw elements, mostly for testing or searching.\"\"\"\n        buff = []\n        for s in self.segments:\n            buff += s.raw_list()\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "raw_list", "self", "pragma", "no", "cover", "todo", "return", "a", "list", "of", "raw", "elements", "mostly", "for", "testing", "or", "searching", "buff", "for", "s", "in", "self", "segments", "buff", "s", "raw_list", "return", "buff"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.iter_raw_seg", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "iter_raw_seg", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def iter_raw_seg(self):\n        \"\"\"Iterate raw segments, mostly for searching.\"\"\"\n        for s in self.segments:\n            yield from s.iter_raw_seg()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "iter_raw_seg", "self", "iterate", "raw", "segments", "mostly", "for", "searching", "for", "s", "in", "self", "segments", "yield", "from", "s", "iter_raw_seg"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.iter_segments", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "iter_segments", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def iter_segments(self, expanding=None, pass_through=False):\n        \"\"\"Iterate raw segments, optionally expanding some chldren.\"\"\"\n        for s in self.segments:\n            if expanding and s.is_type(*expanding):\n                yield from s.iter_segments(\n                    expanding=expanding if pass_through else None\n                )\n            else:\n                yield s\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "iter_segments", "self", "expanding", "none", "pass_through", "false", "iterate", "raw", "segments", "optionally", "expanding", "some", "chldren", "for", "s", "in", "self", "segments", "if", "expanding", "and", "s", "is_type", "expanding", "yield", "from", "s", "iter_segments", "expanding", "expanding", "if", "pass_through", "else", "none", "else", "yield", "s"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.iter_unparsables", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "iter_unparsables", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def iter_unparsables(self):\n        \"\"\"Iterate through any unparsables this segment may contain.\"\"\"\n        for s in self.segments:\n            yield from s.iter_unparsables()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "iter_unparsables", "self", "iterate", "through", "any", "unparsables", "this", "segment", "may", "contain", "for", "s", "in", "self", "segments", "yield", "from", "s", "iter_unparsables"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.type_set", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "type_set", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def type_set(self):\n        \"\"\"Return a set of the types contained, mostly for testing.\"\"\"\n        typs = {self.type}\n        for s in self.segments:\n            typs |= s.type_set()\n        return typs\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "type_set", "self", "return", "a", "set", "of", "the", "types", "contained", "mostly", "for", "testing", "typs", "self", "type", "for", "s", "in", "self", "segments", "typs", "s", "type_set", "return", "typs"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.is_raw", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "is_raw", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def is_raw(self):\n        \"\"\"Return True if this segment has no children.\"\"\"\n        return len(self.segments) == 0\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "is_raw", "self", "return", "true", "if", "this", "segment", "has", "no", "children", "return", "len", "self", "segments", "0"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_child", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_child", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_child(self, *seg_type):\n        \"\"\"Retrieve the first of the children of this segment with matching type.\"\"\"\n        for seg in self.segments:\n            if seg.is_type(*seg_type):\n                return seg\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_child", "self", "seg_type", "retrieve", "the", "first", "of", "the", "children", "of", "this", "segment", "with", "matching", "type", "for", "seg", "in", "self", "segments", "if", "seg", "is_type", "seg_type", "return", "seg", "return", "none"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.get_children", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "get_children", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def get_children(self, *seg_type):\n        \"\"\"Retrieve the all of the children of this segment with matching type.\"\"\"\n        buff = []\n        for seg in self.segments:\n            if seg.is_type(*seg_type):\n                buff.append(seg)\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "get_children", "self", "seg_type", "retrieve", "the", "all", "of", "the", "children", "of", "this", "segment", "with", "matching", "type", "buff", "for", "seg", "in", "self", "segments", "if", "seg", "is_type", "seg_type", "buff", "append", "seg", "return", "buff"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.select_children", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "select_children", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def select_children(\n        self,\n        start_seg: Optional[\"BaseSegment\"] = None,\n        stop_seg: Optional[\"BaseSegment\"] = None,\n        select_if: Optional[Callable[[\"BaseSegment\"], Any]] = None,\n        loop_while: Optional[Callable[[\"BaseSegment\"], Any]] = None,\n    ):\n        \"\"\"Retrieve subset of children based on range and filters.\n\n        Often useful by linter rules when generating fixes, e.g. to find\n        whitespace segments between two already known segments.\n        \"\"\"\n        start_index = self.segments.index(start_seg) if start_seg else -1\n        stop_index = self.segments.index(stop_seg) if stop_seg else len(self.segments)\n        buff = []\n        for seg in self.segments[start_index + 1 : stop_index]:\n            if loop_while and not loop_while(seg):\n                break\n            if not select_if or select_if(seg):\n                buff.append(seg)\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "select_children", "self", "start_seg", "optional", "basesegment", "none", "stop_seg", "optional", "basesegment", "none", "select_if", "optional", "callable", "basesegment", "any", "none", "loop_while", "optional", "callable", "basesegment", "any", "none", "retrieve", "subset", "of", "children", "based", "on", "range", "and", "filters", "often", "useful", "by", "linter", "rules", "when", "generating", "fixes", "e", "g", "to", "find", "whitespace", "segments", "between", "two", "already", "known", "segments", "start_index", "self", "segments", "index", "start_seg", "if", "start_seg", "else", "1", "stop_index", "self", "segments", "index", "stop_seg", "if", "stop_seg", "else", "len", "self", "segments", "buff", "for", "seg", "in", "self", "segments", "start_index", "1", "stop_index", "if", "loop_while", "and", "not", "loop_while", "seg", "break", "if", "not", "select_if", "or", "select_if", "seg", "buff", "append", "seg", "return", "buff"], "doc_len": 106}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.recursive_crawl", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "recursive_crawl", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def recursive_crawl(self, *seg_type, recurse_into=True):\n        \"\"\"Recursively crawl for segments of a given type.\n\n        Args:\n            seg_type: :obj:`str`: one or more type of segment\n                to look for.\n            recurse_into: :obj:`bool`: When an element of type \"seg_type\" is\n                found, whether to recurse into it.\n        \"\"\"\n        # Check this segment\n        if self.is_type(*seg_type):\n            match = True\n            yield self\n        else:\n            match = False\n        if recurse_into or not match:\n            # Recurse\n            for seg in self.segments:\n                yield from seg.recursive_crawl(*seg_type, recurse_into=recurse_into)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "recursive_crawl", "self", "seg_type", "recurse_into", "true", "recursively", "crawl", "for", "segments", "of", "a", "given", "type", "args", "seg_type", "obj", "str", "one", "or", "more", "type", "of", "segment", "to", "look", "for", "recurse_into", "obj", "bool", "when", "an", "element", "of", "type", "seg_type", "is", "found", "whether", "to", "recurse", "into", "it", "check", "this", "segment", "if", "self", "is_type", "seg_type", "match", "true", "yield", "self", "else", "match", "false", "if", "recurse_into", "or", "not", "match", "recurse", "for", "seg", "in", "self", "segments", "yield", "from", "seg", "recursive_crawl", "seg_type", "recurse_into", "recurse_into"], "doc_len": 83}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.path_to", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "path_to", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def path_to(self, other):\n        \"\"\"Given a segment which is assumed within self, get the intermediate segments.\n\n        Returns:\n            :obj:`list` of segments, including the segment we're looking for.\n            None if not found.\n\n        \"\"\"\n        # Return self if we've found the segment.\n        if self is other:\n            return [self]\n\n        # Are we in the right ballpark?\n        # NB: Comparisons have a higher precedence than `not`.\n        if not self.get_start_loc() <= other.get_start_loc() <= self.get_end_loc():\n            return None\n\n        # Do we have any child segments at all?\n        if not self.segments:\n            return None\n\n        # Check through each of the child segments\n        for seg in self.segments:\n            res = seg.path_to(other)\n            if res:\n                return [self] + res\n        return None  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "path_to", "self", "other", "given", "a", "segment", "which", "is", "assumed", "within", "self", "get", "the", "intermediate", "segments", "returns", "obj", "list", "of", "segments", "including", "the", "segment", "we", "re", "looking", "for", "none", "if", "not", "found", "return", "self", "if", "we", "ve", "found", "the", "segment", "if", "self", "is", "other", "return", "self", "are", "we", "in", "the", "right", "ballpark", "nb", "comparisons", "have", "a", "higher", "precedence", "than", "not", "if", "not", "self", "get_start_loc", "other", "get_start_loc", "self", "get_end_loc", "return", "none", "do", "we", "have", "any", "child", "segments", "at", "all", "if", "not", "self", "segments", "return", "none", "check", "through", "each", "of", "the", "child", "segments", "for", "seg", "in", "self", "segments", "res", "seg", "path_to", "other", "if", "res", "return", "self", "res", "return", "none", "pragma", "no", "cover"], "doc_len": 118}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.parse", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "parse", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def parse(self, parse_context=None, parse_grammar=None):\n        \"\"\"Use the parse grammar to find subsegments within this segment.\n\n        A large chunk of the logic around this can be found in the `expand` method.\n\n        Use the parse setting in the context for testing, mostly to check how deep to go.\n        True/False for yes or no, an integer allows a certain number of levels.\n\n        Optionally, this method allows a custom parse grammar to be\n        provided which will override any existing parse grammar\n        on the segment.\n        \"\"\"\n        # Clear the blacklist cache so avoid missteps\n        if parse_context:\n            parse_context.blacklist.clear()\n\n        # the parse_depth and recurse kwargs control how deep we will recurse for testing.\n        if not self.segments:  # pragma: no cover TODO?\n            # This means we're a root segment, just return an unmutated self\n            return self\n\n        # Check the Parse Grammar\n        parse_grammar = parse_grammar or self.parse_grammar\n        if parse_grammar is None:\n            # No parse grammar, go straight to expansion\n            parse_context.logger.debug(\n                \"{}.parse: no grammar. Going straight to expansion\".format(\n                    self.__class__.__name__\n                )\n            )\n        else:\n            # For debugging purposes. Ensure that we don't have non-code elements\n            # at the start or end of the segments. They should always in the middle,\n            # or in the parent expression.\n            segments = self.segments\n            if self.can_start_end_non_code:\n                pre_nc, segments, post_nc = trim_non_code_segments(segments)\n            else:\n                pre_nc = ()\n                post_nc = ()\n                if (not segments[0].is_code) and (\n                    not segments[0].is_meta\n                ):  # pragma: no cover\n                    raise ValueError(\n                        \"Segment {} starts with non code segment: {!r}.\\n{!r}\".format(\n                            self, segments[0].raw, segments\n                        )\n                    )\n                if (not segments[-1].is_code) and (\n                    not segments[-1].is_meta\n                ):  # pragma: no cover\n                    raise ValueError(\n                        \"Segment {} ends with non code segment: {!r}.\\n{!r}\".format(\n                            self, segments[-1].raw, segments\n                        )\n                    )\n\n            # NOTE: No match_depth kwarg, because this is the start of the matching.\n            with parse_context.matching_segment(self.__class__.__name__) as ctx:\n                m = parse_grammar.match(segments=segments, parse_context=ctx)\n\n            if not isinstance(m, MatchResult):  # pragma: no cover\n                raise TypeError(\n                    \"[PD:{}] {}.match. Result is {}, not a MatchResult!\".format(\n                        parse_context.parse_depth, self.__class__.__name__, type(m)\n                    )\n                )\n\n            # Basic Validation, that we haven't dropped anything.\n            check_still_complete(segments, m.matched_segments, m.unmatched_segments)\n\n            if m.has_match():\n                if m.is_complete():\n                    # Complete match, happy days!\n                    self.segments = pre_nc + m.matched_segments + post_nc\n                else:\n                    # Incomplete match.\n                    # For now this means the parsing has failed. Lets add the unmatched bit at the\n                    # end as something unparsable.\n                    # TODO: Do something more intelligent here.\n                    self.segments = (\n                        pre_nc\n                        + m.matched_segments\n                        + (\n                            UnparsableSegment(\n                                segments=m.unmatched_segments + post_nc,\n                                expected=\"Nothing...\",\n                            ),\n                        )\n                    )\n            elif self.allow_empty and not segments:\n                # Very edge case, but some segments are allowed to be empty other than non-code\n                self.segments = pre_nc + post_nc\n            else:\n                # If there's no match at this stage, then it's unparsable. That's\n                # a problem at this stage so wrap it in an unparsable segment and carry on.\n                self.segments = (\n                    pre_nc\n                    + (\n                        UnparsableSegment(\n                            segments=segments,\n                            expected=self.name,\n                        ),  # NB: tuple\n                    )\n                    + post_nc\n                )\n        # Recurse if allowed (using the expand method to deal with the expansion)\n        parse_context.logger.debug(\n            \"{}.parse: Done Parse. Plotting Recursion. Recurse={!r}\".format(\n                self.__class__.__name__, parse_context.recurse\n            )\n        )\n        parse_depth_msg = \"###\\n#\\n# Beginning Parse Depth {}: {}\\n#\\n###\\nInitial Structure:\\n{}\".format(\n            parse_context.parse_depth + 1, self.__class__.__name__, self.stringify()\n        )\n        if parse_context.may_recurse():\n            parse_context.logger.debug(parse_depth_msg)\n            with parse_context.deeper_parse() as ctx:\n                self.segments = self.expand(self.segments, parse_context=ctx)\n\n        return self\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "parse", "self", "parse_context", "none", "parse_grammar", "none", "use", "the", "parse", "grammar", "to", "find", "subsegments", "within", "this", "segment", "a", "large", "chunk", "of", "the", "logic", "around", "this", "can", "be", "found", "in", "the", "expand", "method", "use", "the", "parse", "setting", "in", "the", "context", "for", "testing", "mostly", "to", "check", "how", "deep", "to", "go", "true", "false", "for", "yes", "or", "no", "an", "integer", "allows", "a", "certain", "number", "of", "levels", "optionally", "this", "method", "allows", "a", "custom", "parse", "grammar", "to", "be", "provided", "which", "will", "override", "any", "existing", "parse", "grammar", "on", "the", "segment", "clear", "the", "blacklist", "cache", "so", "avoid", "missteps", "if", "parse_context", "parse_context", "blacklist", "clear", "the", "parse_depth", "and", "recurse", "kwargs", "control", "how", "deep", "we", "will", "recurse", "for", "testing", "if", "not", "self", "segments", "pragma", "no", "cover", "todo", "this", "means", "we", "re", "a", "root", "segment", "just", "return", "an", "unmutated", "self", "return", "self", "check", "the", "parse", "grammar", "parse_grammar", "parse_grammar", "or", "self", "parse_grammar", "if", "parse_grammar", "is", "none", "no", "parse", "grammar", "go", "straight", "to", "expansion", "parse_context", "logger", "debug", "parse", "no", "grammar", "going", "straight", "to", "expansion", "format", "self", "__class__", "__name__", "else", "for", "debugging", "purposes", "ensure", "that", "we", "don", "t", "have", "non", "code", "elements", "at", "the", "start", "or", "end", "of", "the", "segments", "they", "should", "always", "in", "the", "middle", "or", "in", "the", "parent", "expression", "segments", "self", "segments", "if", "self", "can_start_end_non_code", "pre_nc", "segments", "post_nc", "trim_non_code_segments", "segments", "else", "pre_nc", "post_nc", "if", "not", "segments", "0", "is_code", "and", "not", "segments", "0", "is_meta", "pragma", "no", "cover", "raise", "valueerror", "segment", "starts", "with", "non", "code", "segment", "r", "n", "r", "format", "self", "segments", "0", "raw", "segments", "if", "not", "segments", "1", "is_code", "and", "not", "segments", "1", "is_meta", "pragma", "no", "cover", "raise", "valueerror", "segment", "ends", "with", "non", "code", "segment", "r", "n", "r", "format", "self", "segments", "1", "raw", "segments", "note", "no", "match_depth", "kwarg", "because", "this", "is", "the", "start", "of", "the", "matching", "with", "parse_context", "matching_segment", "self", "__class__", "__name__", "as", "ctx", "m", "parse_grammar", "match", "segments", "segments", "parse_context", "ctx", "if", "not", "isinstance", "m", "matchresult", "pragma", "no", "cover", "raise", "typeerror", "pd", "match", "result", "is", "not", "a", "matchresult", "format", "parse_context", "parse_depth", "self", "__class__", "__name__", "type", "m", "basic", "validation", "that", "we", "haven", "t", "dropped", "anything", "check_still_complete", "segments", "m", "matched_segments", "m", "unmatched_segments", "if", "m", "has_match", "if", "m", "is_complete", "complete", "match", "happy", "days", "self", "segments", "pre_nc", "m", "matched_segments", "post_nc", "else", "incomplete", "match", "for", "now", "this", "means", "the", "parsing", "has", "failed", "lets", "add", "the", "unmatched", "bit", "at", "the", "end", "as", "something", "unparsable", "todo", "do", "something", "more", "intelligent", "here", "self", "segments", "pre_nc", "m", "matched_segments", "unparsablesegment", "segments", "m", "unmatched_segments", "post_nc", "expected", "nothing", "elif", "self", "allow_empty", "and", "not", "segments", "very", "edge", "case", "but", "some", "segments", "are", "allowed", "to", "be", "empty", "other", "than", "non", "code", "self", "segments", "pre_nc", "post_nc", "else", "if", "there", "s", "no", "match", "at", "this", "stage", "then", "it", "s", "unparsable", "that", "s", "a", "problem", "at", "this", "stage", "so", "wrap", "it", "in", "an", "unparsable", "segment", "and", "carry", "on", "self", "segments", "pre_nc", "unparsablesegment", "segments", "segments", "expected", "self", "name", "nb", "tuple", "post_nc", "recurse", "if", "allowed", "using", "the", "expand", "method", "to", "deal", "with", "the", "expansion", "parse_context", "logger", "debug", "parse", "done", "parse", "plotting", "recursion", "recurse", "r", "format", "self", "__class__", "__name__", "parse_context", "recurse", "parse_depth_msg", "n", "n", "beginning", "parse", "depth", "n", "n", "ninitial", "structure", "n", "format", "parse_context", "parse_depth", "1", "self", "__class__", "__name__", "self", "stringify", "if", "parse_context", "may_recurse", "parse_context", "logger", "debug", "parse_depth_msg", "with", "parse_context", "deeper_parse", "as", "ctx", "self", "segments", "self", "expand", "self", "segments", "parse_context", "ctx", "return", "self"], "doc_len": 537}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.apply_fixes", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "apply_fixes", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def apply_fixes(self, fixes):\n        \"\"\"Apply an iterable of fixes to this segment.\n\n        Used in applying fixes if we're fixing linting errors.\n        If anything changes, this should return a new version of the segment\n        rather than mutating the original.\n\n        Note: We need to have fixes to apply AND this must have children. In the case\n        of raw segments, they will be replaced or removed by their parent and\n        so this function should just return self.\n        \"\"\"\n        if fixes and not self.is_raw():\n            # Get a reference to self to start with, but this will rapidly\n            # become a working copy.\n            r = self\n\n            # Make a working copy\n            seg_buffer = []\n            todo_buffer = list(self.segments)\n            while True:\n                if len(todo_buffer) == 0:\n                    break\n                else:\n                    seg = todo_buffer.pop(0)\n\n                    fix_buff = fixes.copy()\n                    unused_fixes = []\n                    while fix_buff:\n                        f = fix_buff.pop()\n                        # Look for identity not just equality.\n                        # This handles potential positioning ambiguity.\n                        if f.anchor is seg:\n                            linter_logger.debug(\n                                \"Matched fix against segment: %s -> %s\", f, seg\n                            )\n                            if f.edit_type == \"delete\":\n                                # We're just getting rid of this segment.\n                                seg = None\n                            elif f.edit_type in (\"edit\", \"create\"):\n                                # We're doing a replacement (it could be a single segment or an iterable)\n                                if isinstance(f.edit, BaseSegment):\n                                    seg_buffer.append(f.edit)  # pragma: no cover TODO?\n                                else:\n                                    for s in f.edit:\n                                        seg_buffer.append(s)\n\n                                if f.edit_type == \"create\":\n                                    # in the case of a creation, also add this segment on the end\n                                    seg_buffer.append(seg)\n                            else:  # pragma: no cover\n                                raise ValueError(\n                                    \"Unexpected edit_type: {!r} in {!r}\".format(\n                                        f.edit_type, f\n                                    )\n                                )\n                            # We've applied a fix here. Move on, this also consumes the fix\n                            # TODO: Maybe deal with overlapping fixes later.\n                            break\n                        else:\n                            # We've not used the fix so we should keep it in the list for later.\n                            unused_fixes.append(f)\n                    else:\n                        seg_buffer.append(seg)\n                # Switch over the the unused list\n                fixes = unused_fixes + fix_buff\n                # Invalidate any caches\n                self.invalidate_caches()\n\n            # Then recurse (i.e. deal with the children) (Requeueing)\n            seg_queue = seg_buffer\n            seg_buffer = []\n            for seg in seg_queue:\n                s, fixes = seg.apply_fixes(fixes)\n                seg_buffer.append(s)\n\n            # Reform into a new segment\n            r = r.__class__(\n                # Realign the segments within\n                segments=self._position_segments(\n                    tuple(seg_buffer), parent_pos=r.pos_marker\n                ),\n                pos_marker=r.pos_marker,\n                # Pass through any additional kwargs\n                **{k: getattr(self, k) for k in self.additional_kwargs},\n            )\n            # Return the new segment with any unused fixes.\n            return r, fixes\n        else:\n            return self, fixes\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "apply_fixes", "self", "fixes", "apply", "an", "iterable", "of", "fixes", "to", "this", "segment", "used", "in", "applying", "fixes", "if", "we", "re", "fixing", "linting", "errors", "if", "anything", "changes", "this", "should", "return", "a", "new", "version", "of", "the", "segment", "rather", "than", "mutating", "the", "original", "note", "we", "need", "to", "have", "fixes", "to", "apply", "and", "this", "must", "have", "children", "in", "the", "case", "of", "raw", "segments", "they", "will", "be", "replaced", "or", "removed", "by", "their", "parent", "and", "so", "this", "function", "should", "just", "return", "self", "if", "fixes", "and", "not", "self", "is_raw", "get", "a", "reference", "to", "self", "to", "start", "with", "but", "this", "will", "rapidly", "become", "a", "working", "copy", "r", "self", "make", "a", "working", "copy", "seg_buffer", "todo_buffer", "list", "self", "segments", "while", "true", "if", "len", "todo_buffer", "0", "break", "else", "seg", "todo_buffer", "pop", "0", "fix_buff", "fixes", "copy", "unused_fixes", "while", "fix_buff", "f", "fix_buff", "pop", "look", "for", "identity", "not", "just", "equality", "this", "handles", "potential", "positioning", "ambiguity", "if", "f", "anchor", "is", "seg", "linter_logger", "debug", "matched", "fix", "against", "segment", "s", "s", "f", "seg", "if", "f", "edit_type", "delete", "we", "re", "just", "getting", "rid", "of", "this", "segment", "seg", "none", "elif", "f", "edit_type", "in", "edit", "create", "we", "re", "doing", "a", "replacement", "it", "could", "be", "a", "single", "segment", "or", "an", "iterable", "if", "isinstance", "f", "edit", "basesegment", "seg_buffer", "append", "f", "edit", "pragma", "no", "cover", "todo", "else", "for", "s", "in", "f", "edit", "seg_buffer", "append", "s", "if", "f", "edit_type", "create", "in", "the", "case", "of", "a", "creation", "also", "add", "this", "segment", "on", "the", "end", "seg_buffer", "append", "seg", "else", "pragma", "no", "cover", "raise", "valueerror", "unexpected", "edit_type", "r", "in", "r", "format", "f", "edit_type", "f", "we", "ve", "applied", "a", "fix", "here", "move", "on", "this", "also", "consumes", "the", "fix", "todo", "maybe", "deal", "with", "overlapping", "fixes", "later", "break", "else", "we", "ve", "not", "used", "the", "fix", "so", "we", "should", "keep", "it", "in", "the", "list", "for", "later", "unused_fixes", "append", "f", "else", "seg_buffer", "append", "seg", "switch", "over", "the", "the", "unused", "list", "fixes", "unused_fixes", "fix_buff", "invalidate", "any", "caches", "self", "invalidate_caches", "then", "recurse", "i", "e", "deal", "with", "the", "children", "requeueing", "seg_queue", "seg_buffer", "seg_buffer", "for", "seg", "in", "seg_queue", "s", "fixes", "seg", "apply_fixes", "fixes", "seg_buffer", "append", "s", "reform", "into", "a", "new", "segment", "r", "r", "__class__", "realign", "the", "segments", "within", "segments", "self", "_position_segments", "tuple", "seg_buffer", "parent_pos", "r", "pos_marker", "pos_marker", "r", "pos_marker", "pass", "through", "any", "additional", "kwargs", "k", "getattr", "self", "k", "for", "k", "in", "self", "additional_kwargs", "return", "the", "new", "segment", "with", "any", "unused", "fixes", "return", "r", "fixes", "else", "return", "self", "fixes"], "doc_len": 389}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.iter_patches", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "iter_patches", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def iter_patches(self, templated_str: str) -> Iterator[FixPatch]:\n        \"\"\"Iterate through the segments generating fix patches.\n\n        The patches are generated in TEMPLATED space. This is important\n        so that we defer dealing with any loops until later. At this stage\n        everything *should* happen in templated order.\n\n        Occasionally we have an insertion around a placeholder, so we also\n        return a hint to deal with that.\n        \"\"\"\n        # Does it match? If so we can ignore it.\n        matches = self.raw == templated_str[self.pos_marker.templated_slice]\n        if matches:\n            return\n\n        # If we're here, the segment doesn't match the original.\n        linter_logger.debug(\n            \"%s at %s: Original: [%r] Fixed: [%r]\",\n            type(self).__name__,\n            self.pos_marker.templated_slice,\n            templated_str[self.pos_marker.templated_slice],\n            self.raw,\n        )\n\n        # If it's all literal, then we don't need to recurse.\n        if self.pos_marker.is_literal():\n            # Yield the position in the source file and the patch\n            yield FixPatch(\n                self.pos_marker.templated_slice, self.raw, patch_category=\"literal\"\n            )\n        # Can we go deeper?\n        elif not self.segments:\n            # It's not literal, but it's also a raw segment. If we're going\n            # to yield a change, we would have done it from the parent, so\n            # we just abort from here.\n            return  # pragma: no cover TODO?\n        else:\n            # This segment isn't a literal, but has changed, we need to go deeper.\n\n            # Iterate through the child segments\n            templated_idx = self.pos_marker.templated_slice.start\n            insert_buff = \"\"\n            for seg_idx, segment in enumerate(self.segments):\n\n                # First check for insertions.\n                # We know it's an insertion if it has length but not in the templated file.\n                if segment.raw and segment.pos_marker.is_point():\n                    # Add it to the insertion buffer if it has length:\n                    if segment.raw:\n                        insert_buff += segment.raw\n                        linter_logger.debug(\n                            \"Appending insertion buffer. %r @idx: %s\",\n                            insert_buff,\n                            templated_idx,\n                        )\n                    continue\n\n                # If we get here, then we know it's an original.\n                # Check for deletions at the point before this segment (vs the TEMPLATED).\n                start_diff = segment.pos_marker.templated_slice.start - templated_idx\n\n                # Check to see whether there's a discontinuity before the current segment\n                if start_diff > 0 or insert_buff:\n                    # If we have an insert buffer, then it's an edit, otherwise a deletion.\n                    yield FixPatch(\n                        slice(\n                            segment.pos_marker.templated_slice.start\n                            - max(start_diff, 0),\n                            segment.pos_marker.templated_slice.start,\n                        ),\n                        insert_buff,\n                        patch_category=\"mid_point\",\n                    )\n                    insert_buff = \"\"\n\n                # Now we deal with any changes *within* the segment itself.\n                yield from segment.iter_patches(templated_str=templated_str)\n\n                # Once we've dealt with any patches from the segment, update\n                # our position markers.\n                templated_idx = segment.pos_marker.templated_slice.stop\n\n            # After the loop, we check whether there's a trailing deletion\n            # or insert. Also valid if we still have an insertion buffer here.\n            end_diff = self.pos_marker.templated_slice.stop - templated_idx\n            if end_diff or insert_buff:\n                yield FixPatch(\n                    slice(\n                        self.pos_marker.templated_slice.stop - end_diff,\n                        self.pos_marker.templated_slice.stop,\n                    ),\n                    insert_buff,\n                    patch_category=\"end_point\",\n                )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "iter_patches", "self", "templated_str", "str", "iterator", "fixpatch", "iterate", "through", "the", "segments", "generating", "fix", "patches", "the", "patches", "are", "generated", "in", "templated", "space", "this", "is", "important", "so", "that", "we", "defer", "dealing", "with", "any", "loops", "until", "later", "at", "this", "stage", "everything", "should", "happen", "in", "templated", "order", "occasionally", "we", "have", "an", "insertion", "around", "a", "placeholder", "so", "we", "also", "return", "a", "hint", "to", "deal", "with", "that", "does", "it", "match", "if", "so", "we", "can", "ignore", "it", "matches", "self", "raw", "templated_str", "self", "pos_marker", "templated_slice", "if", "matches", "return", "if", "we", "re", "here", "the", "segment", "doesn", "t", "match", "the", "original", "linter_logger", "debug", "s", "at", "s", "original", "r", "fixed", "r", "type", "self", "__name__", "self", "pos_marker", "templated_slice", "templated_str", "self", "pos_marker", "templated_slice", "self", "raw", "if", "it", "s", "all", "literal", "then", "we", "don", "t", "need", "to", "recurse", "if", "self", "pos_marker", "is_literal", "yield", "the", "position", "in", "the", "source", "file", "and", "the", "patch", "yield", "fixpatch", "self", "pos_marker", "templated_slice", "self", "raw", "patch_category", "literal", "can", "we", "go", "deeper", "elif", "not", "self", "segments", "it", "s", "not", "literal", "but", "it", "s", "also", "a", "raw", "segment", "if", "we", "re", "going", "to", "yield", "a", "change", "we", "would", "have", "done", "it", "from", "the", "parent", "so", "we", "just", "abort", "from", "here", "return", "pragma", "no", "cover", "todo", "else", "this", "segment", "isn", "t", "a", "literal", "but", "has", "changed", "we", "need", "to", "go", "deeper", "iterate", "through", "the", "child", "segments", "templated_idx", "self", "pos_marker", "templated_slice", "start", "insert_buff", "for", "seg_idx", "segment", "in", "enumerate", "self", "segments", "first", "check", "for", "insertions", "we", "know", "it", "s", "an", "insertion", "if", "it", "has", "length", "but", "not", "in", "the", "templated", "file", "if", "segment", "raw", "and", "segment", "pos_marker", "is_point", "add", "it", "to", "the", "insertion", "buffer", "if", "it", "has", "length", "if", "segment", "raw", "insert_buff", "segment", "raw", "linter_logger", "debug", "appending", "insertion", "buffer", "r", "idx", "s", "insert_buff", "templated_idx", "continue", "if", "we", "get", "here", "then", "we", "know", "it", "s", "an", "original", "check", "for", "deletions", "at", "the", "point", "before", "this", "segment", "vs", "the", "templated", "start_diff", "segment", "pos_marker", "templated_slice", "start", "templated_idx", "check", "to", "see", "whether", "there", "s", "a", "discontinuity", "before", "the", "current", "segment", "if", "start_diff", "0", "or", "insert_buff", "if", "we", "have", "an", "insert", "buffer", "then", "it", "s", "an", "edit", "otherwise", "a", "deletion", "yield", "fixpatch", "slice", "segment", "pos_marker", "templated_slice", "start", "max", "start_diff", "0", "segment", "pos_marker", "templated_slice", "start", "insert_buff", "patch_category", "mid_point", "insert_buff", "now", "we", "deal", "with", "any", "changes", "within", "the", "segment", "itself", "yield", "from", "segment", "iter_patches", "templated_str", "templated_str", "once", "we", "ve", "dealt", "with", "any", "patches", "from", "the", "segment", "update", "our", "position", "markers", "templated_idx", "segment", "pos_marker", "templated_slice", "stop", "after", "the", "loop", "we", "check", "whether", "there", "s", "a", "trailing", "deletion", "or", "insert", "also", "valid", "if", "we", "still", "have", "an", "insertion", "buffer", "here", "end_diff", "self", "pos_marker", "templated_slice", "stop", "templated_idx", "if", "end_diff", "or", "insert_buff", "yield", "fixpatch", "slice", "self", "pos_marker", "templated_slice", "stop", "end_diff", "self", "pos_marker", "templated_slice", "stop", "insert_buff", "patch_category", "end_point"], "doc_len": 449}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseSegment.edit", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseSegment", "func_name": "edit", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseSegment\n    def edit(self, raw):\n        \"\"\"Stub.\"\"\"\n        raise NotImplementedError()\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basesegment", "def", "edit", "self", "raw", "stub", "raise", "notimplementederror"], "doc_len": 15}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BracketedSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BracketedSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BracketedSegment\n    def __init__(\n        self,\n        *args,\n        # These are tuples of segments but we're expecting them to\n        # be tuples of length 1. This is because we'll almost always\n        # be doing tuple arithmetic with the results and constructing\n        # 1-tuples on the fly is very easy to misread.\n        start_bracket: Tuple[BaseSegment] = None,\n        end_bracket: Tuple[BaseSegment] = None,\n        **kwargs,\n    ):\n        \"\"\"Stash the bracket segments for later.\"\"\"\n        if not start_bracket or not end_bracket:  # pragma: no cover\n            raise ValueError(\n                \"Attempted to construct Bracketed segment without specifying brackets.\"\n            )\n        self.start_bracket = start_bracket\n        self.end_bracket = end_bracket\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "bracketedsegment", "def", "__init__", "self", "args", "these", "are", "tuples", "of", "segments", "but", "we", "re", "expecting", "them", "to", "be", "tuples", "of", "length", "1", "this", "is", "because", "we", "ll", "almost", "always", "be", "doing", "tuple", "arithmetic", "with", "the", "results", "and", "constructing", "1", "tuples", "on", "the", "fly", "is", "very", "easy", "to", "misread", "start_bracket", "tuple", "basesegment", "none", "end_bracket", "tuple", "basesegment", "none", "kwargs", "stash", "the", "bracket", "segments", "for", "later", "if", "not", "start_bracket", "or", "not", "end_bracket", "pragma", "no", "cover", "raise", "valueerror", "attempted", "to", "construct", "bracketed", "segment", "without", "specifying", "brackets", "self", "start_bracket", "start_bracket", "self", "end_bracket", "end_bracket", "super", "__init__", "args", "kwargs"], "doc_len": 98}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BracketedSegment.simple", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BracketedSegment", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BracketedSegment\n    def simple(cls, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Simple methods for bracketed and the persitent brackets.\"\"\"\n        start_brackets = [\n            start_bracket\n            for _, start_bracket, _, persistent in parse_context.dialect.sets(\n                \"bracket_pairs\"\n            )\n            if persistent\n        ]\n        start_simple = []\n        for ref in start_brackets:\n            start_simple += parse_context.dialect.ref(ref).simple(parse_context)\n        return start_simple\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "bracketedsegment", "def", "simple", "cls", "parse_context", "parsecontext", "optional", "list", "str", "simple", "methods", "for", "bracketed", "and", "the", "persitent", "brackets", "start_brackets", "start_bracket", "for", "_", "start_bracket", "_", "persistent", "in", "parse_context", "dialect", "sets", "bracket_pairs", "if", "persistent", "start_simple", "for", "ref", "in", "start_brackets", "start_simple", "parse_context", "dialect", "ref", "ref", "simple", "parse_context", "return", "start_simple"], "doc_len": 52}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BracketedSegment.match", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BracketedSegment", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BracketedSegment\n    def match(\n        cls, segments: Tuple[\"BaseSegment\", ...], parse_context: ParseContext\n    ) -> MatchResult:\n        \"\"\"Only useful as a terminator.\"\"\"\n        if segments and isinstance(segments[0], cls):\n            return MatchResult((segments[0],), segments[1:])\n        return MatchResult.from_unmatched(segments)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "bracketedsegment", "def", "match", "cls", "segments", "tuple", "basesegment", "parse_context", "parsecontext", "matchresult", "only", "useful", "as", "a", "terminator", "if", "segments", "and", "isinstance", "segments", "0", "cls", "return", "matchresult", "segments", "0", "segments", "1", "return", "matchresult", "from_unmatched", "segments"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::UnparsableSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "UnparsableSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: UnparsableSegment\n    def __init__(self, *args, expected=\"\", **kwargs):\n        self._expected = expected\n        super().__init__(*args, **kwargs)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "unparsablesegment", "def", "__init__", "self", "args", "expected", "kwargs", "self", "_expected", "expected", "super", "__init__", "args", "kwargs"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::UnparsableSegment._suffix", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "UnparsableSegment", "func_name": "_suffix", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: UnparsableSegment\n    def _suffix(self):\n        \"\"\"Return any extra output required at the end when logging.\n\n        NB Override this for specific subclasses if we want extra output.\n        \"\"\"\n        return f\"!! Expected: {self._expected!r}\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "unparsablesegment", "def", "_suffix", "self", "return", "any", "extra", "output", "required", "at", "the", "end", "when", "logging", "nb", "override", "this", "for", "specific", "subclasses", "if", "we", "want", "extra", "output", "return", "f", "expected", "self", "_expected", "r"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::UnparsableSegment.iter_unparsables", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "UnparsableSegment", "func_name": "iter_unparsables", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: UnparsableSegment\n    def iter_unparsables(self):\n        \"\"\"Iterate through any unparsables.\n\n        As this is an unparsable, it should yield itself.\n        \"\"\"\n        yield self\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "unparsablesegment", "def", "iter_unparsables", "self", "iterate", "through", "any", "unparsables", "as", "this", "is", "an", "unparsable", "it", "should", "yield", "itself", "yield", "self"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseFileSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseFileSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseFileSegment\n    def __init__(\n        self,\n        segments,\n        pos_marker=None,\n        name: Optional[str] = None,\n        fname: Optional[str] = None,\n    ):\n        self._file_path = fname\n        super().__init__(segments, pos_marker=pos_marker, name=name)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basefilesegment", "def", "__init__", "self", "segments", "pos_marker", "none", "name", "optional", "str", "none", "fname", "optional", "str", "none", "self", "_file_path", "fname", "super", "__init__", "segments", "pos_marker", "pos_marker", "name", "name"], "doc_len": 32}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseFileSegment.file_path", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseFileSegment", "func_name": "file_path", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseFileSegment\n    def file_path(self):\n        \"\"\"File path of a parsed SQL file.\"\"\"\n        return self._file_path\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basefilesegment", "def", "file_path", "self", "file", "path", "of", "a", "parsed", "sql", "file", "return", "self", "_file_path"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/base.py::BaseFileSegment.get_table_references", "file_path": "src/sqlfluff/core/parser/segments/base.py", "class_name": "BaseFileSegment", "func_name": "get_table_references", "text": "文件路径: src/sqlfluff/core/parser/segments/base.py, 类名: BaseFileSegment\n    def get_table_references(self):\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n        references = set()\n        for stmt in self.get_children(\"statement\"):\n            references |= stmt.get_table_references()\n        return references\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "base", "py", "basefilesegment", "def", "get_table_references", "self", "use", "parsed", "tree", "to", "extract", "table", "references", "references", "set", "for", "stmt", "in", "self", "get_children", "statement", "references", "stmt", "get_table_references", "return", "references"], "doc_len": 31}
{"doc_id": "src/sqlfluff/core/parser/segments/ephemeral.py::EphemeralSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/ephemeral.py", "class_name": "EphemeralSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/ephemeral.py, 类名: EphemeralSegment\n    def __init__(self, segments, pos_marker, parse_grammar, name: Optional[str] = None):\n        # Stash the parse grammar for now.\n        self._parse_grammar = parse_grammar\n        super().__init__(segments, pos_marker=pos_marker, name=name)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "ephemeral", "py", "ephemeralsegment", "def", "__init__", "self", "segments", "pos_marker", "parse_grammar", "name", "optional", "str", "none", "stash", "the", "parse", "grammar", "for", "now", "self", "_parse_grammar", "parse_grammar", "super", "__init__", "segments", "pos_marker", "pos_marker", "name", "name"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/parser/segments/ephemeral.py::EphemeralSegment.is_expandable", "file_path": "src/sqlfluff/core/parser/segments/ephemeral.py", "class_name": "EphemeralSegment", "func_name": "is_expandable", "text": "文件路径: src/sqlfluff/core/parser/segments/ephemeral.py, 类名: EphemeralSegment\n    def is_expandable(self):\n        \"\"\"Ephemeral segments are always expandable.\n\n        They should dissolve after expansion. So if it exists, it's expandable.\n        We need to redefine this here because the usual introspection doesn't\n        handle the custom parse_grammar properly.\n        \"\"\"\n        return True\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "ephemeral", "py", "ephemeralsegment", "def", "is_expandable", "self", "ephemeral", "segments", "are", "always", "expandable", "they", "should", "dissolve", "after", "expansion", "so", "if", "it", "exists", "it", "s", "expandable", "we", "need", "to", "redefine", "this", "here", "because", "the", "usual", "introspection", "doesn", "t", "handle", "the", "custom", "parse_grammar", "properly", "return", "true"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/parser/segments/ephemeral.py::EphemeralSegment.parse", "file_path": "src/sqlfluff/core/parser/segments/ephemeral.py", "class_name": "EphemeralSegment", "func_name": "parse", "text": "文件路径: src/sqlfluff/core/parser/segments/ephemeral.py, 类名: EphemeralSegment\n    def parse(self, parse_context):\n        \"\"\"Use the parse grammar to find subsegments within this segment.\n\n        Return the content of the result, rather than itself.\n        \"\"\"\n        # Call the usual parse function, but overriding the parse grammar.\n        new_self = super().parse(parse_context, parse_grammar=self._parse_grammar)\n        # Return the content of that result rather than self\n        return new_self.segments\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "ephemeral", "py", "ephemeralsegment", "def", "parse", "self", "parse_context", "use", "the", "parse", "grammar", "to", "find", "subsegments", "within", "this", "segment", "return", "the", "content", "of", "the", "result", "rather", "than", "itself", "call", "the", "usual", "parse", "function", "but", "overriding", "the", "parse", "grammar", "new_self", "super", "parse", "parse_context", "parse_grammar", "self", "_parse_grammar", "return", "the", "content", "of", "that", "result", "rather", "than", "self", "return", "new_self", "segments"], "doc_len": 60}
{"doc_id": "src/sqlfluff/core/parser/segments/ephemeral.py::allow_ephemeral", "file_path": "src/sqlfluff/core/parser/segments/ephemeral.py", "class_name": null, "func_name": "allow_ephemeral", "text": "文件路径: src/sqlfluff/core/parser/segments/ephemeral.py\ndef allow_ephemeral(func):\n    \"\"\"Wraps a .match() method to the option of ephemeral matching for grammars.\n\n    This is designed to be used as follows:\n\n        class SomeMatchableObject(object):\n            @match_wrapper()\n            @allow_ephemeral\n            def match(self, segments, parse_context):\n                ...\n                return m\n\n    NOTE: This should come inside the match_wrapper.\n    \"\"\"\n\n    def wrapped_match_method(self, segments: tuple, parse_context):\n        \"\"\"A wrapper on the match function to do some basic validation.\"\"\"\n        # Use the ephemeral_segment if present. This should only\n        # be the case for grammars where `ephemeral_name` is defined.\n        if self.ephemeral_name:\n            # We're going to return as though it's a full match, similar to Anything().\n            new_grammar = copy.copy(self)\n            # Reset the ephemeral name on the new version of the grammar otherwise\n            # we get infinite recursion.\n            new_grammar.ephemeral_name = None\n            # We shouldn't allow nested ephemerals. If they're present, don't create another.\n            # This can happen when grammars call super() on their match method.\n            if len(segments) == 1 and segments[0].is_type(\"ephemeral\"):\n                return MatchResult.from_matched(segments)\n            else:\n                return MatchResult.from_matched(\n                    (\n                        EphemeralSegment(\n                            segments=segments,\n                            pos_marker=None,\n                            # Ephemeral segments get a copy of the parent grammar.\n                            parse_grammar=new_grammar,\n                            name=self.ephemeral_name,\n                        ),\n                    )\n                )\n        else:\n            # Otherwise carry on through with wrapping the function.\n            return func(self, segments, parse_context=parse_context)\n\n    return wrapped_match_method\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "ephemeral", "py", "def", "allow_ephemeral", "func", "wraps", "a", "match", "method", "to", "the", "option", "of", "ephemeral", "matching", "for", "grammars", "this", "is", "designed", "to", "be", "used", "as", "follows", "class", "somematchableobject", "object", "match_wrapper", "allow_ephemeral", "def", "match", "self", "segments", "parse_context", "return", "m", "note", "this", "should", "come", "inside", "the", "match_wrapper", "def", "wrapped_match_method", "self", "segments", "tuple", "parse_context", "a", "wrapper", "on", "the", "match", "function", "to", "do", "some", "basic", "validation", "use", "the", "ephemeral_segment", "if", "present", "this", "should", "only", "be", "the", "case", "for", "grammars", "where", "ephemeral_name", "is", "defined", "if", "self", "ephemeral_name", "we", "re", "going", "to", "return", "as", "though", "it", "s", "a", "full", "match", "similar", "to", "anything", "new_grammar", "copy", "copy", "self", "reset", "the", "ephemeral", "name", "on", "the", "new", "version", "of", "the", "grammar", "otherwise", "we", "get", "infinite", "recursion", "new_grammar", "ephemeral_name", "none", "we", "shouldn", "t", "allow", "nested", "ephemerals", "if", "they", "re", "present", "don", "t", "create", "another", "this", "can", "happen", "when", "grammars", "call", "super", "on", "their", "match", "method", "if", "len", "segments", "1", "and", "segments", "0", "is_type", "ephemeral", "return", "matchresult", "from_matched", "segments", "else", "return", "matchresult", "from_matched", "ephemeralsegment", "segments", "segments", "pos_marker", "none", "ephemeral", "segments", "get", "a", "copy", "of", "the", "parent", "grammar", "parse_grammar", "new_grammar", "name", "self", "ephemeral_name", "else", "otherwise", "carry", "on", "through", "with", "wrapping", "the", "function", "return", "func", "self", "segments", "parse_context", "parse_context", "return", "wrapped_match_method"], "doc_len": 202}
{"doc_id": "src/sqlfluff/core/parser/segments/generator.py::SegmentGenerator.__init__", "file_path": "src/sqlfluff/core/parser/segments/generator.py", "class_name": "SegmentGenerator", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/generator.py, 类名: SegmentGenerator\n    def __init__(self, func):\n        self.func = func\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "generator", "py", "segmentgenerator", "def", "__init__", "self", "func", "self", "func", "func"], "doc_len": 15}
{"doc_id": "src/sqlfluff/core/parser/segments/generator.py::SegmentGenerator.expand", "file_path": "src/sqlfluff/core/parser/segments/generator.py", "class_name": "SegmentGenerator", "func_name": "expand", "text": "文件路径: src/sqlfluff/core/parser/segments/generator.py, 类名: SegmentGenerator\n    def expand(self, dialect):\n        \"\"\"Expand this object into its true dialect object.\n\n        The inner function is passed an instance of the current dialect\n        and so has access to the current sets of that dialect.\n        \"\"\"\n        return self.func(dialect)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "generator", "py", "segmentgenerator", "def", "expand", "self", "dialect", "expand", "this", "object", "into", "its", "true", "dialect", "object", "the", "inner", "function", "is", "passed", "an", "instance", "of", "the", "current", "dialect", "and", "so", "has", "access", "to", "the", "current", "sets", "of", "that", "dialect", "return", "self", "func", "dialect"], "doc_len": 46}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::MetaSegment._suffix", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "MetaSegment", "func_name": "_suffix", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: MetaSegment\n    def _suffix():\n        \"\"\"Return any extra output required at the end when logging.\n\n        Meta classes have not much to say here so just stay blank.\n        \"\"\"\n        return \"\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "metasegment", "def", "_suffix", "return", "any", "extra", "output", "required", "at", "the", "end", "when", "logging", "meta", "classes", "have", "not", "much", "to", "say", "here", "so", "just", "stay", "blank", "return"], "doc_len": 33}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::MetaSegment.match", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "MetaSegment", "func_name": "match", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: MetaSegment\n    def match(cls, segments, parse_context):  # pragma: no cover\n        \"\"\"This will never be called. If it is then we're using it wrong.\"\"\"\n        raise NotImplementedError(\n            \"{} has no match method, it should only be used in a Sequence!\".format(\n                cls.__name__\n            )\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "metasegment", "def", "match", "cls", "segments", "parse_context", "pragma", "no", "cover", "this", "will", "never", "be", "called", "if", "it", "is", "then", "we", "re", "using", "it", "wrong", "raise", "notimplementederror", "has", "no", "match", "method", "it", "should", "only", "be", "used", "in", "a", "sequence", "format", "cls", "__name__"], "doc_len": 47}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::MetaSegment.simple", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "MetaSegment", "func_name": "simple", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: MetaSegment\n    def simple(cls, parse_context: ParseContext) -> Optional[List[str]]:\n        \"\"\"Does this matcher support an uppercase hash matching route?\n\n        This should be true if the MATCH grammar is simple. Most more\n        complicated segments will be assumed to overwrite this method\n        if they wish to be considered simple.\n        \"\"\"\n        return None\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "metasegment", "def", "simple", "cls", "parse_context", "parsecontext", "optional", "list", "str", "does", "this", "matcher", "support", "an", "uppercase", "hash", "matching", "route", "this", "should", "be", "true", "if", "the", "match", "grammar", "is", "simple", "most", "more", "complicated", "segments", "will", "be", "assumed", "to", "overwrite", "this", "method", "if", "they", "wish", "to", "be", "considered", "simple", "return", "none"], "doc_len": 55}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::TemplateSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "TemplateSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: TemplateSegment\n    def __init__(self, pos_marker=None, source_str=\"\", block_type=\"\"):\n        \"\"\"Initialise a placeholder with the source code embedded.\"\"\"\n        if not source_str:  # pragma: no cover\n            raise ValueError(\"Cannot instantiate TemplateSegment without a source_str.\")\n        self.source_str = source_str\n        self.block_type = block_type\n        # Call the super of the pos_marker.\n        super().__init__(pos_marker=pos_marker)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "templatesegment", "def", "__init__", "self", "pos_marker", "none", "source_str", "block_type", "initialise", "a", "placeholder", "with", "the", "source", "code", "embedded", "if", "not", "source_str", "pragma", "no", "cover", "raise", "valueerror", "cannot", "instantiate", "templatesegment", "without", "a", "source_str", "self", "source_str", "source_str", "self", "block_type", "block_type", "call", "the", "super", "of", "the", "pos_marker", "super", "__init__", "pos_marker", "pos_marker"], "doc_len": 53}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::TemplateSegment._suffix", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "TemplateSegment", "func_name": "_suffix", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: TemplateSegment\n    def _suffix(self):\n        \"\"\"Also output what it's a placeholder for.\"\"\"\n        return f\"[Type: {self.block_type!r}, Raw: {self.source_str!r}]\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "templatesegment", "def", "_suffix", "self", "also", "output", "what", "it", "s", "a", "placeholder", "for", "return", "f", "type", "self", "block_type", "r", "raw", "self", "source_str", "r"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/parser/segments/meta.py::TemplateSegment.to_tuple", "file_path": "src/sqlfluff/core/parser/segments/meta.py", "class_name": "TemplateSegment", "func_name": "to_tuple", "text": "文件路径: src/sqlfluff/core/parser/segments/meta.py, 类名: TemplateSegment\n    def to_tuple(self, code_only=False, show_raw=False, include_meta=False):\n        \"\"\"Return a tuple structure from this segment.\n\n        Unlike most segments, we return the _source_ content for placeholders\n        if viewing metas is allowed. This allows verification of the content\n        of those placeholders for inspection or debugging.\n        \"\"\"\n        if include_meta:\n            return (self.get_type(), self.source_str)\n        else:  # pragma: no cover TODO?\n            return (self.get_type(), self.raw)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "meta", "py", "templatesegment", "def", "to_tuple", "self", "code_only", "false", "show_raw", "false", "include_meta", "false", "return", "a", "tuple", "structure", "from", "this", "segment", "unlike", "most", "segments", "we", "return", "the", "_source_", "content", "for", "placeholders", "if", "viewing", "metas", "is", "allowed", "this", "allows", "verification", "of", "the", "content", "of", "those", "placeholders", "for", "inspection", "or", "debugging", "if", "include_meta", "return", "self", "get_type", "self", "source_str", "else", "pragma", "no", "cover", "todo", "return", "self", "get_type", "self", "raw"], "doc_len": 69}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        type: Optional[str] = None,\n        name: Optional[str] = None,\n        trim_start: Optional[Tuple[str, ...]] = None,\n        trim_chars: Optional[Tuple[str, ...]] = None,\n    ):\n        \"\"\"Initialise raw segment.\n\n        If raw is not provided, we default to _default_raw if present.\n        If pos_marker is not provided, it is assume that this will be\n        inserted later as part of a reposition phase.\n        \"\"\"\n        if raw is not None:  # NB, raw *can* be an empty string and be valid\n            self._raw = raw\n        else:\n            self._raw = self._default_raw\n        self._raw_upper = self._raw.upper()\n        # pos marker is required here. We ignore the typing initially\n        # because it might *initially* be unset, but it will be reset\n        # later.\n        self.pos_marker: PositionMarker = pos_marker  # type: ignore\n        # if a surrogate type is provided, store it for later.\n        self._surrogate_type = type\n        self._surrogate_name = name\n        # What should we trim off the ends to get to content\n        self.trim_start = trim_start\n        self.trim_chars = trim_chars\n        # A cache variable for expandable\n        self._is_expandable = None\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "__init__", "self", "raw", "optional", "str", "none", "pos_marker", "optional", "positionmarker", "none", "type", "optional", "str", "none", "name", "optional", "str", "none", "trim_start", "optional", "tuple", "str", "none", "trim_chars", "optional", "tuple", "str", "none", "initialise", "raw", "segment", "if", "raw", "is", "not", "provided", "we", "default", "to", "_default_raw", "if", "present", "if", "pos_marker", "is", "not", "provided", "it", "is", "assume", "that", "this", "will", "be", "inserted", "later", "as", "part", "of", "a", "reposition", "phase", "if", "raw", "is", "not", "none", "nb", "raw", "can", "be", "an", "empty", "string", "and", "be", "valid", "self", "_raw", "raw", "else", "self", "_raw", "self", "_default_raw", "self", "_raw_upper", "self", "_raw", "upper", "pos", "marker", "is", "required", "here", "we", "ignore", "the", "typing", "initially", "because", "it", "might", "initially", "be", "unset", "but", "it", "will", "be", "reset", "later", "self", "pos_marker", "positionmarker", "pos_marker", "type", "ignore", "if", "a", "surrogate", "type", "is", "provided", "store", "it", "for", "later", "self", "_surrogate_type", "type", "self", "_surrogate_name", "name", "what", "should", "we", "trim", "off", "the", "ends", "to", "get", "to", "content", "self", "trim_start", "trim_start", "self", "trim_chars", "trim_chars", "a", "cache", "variable", "for", "expandable", "self", "_is_expandable", "none"], "doc_len": 168}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.__repr__", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def __repr__(self):\n        return \"<{}: ({}) {!r}>\".format(\n            self.__class__.__name__, self.pos_marker, self.raw\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "__repr__", "self", "return", "r", "format", "self", "__class__", "__name__", "self", "pos_marker", "self", "raw"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.matched_length", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "matched_length", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def matched_length(self):\n        \"\"\"Return the length of the segment in characters.\"\"\"\n        return len(self._raw)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "matched_length", "self", "return", "the", "length", "of", "the", "segment", "in", "characters", "return", "len", "self", "_raw"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.is_expandable", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "is_expandable", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def is_expandable(self):\n        \"\"\"Return true if it is meaningful to call `expand` on this segment.\"\"\"\n        return False\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "is_expandable", "self", "return", "true", "if", "it", "is", "meaningful", "to", "call", "expand", "on", "this", "segment", "return", "false"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.is_code", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "is_code", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def is_code(self):\n        \"\"\"Return True if this segment is code.\"\"\"\n        return self._is_code\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "is_code", "self", "return", "true", "if", "this", "segment", "is", "code", "return", "self", "_is_code"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.is_comment", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "is_comment", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def is_comment(self):\n        \"\"\"Return True if this segment is a comment.\"\"\"\n        return self._is_comment\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "is_comment", "self", "return", "true", "if", "this", "segment", "is", "a", "comment", "return", "self", "_is_comment"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.is_whitespace", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "is_whitespace", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def is_whitespace(self):\n        \"\"\"Return True if this segment is whitespace.\"\"\"\n        return self._is_whitespace\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "is_whitespace", "self", "return", "true", "if", "this", "segment", "is", "whitespace", "return", "self", "_is_whitespace"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.raw_upper", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "raw_upper", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def raw_upper(self):\n        \"\"\"Make an uppercase string from the segments of this segment.\"\"\"\n        return self._raw_upper\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "raw_upper", "self", "make", "an", "uppercase", "string", "from", "the", "segments", "of", "this", "segment", "return", "self", "_raw_upper"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.segments", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "segments", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def segments(self):\n        \"\"\"Return an empty list of child segments.\n\n        This is in case something tries to iterate on this segment.\n        \"\"\"\n        return []\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "segments", "self", "return", "an", "empty", "list", "of", "child", "segments", "this", "is", "in", "case", "something", "tries", "to", "iterate", "on", "this", "segment", "return"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.get_type", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "get_type", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def get_type(self):\n        \"\"\"Returns the type of this segment as a string.\"\"\"\n        return self._surrogate_type or self.type\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "get_type", "self", "returns", "the", "type", "of", "this", "segment", "as", "a", "string", "return", "self", "_surrogate_type", "or", "self", "type"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.is_type", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "is_type", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def is_type(self, *seg_type):\n        \"\"\"Extend the parent class method with the surrogate types.\"\"\"\n        if self._surrogate_type and self._surrogate_type in seg_type:\n            return True\n        return self.class_is_type(*seg_type)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "is_type", "self", "seg_type", "extend", "the", "parent", "class", "method", "with", "the", "surrogate", "types", "if", "self", "_surrogate_type", "and", "self", "_surrogate_type", "in", "seg_type", "return", "true", "return", "self", "class_is_type", "seg_type"], "doc_len": 35}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.iter_raw_seg", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "iter_raw_seg", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def iter_raw_seg(self):\n        \"\"\"Iterate raw segments, mostly for searching.\"\"\"\n        yield self\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "iter_raw_seg", "self", "iterate", "raw", "segments", "mostly", "for", "searching", "yield", "self"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.raw_trimmed", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "raw_trimmed", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def raw_trimmed(self):\n        \"\"\"Return a trimmed version of the raw content.\"\"\"\n        raw_buff = self.raw\n        if self.trim_start:\n            for seq in self.trim_start:\n                if raw_buff.startswith(seq):\n                    raw_buff = raw_buff[len(seq) :]\n        if self.trim_chars:\n            raw_buff = self.raw\n            # for each thing to trim\n            for seq in self.trim_chars:\n                # trim start\n                while raw_buff.startswith(seq):\n                    raw_buff = raw_buff[len(seq) :]\n                # trim end\n                while raw_buff.endswith(seq):\n                    raw_buff = raw_buff[: -len(seq)]\n            return raw_buff\n        return raw_buff\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "raw_trimmed", "self", "return", "a", "trimmed", "version", "of", "the", "raw", "content", "raw_buff", "self", "raw", "if", "self", "trim_start", "for", "seq", "in", "self", "trim_start", "if", "raw_buff", "startswith", "seq", "raw_buff", "raw_buff", "len", "seq", "if", "self", "trim_chars", "raw_buff", "self", "raw", "for", "each", "thing", "to", "trim", "for", "seq", "in", "self", "trim_chars", "trim", "start", "while", "raw_buff", "startswith", "seq", "raw_buff", "raw_buff", "len", "seq", "trim", "end", "while", "raw_buff", "endswith", "seq", "raw_buff", "raw_buff", "len", "seq", "return", "raw_buff", "return", "raw_buff"], "doc_len": 78}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.raw_list", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "raw_list", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def raw_list(self):  # pragma: no cover TODO?\n        \"\"\"Return a list of the raw content of this segment.\"\"\"\n        return [self.raw]\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "raw_list", "self", "pragma", "no", "cover", "todo", "return", "a", "list", "of", "the", "raw", "content", "of", "this", "segment", "return", "self", "raw"], "doc_len": 28}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment._reconstruct", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "_reconstruct", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def _reconstruct(self):\n        \"\"\"Return a string of the raw content of this segment.\"\"\"\n        return self._raw\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "_reconstruct", "self", "return", "a", "string", "of", "the", "raw", "content", "of", "this", "segment", "return", "self", "_raw"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.stringify", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "stringify", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def stringify(self, ident=0, tabsize=4, code_only=False):\n        \"\"\"Use indentation to render this segment and its children as a string.\"\"\"\n        preface = self._preface(ident=ident, tabsize=tabsize)\n        return preface + \"\\n\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "stringify", "self", "ident", "0", "tabsize", "4", "code_only", "false", "use", "indentation", "to", "render", "this", "segment", "and", "its", "children", "as", "a", "string", "preface", "self", "_preface", "ident", "ident", "tabsize", "tabsize", "return", "preface", "n"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment._suffix", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "_suffix", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def _suffix(self):\n        \"\"\"Return any extra output required at the end when logging.\n\n        NB Override this for specific subclasses if we want extra output.\n        \"\"\"\n        return f\"{self.raw!r}\"\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "_suffix", "self", "return", "any", "extra", "output", "required", "at", "the", "end", "when", "logging", "nb", "override", "this", "for", "specific", "subclasses", "if", "we", "want", "extra", "output", "return", "f", "self", "raw", "r"], "doc_len": 37}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::RawSegment.edit", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "RawSegment", "func_name": "edit", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: RawSegment\n    def edit(self, raw):\n        \"\"\"Create a new segment, with exactly the same position but different content.\n\n        Returns:\n            A copy of this object with new contents.\n\n        Used mostly by fixes.\n\n        \"\"\"\n        return self.__class__(\n            raw=raw,\n            pos_marker=self.pos_marker,\n            type=self._surrogate_type,\n            name=self._surrogate_name,\n            trim_start=self.trim_start,\n            trim_chars=self.trim_chars,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "rawsegment", "def", "edit", "self", "raw", "create", "a", "new", "segment", "with", "exactly", "the", "same", "position", "but", "different", "content", "returns", "a", "copy", "of", "this", "object", "with", "new", "contents", "used", "mostly", "by", "fixes", "return", "self", "__class__", "raw", "raw", "pos_marker", "self", "pos_marker", "type", "self", "_surrogate_type", "name", "self", "_surrogate_name", "trim_start", "self", "trim_start", "trim_chars", "self", "trim_chars"], "doc_len": 57}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::KeywordSegment.__init__", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "KeywordSegment", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: KeywordSegment\n    def __init__(\n        self,\n        raw: Optional[str] = None,\n        pos_marker: Optional[PositionMarker] = None,\n        type: Optional[str] = None,\n        name: Optional[str] = None,\n    ):\n        \"\"\"If no other name is provided we extrapolate it from the raw.\"\"\"\n        if raw and not name:\n            # names are all lowercase by convention.\n            name = raw.lower()\n        super().__init__(raw=raw, pos_marker=pos_marker, type=type, name=name)\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "keywordsegment", "def", "__init__", "self", "raw", "optional", "str", "none", "pos_marker", "optional", "positionmarker", "none", "type", "optional", "str", "none", "name", "optional", "str", "none", "if", "no", "other", "name", "is", "provided", "we", "extrapolate", "it", "from", "the", "raw", "if", "raw", "and", "not", "name", "names", "are", "all", "lowercase", "by", "convention", "name", "raw", "lower", "super", "__init__", "raw", "raw", "pos_marker", "pos_marker", "type", "type", "name", "name"], "doc_len": 63}
{"doc_id": "src/sqlfluff/core/parser/segments/raw.py::KeywordSegment.edit", "file_path": "src/sqlfluff/core/parser/segments/raw.py", "class_name": "KeywordSegment", "func_name": "edit", "text": "文件路径: src/sqlfluff/core/parser/segments/raw.py, 类名: KeywordSegment\n    def edit(self, raw):\n        \"\"\"Create a new segment, with exactly the same position but different content.\n\n        Returns:\n            A copy of this object with new contents.\n\n        Used mostly by fixes.\n\n        \"\"\"\n        return self.__class__(\n            raw=raw,\n            pos_marker=self.pos_marker,\n            type=self._surrogate_type,\n            name=self._surrogate_name,\n        )\n", "tokens": ["src", "sqlfluff", "core", "parser", "segments", "raw", "py", "keywordsegment", "def", "edit", "self", "raw", "create", "a", "new", "segment", "with", "exactly", "the", "same", "position", "but", "different", "content", "returns", "a", "copy", "of", "this", "object", "with", "new", "contents", "used", "mostly", "by", "fixes", "return", "self", "__class__", "raw", "raw", "pos_marker", "self", "pos_marker", "type", "self", "_surrogate_type", "name", "self", "_surrogate_name"], "doc_len": 51}
{"doc_id": "src/sqlfluff/core/plugin/hookspecs.py::PluginSpec.get_rules", "file_path": "src/sqlfluff/core/plugin/hookspecs.py", "class_name": "PluginSpec", "func_name": "get_rules", "text": "文件路径: src/sqlfluff/core/plugin/hookspecs.py, 类名: PluginSpec\n    def get_rules(self):\n        \"\"\"Get plugin rules.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "plugin", "hookspecs", "py", "pluginspec", "def", "get_rules", "self", "get", "plugin", "rules"], "doc_len": 13}
{"doc_id": "src/sqlfluff/core/plugin/hookspecs.py::PluginSpec.load_default_config", "file_path": "src/sqlfluff/core/plugin/hookspecs.py", "class_name": "PluginSpec", "func_name": "load_default_config", "text": "文件路径: src/sqlfluff/core/plugin/hookspecs.py, 类名: PluginSpec\n    def load_default_config(self) -> dict:\n        \"\"\"Loads the default configuration for the plugin.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "plugin", "hookspecs", "py", "pluginspec", "def", "load_default_config", "self", "dict", "loads", "the", "default", "configuration", "for", "the", "plugin"], "doc_len": 18}
{"doc_id": "src/sqlfluff/core/plugin/hookspecs.py::PluginSpec.get_configs_info", "file_path": "src/sqlfluff/core/plugin/hookspecs.py", "class_name": "PluginSpec", "func_name": "get_configs_info", "text": "文件路径: src/sqlfluff/core/plugin/hookspecs.py, 类名: PluginSpec\n    def get_configs_info(self) -> dict:\n        \"\"\"Get rule config validations and descriptions.\"\"\"\n", "tokens": ["src", "sqlfluff", "core", "plugin", "hookspecs", "py", "pluginspec", "def", "get_configs_info", "self", "dict", "get", "rule", "config", "validations", "and", "descriptions"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/plugin/host.py::get_plugin_manager", "file_path": "src/sqlfluff/core/plugin/host.py", "class_name": null, "func_name": "get_plugin_manager", "text": "文件路径: src/sqlfluff/core/plugin/host.py\ndef get_plugin_manager() -> pluggy.PluginManager:\n    \"\"\"Initializes the PluginManager.\"\"\"\n    pm = pluggy.PluginManager(plugin_base_name)\n    pm.add_hookspecs(PluginSpec)\n    pm.load_setuptools_entrypoints(project_name)\n    return pm\n", "tokens": ["src", "sqlfluff", "core", "plugin", "host", "py", "def", "get_plugin_manager", "pluggy", "pluginmanager", "initializes", "the", "pluginmanager", "pm", "pluggy", "pluginmanager", "plugin_base_name", "pm", "add_hookspecs", "pluginspec", "pm", "load_setuptools_entrypoints", "project_name", "return", "pm"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/plugin/lib.py::get_rules", "file_path": "src/sqlfluff/core/plugin/lib.py", "class_name": null, "func_name": "get_rules", "text": "文件路径: src/sqlfluff/core/plugin/lib.py\ndef get_rules():\n    \"\"\"Get plugin rules.\"\"\"\n    return get_rules_from_path()\n", "tokens": ["src", "sqlfluff", "core", "plugin", "lib", "py", "def", "get_rules", "get", "plugin", "rules", "return", "get_rules_from_path"], "doc_len": 13}
{"doc_id": "src/sqlfluff/core/plugin/lib.py::get_templaters", "file_path": "src/sqlfluff/core/plugin/lib.py", "class_name": null, "func_name": "get_templaters", "text": "文件路径: src/sqlfluff/core/plugin/lib.py\ndef get_templaters():\n    \"\"\"Get templaters.\"\"\"\n    return core_templaters()\n", "tokens": ["src", "sqlfluff", "core", "plugin", "lib", "py", "def", "get_templaters", "get", "templaters", "return", "core_templaters"], "doc_len": 12}
{"doc_id": "src/sqlfluff/core/plugin/lib.py::load_default_config", "file_path": "src/sqlfluff/core/plugin/lib.py", "class_name": null, "func_name": "load_default_config", "text": "文件路径: src/sqlfluff/core/plugin/lib.py\ndef load_default_config() -> dict:\n    \"\"\"Loads the default configuration for the plugin.\"\"\"\n    return ConfigLoader.get_global().load_default_config_file(\n        file_dir=os.path.join(os.path.dirname(os.path.dirname(__file__))),\n        file_name=\"default_config.cfg\",\n    )\n", "tokens": ["src", "sqlfluff", "core", "plugin", "lib", "py", "def", "load_default_config", "dict", "loads", "the", "default", "configuration", "for", "the", "plugin", "return", "configloader", "get_global", "load_default_config_file", "file_dir", "os", "path", "join", "os", "path", "dirname", "os", "path", "dirname", "__file__", "file_name", "default_config", "cfg"], "doc_len": 34}
{"doc_id": "src/sqlfluff/core/plugin/lib.py::get_configs_info", "file_path": "src/sqlfluff/core/plugin/lib.py", "class_name": null, "func_name": "get_configs_info", "text": "文件路径: src/sqlfluff/core/plugin/lib.py\ndef get_configs_info() -> dict:\n    \"\"\"Get rule config validations and descriptions.\"\"\"\n    return STANDARD_CONFIG_INFO_DICT\n", "tokens": ["src", "sqlfluff", "core", "plugin", "lib", "py", "def", "get_configs_info", "dict", "get", "rule", "config", "validations", "and", "descriptions", "return", "standard_config_info_dict"], "doc_len": 17}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleLoggingAdapter.process", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleLoggingAdapter", "func_name": "process", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleLoggingAdapter\n    def process(self, msg, kwargs):\n        \"\"\"Add the code element to the logging message before emit.\"\"\"\n        return \"[{}] {}\".format(self.extra[\"code\"], msg), kwargs\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleloggingadapter", "def", "process", "self", "msg", "kwargs", "add", "the", "code", "element", "to", "the", "logging", "message", "before", "emit", "return", "format", "self", "extra", "code", "msg", "kwargs"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintResult.__init__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintResult", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintResult\n    def __init__(self, anchor=None, fixes=None, memory=None, description=None):\n        # An anchor of none, means no issue\n        self.anchor = anchor\n        # Fixes might be blank\n        self.fixes = fixes or []\n        # When instantiating the result, we filter any fixes which are \"trivial\".\n        self.fixes = [f for f in self.fixes if not f.is_trivial()]\n        # Memory is passed back in the linting result\n        self.memory = memory\n        # store a description_override for later\n        self.description = description\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintresult", "def", "__init__", "self", "anchor", "none", "fixes", "none", "memory", "none", "description", "none", "an", "anchor", "of", "none", "means", "no", "issue", "self", "anchor", "anchor", "fixes", "might", "be", "blank", "self", "fixes", "fixes", "or", "when", "instantiating", "the", "result", "we", "filter", "any", "fixes", "which", "are", "trivial", "self", "fixes", "f", "for", "f", "in", "self", "fixes", "if", "not", "f", "is_trivial", "memory", "is", "passed", "back", "in", "the", "linting", "result", "self", "memory", "memory", "store", "a", "description_override", "for", "later", "self", "description", "description"], "doc_len": 78}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintResult.to_linting_error", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintResult", "func_name": "to_linting_error", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintResult\n    def to_linting_error(self, rule) -> Optional[SQLLintError]:\n        \"\"\"Convert a linting result to a :exc:`SQLLintError` if appropriate.\"\"\"\n        if self.anchor:\n            # Allow description override from the LintResult\n            description = self.description or rule.description\n            return SQLLintError(\n                rule=rule,\n                segment=self.anchor,\n                fixes=self.fixes,\n                description=description,\n            )\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintresult", "def", "to_linting_error", "self", "rule", "optional", "sqllinterror", "convert", "a", "linting", "result", "to", "a", "exc", "sqllinterror", "if", "appropriate", "if", "self", "anchor", "allow", "description", "override", "from", "the", "lintresult", "description", "self", "description", "or", "rule", "description", "return", "sqllinterror", "rule", "rule", "segment", "self", "anchor", "fixes", "self", "fixes", "description", "description", "else", "return", "none"], "doc_len": 53}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintFix.__init__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintFix", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintFix\n    def __init__(self, edit_type, anchor: BaseSegment, edit=None):\n        if edit_type not in [\"create\", \"edit\", \"delete\"]:  # pragma: no cover\n            raise ValueError(f\"Unexpected edit_type: {edit_type}\")\n        self.edit_type = edit_type\n        if not anchor:  # pragma: no cover\n            raise ValueError(\"Fixes must provide an anchor.\")\n        self.anchor = anchor\n        # Coerce to list\n        if isinstance(edit, BaseSegment):\n            edit = [edit]\n        # Copy all the elements of edit to stop contamination.\n        # We're about to start stripping the position markers\n        # of some of the elements and we don't want to end up\n        # stripping the positions of the original elements of\n        # the parsed structure.\n        self.edit = copy.deepcopy(edit)\n        if self.edit:\n            # Check that any edits don't have a position marker set.\n            # We should rely on realignment to make position markers.\n            # Strip position markers of anything enriched, otherwise things can get blurry\n            for seg in self.edit:\n                if seg.pos_marker:\n                    # Developer warning.\n                    rules_logger.debug(\n                        \"Developer Note: Edit segment found with preset position marker. \"\n                        \"These should be unset and calculated later.\"\n                    )\n                    seg.pos_marker = None\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintfix", "def", "__init__", "self", "edit_type", "anchor", "basesegment", "edit", "none", "if", "edit_type", "not", "in", "create", "edit", "delete", "pragma", "no", "cover", "raise", "valueerror", "f", "unexpected", "edit_type", "edit_type", "self", "edit_type", "edit_type", "if", "not", "anchor", "pragma", "no", "cover", "raise", "valueerror", "fixes", "must", "provide", "an", "anchor", "self", "anchor", "anchor", "coerce", "to", "list", "if", "isinstance", "edit", "basesegment", "edit", "edit", "copy", "all", "the", "elements", "of", "edit", "to", "stop", "contamination", "we", "re", "about", "to", "start", "stripping", "the", "position", "markers", "of", "some", "of", "the", "elements", "and", "we", "don", "t", "want", "to", "end", "up", "stripping", "the", "positions", "of", "the", "original", "elements", "of", "the", "parsed", "structure", "self", "edit", "copy", "deepcopy", "edit", "if", "self", "edit", "check", "that", "any", "edits", "don", "t", "have", "a", "position", "marker", "set", "we", "should", "rely", "on", "realignment", "to", "make", "position", "markers", "strip", "position", "markers", "of", "anything", "enriched", "otherwise", "things", "can", "get", "blurry", "for", "seg", "in", "self", "edit", "if", "seg", "pos_marker", "developer", "warning", "rules_logger", "debug", "developer", "note", "edit", "segment", "found", "with", "preset", "position", "marker", "these", "should", "be", "unset", "and", "calculated", "later", "seg", "pos_marker", "none"], "doc_len": 171}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintFix.is_trivial", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintFix", "func_name": "is_trivial", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintFix\n    def is_trivial(self):\n        \"\"\"Return true if the fix is trivial.\n\n        Trivial edits are:\n        - Anything of zero length.\n        - Any edits which result in themselves.\n\n        Removing these makes the routines which process fixes much faster.\n        \"\"\"\n        if self.edit_type == \"create\":\n            if isinstance(self.edit, BaseSegment):\n                if len(self.edit.raw) == 0:  # pragma: no cover TODO?\n                    return True\n            elif all(len(elem.raw) == 0 for elem in self.edit):\n                return True\n        elif self.edit_type == \"edit\" and self.edit == self.anchor:\n            return True  # pragma: no cover TODO?\n        return False\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintfix", "def", "is_trivial", "self", "return", "true", "if", "the", "fix", "is", "trivial", "trivial", "edits", "are", "anything", "of", "zero", "length", "any", "edits", "which", "result", "in", "themselves", "removing", "these", "makes", "the", "routines", "which", "process", "fixes", "much", "faster", "if", "self", "edit_type", "create", "if", "isinstance", "self", "edit", "basesegment", "if", "len", "self", "edit", "raw", "0", "pragma", "no", "cover", "todo", "return", "true", "elif", "all", "len", "elem", "raw", "0", "for", "elem", "in", "self", "edit", "return", "true", "elif", "self", "edit_type", "edit", "and", "self", "edit", "self", "anchor", "return", "true", "pragma", "no", "cover", "todo", "return", "false"], "doc_len": 91}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintFix.__repr__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintFix", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintFix\n    def __repr__(self):\n        if self.edit_type == \"delete\":\n            detail = f\"delete:{self.anchor.raw!r}\"\n        elif self.edit_type in (\"edit\", \"create\"):\n            if hasattr(self.edit, \"raw\"):\n                new_detail = self.edit.raw  # pragma: no cover TODO?\n            else:\n                new_detail = \"\".join(s.raw for s in self.edit)\n\n            if self.edit_type == \"edit\":\n                detail = f\"edt:{self.anchor.raw!r}->{new_detail!r}\"\n            else:\n                detail = f\"create:{new_detail!r}\"\n        else:\n            detail = \"\"  # pragma: no cover TODO?\n        return \"<LintFix: {} @{} {}>\".format(\n            self.edit_type, self.anchor.pos_marker, detail\n        )\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintfix", "def", "__repr__", "self", "if", "self", "edit_type", "delete", "detail", "f", "delete", "self", "anchor", "raw", "r", "elif", "self", "edit_type", "in", "edit", "create", "if", "hasattr", "self", "edit", "raw", "new_detail", "self", "edit", "raw", "pragma", "no", "cover", "todo", "else", "new_detail", "join", "s", "raw", "for", "s", "in", "self", "edit", "if", "self", "edit_type", "edit", "detail", "f", "edt", "self", "anchor", "raw", "r", "new_detail", "r", "else", "detail", "f", "create", "new_detail", "r", "else", "detail", "pragma", "no", "cover", "todo", "return", "lintfix", "format", "self", "edit_type", "self", "anchor", "pos_marker", "detail"], "doc_len": 84}
{"doc_id": "src/sqlfluff/core/rules/base.py::LintFix.__eq__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "LintFix", "func_name": "__eq__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: LintFix\n    def __eq__(self, other):\n        \"\"\"Compare equality with another fix.\n\n        A fix is equal to another if is in the same place (position), with the\n        same type and (if appropriate) the same edit values.\n\n        \"\"\"\n        if not self.edit_type == other.edit_type:\n            return False\n        if not self.anchor == other.anchor:\n            return False\n        if not self.edit == other.edit:\n            return False\n        return True  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "lintfix", "def", "__eq__", "self", "other", "compare", "equality", "with", "another", "fix", "a", "fix", "is", "equal", "to", "another", "if", "is", "in", "the", "same", "place", "position", "with", "the", "same", "type", "and", "if", "appropriate", "the", "same", "edit", "values", "if", "not", "self", "edit_type", "other", "edit_type", "return", "false", "if", "not", "self", "anchor", "other", "anchor", "return", "false", "if", "not", "self", "edit", "other", "edit", "return", "false", "return", "true", "pragma", "no", "cover", "todo"], "doc_len": 70}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.__init__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def __init__(self, code, description, **kwargs):\n        self.description = description\n        self.code = code\n        # kwargs represents the config passed to the rule. Add all kwargs as class attributes\n        # so they can be accessed in rules which inherit from this class\n        for key, value in kwargs.items():\n            self.__dict__[key] = value\n\n        # We also define a custom logger here, which also includes the code\n        # of the rule in the logging.\n        self.logger = RuleLoggingAdapter(rules_logger, {\"code\": code})\n        # Validate that declared configuration options exist\n        try:\n            for keyword in self.config_keywords:\n                if keyword not in kwargs.keys():\n                    raise ValueError(\n                        (\n                            \"Unrecognized config '{}' for Rule {}. If this \"\n                            \"is a new option, please add it to \"\n                            \"`default_config.cfg`\"\n                        ).format(keyword, code)\n                    )\n        except AttributeError:\n            self.logger.info(f\"No config_keywords defined for {code}\")\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "__init__", "self", "code", "description", "kwargs", "self", "description", "description", "self", "code", "code", "kwargs", "represents", "the", "config", "passed", "to", "the", "rule", "add", "all", "kwargs", "as", "class", "attributes", "so", "they", "can", "be", "accessed", "in", "rules", "which", "inherit", "from", "this", "class", "for", "key", "value", "in", "kwargs", "items", "self", "__dict__", "key", "value", "we", "also", "define", "a", "custom", "logger", "here", "which", "also", "includes", "the", "code", "of", "the", "rule", "in", "the", "logging", "self", "logger", "ruleloggingadapter", "rules_logger", "code", "code", "validate", "that", "declared", "configuration", "options", "exist", "try", "for", "keyword", "in", "self", "config_keywords", "if", "keyword", "not", "in", "kwargs", "keys", "raise", "valueerror", "unrecognized", "config", "for", "rule", "if", "this", "is", "a", "new", "option", "please", "add", "it", "to", "default_config", "cfg", "format", "keyword", "code", "except", "attributeerror", "self", "logger", "info", "f", "no", "config_keywords", "defined", "for", "code"], "doc_len": 129}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule._eval", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "_eval", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Evaluate this rule against the current context.\n\n        This should indicate whether a linting violation has occurred and/or\n        whether there is something to remember from this evaluation.\n\n        Note that an evaluate function should always accept `**kwargs`, but\n        if it relies on any available kwargs, it should explicitly call\n        them out at definition.\n\n        Returns:\n            :obj:`LintResult`, list of :obj:`LintResult` or :obj:`None`.\n\n        The reason that this method is called :meth:`_eval` and not `eval` is\n        a bit of a hack with sphinx autodoc, to make it so that the rule\n        documentation auto-generates nicely.\n\n        \"\"\"\n        raise NotImplementedError(\n            (\n                \"{} has not had its `eval` function defined. This is a problem \"\n                \"with the rule setup.\"\n            ).format(self.__class__.__name__)\n        )  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "_eval", "self", "context", "rulecontext", "evalresulttype", "evaluate", "this", "rule", "against", "the", "current", "context", "this", "should", "indicate", "whether", "a", "linting", "violation", "has", "occurred", "and", "or", "whether", "there", "is", "something", "to", "remember", "from", "this", "evaluation", "note", "that", "an", "evaluate", "function", "should", "always", "accept", "kwargs", "but", "if", "it", "relies", "on", "any", "available", "kwargs", "it", "should", "explicitly", "call", "them", "out", "at", "definition", "returns", "obj", "lintresult", "list", "of", "obj", "lintresult", "or", "obj", "none", "the", "reason", "that", "this", "method", "is", "called", "meth", "_eval", "and", "not", "eval", "is", "a", "bit", "of", "a", "hack", "with", "sphinx", "autodoc", "to", "make", "it", "so", "that", "the", "rule", "documentation", "auto", "generates", "nicely", "raise", "notimplementederror", "has", "not", "had", "its", "eval", "function", "defined", "this", "is", "a", "problem", "with", "the", "rule", "setup", "format", "self", "__class__", "__name__", "pragma", "no", "cover"], "doc_len": 131}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.crawl", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "crawl", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def crawl(\n        self,\n        segment,\n        ignore_mask,\n        dialect,\n        parent_stack=None,\n        siblings_pre=None,\n        siblings_post=None,\n        raw_stack=None,\n        memory=None,\n        fname=None,\n        templated_file: Optional[\"TemplatedFile\"] = None,\n    ):\n        \"\"\"Recursively perform the crawl operation on a given segment.\n\n        Returns:\n            A tuple of (vs, raw_stack, fixes, memory)\n\n        \"\"\"\n        # parent stack should be a tuple if it exists\n\n        # Rules should evaluate on segments FIRST, before evaluating on their\n        # children. They should also return a list of violations.\n\n        parent_stack = parent_stack or ()\n        raw_stack = raw_stack or ()\n        siblings_post = siblings_post or ()\n        siblings_pre = siblings_pre or ()\n        memory = memory or {}\n        vs: List[SQLLintError] = []\n        fixes: List[LintFix] = []\n\n        # First, check whether we're looking at an unparsable and whether\n        # this rule will still operate on that.\n        if not self._works_on_unparsable and segment.is_type(\"unparsable\"):\n            # Abort here if it doesn't. Otherwise we'll get odd results.\n            return vs, raw_stack, [], memory\n\n        # TODO: Document what options are available to the evaluation function.\n        try:\n            res = self._eval(\n                context=RuleContext(\n                    segment=segment,\n                    parent_stack=parent_stack,\n                    siblings_pre=siblings_pre,\n                    siblings_post=siblings_post,\n                    raw_stack=raw_stack,\n                    memory=memory,\n                    dialect=dialect,\n                    path=pathlib.Path(fname) if fname else None,\n                    templated_file=templated_file,\n                )\n            )\n        except (bdb.BdbQuit, KeyboardInterrupt):  # pragma: no cover\n            raise\n        # Any exception at this point would halt the linter and\n        # cause the user to get no results\n        except Exception as e:\n            self.logger.critical(\n                f\"Applying rule {self.code} threw an Exception: {e}\", exc_info=True\n            )\n            exception_line, _ = segment.pos_marker.source_position()\n            vs.append(\n                SQLLintError(\n                    rule=self,\n                    segment=segment,\n                    fixes=[],\n                    description=(\n                        f\"\"\"Unexpected exception: {str(e)};\n                        Could you open an issue at https://github.com/sqlfluff/sqlfluff/issues ?\n                        You can ignore this exception for now, by adding '--noqa: {self.code}' at the end\n                        of line {exception_line}\n                        \"\"\"\n                    ),\n                )\n            )\n            return vs, raw_stack, fixes, memory\n\n        new_lerrs = []\n        new_fixes = []\n\n        def _process_lint_result(res):\n            self.discard_unsafe_fixes(res, templated_file)\n            lerr = res.to_linting_error(rule=self)\n            ignored = False\n            if lerr:\n                if ignore_mask:\n                    filtered = LintedFile.ignore_masked_violations([lerr], ignore_mask)\n                    if not filtered:\n                        lerr = None\n                        ignored = True\n            if lerr:\n                new_lerrs.append(lerr)\n            if not ignored:\n                new_fixes.extend(res.fixes)\n\n        if res is None:\n            # Assume this means no problems (also means no memory)\n            pass\n        elif isinstance(res, LintResult):\n            # Extract any memory\n            memory = res.memory\n            _process_lint_result(res)\n        elif isinstance(res, list) and all(\n            isinstance(elem, LintResult) for elem in res\n        ):\n            # Extract any memory from the *last* one, assuming\n            # it was the last to be added\n            memory = res[-1].memory\n            for elem in res:\n                _process_lint_result(elem)\n        else:  # pragma: no cover\n            raise TypeError(\n                \"Got unexpected result [{!r}] back from linting rule: {!r}\".format(\n                    res, self.code\n                )\n            )\n\n        for lerr in new_lerrs:\n            self.logger.debug(\"!! Violation Found: %r\", lerr.description)\n        for fix in new_fixes:\n            self.logger.debug(\"!! Fix Proposed: %r\", fix)\n\n        # Consume the new results\n        vs += new_lerrs\n        fixes += new_fixes\n\n        # The raw stack only keeps track of the previous raw segments\n        if len(segment.segments) == 0:\n            raw_stack += (segment,)\n        # Parent stack keeps track of all the parent segments\n        parent_stack += (segment,)\n\n        for idx, child in enumerate(segment.segments):\n            dvs, raw_stack, child_fixes, memory = self.crawl(\n                segment=child,\n                ignore_mask=ignore_mask,\n                parent_stack=parent_stack,\n                siblings_pre=segment.segments[:idx],\n                siblings_post=segment.segments[idx + 1 :],\n                raw_stack=raw_stack,\n                memory=memory,\n                dialect=dialect,\n                fname=fname,\n                templated_file=templated_file,\n            )\n            vs += dvs\n            fixes += child_fixes\n        return vs, raw_stack, fixes, memory\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "crawl", "self", "segment", "ignore_mask", "dialect", "parent_stack", "none", "siblings_pre", "none", "siblings_post", "none", "raw_stack", "none", "memory", "none", "fname", "none", "templated_file", "optional", "templatedfile", "none", "recursively", "perform", "the", "crawl", "operation", "on", "a", "given", "segment", "returns", "a", "tuple", "of", "vs", "raw_stack", "fixes", "memory", "parent", "stack", "should", "be", "a", "tuple", "if", "it", "exists", "rules", "should", "evaluate", "on", "segments", "first", "before", "evaluating", "on", "their", "children", "they", "should", "also", "return", "a", "list", "of", "violations", "parent_stack", "parent_stack", "or", "raw_stack", "raw_stack", "or", "siblings_post", "siblings_post", "or", "siblings_pre", "siblings_pre", "or", "memory", "memory", "or", "vs", "list", "sqllinterror", "fixes", "list", "lintfix", "first", "check", "whether", "we", "re", "looking", "at", "an", "unparsable", "and", "whether", "this", "rule", "will", "still", "operate", "on", "that", "if", "not", "self", "_works_on_unparsable", "and", "segment", "is_type", "unparsable", "abort", "here", "if", "it", "doesn", "t", "otherwise", "we", "ll", "get", "odd", "results", "return", "vs", "raw_stack", "memory", "todo", "document", "what", "options", "are", "available", "to", "the", "evaluation", "function", "try", "res", "self", "_eval", "context", "rulecontext", "segment", "segment", "parent_stack", "parent_stack", "siblings_pre", "siblings_pre", "siblings_post", "siblings_post", "raw_stack", "raw_stack", "memory", "memory", "dialect", "dialect", "path", "pathlib", "path", "fname", "if", "fname", "else", "none", "templated_file", "templated_file", "except", "bdb", "bdbquit", "keyboardinterrupt", "pragma", "no", "cover", "raise", "any", "exception", "at", "this", "point", "would", "halt", "the", "linter", "and", "cause", "the", "user", "to", "get", "no", "results", "except", "exception", "as", "e", "self", "logger", "critical", "f", "applying", "rule", "self", "code", "threw", "an", "exception", "e", "exc_info", "true", "exception_line", "_", "segment", "pos_marker", "source_position", "vs", "append", "sqllinterror", "rule", "self", "segment", "segment", "fixes", "description", "f", "unexpected", "exception", "str", "e", "could", "you", "open", "an", "issue", "at", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "you", "can", "ignore", "this", "exception", "for", "now", "by", "adding", "noqa", "self", "code", "at", "the", "end", "of", "line", "exception_line", "return", "vs", "raw_stack", "fixes", "memory", "new_lerrs", "new_fixes", "def", "_process_lint_result", "res", "self", "discard_unsafe_fixes", "res", "templated_file", "lerr", "res", "to_linting_error", "rule", "self", "ignored", "false", "if", "lerr", "if", "ignore_mask", "filtered", "lintedfile", "ignore_masked_violations", "lerr", "ignore_mask", "if", "not", "filtered", "lerr", "none", "ignored", "true", "if", "lerr", "new_lerrs", "append", "lerr", "if", "not", "ignored", "new_fixes", "extend", "res", "fixes", "if", "res", "is", "none", "assume", "this", "means", "no", "problems", "also", "means", "no", "memory", "pass", "elif", "isinstance", "res", "lintresult", "extract", "any", "memory", "memory", "res", "memory", "_process_lint_result", "res", "elif", "isinstance", "res", "list", "and", "all", "isinstance", "elem", "lintresult", "for", "elem", "in", "res", "extract", "any", "memory", "from", "the", "last", "one", "assuming", "it", "was", "the", "last", "to", "be", "added", "memory", "res", "1", "memory", "for", "elem", "in", "res", "_process_lint_result", "elem", "else", "pragma", "no", "cover", "raise", "typeerror", "got", "unexpected", "result", "r", "back", "from", "linting", "rule", "r", "format", "res", "self", "code", "for", "lerr", "in", "new_lerrs", "self", "logger", "debug", "violation", "found", "r", "lerr", "description", "for", "fix", "in", "new_fixes", "self", "logger", "debug", "fix", "proposed", "r", "fix", "consume", "the", "new", "results", "vs", "new_lerrs", "fixes", "new_fixes", "the", "raw", "stack", "only", "keeps", "track", "of", "the", "previous", "raw", "segments", "if", "len", "segment", "segments", "0", "raw_stack", "segment", "parent", "stack", "keeps", "track", "of", "all", "the", "parent", "segments", "parent_stack", "segment", "for", "idx", "child", "in", "enumerate", "segment", "segments", "dvs", "raw_stack", "child_fixes", "memory", "self", "crawl", "segment", "child", "ignore_mask", "ignore_mask", "parent_stack", "parent_stack", "siblings_pre", "segment", "segments", "idx", "siblings_post", "segment", "segments", "idx", "1", "raw_stack", "raw_stack", "memory", "memory", "dialect", "dialect", "fname", "fname", "templated_file", "templated_file", "vs", "dvs", "fixes", "child_fixes", "return", "vs", "raw_stack", "fixes", "memory"], "doc_len": 508}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.filter_meta", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "filter_meta", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def filter_meta(segments, keep_meta=False):\n        \"\"\"Filter the segments to non-meta.\n\n        Or optionally the opposite if keep_meta is True.\n        \"\"\"\n        buff = []\n        for elem in segments:\n            if elem.is_meta is keep_meta:\n                buff.append(elem)\n        return tuple(buff)\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "filter_meta", "segments", "keep_meta", "false", "filter", "the", "segments", "to", "non", "meta", "or", "optionally", "the", "opposite", "if", "keep_meta", "is", "true", "buff", "for", "elem", "in", "segments", "if", "elem", "is_meta", "is", "keep_meta", "buff", "append", "elem", "return", "tuple", "buff"], "doc_len": 42}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.get_parent_of", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "get_parent_of", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def get_parent_of(cls, segment, root_segment):  # pragma: no cover TODO?\n        \"\"\"Return the segment immediately containing segment.\n\n        NB: This is recursive.\n\n        Args:\n            segment: The segment to look for.\n            root_segment: Some known parent of the segment\n                we're looking for (although likely not the\n                direct parent in question).\n\n        \"\"\"\n        if segment in root_segment.segments:\n            return root_segment\n        elif root_segment.segments:\n            # try each of the subsegments\n            for sub in root_segment.segments:\n                p = cls.get_parent_of(segment, sub)\n                if p:\n                    return p\n        # Not directly in the segment and\n        # no subsegments to check. Return None.\n        return None\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "get_parent_of", "cls", "segment", "root_segment", "pragma", "no", "cover", "todo", "return", "the", "segment", "immediately", "containing", "segment", "nb", "this", "is", "recursive", "args", "segment", "the", "segment", "to", "look", "for", "root_segment", "some", "known", "parent", "of", "the", "segment", "we", "re", "looking", "for", "although", "likely", "not", "the", "direct", "parent", "in", "question", "if", "segment", "in", "root_segment", "segments", "return", "root_segment", "elif", "root_segment", "segments", "try", "each", "of", "the", "subsegments", "for", "sub", "in", "root_segment", "segments", "p", "cls", "get_parent_of", "segment", "sub", "if", "p", "return", "p", "not", "directly", "in", "the", "segment", "and", "no", "subsegments", "to", "check", "return", "none", "return", "none"], "doc_len": 95}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.matches_target_tuples", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "matches_target_tuples", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def matches_target_tuples(seg: BaseSegment, target_tuples: List[Tuple[str, str]]):\n        \"\"\"Does the given segment match any of the given type tuples.\"\"\"\n        if seg.name in [elem[1] for elem in target_tuples if elem[0] == \"name\"]:\n            return True\n        elif seg.is_type(*[elem[1] for elem in target_tuples if elem[0] == \"type\"]):\n            return True\n        return False\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "matches_target_tuples", "seg", "basesegment", "target_tuples", "list", "tuple", "str", "str", "does", "the", "given", "segment", "match", "any", "of", "the", "given", "type", "tuples", "if", "seg", "name", "in", "elem", "1", "for", "elem", "in", "target_tuples", "if", "elem", "0", "name", "return", "true", "elif", "seg", "is_type", "elem", "1", "for", "elem", "in", "target_tuples", "if", "elem", "0", "type", "return", "true", "return", "false"], "doc_len": 60}
{"doc_id": "src/sqlfluff/core/rules/base.py::BaseRule.discard_unsafe_fixes", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "BaseRule", "func_name": "discard_unsafe_fixes", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: BaseRule\n    def discard_unsafe_fixes(\n        lint_result: LintResult, templated_file: Optional[TemplatedFile]\n    ):\n        \"\"\"Remove (discard) LintResult fixes if they are \"unsafe\".\n\n        By removing its fixes, a LintResult will still be reported, but it\n        will be treated as _unfixable_.\n        \"\"\"\n        if not lint_result.fixes or not templated_file:\n            return\n\n        # Get the set of slices touched by any of the fixes.\n        fix_slices = set()\n        for fix in lint_result.fixes:\n            if fix.anchor:\n                fix_slices.update(\n                    templated_file.raw_slices_spanning_source_slice(\n                        fix.anchor.pos_marker.source_slice\n                    )\n                )\n\n        # Compute the set of block IDs affected by the fixes. If it's more than\n        # one, discard the fixes. Rationale: Fixes that span block boundaries\n        # may corrupt the file, e.g. by moving code in or out of a template\n        # loop.\n        block_info = templated_file.raw_slice_block_info\n        fix_block_ids = set(block_info.block_ids[slice_] for slice_ in fix_slices)\n        if len(fix_block_ids) > 1:\n            linter_logger.info(\n                \"      * Discarding fixes that span blocks: %s\",\n                lint_result.fixes,\n            )\n            lint_result.fixes = []\n            return\n\n        # If the fixes touch a literal-only loop, discard the fixes.\n        # Rationale: Fixes to a template loop that contains only literals are:\n        # - Difficult to correctly back to source code, so there's a risk of\n        #   accidentally \"expanding\" the loop body if we apply them.\n        # - Highly unusual (In practice, templated loops in SQL are usually for\n        #   expanding the same code using different column names, types, etc.,\n        #   in which case the loop body contains template variables.\n        for block_id in fix_block_ids:\n            if block_id in block_info.literal_only_loops:\n                linter_logger.info(\n                    \"      * Discarding fixes to literal-only loop: %s\",\n                    lint_result.fixes,\n                )\n                lint_result.fixes = []\n                return\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "baserule", "def", "discard_unsafe_fixes", "lint_result", "lintresult", "templated_file", "optional", "templatedfile", "remove", "discard", "lintresult", "fixes", "if", "they", "are", "unsafe", "by", "removing", "its", "fixes", "a", "lintresult", "will", "still", "be", "reported", "but", "it", "will", "be", "treated", "as", "_unfixable_", "if", "not", "lint_result", "fixes", "or", "not", "templated_file", "return", "get", "the", "set", "of", "slices", "touched", "by", "any", "of", "the", "fixes", "fix_slices", "set", "for", "fix", "in", "lint_result", "fixes", "if", "fix", "anchor", "fix_slices", "update", "templated_file", "raw_slices_spanning_source_slice", "fix", "anchor", "pos_marker", "source_slice", "compute", "the", "set", "of", "block", "ids", "affected", "by", "the", "fixes", "if", "it", "s", "more", "than", "one", "discard", "the", "fixes", "rationale", "fixes", "that", "span", "block", "boundaries", "may", "corrupt", "the", "file", "e", "g", "by", "moving", "code", "in", "or", "out", "of", "a", "template", "loop", "block_info", "templated_file", "raw_slice_block_info", "fix_block_ids", "set", "block_info", "block_ids", "slice_", "for", "slice_", "in", "fix_slices", "if", "len", "fix_block_ids", "1", "linter_logger", "info", "discarding", "fixes", "that", "span", "blocks", "s", "lint_result", "fixes", "lint_result", "fixes", "return", "if", "the", "fixes", "touch", "a", "literal", "only", "loop", "discard", "the", "fixes", "rationale", "fixes", "to", "a", "template", "loop", "that", "contains", "only", "literals", "are", "difficult", "to", "correctly", "back", "to", "source", "code", "so", "there", "s", "a", "risk", "of", "accidentally", "expanding", "the", "loop", "body", "if", "we", "apply", "them", "highly", "unusual", "in", "practice", "templated", "loops", "in", "sql", "are", "usually", "for", "expanding", "the", "same", "code", "using", "different", "column", "names", "types", "etc", "in", "which", "case", "the", "loop", "body", "contains", "template", "variables", "for", "block_id", "in", "fix_block_ids", "if", "block_id", "in", "block_info", "literal_only_loops", "linter_logger", "info", "discarding", "fixes", "to", "literal", "only", "loop", "s", "lint_result", "fixes", "lint_result", "fixes", "return"], "doc_len": 243}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet.__init__", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def __init__(self, name, config_info):\n        self.name = name\n        self.config_info = config_info\n        self._register = {}\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "__init__", "self", "name", "config_info", "self", "name", "name", "self", "config_info", "config_info", "self", "_register"], "doc_len": 20}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet._validate_config_options", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "_validate_config_options", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def _validate_config_options(self, config, rule=None):\n        \"\"\"Ensure that all config options are valid.\n\n        Config options can also be checked for a specific rule e.g L010.\n        \"\"\"\n        rule_config = config.get_section(\"rules\")\n        for config_name, info_dict in self.config_info.items():\n            config_option = (\n                rule_config.get(config_name)\n                if not rule\n                else rule_config.get(rule).get(config_name)\n            )\n            valid_options = info_dict.get(\"validation\")\n            if (\n                valid_options\n                and config_option not in valid_options\n                and config_option is not None\n            ):\n                raise ValueError(\n                    (\n                        \"Invalid option '{}' for {} configuration. Must be one of {}\"\n                    ).format(\n                        config_option,\n                        config_name,\n                        valid_options,\n                    )\n                )\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "_validate_config_options", "self", "config", "rule", "none", "ensure", "that", "all", "config", "options", "are", "valid", "config", "options", "can", "also", "be", "checked", "for", "a", "specific", "rule", "e", "g", "l010", "rule_config", "config", "get_section", "rules", "for", "config_name", "info_dict", "in", "self", "config_info", "items", "config_option", "rule_config", "get", "config_name", "if", "not", "rule", "else", "rule_config", "get", "rule", "get", "config_name", "valid_options", "info_dict", "get", "validation", "if", "valid_options", "and", "config_option", "not", "in", "valid_options", "and", "config_option", "is", "not", "none", "raise", "valueerror", "invalid", "option", "for", "configuration", "must", "be", "one", "of", "format", "config_option", "config_name", "valid_options"], "doc_len": 87}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet.valid_rule_name_regex", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "valid_rule_name_regex", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def valid_rule_name_regex(self):\n        \"\"\"Defines the accepted pattern for rule names.\n\n        The first group captures the plugin name (optional), which\n        must be capitalized.\n        The second group captures the rule code.\n\n        Examples of valid rule names:\n        * Rule_PluginName_L001\n        * Rule_L001\n        \"\"\"\n        return re.compile(r\"Rule_?([A-Z]{1}[a-zA-Z]+)?_([A-Z][0-9]{3})\")\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "valid_rule_name_regex", "self", "defines", "the", "accepted", "pattern", "for", "rule", "names", "the", "first", "group", "captures", "the", "plugin", "name", "optional", "which", "must", "be", "capitalized", "the", "second", "group", "captures", "the", "rule", "code", "examples", "of", "valid", "rule", "names", "rule_pluginname_l001", "rule_l001", "return", "re", "compile", "r", "rule_", "a", "z", "1", "a", "za", "z", "_", "a", "z", "0", "9", "3"], "doc_len": 60}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet.register", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "register", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def register(self, cls, plugin=None):\n        \"\"\"Decorate a class with this to add it to the ruleset.\n\n        .. code-block:: python\n\n           @myruleset.register\n           class Rule_L001(BaseRule):\n               \"Description of rule.\"\n\n               def eval(self, **kwargs):\n                   return LintResult()\n\n        We expect that rules are defined as classes with the name `Rule_XXXX`\n        where `XXXX` is of the form `LNNN`, where L is a letter (literally L for\n        *linting* by default) and N is a three digit number.\n\n        If this receives classes by any other name, then it will raise a\n        :exc:`ValueError`.\n\n        \"\"\"\n        rule_name_match = self.valid_rule_name_regex.match(cls.__name__)\n        # Validate the name\n        if not rule_name_match:  # pragma: no cover\n            raise ValueError(\n                (\n                    \"Tried to register rule on set {!r} with unexpected \"\n                    \"format: {}, format should be: Rule_PluginName_L123 (for plugins) \"\n                    \"or Rule_L123 (for core rules).\"\n                ).format(self.name, cls.__name__)\n            )\n\n        plugin_name, code = rule_name_match.groups()\n        # If the docstring is multiline, then we extract just summary.\n        description = cls.__doc__.split(\"\\n\")[0]\n\n        if plugin_name:\n            code = f\"{plugin_name}_{code}\"\n\n        # Keep track of the *class* in the register. Don't instantiate yet.\n        if code in self._register:  # pragma: no cover\n            raise ValueError(\n                \"Rule {!r} has already been registered on RuleSet {!r}!\".format(\n                    code, self.name\n                )\n            )\n        self._register[code] = dict(code=code, description=description, cls=cls)\n\n        # Make sure we actually return the original class\n        return cls\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "register", "self", "cls", "plugin", "none", "decorate", "a", "class", "with", "this", "to", "add", "it", "to", "the", "ruleset", "code", "block", "python", "myruleset", "register", "class", "rule_l001", "baserule", "description", "of", "rule", "def", "eval", "self", "kwargs", "return", "lintresult", "we", "expect", "that", "rules", "are", "defined", "as", "classes", "with", "the", "name", "rule_xxxx", "where", "xxxx", "is", "of", "the", "form", "lnnn", "where", "l", "is", "a", "letter", "literally", "l", "for", "linting", "by", "default", "and", "n", "is", "a", "three", "digit", "number", "if", "this", "receives", "classes", "by", "any", "other", "name", "then", "it", "will", "raise", "a", "exc", "valueerror", "rule_name_match", "self", "valid_rule_name_regex", "match", "cls", "__name__", "validate", "the", "name", "if", "not", "rule_name_match", "pragma", "no", "cover", "raise", "valueerror", "tried", "to", "register", "rule", "on", "set", "r", "with", "unexpected", "format", "format", "should", "be", "rule_pluginname_l123", "for", "plugins", "or", "rule_l123", "for", "core", "rules", "format", "self", "name", "cls", "__name__", "plugin_name", "code", "rule_name_match", "groups", "if", "the", "docstring", "is", "multiline", "then", "we", "extract", "just", "summary", "description", "cls", "__doc__", "split", "n", "0", "if", "plugin_name", "code", "f", "plugin_name", "_", "code", "keep", "track", "of", "the", "class", "in", "the", "register", "don", "t", "instantiate", "yet", "if", "code", "in", "self", "_register", "pragma", "no", "cover", "raise", "valueerror", "rule", "r", "has", "already", "been", "registered", "on", "ruleset", "r", "format", "code", "self", "name", "self", "_register", "code", "dict", "code", "code", "description", "description", "cls", "cls", "make", "sure", "we", "actually", "return", "the", "original", "class", "return", "cls"], "doc_len": 218}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet.get_rulelist", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "get_rulelist", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def get_rulelist(self, config) -> List[BaseRule]:\n        \"\"\"Use the config to return the appropriate rules.\n\n        We use the config both for whitelisting and blacklisting, but also\n        for configuring the rules given the given config.\n\n        Returns:\n            :obj:`list` of instantiated :obj:`BaseRule`.\n\n        \"\"\"\n        # Validate all generic rule configs\n        self._validate_config_options(config)\n        # default the whitelist to all the rules if not set\n        whitelist = config.get(\"rule_whitelist\") or list(self._register.keys())\n        blacklist = config.get(\"rule_blacklist\") or []\n\n        whitelisted_unknown_rule_codes = [\n            r for r in whitelist if r not in self._register\n        ]\n        if any(whitelisted_unknown_rule_codes):\n            rules_logger.warning(\n                \"Tried to whitelist unknown rules: {!r}\".format(\n                    whitelisted_unknown_rule_codes\n                )\n            )\n\n        blacklisted_unknown_rule_codes = [\n            r for r in blacklist if r not in self._register\n        ]\n        if any(blacklisted_unknown_rule_codes):  # pragma: no cover\n            rules_logger.warning(\n                \"Tried to blacklist unknown rules: {!r}\".format(\n                    blacklisted_unknown_rule_codes\n                )\n            )\n\n        keylist = sorted(self._register.keys())\n        # First we filter the rules\n        keylist = [r for r in keylist if r in whitelist and r not in blacklist]\n\n        # Construct the kwargs for instantiation before we actually do it.\n        rule_kwargs = {}\n        for k in keylist:\n            kwargs = {}\n            generic_rule_config = config.get_section(\"rules\")\n            specific_rule_config = config.get_section(\n                (\"rules\", self._register[k][\"code\"])\n            )\n            if generic_rule_config:\n                kwargs.update(generic_rule_config)\n            if specific_rule_config:\n                # Validate specific rule config before adding\n                self._validate_config_options(config, self._register[k][\"code\"])\n                kwargs.update(specific_rule_config)\n            kwargs[\"code\"] = self._register[k][\"code\"]\n            # Allow variable substitution in making the description\n            kwargs[\"description\"] = self._register[k][\"description\"].format(**kwargs)\n            rule_kwargs[k] = kwargs\n\n        # Instantiate in the final step\n        return [self._register[k][\"cls\"](**rule_kwargs[k]) for k in keylist]\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "get_rulelist", "self", "config", "list", "baserule", "use", "the", "config", "to", "return", "the", "appropriate", "rules", "we", "use", "the", "config", "both", "for", "whitelisting", "and", "blacklisting", "but", "also", "for", "configuring", "the", "rules", "given", "the", "given", "config", "returns", "obj", "list", "of", "instantiated", "obj", "baserule", "validate", "all", "generic", "rule", "configs", "self", "_validate_config_options", "config", "default", "the", "whitelist", "to", "all", "the", "rules", "if", "not", "set", "whitelist", "config", "get", "rule_whitelist", "or", "list", "self", "_register", "keys", "blacklist", "config", "get", "rule_blacklist", "or", "whitelisted_unknown_rule_codes", "r", "for", "r", "in", "whitelist", "if", "r", "not", "in", "self", "_register", "if", "any", "whitelisted_unknown_rule_codes", "rules_logger", "warning", "tried", "to", "whitelist", "unknown", "rules", "r", "format", "whitelisted_unknown_rule_codes", "blacklisted_unknown_rule_codes", "r", "for", "r", "in", "blacklist", "if", "r", "not", "in", "self", "_register", "if", "any", "blacklisted_unknown_rule_codes", "pragma", "no", "cover", "rules_logger", "warning", "tried", "to", "blacklist", "unknown", "rules", "r", "format", "blacklisted_unknown_rule_codes", "keylist", "sorted", "self", "_register", "keys", "first", "we", "filter", "the", "rules", "keylist", "r", "for", "r", "in", "keylist", "if", "r", "in", "whitelist", "and", "r", "not", "in", "blacklist", "construct", "the", "kwargs", "for", "instantiation", "before", "we", "actually", "do", "it", "rule_kwargs", "for", "k", "in", "keylist", "kwargs", "generic_rule_config", "config", "get_section", "rules", "specific_rule_config", "config", "get_section", "rules", "self", "_register", "k", "code", "if", "generic_rule_config", "kwargs", "update", "generic_rule_config", "if", "specific_rule_config", "validate", "specific", "rule", "config", "before", "adding", "self", "_validate_config_options", "config", "self", "_register", "k", "code", "kwargs", "update", "specific_rule_config", "kwargs", "code", "self", "_register", "k", "code", "allow", "variable", "substitution", "in", "making", "the", "description", "kwargs", "description", "self", "_register", "k", "description", "format", "kwargs", "rule_kwargs", "k", "kwargs", "instantiate", "in", "the", "final", "step", "return", "self", "_register", "k", "cls", "rule_kwargs", "k", "for", "k", "in", "keylist"], "doc_len": 248}
{"doc_id": "src/sqlfluff/core/rules/base.py::RuleSet.copy", "file_path": "src/sqlfluff/core/rules/base.py", "class_name": "RuleSet", "func_name": "copy", "text": "文件路径: src/sqlfluff/core/rules/base.py, 类名: RuleSet\n    def copy(self):\n        \"\"\"Return a copy of self with a separate register.\"\"\"\n        new_ruleset = copy.copy(self)\n        new_ruleset._register = self._register.copy()\n        return new_ruleset\n", "tokens": ["src", "sqlfluff", "core", "rules", "base", "py", "ruleset", "def", "copy", "self", "return", "a", "copy", "of", "self", "with", "a", "separate", "register", "new_ruleset", "copy", "copy", "self", "new_ruleset", "_register", "self", "_register", "copy", "return", "new_ruleset"], "doc_len": 30}
{"doc_id": "src/sqlfluff/core/rules/config_info.py::get_config_info", "file_path": "src/sqlfluff/core/rules/config_info.py", "class_name": null, "func_name": "get_config_info", "text": "文件路径: src/sqlfluff/core/rules/config_info.py\ndef get_config_info() -> dict:\n    \"\"\"Gets the config from core sqlfluff and sqlfluff plugins and merges them.\"\"\"\n    plugin_manager = get_plugin_manager()\n    configs_info = plugin_manager.hook.get_configs_info()\n    return {\n        k: v for config_info_dict in configs_info for k, v in config_info_dict.items()\n    }\n", "tokens": ["src", "sqlfluff", "core", "rules", "config_info", "py", "def", "get_config_info", "dict", "gets", "the", "config", "from", "core", "sqlfluff", "and", "sqlfluff", "plugins", "and", "merges", "them", "plugin_manager", "get_plugin_manager", "configs_info", "plugin_manager", "hook", "get_configs_info", "return", "k", "v", "for", "config_info_dict", "in", "configs_info", "for", "k", "v", "in", "config_info_dict", "items"], "doc_len": 40}
{"doc_id": "src/sqlfluff/core/rules/doc_decorators.py::document_fix_compatible", "file_path": "src/sqlfluff/core/rules/doc_decorators.py", "class_name": null, "func_name": "document_fix_compatible", "text": "文件路径: src/sqlfluff/core/rules/doc_decorators.py\ndef document_fix_compatible(cls):\n    \"\"\"Mark the rule as fixable in the documentation.\"\"\"\n    cls.__doc__ = cls.__doc__.replace(\"\\n\", f\"\\n\\n{FIX_COMPATIBLE}\\n\\n\", 1)\n    return cls\n", "tokens": ["src", "sqlfluff", "core", "rules", "doc_decorators", "py", "def", "document_fix_compatible", "cls", "mark", "the", "rule", "as", "fixable", "in", "the", "documentation", "cls", "__doc__", "cls", "__doc__", "replace", "n", "f", "n", "n", "fix_compatible", "n", "n", "1", "return", "cls"], "doc_len": 32}
{"doc_id": "src/sqlfluff/core/rules/doc_decorators.py::is_fix_compatible", "file_path": "src/sqlfluff/core/rules/doc_decorators.py", "class_name": null, "func_name": "is_fix_compatible", "text": "文件路径: src/sqlfluff/core/rules/doc_decorators.py\ndef is_fix_compatible(cls) -> bool:  # pragma: no cover TODO?\n    \"\"\"Return whether the rule is documented as fixable.\"\"\"\n    return FIX_COMPATIBLE in cls.__doc__\n", "tokens": ["src", "sqlfluff", "core", "rules", "doc_decorators", "py", "def", "is_fix_compatible", "cls", "bool", "pragma", "no", "cover", "todo", "return", "whether", "the", "rule", "is", "documented", "as", "fixable", "return", "fix_compatible", "in", "cls", "__doc__"], "doc_len": 27}
{"doc_id": "src/sqlfluff/core/rules/doc_decorators.py::document_configuration", "file_path": "src/sqlfluff/core/rules/doc_decorators.py", "class_name": null, "func_name": "document_configuration", "text": "文件路径: src/sqlfluff/core/rules/doc_decorators.py\ndef document_configuration(cls, ruleset=\"std\"):\n    \"\"\"Add a 'Configuration' section to a Rule docstring.\n\n    Utilize the the metadata in config_info to dynamically\n    document the configuration options for a given rule.\n\n    This is a little hacky, but it allows us to propagate configuration\n    options in the docs, from a single source of truth.\n    \"\"\"\n    if ruleset == \"std\":\n        config_info = get_config_info()\n    else:  # pragma: no cover\n        raise (\n            NotImplementedError(\n                \"Add another config info dict for the new ruleset here!\"\n            )\n        )\n\n    config_doc = \"\\n    | **Configuration**\"\n    try:\n        for keyword in sorted(cls.config_keywords):\n            try:\n                info_dict = config_info[keyword]\n            except KeyError:  # pragma: no cover\n                raise KeyError(\n                    \"Config value {!r} for rule {} is not configured in `config_info`.\".format(\n                        keyword, cls.__name__\n                    )\n                )\n            config_doc += \"\\n    |     `{}`: {}.\".format(\n                keyword, info_dict[\"definition\"]\n            )\n            if \"validation\" in info_dict:\n                config_doc += \" Must be one of {}.\".format(info_dict[\"validation\"])\n            config_doc += \"\\n    |\"\n    except AttributeError:\n        rules_logger.info(f\"No config_keywords defined for {cls.__name__}\")\n        return cls\n    # Add final blank line\n    config_doc += \"\\n\"\n    # Add the configuration section immediately after the class description\n    # docstring by inserting after the first line break, or first period,\n    # if there is no line break.\n    end_of_class_description = \".\" if \"\\n\" not in cls.__doc__ else \"\\n\"\n\n    cls.__doc__ = cls.__doc__.replace(end_of_class_description, \"\\n\" + config_doc, 1)\n    return cls\n", "tokens": ["src", "sqlfluff", "core", "rules", "doc_decorators", "py", "def", "document_configuration", "cls", "ruleset", "std", "add", "a", "configuration", "section", "to", "a", "rule", "docstring", "utilize", "the", "the", "metadata", "in", "config_info", "to", "dynamically", "document", "the", "configuration", "options", "for", "a", "given", "rule", "this", "is", "a", "little", "hacky", "but", "it", "allows", "us", "to", "propagate", "configuration", "options", "in", "the", "docs", "from", "a", "single", "source", "of", "truth", "if", "ruleset", "std", "config_info", "get_config_info", "else", "pragma", "no", "cover", "raise", "notimplementederror", "add", "another", "config", "info", "dict", "for", "the", "new", "ruleset", "here", "config_doc", "n", "configuration", "try", "for", "keyword", "in", "sorted", "cls", "config_keywords", "try", "info_dict", "config_info", "keyword", "except", "keyerror", "pragma", "no", "cover", "raise", "keyerror", "config", "value", "r", "for", "rule", "is", "not", "configured", "in", "config_info", "format", "keyword", "cls", "__name__", "config_doc", "n", "format", "keyword", "info_dict", "definition", "if", "validation", "in", "info_dict", "config_doc", "must", "be", "one", "of", "format", "info_dict", "validation", "config_doc", "n", "except", "attributeerror", "rules_logger", "info", "f", "no", "config_keywords", "defined", "for", "cls", "__name__", "return", "cls", "add", "final", "blank", "line", "config_doc", "n", "add", "the", "configuration", "section", "immediately", "after", "the", "class", "description", "docstring", "by", "inserting", "after", "the", "first", "line", "break", "or", "first", "period", "if", "there", "is", "no", "line", "break", "end_of_class_description", "if", "n", "not", "in", "cls", "__doc__", "else", "n", "cls", "__doc__", "cls", "__doc__", "replace", "end_of_class_description", "n", "config_doc", "1", "return", "cls"], "doc_len": 198}
{"doc_id": "src/sqlfluff/core/rules/loader.py::get_rules_from_path", "file_path": "src/sqlfluff/core/rules/loader.py", "class_name": null, "func_name": "get_rules_from_path", "text": "文件路径: src/sqlfluff/core/rules/loader.py\ndef get_rules_from_path(\n    # All rule files are expected in the format of L*.py\n    rules_path=os.path.abspath(\n        os.path.join(os.path.dirname(__file__), \"../../rules\", \"L*.py\")\n    ),\n    base_module=\"sqlfluff.rules\",\n):\n    \"\"\"Reads all of the Rule classes from a path into a list.\"\"\"\n    # Create a rules dictionary for importing in sqlfluff/src/sqlfluff/core/rules/__init__.py\n    rules = []\n\n    for module in sorted(glob(rules_path)):\n        # Manipulate the module path to extract the filename without the .py\n        rule_id = os.path.splitext(os.path.basename(module))[0]\n        # All rule classes are expected in the format of Rule_L*\n        rule_class_name = f\"Rule_{rule_id}\"\n        # NOTE: We import the module outside of the try clause to\n        # properly catch any import errors.\n        rule_module = import_module(f\"{base_module}.{rule_id}\")\n        try:\n            rule_class = getattr(rule_module, rule_class_name)\n        except AttributeError as e:\n            raise AttributeError(\n                f\"Rule classes must be named in the format of Rule_L*. [{rule_class_name}]\"\n            ) from e\n        # Add the rules to the rules dictionary for sqlfluff/src/sqlfluff/core/rules/__init__.py\n        rules.append(rule_class)\n\n    return rules\n", "tokens": ["src", "sqlfluff", "core", "rules", "loader", "py", "def", "get_rules_from_path", "all", "rule", "files", "are", "expected", "in", "the", "format", "of", "l", "py", "rules_path", "os", "path", "abspath", "os", "path", "join", "os", "path", "dirname", "__file__", "rules", "l", "py", "base_module", "sqlfluff", "rules", "reads", "all", "of", "the", "rule", "classes", "from", "a", "path", "into", "a", "list", "create", "a", "rules", "dictionary", "for", "importing", "in", "sqlfluff", "src", "sqlfluff", "core", "rules", "__init__", "py", "rules", "for", "module", "in", "sorted", "glob", "rules_path", "manipulate", "the", "module", "path", "to", "extract", "the", "filename", "without", "the", "py", "rule_id", "os", "path", "splitext", "os", "path", "basename", "module", "0", "all", "rule", "classes", "are", "expected", "in", "the", "format", "of", "rule_l", "rule_class_name", "f", "rule_", "rule_id", "note", "we", "import", "the", "module", "outside", "of", "the", "try", "clause", "to", "properly", "catch", "any", "import", "errors", "rule_module", "import_module", "f", "base_module", "rule_id", "try", "rule_class", "getattr", "rule_module", "rule_class_name", "except", "attributeerror", "as", "e", "raise", "attributeerror", "f", "rule", "classes", "must", "be", "named", "in", "the", "format", "of", "rule_l", "rule_class_name", "from", "e", "add", "the", "rules", "to", "the", "rules", "dictionary", "for", "sqlfluff", "src", "sqlfluff", "core", "rules", "__init__", "py", "rules", "append", "rule_class", "return", "rules"], "doc_len": 169}
{"doc_id": "src/sqlfluff/core/rules/__init__.py::_load_standard_rules", "file_path": "src/sqlfluff/core/rules/__init__.py", "class_name": null, "func_name": "_load_standard_rules", "text": "文件路径: src/sqlfluff/core/rules/__init__.py\ndef _load_standard_rules():\n    \"\"\"Initialise the standard ruleset.\n\n    We do this on each call so that dynamic rules changes\n    are possible.\n    \"\"\"\n    std_rule_set = RuleSet(name=\"standard\", config_info=STANDARD_CONFIG_INFO_DICT)\n\n    # Iterate through the rules list and register each rule with the standard set.\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            std_rule_set.register(rule)\n\n    return std_rule_set\n", "tokens": ["src", "sqlfluff", "core", "rules", "__init__", "py", "def", "_load_standard_rules", "initialise", "the", "standard", "ruleset", "we", "do", "this", "on", "each", "call", "so", "that", "dynamic", "rules", "changes", "are", "possible", "std_rule_set", "ruleset", "name", "standard", "config_info", "standard_config_info_dict", "iterate", "through", "the", "rules", "list", "and", "register", "each", "rule", "with", "the", "standard", "set", "for", "plugin_rules", "in", "get_plugin_manager", "hook", "get_rules", "for", "rule", "in", "plugin_rules", "std_rule_set", "register", "rule", "return", "std_rule_set"], "doc_len": 59}
{"doc_id": "src/sqlfluff/core/rules/__init__.py::get_ruleset", "file_path": "src/sqlfluff/core/rules/__init__.py", "class_name": null, "func_name": "get_ruleset", "text": "文件路径: src/sqlfluff/core/rules/__init__.py\ndef get_ruleset(name: str = \"standard\") -> RuleSet:\n    \"\"\"Get a ruleset by name.\"\"\"\n    std_rules = _load_standard_rules()\n    lookup = {std_rules.name: std_rules}\n    # Return a copy in case someone modifies the register.\n    return lookup[name].copy()\n", "tokens": ["src", "sqlfluff", "core", "rules", "__init__", "py", "def", "get_ruleset", "name", "str", "standard", "ruleset", "get", "a", "ruleset", "by", "name", "std_rules", "_load_standard_rules", "lookup", "std_rules", "name", "std_rules", "return", "a", "copy", "in", "case", "someone", "modifies", "the", "register", "return", "lookup", "name", "copy"], "doc_len": 36}
{"doc_id": "src/sqlfluff/core/rules/analysis/select.py::get_select_statement_info", "file_path": "src/sqlfluff/core/rules/analysis/select.py", "class_name": null, "func_name": "get_select_statement_info", "text": "文件路径: src/sqlfluff/core/rules/analysis/select.py\ndef get_select_statement_info(\n    segment: BaseSegment, dialect: Optional[Dialect], early_exit: bool = True\n) -> Optional[SelectStatementColumnsAndTables]:\n    \"\"\"Analyze a select statement: targets, aliases, etc. Return info.\"\"\"\n    assert segment.is_type(\"select_statement\")\n    table_aliases, standalone_aliases = get_aliases_from_select(segment, dialect)\n    if early_exit and not table_aliases and not standalone_aliases:\n        return None\n\n    # Iterate through all the references, both in the select clause, but also\n    # potential others.\n    sc = segment.get_child(\"select_clause\")\n    reference_buffer = list(sc.recursive_crawl(\"object_reference\"))\n    # Add any wildcard references\n    reference_buffer += list(sc.recursive_crawl(\"wildcard_identifier\"))\n    for potential_clause in (\n        \"where_clause\",\n        \"groupby_clause\",\n        \"having_clause\",\n        \"orderby_clause\",\n    ):\n        clause = segment.get_child(potential_clause)\n        if clause:\n            reference_buffer += list(clause.recursive_crawl(\"object_reference\"))\n    # PURGE any references which are in nested select statements\n    for ref in reference_buffer.copy():\n        ref_path = segment.path_to(ref)\n        # is it in a subselect? i.e. a select which isn't this one.\n        if any(\n            seg.is_type(\"select_statement\") and seg is not segment for seg in ref_path\n        ):\n            reference_buffer.remove(ref)\n\n    # Get all select targets.\n    select_targets = segment.get_child(\"select_clause\").get_children(\n        \"select_clause_element\"\n    )\n\n    # Get all column aliases\n    col_aliases = []\n    for col_seg in list(sc.recursive_crawl(\"alias_expression\")):\n        for seg in col_seg.segments:\n            if seg.is_type(\"identifier\"):\n                col_aliases.append(seg.raw)\n\n    # Get any columns referred to in a using clause, and extract anything\n    # from ON clauses.\n    using_cols = []\n    fc = segment.get_child(\"from_clause\")\n    if fc:\n        for join_clause in fc.recursive_crawl(\"join_clause\"):\n            seen_using = False\n            for seg in join_clause.iter_segments():\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    seen_using = True\n                elif seg.is_type(\"join_on_condition\"):\n                    for on_seg in seg.segments:\n                        if on_seg.is_type(\"expression\"):\n                            # Deal with expressions\n                            reference_buffer += list(\n                                seg.recursive_crawl(\"object_reference\")\n                            )\n                elif seen_using and seg.is_type(\"bracketed\"):\n                    for subseg in seg.segments:\n                        if subseg.is_type(\"identifier\"):\n                            using_cols.append(subseg.raw)\n                    seen_using = False\n\n    return SelectStatementColumnsAndTables(\n        select_statement=segment,\n        table_aliases=table_aliases or [],\n        standalone_aliases=standalone_aliases or [],\n        reference_buffer=reference_buffer,\n        select_targets=select_targets,\n        col_aliases=col_aliases,\n        using_cols=using_cols,\n    )\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select", "py", "def", "get_select_statement_info", "segment", "basesegment", "dialect", "optional", "dialect", "early_exit", "bool", "true", "optional", "selectstatementcolumnsandtables", "analyze", "a", "select", "statement", "targets", "aliases", "etc", "return", "info", "assert", "segment", "is_type", "select_statement", "table_aliases", "standalone_aliases", "get_aliases_from_select", "segment", "dialect", "if", "early_exit", "and", "not", "table_aliases", "and", "not", "standalone_aliases", "return", "none", "iterate", "through", "all", "the", "references", "both", "in", "the", "select", "clause", "but", "also", "potential", "others", "sc", "segment", "get_child", "select_clause", "reference_buffer", "list", "sc", "recursive_crawl", "object_reference", "add", "any", "wildcard", "references", "reference_buffer", "list", "sc", "recursive_crawl", "wildcard_identifier", "for", "potential_clause", "in", "where_clause", "groupby_clause", "having_clause", "orderby_clause", "clause", "segment", "get_child", "potential_clause", "if", "clause", "reference_buffer", "list", "clause", "recursive_crawl", "object_reference", "purge", "any", "references", "which", "are", "in", "nested", "select", "statements", "for", "ref", "in", "reference_buffer", "copy", "ref_path", "segment", "path_to", "ref", "is", "it", "in", "a", "subselect", "i", "e", "a", "select", "which", "isn", "t", "this", "one", "if", "any", "seg", "is_type", "select_statement", "and", "seg", "is", "not", "segment", "for", "seg", "in", "ref_path", "reference_buffer", "remove", "ref", "get", "all", "select", "targets", "select_targets", "segment", "get_child", "select_clause", "get_children", "select_clause_element", "get", "all", "column", "aliases", "col_aliases", "for", "col_seg", "in", "list", "sc", "recursive_crawl", "alias_expression", "for", "seg", "in", "col_seg", "segments", "if", "seg", "is_type", "identifier", "col_aliases", "append", "seg", "raw", "get", "any", "columns", "referred", "to", "in", "a", "using", "clause", "and", "extract", "anything", "from", "on", "clauses", "using_cols", "fc", "segment", "get_child", "from_clause", "if", "fc", "for", "join_clause", "in", "fc", "recursive_crawl", "join_clause", "seen_using", "false", "for", "seg", "in", "join_clause", "iter_segments", "if", "seg", "is_type", "keyword", "and", "seg", "name", "using", "seen_using", "true", "elif", "seg", "is_type", "join_on_condition", "for", "on_seg", "in", "seg", "segments", "if", "on_seg", "is_type", "expression", "deal", "with", "expressions", "reference_buffer", "list", "seg", "recursive_crawl", "object_reference", "elif", "seen_using", "and", "seg", "is_type", "bracketed", "for", "subseg", "in", "seg", "segments", "if", "subseg", "is_type", "identifier", "using_cols", "append", "subseg", "raw", "seen_using", "false", "return", "selectstatementcolumnsandtables", "select_statement", "segment", "table_aliases", "table_aliases", "or", "standalone_aliases", "standalone_aliases", "or", "reference_buffer", "reference_buffer", "select_targets", "select_targets", "col_aliases", "col_aliases", "using_cols", "using_cols"], "doc_len": 286}
{"doc_id": "src/sqlfluff/core/rules/analysis/select.py::get_aliases_from_select", "file_path": "src/sqlfluff/core/rules/analysis/select.py", "class_name": null, "func_name": "get_aliases_from_select", "text": "文件路径: src/sqlfluff/core/rules/analysis/select.py\ndef get_aliases_from_select(segment, dialect=None):\n    \"\"\"Gets the aliases referred to in the FROM clause.\n\n    Returns a tuple of two lists:\n    - Table aliases\n    - Value table function aliases\n    \"\"\"\n    fc = segment.get_child(\"from_clause\")\n    if not fc:\n        # If there's no from clause then just abort.\n        return None, None\n    aliases = fc.get_eventual_aliases()\n\n    # We only want table aliases, so filter out aliases for value table\n    # functions and pivot columns.\n    table_aliases = []\n    standalone_aliases = _get_pivot_table_columns(segment, dialect)\n    for table_expr, alias_info in aliases:\n        if _has_value_table_function(table_expr, dialect):\n            if alias_info[0] not in standalone_aliases:\n                standalone_aliases.append(alias_info[0])\n        elif alias_info not in standalone_aliases:\n            table_aliases.append(alias_info)\n\n    return table_aliases, standalone_aliases\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select", "py", "def", "get_aliases_from_select", "segment", "dialect", "none", "gets", "the", "aliases", "referred", "to", "in", "the", "from", "clause", "returns", "a", "tuple", "of", "two", "lists", "table", "aliases", "value", "table", "function", "aliases", "fc", "segment", "get_child", "from_clause", "if", "not", "fc", "if", "there", "s", "no", "from", "clause", "then", "just", "abort", "return", "none", "none", "aliases", "fc", "get_eventual_aliases", "we", "only", "want", "table", "aliases", "so", "filter", "out", "aliases", "for", "value", "table", "functions", "and", "pivot", "columns", "table_aliases", "standalone_aliases", "_get_pivot_table_columns", "segment", "dialect", "for", "table_expr", "alias_info", "in", "aliases", "if", "_has_value_table_function", "table_expr", "dialect", "if", "alias_info", "0", "not", "in", "standalone_aliases", "standalone_aliases", "append", "alias_info", "0", "elif", "alias_info", "not", "in", "standalone_aliases", "table_aliases", "append", "alias_info", "return", "table_aliases", "standalone_aliases"], "doc_len": 106}
{"doc_id": "src/sqlfluff/core/rules/analysis/select.py::_has_value_table_function", "file_path": "src/sqlfluff/core/rules/analysis/select.py", "class_name": null, "func_name": "_has_value_table_function", "text": "文件路径: src/sqlfluff/core/rules/analysis/select.py\ndef _has_value_table_function(table_expr, dialect):\n    if not dialect:\n        # We need the dialect to get the value table function names. If\n        # we don't have it, assume the clause does not have a value table\n        # function.\n        return False\n\n    for function_name in table_expr.recursive_crawl(\"function_name\"):\n        # Other rules can increase whitespace in the function name, so use strip to remove\n        # See: https://github.com/sqlfluff/sqlfluff/issues/1304\n        if function_name.raw.lower().strip() in dialect.sets(\"value_table_functions\"):\n            return True\n    return False\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select", "py", "def", "_has_value_table_function", "table_expr", "dialect", "if", "not", "dialect", "we", "need", "the", "dialect", "to", "get", "the", "value", "table", "function", "names", "if", "we", "don", "t", "have", "it", "assume", "the", "clause", "does", "not", "have", "a", "value", "table", "function", "return", "false", "for", "function_name", "in", "table_expr", "recursive_crawl", "function_name", "other", "rules", "can", "increase", "whitespace", "in", "the", "function", "name", "so", "use", "strip", "to", "remove", "see", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "1304", "if", "function_name", "raw", "lower", "strip", "in", "dialect", "sets", "value_table_functions", "return", "true", "return", "false"], "doc_len": 84}
{"doc_id": "src/sqlfluff/core/rules/analysis/select.py::_get_pivot_table_columns", "file_path": "src/sqlfluff/core/rules/analysis/select.py", "class_name": null, "func_name": "_get_pivot_table_columns", "text": "文件路径: src/sqlfluff/core/rules/analysis/select.py\ndef _get_pivot_table_columns(segment, dialect):\n    if not dialect:\n        # We need the dialect to get the pivot table column names. If\n        # we don't have it, assume the clause does not have a pivot table\n        return []\n\n    fc = segment.get_child(\"from_pivot_expression\")\n    if not fc:\n        # If there's no pivot clause then just abort.\n        return []\n\n    pivot_table_column_aliases = []\n\n    for pivot_table_column_alias in segment.recursive_crawl(\"pivot_column_reference\"):\n        if pivot_table_column_alias.raw not in pivot_table_column_aliases:\n            pivot_table_column_aliases.append(pivot_table_column_alias.raw)\n\n    return pivot_table_column_aliases\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select", "py", "def", "_get_pivot_table_columns", "segment", "dialect", "if", "not", "dialect", "we", "need", "the", "dialect", "to", "get", "the", "pivot", "table", "column", "names", "if", "we", "don", "t", "have", "it", "assume", "the", "clause", "does", "not", "have", "a", "pivot", "table", "return", "fc", "segment", "get_child", "from_pivot_expression", "if", "not", "fc", "if", "there", "s", "no", "pivot", "clause", "then", "just", "abort", "return", "pivot_table_column_aliases", "for", "pivot_table_column_alias", "in", "segment", "recursive_crawl", "pivot_column_reference", "if", "pivot_table_column_alias", "raw", "not", "in", "pivot_table_column_aliases", "pivot_table_column_aliases", "append", "pivot_table_column_alias", "raw", "return", "pivot_table_column_aliases"], "doc_len": 77}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.gather", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "gather", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def gather(\n        cls, segment: BaseSegment, dialect: Dialect\n    ) -> Dict[Optional[str], List[\"SelectCrawler\"]]:\n        \"\"\"Find top-level SELECTs and CTEs, return info.\"\"\"\n        queries = defaultdict(list)\n        # We specify recurse_into=False because we only want top-level select\n        # statmeents and CTEs. We'll deal with nested selects later as needed,\n        # when processing their top-level parent.\n        for select_statement in segment.recursive_crawl(\n            \"select_statement\", recurse_into=False\n        ):\n            select_name = cls._get_name_if_cte(select_statement, segment)\n            queries[select_name].append(SelectCrawler(select_statement, dialect))\n        return dict(queries)\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "gather", "cls", "segment", "basesegment", "dialect", "dialect", "dict", "optional", "str", "list", "selectcrawler", "find", "top", "level", "selects", "and", "ctes", "return", "info", "queries", "defaultdict", "list", "we", "specify", "recurse_into", "false", "because", "we", "only", "want", "top", "level", "select", "statmeents", "and", "ctes", "we", "ll", "deal", "with", "nested", "selects", "later", "as", "needed", "when", "processing", "their", "top", "level", "parent", "for", "select_statement", "in", "segment", "recursive_crawl", "select_statement", "recurse_into", "false", "select_name", "cls", "_get_name_if_cte", "select_statement", "segment", "queries", "select_name", "append", "selectcrawler", "select_statement", "dialect", "return", "dict", "queries"], "doc_len": 82}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.get", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "get", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def get(\n        cls,\n        segment: BaseSegment,\n        queries: Dict[Optional[str], List[\"SelectCrawler\"]],\n        dialect: Dialect,\n    ) -> Union[str, List[\"SelectCrawler\"]]:\n        \"\"\"Find SELECTs, table refs, or value table function calls in segment.\n\n        If we find a SELECT, return info list. Otherwise, return table name\n        or function call string.\n        \"\"\"\n        for o in cls.crawl(segment, queries, dialect, False):\n            return o\n        assert False, \"Should be unreachable\"  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "get", "cls", "segment", "basesegment", "queries", "dict", "optional", "str", "list", "selectcrawler", "dialect", "dialect", "union", "str", "list", "selectcrawler", "find", "selects", "table", "refs", "or", "value", "table", "function", "calls", "in", "segment", "if", "we", "find", "a", "select", "return", "info", "list", "otherwise", "return", "table", "name", "or", "function", "call", "string", "for", "o", "in", "cls", "crawl", "segment", "queries", "dialect", "false", "return", "o", "assert", "false", "should", "be", "unreachable", "pragma", "no", "cover"], "doc_len": 71}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.crawl", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "crawl", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def crawl(\n        cls,\n        segment: BaseSegment,\n        queries: Dict[Optional[str], List[\"SelectCrawler\"]],\n        dialect: Dialect,\n        recurse_into=True,\n    ) -> Generator[Union[str, List[\"SelectCrawler\"]], None, None]:\n        \"\"\"Find SELECTs, table refs, or value table function calls in segment.\n\n        For each SELECT, yield a list of SelectCrawlers. As we find table\n        references or function call strings, yield those.\n        \"\"\"\n        buff = []\n        for seg in segment.recursive_crawl(\n            \"table_reference\", \"select_statement\", recurse_into=recurse_into\n        ):\n            if seg is segment:\n                # If we are starting with a select_statement, recursive_crawl()\n                # returns the statement itself. Skip that.\n                continue\n\n            if seg.is_type(\"table_reference\"):\n                if not seg.is_qualified() and seg.raw in queries:\n                    # It's a CTE.\n                    # :TRICKY: Pop the CTE from \"queries\" to help callers avoid\n                    # infinite recursion. We could make this behavior optional\n                    # someday, if necessary.\n                    yield queries.pop(seg.raw)\n                else:\n                    # It's an external table.\n                    yield seg.raw\n            else:\n                assert seg.is_type(\"select_statement\")\n                buff.append(SelectCrawler(seg, dialect))\n        if not buff:\n            # If we reach here, the SELECT may be querying from a value table\n            # function, e.g. UNNEST(). For our purposes, this is basically the\n            # same as an external table. Return the \"table\" part as a string.\n            table_expr = segment.get_child(\"table_expression\")\n            if table_expr:\n                yield table_expr.raw\n        yield buff\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "crawl", "cls", "segment", "basesegment", "queries", "dict", "optional", "str", "list", "selectcrawler", "dialect", "dialect", "recurse_into", "true", "generator", "union", "str", "list", "selectcrawler", "none", "none", "find", "selects", "table", "refs", "or", "value", "table", "function", "calls", "in", "segment", "for", "each", "select", "yield", "a", "list", "of", "selectcrawlers", "as", "we", "find", "table", "references", "or", "function", "call", "strings", "yield", "those", "buff", "for", "seg", "in", "segment", "recursive_crawl", "table_reference", "select_statement", "recurse_into", "recurse_into", "if", "seg", "is", "segment", "if", "we", "are", "starting", "with", "a", "select_statement", "recursive_crawl", "returns", "the", "statement", "itself", "skip", "that", "continue", "if", "seg", "is_type", "table_reference", "if", "not", "seg", "is_qualified", "and", "seg", "raw", "in", "queries", "it", "s", "a", "cte", "tricky", "pop", "the", "cte", "from", "queries", "to", "help", "callers", "avoid", "infinite", "recursion", "we", "could", "make", "this", "behavior", "optional", "someday", "if", "necessary", "yield", "queries", "pop", "seg", "raw", "else", "it", "s", "an", "external", "table", "yield", "seg", "raw", "else", "assert", "seg", "is_type", "select_statement", "buff", "append", "selectcrawler", "seg", "dialect", "if", "not", "buff", "if", "we", "reach", "here", "the", "select", "may", "be", "querying", "from", "a", "value", "table", "function", "e", "g", "unnest", "for", "our", "purposes", "this", "is", "basically", "the", "same", "as", "an", "external", "table", "return", "the", "table", "part", "as", "a", "string", "table_expr", "segment", "get_child", "table_expression", "if", "table_expr", "yield", "table_expr", "raw", "yield", "buff"], "doc_len": 201}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.__init__", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def __init__(self, select_statement, dialect):\n        self.select_statement = select_statement\n        self.dialect = dialect\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "__init__", "self", "select_statement", "dialect", "self", "select_statement", "select_statement", "self", "dialect", "dialect"], "doc_len": 19}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.select_info", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "select_info", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def select_info(self):\n        \"\"\"Returns SelectStatementColumnsAndTables on the SELECT.\"\"\"\n        result = get_select_statement_info(\n            self.select_statement, self.dialect, early_exit=False\n        )\n        return result\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "select_info", "self", "returns", "selectstatementcolumnsandtables", "on", "the", "select", "result", "get_select_statement_info", "self", "select_statement", "self", "dialect", "early_exit", "false", "return", "result"], "doc_len": 26}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.find_alias", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "find_alias", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def find_alias(self, table: str) -> Optional[AliasInfo]:\n        \"\"\"Find corresponding table_aliases entry (if any) matching \"table\".\"\"\"\n        alias_info = [\n            t\n            for t in self.select_info.table_aliases\n            if t.aliased and t.ref_str == table\n        ]\n        assert len(alias_info) <= 1\n        return alias_info[0] if alias_info else None\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "find_alias", "self", "table", "str", "optional", "aliasinfo", "find", "corresponding", "table_aliases", "entry", "if", "any", "matching", "table", "alias_info", "t", "for", "t", "in", "self", "select_info", "table_aliases", "if", "t", "aliased", "and", "t", "ref_str", "table", "assert", "len", "alias_info", "1", "return", "alias_info", "0", "if", "alias_info", "else", "none"], "doc_len": 49}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler.get_wildcard_info", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "get_wildcard_info", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def get_wildcard_info(self) -> List[WildcardInfo]:\n        \"\"\"Find wildcard (*) targets in the SELECT.\"\"\"\n        buff = []\n        for seg in self.select_info.select_targets:\n            if seg.get_child(\"wildcard_expression\"):\n                if \".\" in seg.raw:\n                    # The wildcard specifies a target table.\n                    table = seg.raw.rsplit(\".\", 1)[0]\n                    buff.append(WildcardInfo(seg, [table]))\n                else:\n                    # The wildcard is unqualified (i.e. does not specify a\n                    # table). This means to include all columns from all the\n                    # tables in the query.\n                    buff.append(\n                        WildcardInfo(\n                            seg,\n                            [\n                                alias_info.ref_str\n                                if alias_info.aliased\n                                else alias_info.from_expression_element.raw\n                                for alias_info in self.select_info.table_aliases\n                            ],\n                        )\n                    )\n        return buff\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "get_wildcard_info", "self", "list", "wildcardinfo", "find", "wildcard", "targets", "in", "the", "select", "buff", "for", "seg", "in", "self", "select_info", "select_targets", "if", "seg", "get_child", "wildcard_expression", "if", "in", "seg", "raw", "the", "wildcard", "specifies", "a", "target", "table", "table", "seg", "raw", "rsplit", "1", "0", "buff", "append", "wildcardinfo", "seg", "table", "else", "the", "wildcard", "is", "unqualified", "i", "e", "does", "not", "specify", "a", "table", "this", "means", "to", "include", "all", "columns", "from", "all", "the", "tables", "in", "the", "query", "buff", "append", "wildcardinfo", "seg", "alias_info", "ref_str", "if", "alias_info", "aliased", "else", "alias_info", "from_expression_element", "raw", "for", "alias_info", "in", "self", "select_info", "table_aliases", "return", "buff"], "doc_len": 97}
{"doc_id": "src/sqlfluff/core/rules/analysis/select_crawler.py::SelectCrawler._get_name_if_cte", "file_path": "src/sqlfluff/core/rules/analysis/select_crawler.py", "class_name": "SelectCrawler", "func_name": "_get_name_if_cte", "text": "文件路径: src/sqlfluff/core/rules/analysis/select_crawler.py, 类名: SelectCrawler\n    def _get_name_if_cte(\n        select_statement: BaseSegment, ancestor_segment: BaseSegment\n    ) -> Optional[str]:\n        \"\"\"Return name if CTE. If top-level, return None.\"\"\"\n        cte = None\n        path_to = ancestor_segment.path_to(select_statement)\n        for seg in path_to:\n            if seg.is_type(\"common_table_expression\"):\n                cte = seg\n                break\n        select_name = cte.segments[0].raw if cte else None\n        return select_name\n", "tokens": ["src", "sqlfluff", "core", "rules", "analysis", "select_crawler", "py", "selectcrawler", "def", "_get_name_if_cte", "select_statement", "basesegment", "ancestor_segment", "basesegment", "optional", "str", "return", "name", "if", "cte", "if", "top", "level", "return", "none", "cte", "none", "path_to", "ancestor_segment", "path_to", "select_statement", "for", "seg", "in", "path_to", "if", "seg", "is_type", "common_table_expression", "cte", "seg", "break", "select_name", "cte", "segments", "0", "raw", "if", "cte", "else", "none", "return", "select_name"], "doc_len": 53}
{"doc_id": "src/sqlfluff/core/templaters/base.py::iter_indices_of_newlines", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": null, "func_name": "iter_indices_of_newlines", "text": "文件路径: src/sqlfluff/core/templaters/base.py\ndef iter_indices_of_newlines(raw_str: str) -> Iterator[int]:\n    \"\"\"Find the indices of all newlines in a string.\"\"\"\n    init_idx = -1\n    while True:\n        nl_pos = raw_str.find(\"\\n\", init_idx + 1)\n        if nl_pos >= 0:\n            yield nl_pos\n            init_idx = nl_pos\n        else:\n            break  # pragma: no cover TODO?\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "def", "iter_indices_of_newlines", "raw_str", "str", "iterator", "int", "find", "the", "indices", "of", "all", "newlines", "in", "a", "string", "init_idx", "1", "while", "true", "nl_pos", "raw_str", "find", "n", "init_idx", "1", "if", "nl_pos", "0", "yield", "nl_pos", "init_idx", "nl_pos", "else", "break", "pragma", "no", "cover", "todo"], "doc_len": 44}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawFileSlice.end_source_idx", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawFileSlice", "func_name": "end_source_idx", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawFileSlice\n    def end_source_idx(self):\n        \"\"\"Return the closing index of this slice.\"\"\"\n        return self.source_idx + len(self.raw)\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawfileslice", "def", "end_source_idx", "self", "return", "the", "closing", "index", "of", "this", "slice", "return", "self", "source_idx", "len", "self", "raw"], "doc_len": 23}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawFileSlice.source_slice", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawFileSlice", "func_name": "source_slice", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawFileSlice\n    def source_slice(self):\n        \"\"\"Return the a slice object for this slice.\"\"\"\n        return slice(self.source_idx, self.end_source_idx())\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawfileslice", "def", "source_slice", "self", "return", "the", "a", "slice", "object", "for", "this", "slice", "return", "slice", "self", "source_idx", "self", "end_source_idx"], "doc_len": 24}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.__init__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def __init__(\n        self,\n        source_str: str,\n        fname: str,\n        templated_str: Optional[str] = None,\n        sliced_file: Optional[List[TemplatedFileSlice]] = None,\n        raw_sliced: Optional[List[RawFileSlice]] = None,\n    ):\n        \"\"\"Initialise the TemplatedFile.\n\n        If no templated_str is provided then we assume that\n        the file is NOT templated and that the templated view\n        is the same as the source view.\n        \"\"\"\n        self.source_str = source_str\n        # An empty string is still allowed as the templated string.\n        self.templated_str = source_str if templated_str is None else templated_str\n        # If no fname, we assume this is from a string or stdin.\n        self.fname = fname\n        # Assume that no sliced_file, means the file is not templated\n        # TODO: Enable error handling.\n        if (\n            not sliced_file\n        ) and self.templated_str != self.source_str:  # pragma: no cover\n            raise ValueError(\"Cannot instantiate a templated file unsliced!\")\n        # If we get here and we don't have sliced files, then it's raw, so create them.\n        self.sliced_file: List[TemplatedFileSlice] = sliced_file or [\n            TemplatedFileSlice(\n                \"literal\", slice(0, len(source_str)), slice(0, len(source_str))\n            )\n        ]\n        self.raw_sliced: List[RawFileSlice] = raw_sliced or [\n            RawFileSlice(source_str, \"literal\", 0)\n        ]\n        # Precalculate newlines, character positions.\n        self._source_newlines = list(iter_indices_of_newlines(self.source_str))\n        self._templated_newlines = list(iter_indices_of_newlines(self.templated_str))\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "__init__", "self", "source_str", "str", "fname", "str", "templated_str", "optional", "str", "none", "sliced_file", "optional", "list", "templatedfileslice", "none", "raw_sliced", "optional", "list", "rawfileslice", "none", "initialise", "the", "templatedfile", "if", "no", "templated_str", "is", "provided", "then", "we", "assume", "that", "the", "file", "is", "not", "templated", "and", "that", "the", "templated", "view", "is", "the", "same", "as", "the", "source", "view", "self", "source_str", "source_str", "an", "empty", "string", "is", "still", "allowed", "as", "the", "templated", "string", "self", "templated_str", "source_str", "if", "templated_str", "is", "none", "else", "templated_str", "if", "no", "fname", "we", "assume", "this", "is", "from", "a", "string", "or", "stdin", "self", "fname", "fname", "assume", "that", "no", "sliced_file", "means", "the", "file", "is", "not", "templated", "todo", "enable", "error", "handling", "if", "not", "sliced_file", "and", "self", "templated_str", "self", "source_str", "pragma", "no", "cover", "raise", "valueerror", "cannot", "instantiate", "a", "templated", "file", "unsliced", "if", "we", "get", "here", "and", "we", "don", "t", "have", "sliced", "files", "then", "it", "s", "raw", "so", "create", "them", "self", "sliced_file", "list", "templatedfileslice", "sliced_file", "or", "templatedfileslice", "literal", "slice", "0", "len", "source_str", "slice", "0", "len", "source_str", "self", "raw_sliced", "list", "rawfileslice", "raw_sliced", "or", "rawfileslice", "source_str", "literal", "0", "precalculate", "newlines", "character", "positions", "self", "_source_newlines", "list", "iter_indices_of_newlines", "self", "source_str", "self", "_templated_newlines", "list", "iter_indices_of_newlines", "self", "templated_str"], "doc_len": 187}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.from_string", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "from_string", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def from_string(cls, raw):\n        \"\"\"Create TemplatedFile from a string.\"\"\"\n        return cls(source_str=raw, fname=\"<string>\")\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "from_string", "cls", "raw", "create", "templatedfile", "from", "a", "string", "return", "cls", "source_str", "raw", "fname", "string"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.__bool__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "__bool__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def __bool__(self):\n        \"\"\"Return true if there's a templated file.\"\"\"\n        return bool(self.templated_str)\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "__bool__", "self", "return", "true", "if", "there", "s", "a", "templated", "file", "return", "bool", "self", "templated_str"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.__repr__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "__repr__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def __repr__(self):  # pragma: no cover TODO?\n        return \"<TemplatedFile>\"\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "__repr__", "self", "pragma", "no", "cover", "todo", "return", "templatedfile"], "doc_len": 16}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.__str__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "__str__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def __str__(self):\n        \"\"\"Return the templated file if coerced to string.\"\"\"\n        return self.templated_str\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "__str__", "self", "return", "the", "templated", "file", "if", "coerced", "to", "string", "return", "self", "templated_str"], "doc_len": 21}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.get_line_pos_of_char_pos", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "get_line_pos_of_char_pos", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def get_line_pos_of_char_pos(\n        self, char_pos: int, source: bool = True\n    ) -> Tuple[int, int]:\n        \"\"\"Get the line number and position of a point in the source file.\n\n        Args:\n            char_pos: The character position in the relevant file.\n            source: Are we checking the source file (as opposed to the\n                templated file)\n\n        Returns:\n            line_number, line_position\n\n        \"\"\"\n        if source:\n            ref_str = self._source_newlines\n        else:\n            ref_str = self._templated_newlines\n\n        nl_idx = bisect_left(ref_str, char_pos)\n\n        if nl_idx > 0:\n            return nl_idx + 1, char_pos - ref_str[nl_idx - 1]\n        else:\n            # NB: line_pos is char_pos+1 because character position is 0-indexed,\n            # but the line position is 1-indexed.\n            return 1, char_pos + 1\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "get_line_pos_of_char_pos", "self", "char_pos", "int", "source", "bool", "true", "tuple", "int", "int", "get", "the", "line", "number", "and", "position", "of", "a", "point", "in", "the", "source", "file", "args", "char_pos", "the", "character", "position", "in", "the", "relevant", "file", "source", "are", "we", "checking", "the", "source", "file", "as", "opposed", "to", "the", "templated", "file", "returns", "line_number", "line_position", "if", "source", "ref_str", "self", "_source_newlines", "else", "ref_str", "self", "_templated_newlines", "nl_idx", "bisect_left", "ref_str", "char_pos", "if", "nl_idx", "0", "return", "nl_idx", "1", "char_pos", "ref_str", "nl_idx", "1", "else", "nb", "line_pos", "is", "char_pos", "1", "because", "character", "position", "is", "0", "indexed", "but", "the", "line", "position", "is", "1", "indexed", "return", "1", "char_pos", "1"], "doc_len": 102}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile._find_slice_indices_of_templated_pos", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "_find_slice_indices_of_templated_pos", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def _find_slice_indices_of_templated_pos(\n        self,\n        templated_pos: int,\n        start_idx: Optional[int] = None,\n        inclusive: bool = True,\n    ) -> Tuple[int, int]:\n        \"\"\"Find a subset of the sliced file which touch this point.\n\n        NB: the last_idx is exclusive, as the intent is to use this as a slice.\n        \"\"\"\n        start_idx = start_idx or 0\n        first_idx = None\n        last_idx = start_idx\n        for idx, elem in enumerate(self.sliced_file[start_idx:]):\n            last_idx = idx + start_idx\n            if elem[2].stop >= templated_pos:\n                if first_idx is None:\n                    first_idx = idx + start_idx\n                if elem[2].start > templated_pos:\n                    break\n                elif not inclusive and elem[2].start >= templated_pos:\n                    break\n        # If we got to the end add another index\n        else:\n            last_idx += 1\n        if first_idx is None:  # pragma: no cover\n            raise ValueError(\"Position Not Found\")\n        return first_idx, last_idx\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "_find_slice_indices_of_templated_pos", "self", "templated_pos", "int", "start_idx", "optional", "int", "none", "inclusive", "bool", "true", "tuple", "int", "int", "find", "a", "subset", "of", "the", "sliced", "file", "which", "touch", "this", "point", "nb", "the", "last_idx", "is", "exclusive", "as", "the", "intent", "is", "to", "use", "this", "as", "a", "slice", "start_idx", "start_idx", "or", "0", "first_idx", "none", "last_idx", "start_idx", "for", "idx", "elem", "in", "enumerate", "self", "sliced_file", "start_idx", "last_idx", "idx", "start_idx", "if", "elem", "2", "stop", "templated_pos", "if", "first_idx", "is", "none", "first_idx", "idx", "start_idx", "if", "elem", "2", "start", "templated_pos", "break", "elif", "not", "inclusive", "and", "elem", "2", "start", "templated_pos", "break", "if", "we", "got", "to", "the", "end", "add", "another", "index", "else", "last_idx", "1", "if", "first_idx", "is", "none", "pragma", "no", "cover", "raise", "valueerror", "position", "not", "found", "return", "first_idx", "last_idx"], "doc_len": 121}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.raw_slice_block_info", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "raw_slice_block_info", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def raw_slice_block_info(self) -> RawSliceBlockInfo:\n        \"\"\"Returns a dict with a unique ID for each template block.\"\"\"\n        block_ids = {}\n        block_content_types = defaultdict(set)\n        loops = set()\n        blocks = []\n        block_id = 0\n        for idx, raw_slice in enumerate(self.raw_sliced):\n            if raw_slice.slice_type != \"block_end\":\n                block_content_types[block_id].add(raw_slice.slice_type)\n            if raw_slice.slice_type == \"block_start\":\n                blocks.append(raw_slice)\n                templater_logger.info(\"%d -> %r\", block_id, raw_slice.raw)\n                block_ids[raw_slice] = block_id\n                block_id += 1\n                if raw_slice.slice_subtype == \"loop\":\n                    loops.add(block_id)\n            elif raw_slice.slice_type == \"block_end\":\n                blocks.pop()\n                block_id += 1\n                templater_logger.info(\"%d -> %r\", block_id, raw_slice.raw)\n                block_ids[raw_slice] = block_id\n            else:\n                templater_logger.info(\"%d -> %r\", block_id, raw_slice.raw)\n                block_ids[raw_slice] = block_id\n        literal_only_loops = [\n            block_id\n            for block_id in set(block_ids.values())\n            if block_id in loops and block_content_types[block_id] == {\"literal\"}\n        ]\n        return RawSliceBlockInfo(block_ids, literal_only_loops)\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "raw_slice_block_info", "self", "rawsliceblockinfo", "returns", "a", "dict", "with", "a", "unique", "id", "for", "each", "template", "block", "block_ids", "block_content_types", "defaultdict", "set", "loops", "set", "blocks", "block_id", "0", "for", "idx", "raw_slice", "in", "enumerate", "self", "raw_sliced", "if", "raw_slice", "slice_type", "block_end", "block_content_types", "block_id", "add", "raw_slice", "slice_type", "if", "raw_slice", "slice_type", "block_start", "blocks", "append", "raw_slice", "templater_logger", "info", "d", "r", "block_id", "raw_slice", "raw", "block_ids", "raw_slice", "block_id", "block_id", "1", "if", "raw_slice", "slice_subtype", "loop", "loops", "add", "block_id", "elif", "raw_slice", "slice_type", "block_end", "blocks", "pop", "block_id", "1", "templater_logger", "info", "d", "r", "block_id", "raw_slice", "raw", "block_ids", "raw_slice", "block_id", "else", "templater_logger", "info", "d", "r", "block_id", "raw_slice", "raw", "block_ids", "raw_slice", "block_id", "literal_only_loops", "block_id", "for", "block_id", "in", "set", "block_ids", "values", "if", "block_id", "in", "loops", "and", "block_content_types", "block_id", "literal", "return", "rawsliceblockinfo", "block_ids", "literal_only_loops"], "doc_len": 122}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.raw_slices_spanning_source_slice", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "raw_slices_spanning_source_slice", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def raw_slices_spanning_source_slice(self, source_slice: slice):\n        \"\"\"Return a list of the raw slices spanning a set of indices.\"\"\"\n        # First find the start index\n        raw_slice_idx = 0\n        # Move the raw pointer forward to the start of this patch\n        while (\n            raw_slice_idx + 1 < len(self.raw_sliced)\n            and self.raw_sliced[raw_slice_idx + 1].source_idx <= source_slice.start\n        ):\n            raw_slice_idx += 1\n        # Find slice index of the end of this patch.\n        slice_span = 1\n        while (\n            raw_slice_idx + slice_span < len(self.raw_sliced)\n            and self.raw_sliced[raw_slice_idx + slice_span].source_idx\n            < source_slice.stop\n        ):\n            slice_span += 1\n        # Return the raw slices:\n        return self.raw_sliced[raw_slice_idx : raw_slice_idx + slice_span]\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "raw_slices_spanning_source_slice", "self", "source_slice", "slice", "return", "a", "list", "of", "the", "raw", "slices", "spanning", "a", "set", "of", "indices", "first", "find", "the", "start", "index", "raw_slice_idx", "0", "move", "the", "raw", "pointer", "forward", "to", "the", "start", "of", "this", "patch", "while", "raw_slice_idx", "1", "len", "self", "raw_sliced", "and", "self", "raw_sliced", "raw_slice_idx", "1", "source_idx", "source_slice", "start", "raw_slice_idx", "1", "find", "slice", "index", "of", "the", "end", "of", "this", "patch", "slice_span", "1", "while", "raw_slice_idx", "slice_span", "len", "self", "raw_sliced", "and", "self", "raw_sliced", "raw_slice_idx", "slice_span", "source_idx", "source_slice", "stop", "slice_span", "1", "return", "the", "raw", "slices", "return", "self", "raw_sliced", "raw_slice_idx", "raw_slice_idx", "slice_span"], "doc_len": 95}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.templated_slice_to_source_slice", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "templated_slice_to_source_slice", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def templated_slice_to_source_slice(\n        self,\n        template_slice: slice,\n    ) -> slice:\n        \"\"\"Convert a template slice to a source slice.\"\"\"\n        if not self.sliced_file:\n            return template_slice  # pragma: no cover TODO?\n\n        ts_start_sf_start, ts_start_sf_stop = self._find_slice_indices_of_templated_pos(\n            template_slice.start\n        )\n\n        ts_start_subsliced_file = self.sliced_file[ts_start_sf_start:ts_start_sf_stop]\n\n        # Work out the insertion point\n        insertion_point = -1\n        for elem in ts_start_subsliced_file:\n            # Do slice starts and ends:\n            for slice_elem in (\"start\", \"stop\"):\n                if getattr(elem[2], slice_elem) == template_slice.start:\n                    # Store the lowest.\n                    point = getattr(elem[1], slice_elem)\n                    if insertion_point < 0 or point < insertion_point:\n                        insertion_point = point\n                    # We don't break here, because we might find ANOTHER\n                    # later which is actually earlier.\n\n        # Zero length slice.\n        if template_slice.start == template_slice.stop:\n            # Is it on a join?\n            if insertion_point >= 0:\n                return slice(insertion_point, insertion_point)\n            # It's within a segment.\n            else:\n                if (\n                    ts_start_subsliced_file\n                    and ts_start_subsliced_file[0][0] == \"literal\"\n                ):\n                    offset = template_slice.start - ts_start_subsliced_file[0][2].start\n                    return slice(\n                        ts_start_subsliced_file[0][1].start + offset,\n                        ts_start_subsliced_file[0][1].start + offset,\n                    )\n                else:\n                    raise ValueError(\n                        \"Attempting a single length slice within a templated section!\"\n                    )\n\n        # Otherwise it's a slice with length.\n\n        # Use a non inclusive match to get the end point.\n        ts_stop_sf_start, ts_stop_sf_stop = self._find_slice_indices_of_templated_pos(\n            template_slice.stop, inclusive=False\n        )\n\n        # Update starting position based on insertion point:\n        if insertion_point >= 0:\n            for elem in self.sliced_file[ts_start_sf_start:]:\n                if elem[1].start != insertion_point:\n                    ts_start_sf_start += 1\n                else:\n                    break\n\n        subslices = self.sliced_file[\n            # Very inclusive slice\n            min(ts_start_sf_start, ts_stop_sf_start) : max(\n                ts_start_sf_stop, ts_stop_sf_stop\n            )\n        ]\n        if ts_start_sf_start == ts_start_sf_stop:\n            if ts_start_sf_start > len(self.sliced_file):  # pragma: no cover\n                # We should never get here\n                raise ValueError(\"Starting position higher than sliced file position\")\n            if ts_start_sf_start < len(self.sliced_file):\n                return self.sliced_file[1].source_slice\n            else:\n                return self.sliced_file[-1].source_slice  # pragma: no cover\n        else:\n            start_slices = self.sliced_file[ts_start_sf_start:ts_start_sf_stop]\n        if ts_stop_sf_start == ts_stop_sf_stop:  # pragma: no cover TODO?\n            stop_slices = [self.sliced_file[ts_stop_sf_start]]\n        else:\n            stop_slices = self.sliced_file[ts_stop_sf_start:ts_stop_sf_stop]\n\n        # if it's a literal segment then we can get the exact position\n        # otherwise we're greedy.\n\n        # Start.\n        if insertion_point >= 0:\n            source_start = insertion_point\n        elif start_slices[0][0] == \"literal\":\n            offset = template_slice.start - start_slices[0][2].start\n            source_start = start_slices[0][1].start + offset\n        else:\n            source_start = start_slices[0][1].start\n        # Stop.\n        if stop_slices[-1][0] == \"literal\":\n            offset = stop_slices[-1][2].stop - template_slice.stop\n            source_stop = stop_slices[-1][1].stop - offset\n        else:\n            source_stop = stop_slices[-1][1].stop\n\n        # Does this slice go backward?\n        if source_start > source_stop:\n            # If this happens, it's because one was templated and\n            # the other isn't, or because a loop means that the segments\n            # are in a different order.\n\n            # Take the widest possible span in this case.\n            source_start = min(elem[1].start for elem in subslices)\n            source_stop = max(elem[1].stop for elem in subslices)\n\n        source_slice = slice(source_start, source_stop)\n\n        return source_slice\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "templated_slice_to_source_slice", "self", "template_slice", "slice", "slice", "convert", "a", "template", "slice", "to", "a", "source", "slice", "if", "not", "self", "sliced_file", "return", "template_slice", "pragma", "no", "cover", "todo", "ts_start_sf_start", "ts_start_sf_stop", "self", "_find_slice_indices_of_templated_pos", "template_slice", "start", "ts_start_subsliced_file", "self", "sliced_file", "ts_start_sf_start", "ts_start_sf_stop", "work", "out", "the", "insertion", "point", "insertion_point", "1", "for", "elem", "in", "ts_start_subsliced_file", "do", "slice", "starts", "and", "ends", "for", "slice_elem", "in", "start", "stop", "if", "getattr", "elem", "2", "slice_elem", "template_slice", "start", "store", "the", "lowest", "point", "getattr", "elem", "1", "slice_elem", "if", "insertion_point", "0", "or", "point", "insertion_point", "insertion_point", "point", "we", "don", "t", "break", "here", "because", "we", "might", "find", "another", "later", "which", "is", "actually", "earlier", "zero", "length", "slice", "if", "template_slice", "start", "template_slice", "stop", "is", "it", "on", "a", "join", "if", "insertion_point", "0", "return", "slice", "insertion_point", "insertion_point", "it", "s", "within", "a", "segment", "else", "if", "ts_start_subsliced_file", "and", "ts_start_subsliced_file", "0", "0", "literal", "offset", "template_slice", "start", "ts_start_subsliced_file", "0", "2", "start", "return", "slice", "ts_start_subsliced_file", "0", "1", "start", "offset", "ts_start_subsliced_file", "0", "1", "start", "offset", "else", "raise", "valueerror", "attempting", "a", "single", "length", "slice", "within", "a", "templated", "section", "otherwise", "it", "s", "a", "slice", "with", "length", "use", "a", "non", "inclusive", "match", "to", "get", "the", "end", "point", "ts_stop_sf_start", "ts_stop_sf_stop", "self", "_find_slice_indices_of_templated_pos", "template_slice", "stop", "inclusive", "false", "update", "starting", "position", "based", "on", "insertion", "point", "if", "insertion_point", "0", "for", "elem", "in", "self", "sliced_file", "ts_start_sf_start", "if", "elem", "1", "start", "insertion_point", "ts_start_sf_start", "1", "else", "break", "subslices", "self", "sliced_file", "very", "inclusive", "slice", "min", "ts_start_sf_start", "ts_stop_sf_start", "max", "ts_start_sf_stop", "ts_stop_sf_stop", "if", "ts_start_sf_start", "ts_start_sf_stop", "if", "ts_start_sf_start", "len", "self", "sliced_file", "pragma", "no", "cover", "we", "should", "never", "get", "here", "raise", "valueerror", "starting", "position", "higher", "than", "sliced", "file", "position", "if", "ts_start_sf_start", "len", "self", "sliced_file", "return", "self", "sliced_file", "1", "source_slice", "else", "return", "self", "sliced_file", "1", "source_slice", "pragma", "no", "cover", "else", "start_slices", "self", "sliced_file", "ts_start_sf_start", "ts_start_sf_stop", "if", "ts_stop_sf_start", "ts_stop_sf_stop", "pragma", "no", "cover", "todo", "stop_slices", "self", "sliced_file", "ts_stop_sf_start", "else", "stop_slices", "self", "sliced_file", "ts_stop_sf_start", "ts_stop_sf_stop", "if", "it", "s", "a", "literal", "segment", "then", "we", "can", "get", "the", "exact", "position", "otherwise", "we", "re", "greedy", "start", "if", "insertion_point", "0", "source_start", "insertion_point", "elif", "start_slices", "0", "0", "literal", "offset", "template_slice", "start", "start_slices", "0", "2", "start", "source_start", "start_slices", "0", "1", "start", "offset", "else", "source_start", "start_slices", "0", "1", "start", "stop", "if", "stop_slices", "1", "0", "literal", "offset", "stop_slices", "1", "2", "stop", "template_slice", "stop", "source_stop", "stop_slices", "1", "1", "stop", "offset", "else", "source_stop", "stop_slices", "1", "1", "stop", "does", "this", "slice", "go", "backward", "if", "source_start", "source_stop", "if", "this", "happens", "it", "s", "because", "one", "was", "templated", "and", "the", "other", "isn", "t", "or", "because", "a", "loop", "means", "that", "the", "segments", "are", "in", "a", "different", "order", "take", "the", "widest", "possible", "span", "in", "this", "case", "source_start", "min", "elem", "1", "start", "for", "elem", "in", "subslices", "source_stop", "max", "elem", "1", "stop", "for", "elem", "in", "subslices", "source_slice", "slice", "source_start", "source_stop", "return", "source_slice"], "doc_len": 433}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.is_source_slice_literal", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "is_source_slice_literal", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def is_source_slice_literal(self, source_slice: slice) -> bool:\n        \"\"\"Work out whether a slice of the source file is a literal or not.\"\"\"\n        # No sliced file? Everything is literal\n        if not self.raw_sliced:  # pragma: no cover TODO?\n            return True\n        # Zero length slice. It's a literal, because it's definitely not templated.\n        if source_slice.start == source_slice.stop:\n            return True\n        is_literal = True\n        for _, seg_type, seg_idx, _ in self.raw_sliced:\n            # Reset if we find a literal and we're up to the start\n            # otherwise set false.\n            if seg_idx <= source_slice.start:\n                is_literal = seg_type == \"literal\"\n            elif seg_idx >= source_slice.stop:\n                # We've gone past the end. Break and Return.\n                break\n            else:\n                # We're in the middle. Check type\n                if seg_type != \"literal\":\n                    is_literal = False\n        return is_literal\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "is_source_slice_literal", "self", "source_slice", "slice", "bool", "work", "out", "whether", "a", "slice", "of", "the", "source", "file", "is", "a", "literal", "or", "not", "no", "sliced", "file", "everything", "is", "literal", "if", "not", "self", "raw_sliced", "pragma", "no", "cover", "todo", "return", "true", "zero", "length", "slice", "it", "s", "a", "literal", "because", "it", "s", "definitely", "not", "templated", "if", "source_slice", "start", "source_slice", "stop", "return", "true", "is_literal", "true", "for", "_", "seg_type", "seg_idx", "_", "in", "self", "raw_sliced", "reset", "if", "we", "find", "a", "literal", "and", "we", "re", "up", "to", "the", "start", "otherwise", "set", "false", "if", "seg_idx", "source_slice", "start", "is_literal", "seg_type", "literal", "elif", "seg_idx", "source_slice", "stop", "we", "ve", "gone", "past", "the", "end", "break", "and", "return", "break", "else", "we", "re", "in", "the", "middle", "check", "type", "if", "seg_type", "literal", "is_literal", "false", "return", "is_literal"], "doc_len": 125}
{"doc_id": "src/sqlfluff/core/templaters/base.py::TemplatedFile.source_only_slices", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "TemplatedFile", "func_name": "source_only_slices", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: TemplatedFile\n    def source_only_slices(self) -> List[RawFileSlice]:\n        \"\"\"Return a list a slices which reference the parts only in the source.\n\n        All of these slices should be expected to have zero-length\n        in the templated file.\n\n        The results are NECESSARILY sorted.\n        \"\"\"\n        ret_buff = []\n        for elem in self.raw_sliced:\n            if elem.slice_type in (\"comment\", \"block_end\", \"block_start\", \"block_mid\"):\n                ret_buff.append(elem)\n        return ret_buff\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "templatedfile", "def", "source_only_slices", "self", "list", "rawfileslice", "return", "a", "list", "a", "slices", "which", "reference", "the", "parts", "only", "in", "the", "source", "all", "of", "these", "slices", "should", "be", "expected", "to", "have", "zero", "length", "in", "the", "templated", "file", "the", "results", "are", "necessarily", "sorted", "ret_buff", "for", "elem", "in", "self", "raw_sliced", "if", "elem", "slice_type", "in", "comment", "block_end", "block_start", "block_mid", "ret_buff", "append", "elem", "return", "ret_buff"], "doc_len": 64}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawTemplater.__init__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawTemplater", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawTemplater\n    def __init__(self, **kwargs):\n        \"\"\"Placeholder init function.\n\n        Here we should load any initial config found in the root directory. The init\n        function shouldn't take any arguments at this stage as we assume that it will load\n        its own config. Maybe at this stage we might allow override parameters to be passed\n        to the linter at runtime from the cli - that would be the only time we would pass\n        arguments in here.\n        \"\"\"\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawtemplater", "def", "__init__", "self", "kwargs", "placeholder", "init", "function", "here", "we", "should", "load", "any", "initial", "config", "found", "in", "the", "root", "directory", "the", "init", "function", "shouldn", "t", "take", "any", "arguments", "at", "this", "stage", "as", "we", "assume", "that", "it", "will", "load", "its", "own", "config", "maybe", "at", "this", "stage", "we", "might", "allow", "override", "parameters", "to", "be", "passed", "to", "the", "linter", "at", "runtime", "from", "the", "cli", "that", "would", "be", "the", "only", "time", "we", "would", "pass", "arguments", "in", "here"], "doc_len": 79}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawTemplater.sequence_files", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawTemplater", "func_name": "sequence_files", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawTemplater\n    def sequence_files(\n        self, fnames: List[str], config=None, formatter=None\n    ) -> Iterable[str]:\n        \"\"\"Given files to be processed, return a valid processing sequence.\"\"\"\n        # Default is to process in the original order.\n        return fnames\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawtemplater", "def", "sequence_files", "self", "fnames", "list", "str", "config", "none", "formatter", "none", "iterable", "str", "given", "files", "to", "be", "processed", "return", "a", "valid", "processing", "sequence", "default", "is", "to", "process", "in", "the", "original", "order", "return", "fnames"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawTemplater.process", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawTemplater", "func_name": "process", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawTemplater\n    def process(\n        self, *, in_str: str, fname: str, config=None, formatter=None\n    ) -> Tuple[Optional[TemplatedFile], list]:\n        \"\"\"Process a string and return a TemplatedFile.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        return TemplatedFile(in_str, fname=fname), []\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawtemplater", "def", "process", "self", "in_str", "str", "fname", "str", "config", "none", "formatter", "none", "tuple", "optional", "templatedfile", "list", "process", "a", "string", "and", "return", "a", "templatedfile", "note", "that", "the", "arguments", "are", "enforced", "as", "keywords", "because", "templaters", "can", "have", "differences", "in", "their", "process", "method", "signature", "a", "templater", "that", "only", "supports", "reading", "from", "a", "file", "would", "need", "the", "following", "signature", "process", "fname", "in_str", "none", "config", "none", "arguments", "are", "swapped", "args", "in_str", "obj", "str", "the", "input", "string", "fname", "obj", "str", "optional", "the", "filename", "of", "this", "string", "this", "is", "mostly", "for", "loading", "config", "files", "at", "runtime", "config", "obj", "fluffconfig", "a", "specific", "config", "to", "use", "for", "this", "templating", "operation", "only", "necessary", "for", "some", "templaters", "formatter", "obj", "callbackformatter", "optional", "object", "for", "output", "return", "templatedfile", "in_str", "fname", "fname"], "doc_len": 124}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawTemplater.__eq__", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawTemplater", "func_name": "__eq__", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawTemplater\n    def __eq__(self, other):\n        \"\"\"Return true if `other` is of the same class as this one.\n\n        NB: This is useful in comparing configs.\n        \"\"\"\n        return isinstance(other, self.__class__)\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawtemplater", "def", "__eq__", "self", "other", "return", "true", "if", "other", "is", "of", "the", "same", "class", "as", "this", "one", "nb", "this", "is", "useful", "in", "comparing", "configs", "return", "isinstance", "other", "self", "__class__"], "doc_len": 35}
{"doc_id": "src/sqlfluff/core/templaters/base.py::RawTemplater.config_pairs", "file_path": "src/sqlfluff/core/templaters/base.py", "class_name": "RawTemplater", "func_name": "config_pairs", "text": "文件路径: src/sqlfluff/core/templaters/base.py, 类名: RawTemplater\n    def config_pairs(self):\n        \"\"\"Returns info about the given templater for output by the cli.\"\"\"\n        return [(\"templater\", self.name)]\n", "tokens": ["src", "sqlfluff", "core", "templaters", "base", "py", "rawtemplater", "def", "config_pairs", "self", "returns", "info", "about", "the", "given", "templater", "for", "output", "by", "the", "cli", "return", "templater", "self", "name"], "doc_len": 25}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._extract_macros_from_template", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_extract_macros_from_template", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _extract_macros_from_template(template, env, ctx):\n        \"\"\"Take a template string and extract any macros from it.\n\n        Lovingly inspired by http://codyaray.com/2015/05/auto-load-jinja2-macros\n        \"\"\"\n        from jinja2.runtime import Macro  # noqa\n\n        # Iterate through keys exported from the loaded template string\n        context = {}\n        macro_template = env.from_string(template, globals=ctx)\n        # This is kind of low level and hacky but it works\n        for k in macro_template.module.__dict__:\n            attr = getattr(macro_template.module, k)\n            # Is it a macro? If so install it at the name of the macro\n            if isinstance(attr, Macro):\n                context[k] = attr\n        # Return the context\n        return context\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_extract_macros_from_template", "template", "env", "ctx", "take", "a", "template", "string", "and", "extract", "any", "macros", "from", "it", "lovingly", "inspired", "by", "http", "codyaray", "com", "2015", "05", "auto", "load", "jinja2", "macros", "from", "jinja2", "runtime", "import", "macro", "noqa", "iterate", "through", "keys", "exported", "from", "the", "loaded", "template", "string", "context", "macro_template", "env", "from_string", "template", "globals", "ctx", "this", "is", "kind", "of", "low", "level", "and", "hacky", "but", "it", "works", "for", "k", "in", "macro_template", "module", "__dict__", "attr", "getattr", "macro_template", "module", "k", "is", "it", "a", "macro", "if", "so", "install", "it", "at", "the", "name", "of", "the", "macro", "if", "isinstance", "attr", "macro", "context", "k", "attr", "return", "the", "context", "return", "context"], "doc_len": 104}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._extract_macros_from_path", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_extract_macros_from_path", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _extract_macros_from_path(cls, path, env, ctx):\n        \"\"\"Take a path and extract macros from it.\"\"\"\n        # Does the path exist? It should as this check was done on config load.\n        if not os.path.exists(path):  # pragma: no cover\n            raise ValueError(f\"Path does not exist: {path}\")\n\n        macro_ctx = {}\n        if os.path.isfile(path):\n            # It's a file. Extract macros from it.\n            with open(path) as opened_file:\n                template = opened_file.read()\n            # Update the context with macros from the file.\n            macro_ctx.update(\n                cls._extract_macros_from_template(template, env=env, ctx=ctx)\n            )\n        else:\n            # It's a directory. Iterate through files in it and extract from them.\n            for dirpath, _, files in os.walk(path):\n                for fname in files:\n                    if fname.endswith(\".sql\"):\n                        macro_ctx.update(\n                            cls._extract_macros_from_path(\n                                os.path.join(dirpath, fname), env=env, ctx=ctx\n                            )\n                        )\n        return macro_ctx\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_extract_macros_from_path", "cls", "path", "env", "ctx", "take", "a", "path", "and", "extract", "macros", "from", "it", "does", "the", "path", "exist", "it", "should", "as", "this", "check", "was", "done", "on", "config", "load", "if", "not", "os", "path", "exists", "path", "pragma", "no", "cover", "raise", "valueerror", "f", "path", "does", "not", "exist", "path", "macro_ctx", "if", "os", "path", "isfile", "path", "it", "s", "a", "file", "extract", "macros", "from", "it", "with", "open", "path", "as", "opened_file", "template", "opened_file", "read", "update", "the", "context", "with", "macros", "from", "the", "file", "macro_ctx", "update", "cls", "_extract_macros_from_template", "template", "env", "env", "ctx", "ctx", "else", "it", "s", "a", "directory", "iterate", "through", "files", "in", "it", "and", "extract", "from", "them", "for", "dirpath", "_", "files", "in", "os", "walk", "path", "for", "fname", "in", "files", "if", "fname", "endswith", "sql", "macro_ctx", "update", "cls", "_extract_macros_from_path", "os", "path", "join", "dirpath", "fname", "env", "env", "ctx", "ctx", "return", "macro_ctx"], "doc_len": 136}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._extract_macros_from_config", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_extract_macros_from_config", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _extract_macros_from_config(self, config, env, ctx):\n        \"\"\"Take a config and load any macros from it.\"\"\"\n        if config:\n            # This is now a nested section\n            loaded_context = (\n                config.get_section((self.templater_selector, self.name, \"macros\")) or {}\n            )\n        else:  # pragma: no cover TODO?\n            loaded_context = {}\n\n        # Iterate to load macros\n        macro_ctx = {}\n        for value in loaded_context.values():\n            macro_ctx.update(\n                self._extract_macros_from_template(value, env=env, ctx=ctx)\n            )\n        return macro_ctx\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_extract_macros_from_config", "self", "config", "env", "ctx", "take", "a", "config", "and", "load", "any", "macros", "from", "it", "if", "config", "this", "is", "now", "a", "nested", "section", "loaded_context", "config", "get_section", "self", "templater_selector", "self", "name", "macros", "or", "else", "pragma", "no", "cover", "todo", "loaded_context", "iterate", "to", "load", "macros", "macro_ctx", "for", "value", "in", "loaded_context", "values", "macro_ctx", "update", "self", "_extract_macros_from_template", "value", "env", "env", "ctx", "ctx", "return", "macro_ctx"], "doc_len": 66}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._extract_libraries_from_config", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_extract_libraries_from_config", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _extract_libraries_from_config(self, config):\n        library_path = config.get_section(\n            (self.templater_selector, self.name, \"library_path\")\n        )\n        if not library_path:\n            return {}\n\n        libraries = {}\n        for file_name in os.listdir(library_path):\n            file_path = os.path.join(library_path, file_name)\n            if not os.path.isfile(file_path) or not file_name.endswith(\".py\"):\n                continue\n\n            module_name = os.path.splitext(file_name)[0]\n            spec = importlib.util.spec_from_file_location(module_name, file_path)\n            lib = importlib.util.module_from_spec(spec)\n            spec.loader.exec_module(lib)\n            libraries[module_name] = lib\n\n        return libraries\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_extract_libraries_from_config", "self", "config", "library_path", "config", "get_section", "self", "templater_selector", "self", "name", "library_path", "if", "not", "library_path", "return", "libraries", "for", "file_name", "in", "os", "listdir", "library_path", "file_path", "os", "path", "join", "library_path", "file_name", "if", "not", "os", "path", "isfile", "file_path", "or", "not", "file_name", "endswith", "py", "continue", "module_name", "os", "path", "splitext", "file_name", "0", "spec", "importlib", "util", "spec_from_file_location", "module_name", "file_path", "lib", "importlib", "util", "module_from_spec", "spec", "spec", "loader", "exec_module", "lib", "libraries", "module_name", "lib", "return", "libraries"], "doc_len": 74}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._generate_dbt_builtins", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_generate_dbt_builtins", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _generate_dbt_builtins():\n        \"\"\"Generate the dbt builtins which are injected in the context.\"\"\"\n        # This feels a bit wrong defining these here, they should probably\n        # be configurable somewhere sensible. But for now they're not.\n        # TODO: Come up with a better solution.\n\n        class ThisEmulator:\n            \"\"\"A class which emulates the `this` class from dbt.\"\"\"\n\n            name = \"this_model\"\n            schema = \"this_schema\"\n            database = \"this_database\"\n\n            def __str__(self):  # pragma: no cover TODO?\n                return self.name\n\n        dbt_builtins = {\n            # `is_incremental()` renders as False, always in this case.\n            # TODO: This means we'll never parse the other part of the query,\n            # so we should find a solution to that. Perhaps forcing the file\n            # to be parsed TWICE if it uses this variable.\n            \"is_incremental\": lambda: False,\n            \"this\": ThisEmulator(),\n        }\n        return dbt_builtins\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_generate_dbt_builtins", "generate", "the", "dbt", "builtins", "which", "are", "injected", "in", "the", "context", "this", "feels", "a", "bit", "wrong", "defining", "these", "here", "they", "should", "probably", "be", "configurable", "somewhere", "sensible", "but", "for", "now", "they", "re", "not", "todo", "come", "up", "with", "a", "better", "solution", "class", "thisemulator", "a", "class", "which", "emulates", "the", "this", "class", "from", "dbt", "name", "this_model", "schema", "this_schema", "database", "this_database", "def", "__str__", "self", "pragma", "no", "cover", "todo", "return", "self", "name", "dbt_builtins", "is_incremental", "renders", "as", "false", "always", "in", "this", "case", "todo", "this", "means", "we", "ll", "never", "parse", "the", "other", "part", "of", "the", "query", "so", "we", "should", "find", "a", "solution", "to", "that", "perhaps", "forcing", "the", "file", "to", "be", "parsed", "twice", "if", "it", "uses", "this", "variable", "is_incremental", "lambda", "false", "this", "thisemulator", "return", "dbt_builtins"], "doc_len": 124}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._crawl_tree", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_crawl_tree", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _crawl_tree(cls, tree, variable_names, raw):\n        \"\"\"Crawl the tree looking for occurrences of the undeclared values.\"\"\"\n        # First iterate through children\n        for elem in tree.iter_child_nodes():\n            yield from cls._crawl_tree(elem, variable_names, raw)\n        # Then assess self\n        if isinstance(tree, jinja2.nodes.Name) and tree.name in variable_names:\n            line_no = tree.lineno\n            line = raw.split(\"\\n\")[line_no - 1]\n            pos = line.index(tree.name) + 1\n            yield SQLTemplaterError(\n                f\"Undefined jinja template variable: {tree.name!r}\",\n                line_no=line_no,\n                line_pos=pos,\n            )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_crawl_tree", "cls", "tree", "variable_names", "raw", "crawl", "the", "tree", "looking", "for", "occurrences", "of", "the", "undeclared", "values", "first", "iterate", "through", "children", "for", "elem", "in", "tree", "iter_child_nodes", "yield", "from", "cls", "_crawl_tree", "elem", "variable_names", "raw", "then", "assess", "self", "if", "isinstance", "tree", "jinja2", "nodes", "name", "and", "tree", "name", "in", "variable_names", "line_no", "tree", "lineno", "line", "raw", "split", "n", "line_no", "1", "pos", "line", "index", "tree", "name", "1", "yield", "sqltemplatererror", "f", "undefined", "jinja", "template", "variable", "tree", "name", "r", "line_no", "line_no", "line_pos", "pos"], "doc_len": 82}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._get_jinja_env", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_get_jinja_env", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _get_jinja_env():\n        \"\"\"Get a properly configured jinja environment.\"\"\"\n        # We explicitly want to preserve newlines.\n        return SandboxedEnvironment(\n            keep_trailing_newline=True,\n            # The do extension allows the \"do\" directive\n            autoescape=False,\n            extensions=[\"jinja2.ext.do\"],\n        )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_get_jinja_env", "get", "a", "properly", "configured", "jinja", "environment", "we", "explicitly", "want", "to", "preserve", "newlines", "return", "sandboxedenvironment", "keep_trailing_newline", "true", "the", "do", "extension", "allows", "the", "do", "directive", "autoescape", "false", "extensions", "jinja2", "ext", "do"], "doc_len": 38}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater.process", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "process", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def process(\n        self, *, in_str: str, fname: str, config=None, formatter=None\n    ) -> Tuple[Optional[TemplatedFile], list]:\n        \"\"\"Process a string and return the new string.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        if not config:  # pragma: no cover\n            raise ValueError(\n                \"For the jinja templater, the `process()` method requires a config object.\"\n            )\n\n        # Load the context\n        live_context = self.get_context(fname=fname, config=config)\n        # Apply dbt builtin functions if we're allowed.\n        apply_dbt_builtins = config.get_section(\n            (self.templater_selector, self.name, \"apply_dbt_builtins\")\n        )\n        if apply_dbt_builtins:\n            # This feels a bit wrong defining these here, they should probably\n            # be configurable somewhere sensible. But for now they're not.\n            # TODO: Come up with a better solution.\n            dbt_builtins = self._generate_dbt_builtins()\n            for name in dbt_builtins:\n                # Only apply if it hasn't already been set at this stage.\n                if name not in live_context:\n                    live_context[name] = dbt_builtins[name]\n\n        env = self._get_jinja_env()\n\n        # Load macros from path (if applicable)\n        macros_path = config.get_section(\n            (self.templater_selector, self.name, \"load_macros_from_path\")\n        )\n        if macros_path:\n            live_context.update(\n                self._extract_macros_from_path(macros_path, env=env, ctx=live_context)\n            )\n\n        # Load config macros, these will take precedence over macros from the path\n        live_context.update(\n            self._extract_macros_from_config(config=config, env=env, ctx=live_context)\n        )\n\n        live_context.update(self._extract_libraries_from_config(config=config))\n\n        # Load the template, passing the global context.\n        try:\n            template = env.from_string(in_str, globals=live_context)\n        except TemplateSyntaxError as err:\n            # Something in the template didn't parse, return the original\n            # and a violation around what happened.\n            (len(line) for line in in_str.split(\"\\n\")[: err.lineno])\n            return (\n                TemplatedFile(source_str=in_str, fname=fname),\n                [\n                    SQLTemplaterError(\n                        f\"Failure to parse jinja template: {err}.\",\n                        line_no=err.lineno,\n                    )\n                ],\n            )\n\n        violations = []\n\n        # Attempt to identify any undeclared variables. The majority\n        # will be found during the _crawl_tree step rather than this\n        # first Exception which serves only to catch catastrophic errors.\n        try:\n            syntax_tree = env.parse(in_str)\n            undefined_variables = meta.find_undeclared_variables(syntax_tree)\n        except Exception as err:  # pragma: no cover\n            # TODO: Add a url here so people can get more help.\n            raise SQLTemplaterError(f\"Failure in identifying Jinja variables: {err}.\")\n\n        # Get rid of any that *are* actually defined.\n        for val in live_context:\n            if val in undefined_variables:\n                undefined_variables.remove(val)\n\n        if undefined_variables:\n            # Lets go through and find out where they are:\n            for val in self._crawl_tree(syntax_tree, undefined_variables, in_str):\n                violations.append(val)\n\n        try:\n            # NB: Passing no context. Everything is loaded when the template is loaded.\n            out_str = template.render()\n            # Slice the file once rendered.\n            raw_sliced, sliced_file, out_str = self.slice_file(\n                in_str, out_str, config=config\n            )\n            return (\n                TemplatedFile(\n                    source_str=in_str,\n                    templated_str=out_str,\n                    fname=fname,\n                    sliced_file=sliced_file,\n                    raw_sliced=raw_sliced,\n                ),\n                violations,\n            )\n        except (TemplateError, TypeError) as err:\n            templater_logger.info(\"Unrecoverable Jinja Error: %s\", err)\n            violations.append(\n                SQLTemplaterError(\n                    (\n                        \"Unrecoverable failure in Jinja templating: {}. Have you configured \"\n                        \"your variables? https://docs.sqlfluff.com/en/latest/configuration.html\"\n                    ).format(err)\n                )\n            )\n            return None, violations\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "process", "self", "in_str", "str", "fname", "str", "config", "none", "formatter", "none", "tuple", "optional", "templatedfile", "list", "process", "a", "string", "and", "return", "the", "new", "string", "note", "that", "the", "arguments", "are", "enforced", "as", "keywords", "because", "templaters", "can", "have", "differences", "in", "their", "process", "method", "signature", "a", "templater", "that", "only", "supports", "reading", "from", "a", "file", "would", "need", "the", "following", "signature", "process", "fname", "in_str", "none", "config", "none", "arguments", "are", "swapped", "args", "in_str", "obj", "str", "the", "input", "string", "fname", "obj", "str", "optional", "the", "filename", "of", "this", "string", "this", "is", "mostly", "for", "loading", "config", "files", "at", "runtime", "config", "obj", "fluffconfig", "a", "specific", "config", "to", "use", "for", "this", "templating", "operation", "only", "necessary", "for", "some", "templaters", "formatter", "obj", "callbackformatter", "optional", "object", "for", "output", "if", "not", "config", "pragma", "no", "cover", "raise", "valueerror", "for", "the", "jinja", "templater", "the", "process", "method", "requires", "a", "config", "object", "load", "the", "context", "live_context", "self", "get_context", "fname", "fname", "config", "config", "apply", "dbt", "builtin", "functions", "if", "we", "re", "allowed", "apply_dbt_builtins", "config", "get_section", "self", "templater_selector", "self", "name", "apply_dbt_builtins", "if", "apply_dbt_builtins", "this", "feels", "a", "bit", "wrong", "defining", "these", "here", "they", "should", "probably", "be", "configurable", "somewhere", "sensible", "but", "for", "now", "they", "re", "not", "todo", "come", "up", "with", "a", "better", "solution", "dbt_builtins", "self", "_generate_dbt_builtins", "for", "name", "in", "dbt_builtins", "only", "apply", "if", "it", "hasn", "t", "already", "been", "set", "at", "this", "stage", "if", "name", "not", "in", "live_context", "live_context", "name", "dbt_builtins", "name", "env", "self", "_get_jinja_env", "load", "macros", "from", "path", "if", "applicable", "macros_path", "config", "get_section", "self", "templater_selector", "self", "name", "load_macros_from_path", "if", "macros_path", "live_context", "update", "self", "_extract_macros_from_path", "macros_path", "env", "env", "ctx", "live_context", "load", "config", "macros", "these", "will", "take", "precedence", "over", "macros", "from", "the", "path", "live_context", "update", "self", "_extract_macros_from_config", "config", "config", "env", "env", "ctx", "live_context", "live_context", "update", "self", "_extract_libraries_from_config", "config", "config", "load", "the", "template", "passing", "the", "global", "context", "try", "template", "env", "from_string", "in_str", "globals", "live_context", "except", "templatesyntaxerror", "as", "err", "something", "in", "the", "template", "didn", "t", "parse", "return", "the", "original", "and", "a", "violation", "around", "what", "happened", "len", "line", "for", "line", "in", "in_str", "split", "n", "err", "lineno", "return", "templatedfile", "source_str", "in_str", "fname", "fname", "sqltemplatererror", "f", "failure", "to", "parse", "jinja", "template", "err", "line_no", "err", "lineno", "violations", "attempt", "to", "identify", "any", "undeclared", "variables", "the", "majority", "will", "be", "found", "during", "the", "_crawl_tree", "step", "rather", "than", "this", "first", "exception", "which", "serves", "only", "to", "catch", "catastrophic", "errors", "try", "syntax_tree", "env", "parse", "in_str", "undefined_variables", "meta", "find_undeclared_variables", "syntax_tree", "except", "exception", "as", "err", "pragma", "no", "cover", "todo", "add", "a", "url", "here", "so", "people", "can", "get", "more", "help", "raise", "sqltemplatererror", "f", "failure", "in", "identifying", "jinja", "variables", "err", "get", "rid", "of", "any", "that", "are", "actually", "defined", "for", "val", "in", "live_context", "if", "val", "in", "undefined_variables", "undefined_variables", "remove", "val", "if", "undefined_variables", "lets", "go", "through", "and", "find", "out", "where", "they", "are", "for", "val", "in", "self", "_crawl_tree", "syntax_tree", "undefined_variables", "in_str", "violations", "append", "val", "try", "nb", "passing", "no", "context", "everything", "is", "loaded", "when", "the", "template", "is", "loaded", "out_str", "template", "render", "slice", "the", "file", "once", "rendered", "raw_sliced", "sliced_file", "out_str", "self", "slice_file", "in_str", "out_str", "config", "config", "return", "templatedfile", "source_str", "in_str", "templated_str", "out_str", "fname", "fname", "sliced_file", "sliced_file", "raw_sliced", "raw_sliced", "violations", "except", "templateerror", "typeerror", "as", "err", "templater_logger", "info", "unrecoverable", "jinja", "error", "s", "err", "violations", "append", "sqltemplatererror", "unrecoverable", "failure", "in", "jinja", "templating", "have", "you", "configured", "your", "variables", "https", "docs", "sqlfluff", "com", "en", "latest", "configuration", "html", "format", "err", "return", "none", "violations"], "doc_len": 526}
{"doc_id": "src/sqlfluff/core/templaters/jinja.py::JinjaTemplater._slice_template", "file_path": "src/sqlfluff/core/templaters/jinja.py", "class_name": "JinjaTemplater", "func_name": "_slice_template", "text": "文件路径: src/sqlfluff/core/templaters/jinja.py, 类名: JinjaTemplater\n    def _slice_template(cls, in_str: str) -> Iterator[RawFileSlice]:\n        \"\"\"Slice template in jinja.\n\n        NB: Starts and ends of blocks are not distinguished.\n        \"\"\"\n        env = cls._get_jinja_env()\n        str_buff = \"\"\n        idx = 0\n        # We decide the \"kind\" of element we're dealing with\n        # using it's _closing_ tag rather than it's opening\n        # tag. The types here map back to similar types of\n        # sections in the python slicer.\n        block_types = {\n            \"variable_end\": \"templated\",\n            \"block_end\": \"block\",\n            \"comment_end\": \"comment\",\n            # Raw tags should behave like blocks. Note that\n            # raw_end and raw_begin are whole tags rather\n            # than blocks and comments where we get partial\n            # tags.\n            \"raw_end\": \"block\",\n            \"raw_begin\": \"block\",\n        }\n\n        # https://jinja.palletsprojects.com/en/2.11.x/api/#jinja2.Environment.lex\n        for _, elem_type, raw in env.lex(in_str):\n            if elem_type == \"data\":\n                yield RawFileSlice(raw, \"literal\", idx)\n                idx += len(raw)\n                continue\n            str_buff += raw\n\n            if elem_type.endswith(\"_begin\"):\n                # When a \"begin\" tag (whether block, comment, or data) uses\n                # whitespace stripping\n                # (https://jinja.palletsprojects.com/en/3.0.x/templates/#whitespace-control),\n                # the Jinja lex() function handles this by discarding adjacent\n                # whitespace from in_str. For more insight, see the tokeniter()\n                # function in this file:\n                # https://github.com/pallets/jinja/blob/main/src/jinja2/lexer.py\n                # We want to detect and correct for this in order to:\n                # - Correctly update \"idx\" (if this is wrong, that's a\n                #   potential DISASTER because lint fixes use this info to\n                #   update the source file, and incorrect values often result in\n                #   CORRUPTING the user's file so it's no longer valid SQL. :-O\n                # - Guarantee that the slices we return fully \"cover\" the\n                #   contents of in_str.\n                #\n                # We detect skipped characters by looking ahead in in_str for\n                # the token just returned from lex(). The token text will either\n                # be at the current 'idx' position (if whitespace stripping did\n                # not occur) OR it'll be farther along in in_str, but we're\n                # GUARANTEED that lex() only skips over WHITESPACE; nothing else.\n\n                # Find the token returned. Did lex() skip over any characters?\n                num_chars_skipped = in_str.index(raw, idx) - idx\n                if num_chars_skipped:\n                    # Yes. It skipped over some characters. Compute a string\n                    # containing the skipped characters.\n                    skipped_str = in_str[idx : idx + num_chars_skipped]\n\n                    # Sanity check: Verify that Jinja only skips over\n                    # WHITESPACE, never anything else.\n                    if not skipped_str.isspace():  # pragma: no cover\n                        templater_logger.warning(\n                            \"Jinja lex() skipped non-whitespace: %s\", skipped_str\n                        )\n                    # Treat the skipped whitespace as a literal.\n                    yield RawFileSlice(skipped_str, \"literal\", idx)\n                    idx += num_chars_skipped\n\n            # raw_end and raw_begin behave a little differently in\n            # that the whole tag shows up in one go rather than getting\n            # parts of the tag at a time.\n            if elem_type.endswith(\"_end\") or elem_type == \"raw_begin\":\n                block_type = block_types[elem_type]\n                block_subtype = None\n                # Handle starts and ends of blocks\n                if block_type == \"block\":\n                    # Trim off the brackets and then the whitespace\n                    m_open = cls.re_open_tag.search(str_buff)\n                    m_close = cls.re_close_tag.search(str_buff)\n                    trimmed_content = \"\"\n                    if m_open and m_close:\n                        trimmed_content = str_buff[\n                            len(m_open.group(0)) : -len(m_close.group(0))\n                        ]\n                    if trimmed_content.startswith(\"end\"):\n                        block_type = \"block_end\"\n                    elif trimmed_content.startswith(\"el\"):\n                        # else, elif\n                        block_type = \"block_mid\"\n                    else:\n                        block_type = \"block_start\"\n                        if trimmed_content.split()[0] == \"for\":\n                            block_subtype = \"loop\"\n                m = re.search(r\"\\s+$\", raw, re.MULTILINE | re.DOTALL)\n                if raw.startswith(\"-\") and m:\n                    # Right whitespace was stripped. Split off the trailing\n                    # whitespace into a separate slice. The desired behavior is\n                    # to behave similarly as the left stripping case above.\n                    # Note that the stakes are a bit different, because lex()\n                    # hasn't *omitted* any characters from the strings it\n                    # returns, it has simply grouped them differently than we\n                    # want.\n                    trailing_chars = len(m.group(0))\n                    yield RawFileSlice(\n                        str_buff[:-trailing_chars], block_type, idx, block_subtype\n                    )\n                    idx += len(str_buff) - trailing_chars\n                    yield RawFileSlice(str_buff[-trailing_chars:], \"literal\", idx)\n                    idx += trailing_chars\n                else:\n                    yield RawFileSlice(str_buff, block_type, idx, block_subtype)\n                    idx += len(str_buff)\n                str_buff = \"\"\n", "tokens": ["src", "sqlfluff", "core", "templaters", "jinja", "py", "jinjatemplater", "def", "_slice_template", "cls", "in_str", "str", "iterator", "rawfileslice", "slice", "template", "in", "jinja", "nb", "starts", "and", "ends", "of", "blocks", "are", "not", "distinguished", "env", "cls", "_get_jinja_env", "str_buff", "idx", "0", "we", "decide", "the", "kind", "of", "element", "we", "re", "dealing", "with", "using", "it", "s", "_closing_", "tag", "rather", "than", "it", "s", "opening", "tag", "the", "types", "here", "map", "back", "to", "similar", "types", "of", "sections", "in", "the", "python", "slicer", "block_types", "variable_end", "templated", "block_end", "block", "comment_end", "comment", "raw", "tags", "should", "behave", "like", "blocks", "note", "that", "raw_end", "and", "raw_begin", "are", "whole", "tags", "rather", "than", "blocks", "and", "comments", "where", "we", "get", "partial", "tags", "raw_end", "block", "raw_begin", "block", "https", "jinja", "palletsprojects", "com", "en", "2", "11", "x", "api", "jinja2", "environment", "lex", "for", "_", "elem_type", "raw", "in", "env", "lex", "in_str", "if", "elem_type", "data", "yield", "rawfileslice", "raw", "literal", "idx", "idx", "len", "raw", "continue", "str_buff", "raw", "if", "elem_type", "endswith", "_begin", "when", "a", "begin", "tag", "whether", "block", "comment", "or", "data", "uses", "whitespace", "stripping", "https", "jinja", "palletsprojects", "com", "en", "3", "0", "x", "templates", "whitespace", "control", "the", "jinja", "lex", "function", "handles", "this", "by", "discarding", "adjacent", "whitespace", "from", "in_str", "for", "more", "insight", "see", "the", "tokeniter", "function", "in", "this", "file", "https", "github", "com", "pallets", "jinja", "blob", "main", "src", "jinja2", "lexer", "py", "we", "want", "to", "detect", "and", "correct", "for", "this", "in", "order", "to", "correctly", "update", "idx", "if", "this", "is", "wrong", "that", "s", "a", "potential", "disaster", "because", "lint", "fixes", "use", "this", "info", "to", "update", "the", "source", "file", "and", "incorrect", "values", "often", "result", "in", "corrupting", "the", "user", "s", "file", "so", "it", "s", "no", "longer", "valid", "sql", "o", "guarantee", "that", "the", "slices", "we", "return", "fully", "cover", "the", "contents", "of", "in_str", "we", "detect", "skipped", "characters", "by", "looking", "ahead", "in", "in_str", "for", "the", "token", "just", "returned", "from", "lex", "the", "token", "text", "will", "either", "be", "at", "the", "current", "idx", "position", "if", "whitespace", "stripping", "did", "not", "occur", "or", "it", "ll", "be", "farther", "along", "in", "in_str", "but", "we", "re", "guaranteed", "that", "lex", "only", "skips", "over", "whitespace", "nothing", "else", "find", "the", "token", "returned", "did", "lex", "skip", "over", "any", "characters", "num_chars_skipped", "in_str", "index", "raw", "idx", "idx", "if", "num_chars_skipped", "yes", "it", "skipped", "over", "some", "characters", "compute", "a", "string", "containing", "the", "skipped", "characters", "skipped_str", "in_str", "idx", "idx", "num_chars_skipped", "sanity", "check", "verify", "that", "jinja", "only", "skips", "over", "whitespace", "never", "anything", "else", "if", "not", "skipped_str", "isspace", "pragma", "no", "cover", "templater_logger", "warning", "jinja", "lex", "skipped", "non", "whitespace", "s", "skipped_str", "treat", "the", "skipped", "whitespace", "as", "a", "literal", "yield", "rawfileslice", "skipped_str", "literal", "idx", "idx", "num_chars_skipped", "raw_end", "and", "raw_begin", "behave", "a", "little", "differently", "in", "that", "the", "whole", "tag", "shows", "up", "in", "one", "go", "rather", "than", "getting", "parts", "of", "the", "tag", "at", "a", "time", "if", "elem_type", "endswith", "_end", "or", "elem_type", "raw_begin", "block_type", "block_types", "elem_type", "block_subtype", "none", "handle", "starts", "and", "ends", "of", "blocks", "if", "block_type", "block", "trim", "off", "the", "brackets", "and", "then", "the", "whitespace", "m_open", "cls", "re_open_tag", "search", "str_buff", "m_close", "cls", "re_close_tag", "search", "str_buff", "trimmed_content", "if", "m_open", "and", "m_close", "trimmed_content", "str_buff", "len", "m_open", "group", "0", "len", "m_close", "group", "0", "if", "trimmed_content", "startswith", "end", "block_type", "block_end", "elif", "trimmed_content", "startswith", "el", "else", "elif", "block_type", "block_mid", "else", "block_type", "block_start", "if", "trimmed_content", "split", "0", "for", "block_subtype", "loop", "m", "re", "search", "r", "s", "raw", "re", "multiline", "re", "dotall", "if", "raw", "startswith", "and", "m", "right", "whitespace", "was", "stripped", "split", "off", "the", "trailing", "whitespace", "into", "a", "separate", "slice", "the", "desired", "behavior", "is", "to", "behave", "similarly", "as", "the", "left", "stripping", "case", "above", "note", "that", "the", "stakes", "are", "a", "bit", "different", "because", "lex", "hasn", "t", "omitted", "any", "characters", "from", "the", "strings", "it", "returns", "it", "has", "simply", "grouped", "them", "differently", "than", "we", "want", "trailing_chars", "len", "m", "group", "0", "yield", "rawfileslice", "str_buff", "trailing_chars", "block_type", "idx", "block_subtype", "idx", "len", "str_buff", "trailing_chars", "yield", "rawfileslice", "str_buff", "trailing_chars", "literal", "idx", "idx", "trailing_chars", "else", "yield", "rawfileslice", "str_buff", "block_type", "idx", "block_subtype", "idx", "len", "str_buff", "str_buff"], "doc_len": 603}
{"doc_id": "src/sqlfluff/core/templaters/python.py::IntermediateFileSlice._trim_end", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "IntermediateFileSlice", "func_name": "_trim_end", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: IntermediateFileSlice\n    def _trim_end(\n        self, templated_str: str, target_end: str = \"head\"\n    ) -> Tuple[\"IntermediateFileSlice\", List[TemplatedFileSlice]]:\n        \"\"\"Trim the ends of a intermediate segment.\"\"\"\n        target_idx = 0 if target_end == \"head\" else -1\n        terminator_types = (\"block_start\") if target_end == \"head\" else (\"block_end\")\n        main_source_slice = self.source_slice\n        main_templated_slice = self.templated_slice\n        slice_buffer = self.slice_buffer\n\n        end_buffer = []\n\n        # Yield any leading literals, comments or blocks.\n        while len(slice_buffer) > 0 and slice_buffer[target_idx].slice_type in (\n            \"literal\",\n            \"block_start\",\n            \"block_end\",\n            \"comment\",\n        ):\n            focus = slice_buffer[target_idx]\n            templater_logger.debug(\"            %s Focus: %s\", target_end, focus)\n            # Is it a zero length item?\n            if focus.slice_type in (\"block_start\", \"block_end\", \"comment\"):\n                # Only add the length in the source space.\n                templated_len = 0\n            else:\n                # Assume it's a literal, check the literal actually matches.\n                templated_len = len(focus.raw)\n                if target_end == \"head\":\n                    check_slice = slice(\n                        main_templated_slice.start,\n                        main_templated_slice.start + templated_len,\n                    )\n                else:\n                    check_slice = slice(\n                        main_templated_slice.stop - templated_len,\n                        main_templated_slice.stop,\n                    )\n\n                if templated_str[check_slice] != focus.raw:\n                    # It doesn't match, we can't use it. break\n                    templater_logger.debug(\"                Nope\")\n                    break\n\n            # If it does match, set up the new slices\n            if target_end == \"head\":\n                division = (\n                    main_source_slice.start + len(focus.raw),\n                    main_templated_slice.start + templated_len,\n                )\n                new_slice = TemplatedFileSlice(\n                    focus.slice_type,\n                    slice(main_source_slice.start, division[0]),\n                    slice(main_templated_slice.start, division[1]),\n                )\n                end_buffer.append(new_slice)\n                main_source_slice = slice(division[0], main_source_slice.stop)\n                main_templated_slice = slice(division[1], main_templated_slice.stop)\n            else:\n                division = (\n                    main_source_slice.stop - len(focus.raw),\n                    main_templated_slice.stop - templated_len,\n                )\n                new_slice = TemplatedFileSlice(\n                    focus.slice_type,\n                    slice(division[0], main_source_slice.stop),\n                    slice(division[1], main_templated_slice.stop),\n                )\n                end_buffer.insert(0, new_slice)\n                main_source_slice = slice(main_source_slice.start, division[0])\n                main_templated_slice = slice(main_templated_slice.start, division[1])\n\n            slice_buffer.pop(target_idx)\n            if focus.slice_type in terminator_types:\n                break\n        # Return a new Intermediate slice and the buffer.\n        # NB: Don't check size of slice buffer here. We can do that later.\n        new_intermediate = self.__class__(\n            \"compound\", main_source_slice, main_templated_slice, slice_buffer\n        )\n        return new_intermediate, end_buffer\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "intermediatefileslice", "def", "_trim_end", "self", "templated_str", "str", "target_end", "str", "head", "tuple", "intermediatefileslice", "list", "templatedfileslice", "trim", "the", "ends", "of", "a", "intermediate", "segment", "target_idx", "0", "if", "target_end", "head", "else", "1", "terminator_types", "block_start", "if", "target_end", "head", "else", "block_end", "main_source_slice", "self", "source_slice", "main_templated_slice", "self", "templated_slice", "slice_buffer", "self", "slice_buffer", "end_buffer", "yield", "any", "leading", "literals", "comments", "or", "blocks", "while", "len", "slice_buffer", "0", "and", "slice_buffer", "target_idx", "slice_type", "in", "literal", "block_start", "block_end", "comment", "focus", "slice_buffer", "target_idx", "templater_logger", "debug", "s", "focus", "s", "target_end", "focus", "is", "it", "a", "zero", "length", "item", "if", "focus", "slice_type", "in", "block_start", "block_end", "comment", "only", "add", "the", "length", "in", "the", "source", "space", "templated_len", "0", "else", "assume", "it", "s", "a", "literal", "check", "the", "literal", "actually", "matches", "templated_len", "len", "focus", "raw", "if", "target_end", "head", "check_slice", "slice", "main_templated_slice", "start", "main_templated_slice", "start", "templated_len", "else", "check_slice", "slice", "main_templated_slice", "stop", "templated_len", "main_templated_slice", "stop", "if", "templated_str", "check_slice", "focus", "raw", "it", "doesn", "t", "match", "we", "can", "t", "use", "it", "break", "templater_logger", "debug", "nope", "break", "if", "it", "does", "match", "set", "up", "the", "new", "slices", "if", "target_end", "head", "division", "main_source_slice", "start", "len", "focus", "raw", "main_templated_slice", "start", "templated_len", "new_slice", "templatedfileslice", "focus", "slice_type", "slice", "main_source_slice", "start", "division", "0", "slice", "main_templated_slice", "start", "division", "1", "end_buffer", "append", "new_slice", "main_source_slice", "slice", "division", "0", "main_source_slice", "stop", "main_templated_slice", "slice", "division", "1", "main_templated_slice", "stop", "else", "division", "main_source_slice", "stop", "len", "focus", "raw", "main_templated_slice", "stop", "templated_len", "new_slice", "templatedfileslice", "focus", "slice_type", "slice", "division", "0", "main_source_slice", "stop", "slice", "division", "1", "main_templated_slice", "stop", "end_buffer", "insert", "0", "new_slice", "main_source_slice", "slice", "main_source_slice", "start", "division", "0", "main_templated_slice", "slice", "main_templated_slice", "start", "division", "1", "slice_buffer", "pop", "target_idx", "if", "focus", "slice_type", "in", "terminator_types", "break", "return", "a", "new", "intermediate", "slice", "and", "the", "buffer", "nb", "don", "t", "check", "size", "of", "slice", "buffer", "here", "we", "can", "do", "that", "later", "new_intermediate", "self", "__class__", "compound", "main_source_slice", "main_templated_slice", "slice_buffer", "return", "new_intermediate", "end_buffer"], "doc_len": 286}
{"doc_id": "src/sqlfluff/core/templaters/python.py::IntermediateFileSlice.trim_ends", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "IntermediateFileSlice", "func_name": "trim_ends", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: IntermediateFileSlice\n    def trim_ends(\n        self, templated_str: str\n    ) -> Tuple[\n        List[TemplatedFileSlice], \"IntermediateFileSlice\", List[TemplatedFileSlice]\n    ]:\n        \"\"\"Trim both ends of an intermediate slice.\"\"\"\n        # Trim start:\n        new_slice, head_buffer = self._trim_end(\n            templated_str=templated_str, target_end=\"head\"\n        )\n        # Trim end:\n        new_slice, tail_buffer = new_slice._trim_end(\n            templated_str=templated_str, target_end=\"tail\"\n        )\n        # Return\n        return head_buffer, new_slice, tail_buffer\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "intermediatefileslice", "def", "trim_ends", "self", "templated_str", "str", "tuple", "list", "templatedfileslice", "intermediatefileslice", "list", "templatedfileslice", "trim", "both", "ends", "of", "an", "intermediate", "slice", "trim", "start", "new_slice", "head_buffer", "self", "_trim_end", "templated_str", "templated_str", "target_end", "head", "trim", "end", "new_slice", "tail_buffer", "new_slice", "_trim_end", "templated_str", "templated_str", "target_end", "tail", "return", "return", "head_buffer", "new_slice", "tail_buffer"], "doc_len": 50}
{"doc_id": "src/sqlfluff/core/templaters/python.py::IntermediateFileSlice.try_simple", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "IntermediateFileSlice", "func_name": "try_simple", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: IntermediateFileSlice\n    def try_simple(self):\n        \"\"\"Try to turn this intermediate slice into a simple slice.\"\"\"\n        # Yield anything simple\n        if len(self.slice_buffer) == 1:\n            return TemplatedFileSlice(\n                self.slice_buffer[0].slice_type,\n                self.source_slice,\n                self.templated_slice,\n            )\n        else:\n            raise ValueError(\"IntermediateFileSlice is not simple!\")\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "intermediatefileslice", "def", "try_simple", "self", "try", "to", "turn", "this", "intermediate", "slice", "into", "a", "simple", "slice", "yield", "anything", "simple", "if", "len", "self", "slice_buffer", "1", "return", "templatedfileslice", "self", "slice_buffer", "0", "slice_type", "self", "source_slice", "self", "templated_slice", "else", "raise", "valueerror", "intermediatefileslice", "is", "not", "simple"], "doc_len": 45}
{"doc_id": "src/sqlfluff/core/templaters/python.py::IntermediateFileSlice.coalesce", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "IntermediateFileSlice", "func_name": "coalesce", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: IntermediateFileSlice\n    def coalesce(self):\n        \"\"\"Coalesce this whole slice into a single one. Brutally.\"\"\"\n        return TemplatedFileSlice(\n            PythonTemplater._coalesce_types(self.slice_buffer),\n            self.source_slice,\n            self.templated_slice,\n        )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "intermediatefileslice", "def", "coalesce", "self", "coalesce", "this", "whole", "slice", "into", "a", "single", "one", "brutally", "return", "templatedfileslice", "pythontemplater", "_coalesce_types", "self", "slice_buffer", "self", "source_slice", "self", "templated_slice"], "doc_len": 29}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater.__init__", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "__init__", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def __init__(self, override_context=None, **kwargs):\n        self.default_context = dict(test_value=\"__test__\")\n        self.override_context = override_context or {}\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "__init__", "self", "override_context", "none", "kwargs", "self", "default_context", "dict", "test_value", "__test__", "self", "override_context", "override_context", "or"], "doc_len": 22}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater.infer_type", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "infer_type", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def infer_type(s):\n        \"\"\"Infer a python type from a string and convert.\n\n        Given a string value, convert it to a more specific built-in Python type\n        (e.g. int, float, list, dictionary) if possible.\n\n        \"\"\"\n        try:\n            return ast.literal_eval(s)\n        except (SyntaxError, ValueError):\n            return s\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "infer_type", "s", "infer", "a", "python", "type", "from", "a", "string", "and", "convert", "given", "a", "string", "value", "convert", "it", "to", "a", "more", "specific", "built", "in", "python", "type", "e", "g", "int", "float", "list", "dictionary", "if", "possible", "try", "return", "ast", "literal_eval", "s", "except", "syntaxerror", "valueerror", "return", "s"], "doc_len": 51}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater.get_context", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "get_context", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def get_context(self, fname=None, config=None):\n        \"\"\"Get the templating context from the config.\"\"\"\n        # TODO: The config loading should be done outside the templater code. Here\n        # is a silly place.\n        if config:\n            # This is now a nested section\n            loaded_context = (\n                config.get_section((self.templater_selector, self.name, \"context\"))\n                or {}\n            )\n        else:\n            loaded_context = {}\n        live_context = {}\n        live_context.update(self.default_context)\n        live_context.update(loaded_context)\n        live_context.update(self.override_context)\n\n        # Infer types\n        for k in loaded_context:\n            live_context[k] = self.infer_type(live_context[k])\n        return live_context\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "get_context", "self", "fname", "none", "config", "none", "get", "the", "templating", "context", "from", "the", "config", "todo", "the", "config", "loading", "should", "be", "done", "outside", "the", "templater", "code", "here", "is", "a", "silly", "place", "if", "config", "this", "is", "now", "a", "nested", "section", "loaded_context", "config", "get_section", "self", "templater_selector", "self", "name", "context", "or", "else", "loaded_context", "live_context", "live_context", "update", "self", "default_context", "live_context", "update", "loaded_context", "live_context", "update", "self", "override_context", "infer", "types", "for", "k", "in", "loaded_context", "live_context", "k", "self", "infer_type", "live_context", "k", "return", "live_context"], "doc_len": 82}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater.process", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "process", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def process(\n        self, *, in_str: str, fname: str, config=None, formatter=None\n    ) -> Tuple[Optional[TemplatedFile], list]:\n        \"\"\"Process a string and return a TemplatedFile.\n\n        Note that the arguments are enforced as keywords\n        because Templaters can have differences in their\n        `process` method signature.\n        A Templater that only supports reading from a file\n        would need the following signature:\n            process(*, fname, in_str=None, config=None)\n        (arguments are swapped)\n\n        Args:\n            in_str (:obj:`str`): The input string.\n            fname (:obj:`str`, optional): The filename of this string. This is\n                mostly for loading config files at runtime.\n            config (:obj:`FluffConfig`): A specific config to use for this\n                templating operation. Only necessary for some templaters.\n            formatter (:obj:`CallbackFormatter`): Optional object for output.\n\n        \"\"\"\n        live_context = self.get_context(fname=fname, config=config)\n        try:\n            new_str = in_str.format(**live_context)\n        except KeyError as err:\n            # TODO: Add a url here so people can get more help.\n            raise SQLTemplaterError(\n                \"Failure in Python templating: {}. Have you configured your variables?\".format(\n                    err\n                )\n            )\n        raw_sliced, sliced_file, new_str = self.slice_file(\n            in_str, new_str, config=config\n        )\n        return (\n            TemplatedFile(\n                source_str=in_str,\n                templated_str=new_str,\n                fname=fname,\n                sliced_file=sliced_file,\n                raw_sliced=raw_sliced,\n            ),\n            [],\n        )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "process", "self", "in_str", "str", "fname", "str", "config", "none", "formatter", "none", "tuple", "optional", "templatedfile", "list", "process", "a", "string", "and", "return", "a", "templatedfile", "note", "that", "the", "arguments", "are", "enforced", "as", "keywords", "because", "templaters", "can", "have", "differences", "in", "their", "process", "method", "signature", "a", "templater", "that", "only", "supports", "reading", "from", "a", "file", "would", "need", "the", "following", "signature", "process", "fname", "in_str", "none", "config", "none", "arguments", "are", "swapped", "args", "in_str", "obj", "str", "the", "input", "string", "fname", "obj", "str", "optional", "the", "filename", "of", "this", "string", "this", "is", "mostly", "for", "loading", "config", "files", "at", "runtime", "config", "obj", "fluffconfig", "a", "specific", "config", "to", "use", "for", "this", "templating", "operation", "only", "necessary", "for", "some", "templaters", "formatter", "obj", "callbackformatter", "optional", "object", "for", "output", "live_context", "self", "get_context", "fname", "fname", "config", "config", "try", "new_str", "in_str", "format", "live_context", "except", "keyerror", "as", "err", "todo", "add", "a", "url", "here", "so", "people", "can", "get", "more", "help", "raise", "sqltemplatererror", "failure", "in", "python", "templating", "have", "you", "configured", "your", "variables", "format", "err", "raw_sliced", "sliced_file", "new_str", "self", "slice_file", "in_str", "new_str", "config", "config", "return", "templatedfile", "source_str", "in_str", "templated_str", "new_str", "fname", "fname", "sliced_file", "sliced_file", "raw_sliced", "raw_sliced"], "doc_len": 180}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater.slice_file", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "slice_file", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def slice_file(\n        cls,\n        raw_str: str,\n        templated_str: str,\n        config=None,\n    ) -> Tuple[List[RawFileSlice], List[TemplatedFileSlice], str]:\n        \"\"\"Slice the file to determine regions where we can fix.\"\"\"\n        templater_logger.info(\"Slicing File Template\")\n        templater_logger.debug(\"    Raw String: %r\", raw_str)\n        templater_logger.debug(\"    Templated String: %r\", templated_str)\n        # Slice the raw file\n        raw_sliced = list(cls._slice_template(raw_str))\n        templater_logger.debug(\"    Raw Sliced:\")\n        for idx, raw_slice in enumerate(raw_sliced):\n            templater_logger.debug(\"        %s: %r\", idx, raw_slice)\n        # Find the literals\n        literals = [\n            raw_slice.raw\n            for raw_slice in raw_sliced\n            if raw_slice.slice_type == \"literal\"\n        ]\n        templater_logger.debug(\"    Literals: %s\", literals)\n        for loop_idx in range(2):\n            templater_logger.debug(\"    # Slice Loop %s\", loop_idx)\n            # Calculate occurrences\n            raw_occurrences = cls._substring_occurrences(raw_str, literals)\n            templated_occurrences = cls._substring_occurrences(templated_str, literals)\n            templater_logger.debug(\n                \"    Occurrences: Raw: %s, Templated: %s\",\n                raw_occurrences,\n                templated_occurrences,\n            )\n            # Split on invariants\n            split_sliced = list(\n                cls._split_invariants(\n                    raw_sliced,\n                    literals,\n                    raw_occurrences,\n                    templated_occurrences,\n                    templated_str,\n                )\n            )\n            templater_logger.debug(\"    Split Sliced:\")\n            for idx, split_slice in enumerate(split_sliced):\n                templater_logger.debug(\"        %s: %r\", idx, split_slice)\n            # Deal with uniques and coalesce the rest\n            sliced_file = list(\n                cls._split_uniques_coalesce_rest(\n                    split_sliced, raw_occurrences, templated_occurrences, templated_str\n                )\n            )\n            templater_logger.debug(\"    Fully Sliced:\")\n            for idx, templ_slice in enumerate(sliced_file):\n                templater_logger.debug(\"        %s: %r\", idx, templ_slice)\n            unwrap_wrapped = (\n                True\n                if config is None\n                else config.get(\n                    \"unwrap_wrapped_queries\", section=\"templater\", default=True\n                )\n            )\n            sliced_file, new_templated_str = cls._check_for_wrapped(\n                sliced_file, templated_str, unwrap_wrapped=unwrap_wrapped\n            )\n            if new_templated_str == templated_str:\n                # If we didn't change it then we're done.\n                break\n            else:\n                # If it's not equal, loop around\n                templated_str = new_templated_str\n        return raw_sliced, sliced_file, new_templated_str\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "slice_file", "cls", "raw_str", "str", "templated_str", "str", "config", "none", "tuple", "list", "rawfileslice", "list", "templatedfileslice", "str", "slice", "the", "file", "to", "determine", "regions", "where", "we", "can", "fix", "templater_logger", "info", "slicing", "file", "template", "templater_logger", "debug", "raw", "string", "r", "raw_str", "templater_logger", "debug", "templated", "string", "r", "templated_str", "slice", "the", "raw", "file", "raw_sliced", "list", "cls", "_slice_template", "raw_str", "templater_logger", "debug", "raw", "sliced", "for", "idx", "raw_slice", "in", "enumerate", "raw_sliced", "templater_logger", "debug", "s", "r", "idx", "raw_slice", "find", "the", "literals", "literals", "raw_slice", "raw", "for", "raw_slice", "in", "raw_sliced", "if", "raw_slice", "slice_type", "literal", "templater_logger", "debug", "literals", "s", "literals", "for", "loop_idx", "in", "range", "2", "templater_logger", "debug", "slice", "loop", "s", "loop_idx", "calculate", "occurrences", "raw_occurrences", "cls", "_substring_occurrences", "raw_str", "literals", "templated_occurrences", "cls", "_substring_occurrences", "templated_str", "literals", "templater_logger", "debug", "occurrences", "raw", "s", "templated", "s", "raw_occurrences", "templated_occurrences", "split", "on", "invariants", "split_sliced", "list", "cls", "_split_invariants", "raw_sliced", "literals", "raw_occurrences", "templated_occurrences", "templated_str", "templater_logger", "debug", "split", "sliced", "for", "idx", "split_slice", "in", "enumerate", "split_sliced", "templater_logger", "debug", "s", "r", "idx", "split_slice", "deal", "with", "uniques", "and", "coalesce", "the", "rest", "sliced_file", "list", "cls", "_split_uniques_coalesce_rest", "split_sliced", "raw_occurrences", "templated_occurrences", "templated_str", "templater_logger", "debug", "fully", "sliced", "for", "idx", "templ_slice", "in", "enumerate", "sliced_file", "templater_logger", "debug", "s", "r", "idx", "templ_slice", "unwrap_wrapped", "true", "if", "config", "is", "none", "else", "config", "get", "unwrap_wrapped_queries", "section", "templater", "default", "true", "sliced_file", "new_templated_str", "cls", "_check_for_wrapped", "sliced_file", "templated_str", "unwrap_wrapped", "unwrap_wrapped", "if", "new_templated_str", "templated_str", "if", "we", "didn", "t", "change", "it", "then", "we", "re", "done", "break", "else", "if", "it", "s", "not", "equal", "loop", "around", "templated_str", "new_templated_str", "return", "raw_sliced", "sliced_file", "new_templated_str"], "doc_len": 234}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._check_for_wrapped", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_check_for_wrapped", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _check_for_wrapped(\n        cls,\n        slices: List[TemplatedFileSlice],\n        templated_str: str,\n        unwrap_wrapped: bool = True,\n    ) -> Tuple[List[TemplatedFileSlice], str]:\n        \"\"\"Identify a wrapped query (e.g. dbt test) and handle it.\n\n        If unwrap_wrapped is true, we trim the wrapping from the templated file.\n        If unwrap_wrapped is false, we add a slice at start and end.\n        \"\"\"\n        if not slices:\n            # If there are no slices, return\n            return slices, templated_str\n        first_slice = slices[0]\n        last_slice = slices[-1]\n\n        if unwrap_wrapped:\n            # If we're unwrapping, there is no need to edit the slices, but we do need to trim\n            # the templated string. We should expect that the template will need to be re-sliced\n            # but we should assume that the function calling this one will deal with that\n            # eventuality.\n            return (\n                slices,\n                templated_str[\n                    first_slice.templated_slice.start : last_slice.templated_slice.stop\n                ],\n            )\n\n        if (\n            first_slice.source_slice.start == 0\n            and first_slice.templated_slice.start != 0\n        ):\n            # This means that there is text at the start of the templated file which doesn't exist\n            # in the raw file. Handle this by adding a templated slice (though it's not really templated)\n            # between 0 and 0 in the raw, and 0 and the current first slice start index in the templated.\n            slices.insert(\n                0,\n                TemplatedFileSlice(\n                    \"templated\",\n                    slice(0, 0),\n                    slice(0, first_slice.templated_slice.start),\n                ),\n            )\n        if last_slice.templated_slice.stop != len(templated_str):\n            #  This means that there is text at the end of the templated file which doesn't exist\n            #  in the raw file. Handle this by adding a templated slice beginning and ending at the\n            #  end of the raw, and the current last slice stop and file end in the templated.\n            slices.append(\n                TemplatedFileSlice(\n                    \"templated\",\n                    slice(last_slice.source_slice.stop, last_slice.source_slice.stop),\n                    slice(last_slice.templated_slice.stop, len(templated_str)),\n                )\n            )\n        return slices, templated_str\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_check_for_wrapped", "cls", "slices", "list", "templatedfileslice", "templated_str", "str", "unwrap_wrapped", "bool", "true", "tuple", "list", "templatedfileslice", "str", "identify", "a", "wrapped", "query", "e", "g", "dbt", "test", "and", "handle", "it", "if", "unwrap_wrapped", "is", "true", "we", "trim", "the", "wrapping", "from", "the", "templated", "file", "if", "unwrap_wrapped", "is", "false", "we", "add", "a", "slice", "at", "start", "and", "end", "if", "not", "slices", "if", "there", "are", "no", "slices", "return", "return", "slices", "templated_str", "first_slice", "slices", "0", "last_slice", "slices", "1", "if", "unwrap_wrapped", "if", "we", "re", "unwrapping", "there", "is", "no", "need", "to", "edit", "the", "slices", "but", "we", "do", "need", "to", "trim", "the", "templated", "string", "we", "should", "expect", "that", "the", "template", "will", "need", "to", "be", "re", "sliced", "but", "we", "should", "assume", "that", "the", "function", "calling", "this", "one", "will", "deal", "with", "that", "eventuality", "return", "slices", "templated_str", "first_slice", "templated_slice", "start", "last_slice", "templated_slice", "stop", "if", "first_slice", "source_slice", "start", "0", "and", "first_slice", "templated_slice", "start", "0", "this", "means", "that", "there", "is", "text", "at", "the", "start", "of", "the", "templated", "file", "which", "doesn", "t", "exist", "in", "the", "raw", "file", "handle", "this", "by", "adding", "a", "templated", "slice", "though", "it", "s", "not", "really", "templated", "between", "0", "and", "0", "in", "the", "raw", "and", "0", "and", "the", "current", "first", "slice", "start", "index", "in", "the", "templated", "slices", "insert", "0", "templatedfileslice", "templated", "slice", "0", "0", "slice", "0", "first_slice", "templated_slice", "start", "if", "last_slice", "templated_slice", "stop", "len", "templated_str", "this", "means", "that", "there", "is", "text", "at", "the", "end", "of", "the", "templated", "file", "which", "doesn", "t", "exist", "in", "the", "raw", "file", "handle", "this", "by", "adding", "a", "templated", "slice", "beginning", "and", "ending", "at", "the", "end", "of", "the", "raw", "and", "the", "current", "last", "slice", "stop", "and", "file", "end", "in", "the", "templated", "slices", "append", "templatedfileslice", "templated", "slice", "last_slice", "source_slice", "stop", "last_slice", "source_slice", "stop", "slice", "last_slice", "templated_slice", "stop", "len", "templated_str", "return", "slices", "templated_str"], "doc_len": 285}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._substring_occurrences", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_substring_occurrences", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _substring_occurrences(\n        cls, in_str: str, substrings: Iterable[str]\n    ) -> Dict[str, List[int]]:\n        \"\"\"Find every occurrence of the given substrings.\"\"\"\n        occurrences = {}\n        for substring in substrings:\n            occurrences[substring] = list(findall(substring, in_str))\n        return occurrences\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_substring_occurrences", "cls", "in_str", "str", "substrings", "iterable", "str", "dict", "str", "list", "int", "find", "every", "occurrence", "of", "the", "given", "substrings", "occurrences", "for", "substring", "in", "substrings", "occurrences", "substring", "list", "findall", "substring", "in_str", "return", "occurrences"], "doc_len": 39}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._sorted_occurrence_tuples", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_sorted_occurrence_tuples", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _sorted_occurrence_tuples(\n        occurrences: Dict[str, List[int]]\n    ) -> List[Tuple[str, int]]:\n        \"\"\"Sort a dict of occurrences into a sorted list of tuples.\"\"\"\n        return sorted(\n            ((raw, idx) for raw in occurrences.keys() for idx in occurrences[raw]),\n            # Sort first by position, then by lexical (for stability)\n            key=lambda x: (x[1], x[0]),\n        )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_sorted_occurrence_tuples", "occurrences", "dict", "str", "list", "int", "list", "tuple", "str", "int", "sort", "a", "dict", "of", "occurrences", "into", "a", "sorted", "list", "of", "tuples", "return", "sorted", "raw", "idx", "for", "raw", "in", "occurrences", "keys", "for", "idx", "in", "occurrences", "raw", "sort", "first", "by", "position", "then", "by", "lexical", "for", "stability", "key", "lambda", "x", "x", "1", "x", "0"], "doc_len": 59}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._slice_template", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_slice_template", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _slice_template(cls, in_str: str) -> Iterator[RawFileSlice]:\n        \"\"\"Slice a templated python string into token tuples.\n\n        This uses Formatter() as per:\n        https://docs.python.org/3/library/string.html#string.Formatter\n        \"\"\"\n        fmt = Formatter()\n        in_idx = 0\n        for literal_text, field_name, format_spec, conversion in fmt.parse(in_str):\n            if literal_text:\n                escape_chars = cls._sorted_occurrence_tuples(\n                    cls._substring_occurrences(literal_text, [\"}\", \"{\"])\n                )\n                idx = 0\n                while escape_chars:\n                    first_char = escape_chars.pop()\n                    # Is there a literal first?\n                    if first_char[1] > idx:\n                        yield RawFileSlice(\n                            literal_text[idx : first_char[1]], \"literal\", in_idx\n                        )\n                        in_idx += first_char[1] - idx\n                    # Add the escaped\n                    idx = first_char[1] + len(first_char[0])\n                    # We double them here to make the raw\n                    yield RawFileSlice(\n                        literal_text[first_char[1] : idx] * 2, \"escaped\", in_idx\n                    )\n                    # Will always be 2 in this case.\n                    # This is because ALL escape sequences in the python formatter\n                    # are two characters which reduce to one.\n                    in_idx += 2\n                # Deal with last one (if present)\n                if literal_text[idx:]:\n                    yield RawFileSlice(literal_text[idx:], \"literal\", in_idx)\n                    in_idx += len(literal_text) - idx\n            # Deal with fields\n            if field_name:\n                constructed_token = \"{{{field_name}{conv}{spec}}}\".format(\n                    field_name=field_name,\n                    conv=f\"!{conversion}\" if conversion else \"\",\n                    spec=f\":{format_spec}\" if format_spec else \"\",\n                )\n                yield RawFileSlice(constructed_token, \"templated\", in_idx)\n                in_idx += len(constructed_token)\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_slice_template", "cls", "in_str", "str", "iterator", "rawfileslice", "slice", "a", "templated", "python", "string", "into", "token", "tuples", "this", "uses", "formatter", "as", "per", "https", "docs", "python", "org", "3", "library", "string", "html", "string", "formatter", "fmt", "formatter", "in_idx", "0", "for", "literal_text", "field_name", "format_spec", "conversion", "in", "fmt", "parse", "in_str", "if", "literal_text", "escape_chars", "cls", "_sorted_occurrence_tuples", "cls", "_substring_occurrences", "literal_text", "idx", "0", "while", "escape_chars", "first_char", "escape_chars", "pop", "is", "there", "a", "literal", "first", "if", "first_char", "1", "idx", "yield", "rawfileslice", "literal_text", "idx", "first_char", "1", "literal", "in_idx", "in_idx", "first_char", "1", "idx", "add", "the", "escaped", "idx", "first_char", "1", "len", "first_char", "0", "we", "double", "them", "here", "to", "make", "the", "raw", "yield", "rawfileslice", "literal_text", "first_char", "1", "idx", "2", "escaped", "in_idx", "will", "always", "be", "2", "in", "this", "case", "this", "is", "because", "all", "escape", "sequences", "in", "the", "python", "formatter", "are", "two", "characters", "which", "reduce", "to", "one", "in_idx", "2", "deal", "with", "last", "one", "if", "present", "if", "literal_text", "idx", "yield", "rawfileslice", "literal_text", "idx", "literal", "in_idx", "in_idx", "len", "literal_text", "idx", "deal", "with", "fields", "if", "field_name", "constructed_token", "field_name", "conv", "spec", "format", "field_name", "field_name", "conv", "f", "conversion", "if", "conversion", "else", "spec", "f", "format_spec", "if", "format_spec", "else", "yield", "rawfileslice", "constructed_token", "templated", "in_idx", "in_idx", "len", "constructed_token"], "doc_len": 189}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._split_invariants", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_split_invariants", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _split_invariants(\n        cls,\n        raw_sliced: List[RawFileSlice],\n        literals: List[str],\n        raw_occurrences: Dict[str, List[int]],\n        templated_occurrences: Dict[str, List[int]],\n        templated_str: str,\n    ) -> Iterator[IntermediateFileSlice]:\n        \"\"\"Split a sliced file on its invariant literals.\n\n        We prioritise the _longest_ invariants first as they\n        are more likely to the the anchors.\n        \"\"\"\n        # Calculate invariants\n        invariants = [\n            literal\n            for literal in literals\n            if len(raw_occurrences[literal]) == 1\n            and len(templated_occurrences[literal]) == 1\n        ]\n        # Work through the invariants and make sure they appear\n        # in order.\n        for linv in sorted(invariants, key=len, reverse=True):\n            # Any invariants which have templated positions, relative\n            # to source positions, which aren't in order, should be\n            # ignored.\n\n            # Is this one still relevant?\n            if linv not in invariants:\n                continue\n\n            source_pos, templ_pos = raw_occurrences[linv], templated_occurrences[linv]\n            # Copy the list before iterating because we're going to edit it.\n            for tinv in invariants.copy():\n                if tinv != linv:\n                    src_dir = source_pos > raw_occurrences[tinv]\n                    tmp_dir = templ_pos > templated_occurrences[tinv]\n                    # If it's not in the same direction in the source and template remove it.\n                    if src_dir != tmp_dir:\n                        templater_logger.debug(\n                            \"          Invariant found out of order: %r\", tinv\n                        )\n                        invariants.remove(tinv)\n\n        # Set up some buffers\n        buffer: List[RawFileSlice] = []\n        idx: Optional[int] = None\n        templ_idx = 0\n        # Loop through\n        for raw, token_type, raw_pos, _ in raw_sliced:\n            if raw in invariants:\n                if buffer:\n                    yield IntermediateFileSlice(\n                        \"compound\",\n                        slice(idx, raw_pos),\n                        slice(templ_idx, templated_occurrences[raw][0]),\n                        buffer,\n                    )\n                buffer = []\n                idx = None\n                yield IntermediateFileSlice(\n                    \"invariant\",\n                    slice(raw_pos, raw_pos + len(raw)),\n                    slice(\n                        templated_occurrences[raw][0],\n                        templated_occurrences[raw][0] + len(raw),\n                    ),\n                    [RawFileSlice(raw, token_type, templated_occurrences[raw][0])],\n                )\n                templ_idx = templated_occurrences[raw][0] + len(raw)\n            else:\n                buffer.append(RawFileSlice(raw, token_type, raw_pos))\n                if idx is None:\n                    idx = raw_pos\n        # If we have a final buffer, yield it\n        if buffer:\n            yield IntermediateFileSlice(\n                \"compound\",\n                slice((idx or 0), (idx or 0) + sum(len(slc.raw) for slc in buffer)),\n                slice(templ_idx, len(templated_str)),\n                buffer,\n            )\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_split_invariants", "cls", "raw_sliced", "list", "rawfileslice", "literals", "list", "str", "raw_occurrences", "dict", "str", "list", "int", "templated_occurrences", "dict", "str", "list", "int", "templated_str", "str", "iterator", "intermediatefileslice", "split", "a", "sliced", "file", "on", "its", "invariant", "literals", "we", "prioritise", "the", "_longest_", "invariants", "first", "as", "they", "are", "more", "likely", "to", "the", "the", "anchors", "calculate", "invariants", "invariants", "literal", "for", "literal", "in", "literals", "if", "len", "raw_occurrences", "literal", "1", "and", "len", "templated_occurrences", "literal", "1", "work", "through", "the", "invariants", "and", "make", "sure", "they", "appear", "in", "order", "for", "linv", "in", "sorted", "invariants", "key", "len", "reverse", "true", "any", "invariants", "which", "have", "templated", "positions", "relative", "to", "source", "positions", "which", "aren", "t", "in", "order", "should", "be", "ignored", "is", "this", "one", "still", "relevant", "if", "linv", "not", "in", "invariants", "continue", "source_pos", "templ_pos", "raw_occurrences", "linv", "templated_occurrences", "linv", "copy", "the", "list", "before", "iterating", "because", "we", "re", "going", "to", "edit", "it", "for", "tinv", "in", "invariants", "copy", "if", "tinv", "linv", "src_dir", "source_pos", "raw_occurrences", "tinv", "tmp_dir", "templ_pos", "templated_occurrences", "tinv", "if", "it", "s", "not", "in", "the", "same", "direction", "in", "the", "source", "and", "template", "remove", "it", "if", "src_dir", "tmp_dir", "templater_logger", "debug", "invariant", "found", "out", "of", "order", "r", "tinv", "invariants", "remove", "tinv", "set", "up", "some", "buffers", "buffer", "list", "rawfileslice", "idx", "optional", "int", "none", "templ_idx", "0", "loop", "through", "for", "raw", "token_type", "raw_pos", "_", "in", "raw_sliced", "if", "raw", "in", "invariants", "if", "buffer", "yield", "intermediatefileslice", "compound", "slice", "idx", "raw_pos", "slice", "templ_idx", "templated_occurrences", "raw", "0", "buffer", "buffer", "idx", "none", "yield", "intermediatefileslice", "invariant", "slice", "raw_pos", "raw_pos", "len", "raw", "slice", "templated_occurrences", "raw", "0", "templated_occurrences", "raw", "0", "len", "raw", "rawfileslice", "raw", "token_type", "templated_occurrences", "raw", "0", "templ_idx", "templated_occurrences", "raw", "0", "len", "raw", "else", "buffer", "append", "rawfileslice", "raw", "token_type", "raw_pos", "if", "idx", "is", "none", "idx", "raw_pos", "if", "we", "have", "a", "final", "buffer", "yield", "it", "if", "buffer", "yield", "intermediatefileslice", "compound", "slice", "idx", "or", "0", "idx", "or", "0", "sum", "len", "slc", "raw", "for", "slc", "in", "buffer", "slice", "templ_idx", "len", "templated_str", "buffer"], "doc_len": 302}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._filter_occurrences", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_filter_occurrences", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _filter_occurrences(\n        file_slice: slice, occurrences: Dict[str, List[int]]\n    ) -> Dict[str, List[int]]:\n        \"\"\"Filter a dict of occurrences to just those within a slice.\"\"\"\n        filtered = {\n            key: [\n                pos\n                for pos in occurrences[key]\n                if pos >= file_slice.start and pos < file_slice.stop\n            ]\n            for key in occurrences.keys()\n        }\n        return {key: filtered[key] for key in filtered.keys() if filtered[key]}\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_filter_occurrences", "file_slice", "slice", "occurrences", "dict", "str", "list", "int", "dict", "str", "list", "int", "filter", "a", "dict", "of", "occurrences", "to", "just", "those", "within", "a", "slice", "filtered", "key", "pos", "for", "pos", "in", "occurrences", "key", "if", "pos", "file_slice", "start", "and", "pos", "file_slice", "stop", "for", "key", "in", "occurrences", "keys", "return", "key", "filtered", "key", "for", "key", "in", "filtered", "keys", "if", "filtered", "key"], "doc_len": 64}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._coalesce_types", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_coalesce_types", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _coalesce_types(elems: List[RawFileSlice]) -> str:\n        \"\"\"Coalesce to the priority type.\"\"\"\n        # Make a set of types\n        types = {elem.slice_type for elem in elems}\n        # Replace block types with templated\n        for typ in list(types):\n            if typ.startswith(\"block_\"):\n                types.remove(typ)\n                types.add(\"templated\")\n        # Take the easy route if they're all the same type\n        if len(types) == 1:\n            return types.pop()\n        # Then deal with priority\n        priority = [\"templated\", \"escaped\", \"literal\"]\n        for p in priority:\n            if p in types:\n                return p\n        raise RuntimeError(\n            f\"Exhausted priorities in _coalesce_types! {types!r}\"\n        )  # pragma: no cover\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_coalesce_types", "elems", "list", "rawfileslice", "str", "coalesce", "to", "the", "priority", "type", "make", "a", "set", "of", "types", "types", "elem", "slice_type", "for", "elem", "in", "elems", "replace", "block", "types", "with", "templated", "for", "typ", "in", "list", "types", "if", "typ", "startswith", "block_", "types", "remove", "typ", "types", "add", "templated", "take", "the", "easy", "route", "if", "they", "re", "all", "the", "same", "type", "if", "len", "types", "1", "return", "types", "pop", "then", "deal", "with", "priority", "priority", "templated", "escaped", "literal", "for", "p", "in", "priority", "if", "p", "in", "types", "return", "p", "raise", "runtimeerror", "f", "exhausted", "priorities", "in", "_coalesce_types", "types", "r", "pragma", "no", "cover"], "doc_len": 98}
{"doc_id": "src/sqlfluff/core/templaters/python.py::PythonTemplater._split_uniques_coalesce_rest", "file_path": "src/sqlfluff/core/templaters/python.py", "class_name": "PythonTemplater", "func_name": "_split_uniques_coalesce_rest", "text": "文件路径: src/sqlfluff/core/templaters/python.py, 类名: PythonTemplater\n    def _split_uniques_coalesce_rest(\n        cls,\n        split_file: List[IntermediateFileSlice],\n        raw_occurrences: Dict[str, List[int]],\n        templ_occurrences: Dict[str, List[int]],\n        templated_str: str,\n    ) -> Iterator[TemplatedFileSlice]:\n        \"\"\"Within each of the compound sections split on unique literals.\n\n        For everything else we coalesce to the dominant type.\n\n        Returns:\n            Iterable of the type of segment, the slice in the raw file\n                and the slice in the templated file.\n\n        \"\"\"\n        # A buffer to capture tail segments\n        tail_buffer: List[TemplatedFileSlice] = []\n\n        templater_logger.debug(\"    _split_uniques_coalesce_rest: %s\", split_file)\n\n        for int_file_slice in split_file:\n            # Yield anything from the tail buffer\n            if tail_buffer:\n                templater_logger.debug(\n                    \"        Yielding Tail Buffer [start]: %s\", tail_buffer\n                )\n                yield from tail_buffer\n                tail_buffer = []\n\n            # Check whether we're handling a zero length slice.\n            if (\n                int_file_slice.templated_slice.stop\n                - int_file_slice.templated_slice.start\n                == 0\n            ):\n                point_combo = int_file_slice.coalesce()\n                templater_logger.debug(\n                    \"        Yielding Point Combination: %s\", point_combo\n                )\n                yield point_combo\n                continue\n\n            # Yield anything simple\n            try:\n                simple_elem = int_file_slice.try_simple()\n                templater_logger.debug(\"        Yielding Simple: %s\", simple_elem)\n                yield simple_elem\n                continue\n            except ValueError:\n                pass\n\n            # Trim ends and overwrite the current working copy.\n            head_buffer, int_file_slice, tail_buffer = int_file_slice.trim_ends(\n                templated_str=templated_str\n            )\n            if head_buffer:\n                yield from head_buffer\n            # Have we consumed the whole thing?\n            if not int_file_slice.slice_buffer:\n                continue\n\n            # Try to yield simply again (post trim)\n            try:\n                simple_elem = int_file_slice.try_simple()\n                templater_logger.debug(\"        Yielding Simple: %s\", simple_elem)\n                yield simple_elem\n                continue\n            except ValueError:\n                pass\n\n            templater_logger.debug(\"        Intermediate Slice: %s\", int_file_slice)\n            # Generate the coalesced version in case we need it\n            coalesced = int_file_slice.coalesce()\n\n            # Look for anchors\n            raw_occs = cls._filter_occurrences(\n                int_file_slice.source_slice, raw_occurrences\n            )\n            templ_occs = cls._filter_occurrences(\n                int_file_slice.templated_slice, templ_occurrences\n            )\n            # Do we have any uniques to split on?\n            # NB: We use `get` on the templated occurrences, because it's possible\n            # that because of an if statement, something is in the source, but\n            # not in the templated at all. In that case, we shouldn't use it.\n            one_way_uniques = [\n                key\n                for key in raw_occs.keys()\n                if len(raw_occs[key]) == 1 and len(templ_occs.get(key, [])) >= 1\n            ]\n            two_way_uniques = [\n                key for key in one_way_uniques if len(templ_occs[key]) == 1\n            ]\n            # if we don't have anything to anchor on, then just return (coalescing types)\n            if not raw_occs or not templ_occs or not one_way_uniques:\n                templater_logger.debug(\n                    \"        No Anchors or Uniques. Yielding Whole: %s\", coalesced\n                )\n                yield coalesced\n                continue\n\n            # Deal with the inner segment itself.\n            templater_logger.debug(\n                \"        Intermediate Slice [post trim]: %s: %r\",\n                int_file_slice,\n                templated_str[int_file_slice.templated_slice],\n            )\n            templater_logger.debug(\"        One Way Uniques: %s\", one_way_uniques)\n            templater_logger.debug(\"        Two Way Uniques: %s\", two_way_uniques)\n\n            # Hang onto the starting position, which we'll advance as we go.\n            starts = (\n                int_file_slice.source_slice.start,\n                int_file_slice.templated_slice.start,\n            )\n\n            # Deal with two way uniques first, because they are easier.\n            # If we do find any we use recursion, because we'll want to do\n            # all of the above checks again.\n            if two_way_uniques:\n                # Yield the uniques and coalesce anything between.\n                bookmark_idx = 0\n                for idx, raw_slice in enumerate(int_file_slice.slice_buffer):\n                    pos = 0\n                    unq: Optional[str] = None\n                    # Does this element contain one of our uniques? If so, where?\n                    for unique in two_way_uniques:\n                        if unique in raw_slice.raw:\n                            pos = raw_slice.raw.index(unique)\n                            unq = unique\n\n                    if unq:\n                        # Yes it does. Handle it.\n\n                        # Get the position of the unique section.\n                        unique_position = (\n                            raw_occs[unq][0],\n                            templ_occs[unq][0],\n                        )\n                        templater_logger.debug(\n                            \"            Handling Unique: %r, %s, %s, %r\",\n                            unq,\n                            pos,\n                            unique_position,\n                            raw_slice,\n                        )\n\n                        # Handle full slices up to this one\n                        if idx > bookmark_idx:\n                            # Recurse to deal with any loops separately\n                            yield from cls._split_uniques_coalesce_rest(\n                                [\n                                    IntermediateFileSlice(\n                                        \"compound\",\n                                        # slice up to this unique\n                                        slice(starts[0], unique_position[0] - pos),\n                                        slice(starts[1], unique_position[1] - pos),\n                                        int_file_slice.slice_buffer[bookmark_idx:idx],\n                                    )\n                                ],\n                                raw_occs,\n                                templ_occs,\n                                templated_str,\n                            )\n\n                        # Handle any potential partial slice if we're part way through this one.\n                        if pos > 0:\n                            yield TemplatedFileSlice(\n                                raw_slice.slice_type,\n                                slice(unique_position[0] - pos, unique_position[0]),\n                                slice(unique_position[1] - pos, unique_position[1]),\n                            )\n\n                        # Handle the unique itself and update the bookmark\n                        starts = (\n                            unique_position[0] + len(unq),\n                            unique_position[1] + len(unq),\n                        )\n                        yield TemplatedFileSlice(\n                            raw_slice.slice_type,\n                            slice(unique_position[0], starts[0]),\n                            slice(unique_position[1], starts[1]),\n                        )\n                        # Move the bookmark after this position\n                        bookmark_idx = idx + 1\n\n                        # Handle any remnant after the unique.\n                        if raw_slice.raw[pos + len(unq) :]:\n                            remnant_length = len(raw_slice.raw) - (len(unq) + pos)\n                            _starts = starts\n                            starts = (\n                                starts[0] + remnant_length,\n                                starts[1] + remnant_length,\n                            )\n                            yield TemplatedFileSlice(\n                                raw_slice.slice_type,\n                                slice(_starts[0], starts[0]),\n                                slice(_starts[1], starts[1]),\n                            )\n\n                if bookmark_idx == 0:  # pragma: no cover\n                    # This is a SAFETY VALVE. In Theory we should never be here\n                    # and if we are it implies an error elsewhere. This clause\n                    # should stop any potential infinite recursion in its tracks\n                    # by simply classifying the whole of the current block as\n                    # templated and just stopping here.\n                    # Bugs triggering this eventuality have been observed in 0.4.0.\n                    templater_logger.info(\n                        \"        Safety Value Info: %s, %r\",\n                        two_way_uniques,\n                        templated_str[int_file_slice.templated_slice],\n                    )\n                    templater_logger.warning(\n                        \"        Python templater safety value unexpectedly triggered. \"\n                        \"Please report your raw and compiled query on github for debugging.\"\n                    )\n                    # NOTE: If a bug is reported here, this will incorrectly\n                    # classify more of the query as \"templated\" than it should.\n                    yield coalesced\n                    continue\n\n                # At the end of the loop deal with any remaining slices.\n                # The above \"Safety Valve\"TM should keep us safe from infinite\n                # recursion.\n                if len(int_file_slice.slice_buffer) > bookmark_idx:\n                    # Recurse to deal with any loops separately\n                    yield from cls._split_uniques_coalesce_rest(\n                        [\n                            IntermediateFileSlice(\n                                \"compound\",\n                                # Slicing is easy here, we have no choice\n                                slice(starts[0], int_file_slice.source_slice.stop),\n                                slice(starts[1], int_file_slice.templated_slice.stop),\n                                # Calculate the subsection to deal with.\n                                int_file_slice.slice_buffer[\n                                    bookmark_idx : len(int_file_slice.slice_buffer)\n                                ],\n                            )\n                        ],\n                        raw_occs,\n                        templ_occs,\n                        templated_str,\n                    )\n                # We continue here because the buffer should be exhausted,\n                # and if there's more to do we'll do it in the recursion.\n                continue\n\n            # If we get here, then there ARE uniques, but they are only ONE WAY.\n            # This means loops. Loops are tricky.\n            # We're very unlikely to get here (impossible?) with just python\n            # formatting, but this class is also the base for the jinja templater\n            # (and others?) so it may be used there.\n            # One way uniques give us landmarks to try and estimate what to do with them.\n            owu_templ_tuples = cls._sorted_occurrence_tuples(\n                {key: templ_occs[key] for key in one_way_uniques}\n            )\n\n            templater_logger.debug(\n                \"        Handling One Way Uniques: %s\", owu_templ_tuples\n            )\n\n            # Hang onto out *ending* position too from here.\n            stops = (\n                int_file_slice.source_slice.stop,\n                int_file_slice.templated_slice.stop,\n            )\n\n            # OWU in this context refers to \"One Way Unique\"\n            this_owu_idx: Optional[int] = None\n            last_owu_idx: Optional[int] = None\n            # Iterate through occurrence tuples of the one-way uniques.\n            for raw, template_idx in owu_templ_tuples:\n                raw_idx = raw_occs[raw][0]\n                raw_len = len(raw)\n\n                # Find the index of this owu in the slice_buffer, store the previous\n                last_owu_idx = this_owu_idx\n                try:\n                    this_owu_idx = next(\n                        idx\n                        for idx, slc in enumerate(int_file_slice.slice_buffer)\n                        if slc.raw == raw\n                    )\n                except StopIteration:\n                    # This can happen if the unique was detected, but was introduced\n                    # by a templater step. This is a false positive. Skip and move on.\n                    templater_logger.info(\n                        \"One Way Unique %r not found in slice buffer. Skipping...\", raw\n                    )\n                    continue\n\n                templater_logger.debug(\n                    \"        Handling OWU: %r @%s (raw @%s) [this_owu_idx: %s, last_owu_dx: %s]\",\n                    raw,\n                    template_idx,\n                    raw_idx,\n                    this_owu_idx,\n                    last_owu_idx,\n                )\n\n                if template_idx > starts[1]:\n                    # Yield the bit before this literal. We yield it\n                    # all as a tuple, because if we could do any better\n                    # we would have done it by now.\n\n                    # Can we identify a meaningful portion of the patch\n                    # to recurse a split?\n                    sub_section: Optional[List[RawFileSlice]] = None\n                    # If it's the start, the slicing is easy\n                    if (\n                        starts[1] == int_file_slice.templated_slice.stop\n                    ):  # pragma: no cover TODO?\n                        sub_section = int_file_slice.slice_buffer[:this_owu_idx]\n                    # If we are AFTER the previous in the template, then it's\n                    # also easy. [assuming it's not the same owu]\n                    elif raw_idx > starts[0] and last_owu_idx != this_owu_idx:\n                        if last_owu_idx:\n                            sub_section = int_file_slice.slice_buffer[\n                                last_owu_idx + 1 : this_owu_idx\n                            ]\n                        else:\n                            sub_section = int_file_slice.slice_buffer[:this_owu_idx]\n\n                    # If we succeeded in one of the above, we can also recurse\n                    # and be more intelligent with the other sections.\n                    if sub_section:\n                        # This assertion makes MyPy happy. In this case, we\n                        # never set source_slice without also setting\n                        # subsection.\n                        templater_logger.debug(\n                            \"        Attempting Subsplit [pre]: %s, %r\",\n                            sub_section,\n                            templated_str[slice(starts[1], template_idx)],\n                        )\n                        yield from cls._split_uniques_coalesce_rest(\n                            [\n                                IntermediateFileSlice(\n                                    \"compound\",\n                                    # Slicing is easy here, we have no choice\n                                    slice(starts[0], raw_idx),\n                                    slice(starts[1], template_idx),\n                                    sub_section,\n                                )\n                            ],\n                            raw_occs,\n                            templ_occs,\n                            templated_str,\n                        )\n                    # Otherwise, it's the tricky case.\n                    else:\n                        # In this case we've found a literal, coming AFTER another\n                        # in the templated version, but BEFORE (or the same) in the\n                        # raw version. This only happens during loops, but it means\n                        # that identifying exactly what the intervening bit refers\n                        # to is a bit arbitrary. In this case we're going to OVER\n                        # estimate and refer to the whole loop segment.\n\n                        # TODO: Maybe this should make two chunks instead, one\n                        # working backward, and one working forward. But that's\n                        # a job for another day.\n\n                        # First find where we are starting this remainder\n                        # in the template (as an index in the buffer).\n                        # Any segments *after* cur_idx are involved.\n                        if last_owu_idx is None or last_owu_idx + 1 >= len(\n                            int_file_slice.slice_buffer\n                        ):\n                            cur_idx = 0  # pragma: no cover\n                        else:\n                            cur_idx = last_owu_idx + 1\n\n                        # We need to know how many block_ends are after this.\n                        block_ends = sum(\n                            slc[1] == \"block_end\"\n                            for slc in int_file_slice.slice_buffer[cur_idx:]\n                        )\n                        # We can allow up to this number of preceding block starts\n                        block_start_indices = [\n                            idx\n                            for idx, slc in enumerate(\n                                int_file_slice.slice_buffer[:cur_idx]\n                            )\n                            if slc[1] == \"block_start\"\n                        ]\n\n                        # Trim anything which we're not allowed to use.\n                        if len(block_start_indices) > block_ends:\n                            offset = block_start_indices[-1 - block_ends] + 1\n                            elem_sub_buffer = int_file_slice.slice_buffer[offset:]\n                            cur_idx -= offset\n                        else:\n                            elem_sub_buffer = int_file_slice.slice_buffer\n\n                        # We also need to know whether any of the *starting*\n                        # segments are involved.\n                        # Anything up to start_idx (exclusive) is included.\n                        include_start = raw_idx > elem_sub_buffer[0][2]\n\n                        # The ending point of this slice, is already decided.\n                        end_point = elem_sub_buffer[-1].end_source_idx()\n\n                        # If start_idx is None, we're in luck. We don't need to include the beginning.\n                        if include_start:\n                            start_point = elem_sub_buffer[0].source_idx\n                        # Otherwise we know it's looped round, we need to include the whole slice.\n                        else:\n                            start_point = elem_sub_buffer[cur_idx].source_idx\n\n                        tricky = TemplatedFileSlice(\n                            \"templated\",\n                            slice(start_point, end_point),\n                            slice(starts[1], template_idx),\n                        )\n\n                        templater_logger.debug(\n                            \"        Yielding Tricky Case : %s\",\n                            tricky,\n                        )\n\n                        yield tricky\n\n                # Yield the literal\n                owu_literal_slice = TemplatedFileSlice(\n                    \"literal\",\n                    slice(raw_idx, raw_idx + raw_len),\n                    slice(template_idx, template_idx + raw_len),\n                )\n                templater_logger.debug(\n                    \"    Yielding Unique: %r, %s\",\n                    raw,\n                    owu_literal_slice,\n                )\n                yield owu_literal_slice\n                # Update our bookmark\n                starts = (\n                    raw_idx + raw_len,\n                    template_idx + raw_len,\n                )\n\n            if starts[1] < stops[1] and last_owu_idx is not None:\n                # Yield the end bit\n                templater_logger.debug(\"        Attempting Subsplit [post].\")\n                yield from cls._split_uniques_coalesce_rest(\n                    [\n                        IntermediateFileSlice(\n                            \"compound\",\n                            # Slicing is easy here, we have no choice\n                            slice(raw_idx + raw_len, stops[0]),\n                            slice(starts[1], stops[1]),\n                            int_file_slice.slice_buffer[last_owu_idx + 1 :],\n                        )\n                    ],\n                    raw_occs,\n                    templ_occs,\n                    templated_str,\n                )\n\n        # Yield anything from the tail buffer\n        if tail_buffer:\n            templater_logger.debug(\n                \"        Yielding Tail Buffer [end]: %s\", tail_buffer\n            )\n            yield from tail_buffer\n", "tokens": ["src", "sqlfluff", "core", "templaters", "python", "py", "pythontemplater", "def", "_split_uniques_coalesce_rest", "cls", "split_file", "list", "intermediatefileslice", "raw_occurrences", "dict", "str", "list", "int", "templ_occurrences", "dict", "str", "list", "int", "templated_str", "str", "iterator", "templatedfileslice", "within", "each", "of", "the", "compound", "sections", "split", "on", "unique", "literals", "for", "everything", "else", "we", "coalesce", "to", "the", "dominant", "type", "returns", "iterable", "of", "the", "type", "of", "segment", "the", "slice", "in", "the", "raw", "file", "and", "the", "slice", "in", "the", "templated", "file", "a", "buffer", "to", "capture", "tail", "segments", "tail_buffer", "list", "templatedfileslice", "templater_logger", "debug", "_split_uniques_coalesce_rest", "s", "split_file", "for", "int_file_slice", "in", "split_file", "yield", "anything", "from", "the", "tail", "buffer", "if", "tail_buffer", "templater_logger", "debug", "yielding", "tail", "buffer", "start", "s", "tail_buffer", "yield", "from", "tail_buffer", "tail_buffer", "check", "whether", "we", "re", "handling", "a", "zero", "length", "slice", "if", "int_file_slice", "templated_slice", "stop", "int_file_slice", "templated_slice", "start", "0", "point_combo", "int_file_slice", "coalesce", "templater_logger", "debug", "yielding", "point", "combination", "s", "point_combo", "yield", "point_combo", "continue", "yield", "anything", "simple", "try", "simple_elem", "int_file_slice", "try_simple", "templater_logger", "debug", "yielding", "simple", "s", "simple_elem", "yield", "simple_elem", "continue", "except", "valueerror", "pass", "trim", "ends", "and", "overwrite", "the", "current", "working", "copy", "head_buffer", "int_file_slice", "tail_buffer", "int_file_slice", "trim_ends", "templated_str", "templated_str", "if", "head_buffer", "yield", "from", "head_buffer", "have", "we", "consumed", "the", "whole", "thing", "if", "not", "int_file_slice", "slice_buffer", "continue", "try", "to", "yield", "simply", "again", "post", "trim", "try", "simple_elem", "int_file_slice", "try_simple", "templater_logger", "debug", "yielding", "simple", "s", "simple_elem", "yield", "simple_elem", "continue", "except", "valueerror", "pass", "templater_logger", "debug", "intermediate", "slice", "s", "int_file_slice", "generate", "the", "coalesced", "version", "in", "case", "we", "need", "it", "coalesced", "int_file_slice", "coalesce", "look", "for", "anchors", "raw_occs", "cls", "_filter_occurrences", "int_file_slice", "source_slice", "raw_occurrences", "templ_occs", "cls", "_filter_occurrences", "int_file_slice", "templated_slice", "templ_occurrences", "do", "we", "have", "any", "uniques", "to", "split", "on", "nb", "we", "use", "get", "on", "the", "templated", "occurrences", "because", "it", "s", "possible", "that", "because", "of", "an", "if", "statement", "something", "is", "in", "the", "source", "but", "not", "in", "the", "templated", "at", "all", "in", "that", "case", "we", "shouldn", "t", "use", "it", "one_way_uniques", "key", "for", "key", "in", "raw_occs", "keys", "if", "len", "raw_occs", "key", "1", "and", "len", "templ_occs", "get", "key", "1", "two_way_uniques", "key", "for", "key", "in", "one_way_uniques", "if", "len", "templ_occs", "key", "1", "if", "we", "don", "t", "have", "anything", "to", "anchor", "on", "then", "just", "return", "coalescing", "types", "if", "not", "raw_occs", "or", "not", "templ_occs", "or", "not", "one_way_uniques", "templater_logger", "debug", "no", "anchors", "or", "uniques", "yielding", "whole", "s", "coalesced", "yield", "coalesced", "continue", "deal", "with", "the", "inner", "segment", "itself", "templater_logger", "debug", "intermediate", "slice", "post", "trim", "s", "r", "int_file_slice", "templated_str", "int_file_slice", "templated_slice", "templater_logger", "debug", "one", "way", "uniques", "s", "one_way_uniques", "templater_logger", "debug", "two", "way", "uniques", "s", "two_way_uniques", "hang", "onto", "the", "starting", "position", "which", "we", "ll", "advance", "as", "we", "go", "starts", "int_file_slice", "source_slice", "start", "int_file_slice", "templated_slice", "start", "deal", "with", "two", "way", "uniques", "first", "because", "they", "are", "easier", "if", "we", "do", "find", "any", "we", "use", "recursion", "because", "we", "ll", "want", "to", "do", "all", "of", "the", "above", "checks", "again", "if", "two_way_uniques", "yield", "the", "uniques", "and", "coalesce", "anything", "between", "bookmark_idx", "0", "for", "idx", "raw_slice", "in", "enumerate", "int_file_slice", "slice_buffer", "pos", "0", "unq", "optional", "str", "none", "does", "this", "element", "contain", "one", "of", "our", "uniques", "if", "so", "where", "for", "unique", "in", "two_way_uniques", "if", "unique", "in", "raw_slice", "raw", "pos", "raw_slice", "raw", "index", "unique", "unq", "unique", "if", "unq", "yes", "it", "does", "handle", "it", "get", "the", "position", "of", "the", "unique", "section", "unique_position", "raw_occs", "unq", "0", "templ_occs", "unq", "0", "templater_logger", "debug", "handling", "unique", "r", "s", "s", "r", "unq", "pos", "unique_position", "raw_slice", "handle", "full", "slices", "up", "to", "this", "one", "if", "idx", "bookmark_idx", "recurse", "to", "deal", "with", "any", "loops", "separately", "yield", "from", "cls", "_split_uniques_coalesce_rest", "intermediatefileslice", "compound", "slice", "up", "to", "this", "unique", "slice", "starts", "0", "unique_position", "0", "pos", "slice", "starts", "1", "unique_position", "1", "pos", "int_file_slice", "slice_buffer", "bookmark_idx", "idx", "raw_occs", "templ_occs", "templated_str", "handle", "any", "potential", "partial", "slice", "if", "we", "re", "part", "way", "through", "this", "one", "if", "pos", "0", "yield", "templatedfileslice", "raw_slice", "slice_type", "slice", "unique_position", "0", "pos", "unique_position", "0", "slice", "unique_position", "1", "pos", "unique_position", "1", "handle", "the", "unique", "itself", "and", "update", "the", "bookmark", "starts", "unique_position", "0", "len", "unq", "unique_position", "1", "len", "unq", "yield", "templatedfileslice", "raw_slice", "slice_type", "slice", "unique_position", "0", "starts", "0", "slice", "unique_position", "1", "starts", "1", "move", "the", "bookmark", "after", "this", "position", "bookmark_idx", "idx", "1", "handle", "any", "remnant", "after", "the", "unique", "if", "raw_slice", "raw", "pos", "len", "unq", "remnant_length", "len", "raw_slice", "raw", "len", "unq", "pos", "_starts", "starts", "starts", "starts", "0", "remnant_length", "starts", "1", "remnant_length", "yield", "templatedfileslice", "raw_slice", "slice_type", "slice", "_starts", "0", "starts", "0", "slice", "_starts", "1", "starts", "1", "if", "bookmark_idx", "0", "pragma", "no", "cover", "this", "is", "a", "safety", "valve", "in", "theory", "we", "should", "never", "be", "here", "and", "if", "we", "are", "it", "implies", "an", "error", "elsewhere", "this", "clause", "should", "stop", "any", "potential", "infinite", "recursion", "in", "its", "tracks", "by", "simply", "classifying", "the", "whole", "of", "the", "current", "block", "as", "templated", "and", "just", "stopping", "here", "bugs", "triggering", "this", "eventuality", "have", "been", "observed", "in", "0", "4", "0", "templater_logger", "info", "safety", "value", "info", "s", "r", "two_way_uniques", "templated_str", "int_file_slice", "templated_slice", "templater_logger", "warning", "python", "templater", "safety", "value", "unexpectedly", "triggered", "please", "report", "your", "raw", "and", "compiled", "query", "on", "github", "for", "debugging", "note", "if", "a", "bug", "is", "reported", "here", "this", "will", "incorrectly", "classify", "more", "of", "the", "query", "as", "templated", "than", "it", "should", "yield", "coalesced", "continue", "at", "the", "end", "of", "the", "loop", "deal", "with", "any", "remaining", "slices", "the", "above", "safety", "valve", "tm", "should", "keep", "us", "safe", "from", "infinite", "recursion", "if", "len", "int_file_slice", "slice_buffer", "bookmark_idx", "recurse", "to", "deal", "with", "any", "loops", "separately", "yield", "from", "cls", "_split_uniques_coalesce_rest", "intermediatefileslice", "compound", "slicing", "is", "easy", "here", "we", "have", "no", "choice", "slice", "starts", "0", "int_file_slice", "source_slice", "stop", "slice", "starts", "1", "int_file_slice", "templated_slice", "stop", "calculate", "the", "subsection", "to", "deal", "with", "int_file_slice", "slice_buffer", "bookmark_idx", "len", "int_file_slice", "slice_buffer", "raw_occs", "templ_occs", "templated_str", "we", "continue", "here", "because", "the", "buffer", "should", "be", "exhausted", "and", "if", "there", "s", "more", "to", "do", "we", "ll", "do", "it", "in", "the", "recursion", "continue", "if", "we", "get", "here", "then", "there", "are", "uniques", "but", "they", "are", "only", "one", "way", "this", "means", "loops", "loops", "are", "tricky", "we", "re", "very", "unlikely", "to", "get", "here", "impossible", "with", "just", "python", "formatting", "but", "this", "class", "is", "also", "the", "base", "for", "the", "jinja", "templater", "and", "others", "so", "it", "may", "be", "used", "there", "one", "way", "uniques", "give", "us", "landmarks", "to", "try", "and", "estimate", "what", "to", "do", "with", "them", "owu_templ_tuples", "cls", "_sorted_occurrence_tuples", "key", "templ_occs", "key", "for", "key", "in", "one_way_uniques", "templater_logger", "debug", "handling", "one", "way", "uniques", "s", "owu_templ_tuples", "hang", "onto", "out", "ending", "position", "too", "from", "here", "stops", "int_file_slice", "source_slice", "stop", "int_file_slice", "templated_slice", "stop", "owu", "in", "this", "context", "refers", "to", "one", "way", "unique", "this_owu_idx", "optional", "int", "none", "last_owu_idx", "optional", "int", "none", "iterate", "through", "occurrence", "tuples", "of", "the", "one", "way", "uniques", "for", "raw", "template_idx", "in", "owu_templ_tuples", "raw_idx", "raw_occs", "raw", "0", "raw_len", "len", "raw", "find", "the", "index", "of", "this", "owu", "in", "the", "slice_buffer", "store", "the", "previous", "last_owu_idx", "this_owu_idx", "try", "this_owu_idx", "next", "idx", "for", "idx", "slc", "in", "enumerate", "int_file_slice", "slice_buffer", "if", "slc", "raw", "raw", "except", "stopiteration", "this", "can", "happen", "if", "the", "unique", "was", "detected", "but", "was", "introduced", "by", "a", "templater", "step", "this", "is", "a", "false", "positive", "skip", "and", "move", "on", "templater_logger", "info", "one", "way", "unique", "r", "not", "found", "in", "slice", "buffer", "skipping", "raw", "continue", "templater_logger", "debug", "handling", "owu", "r", "s", "raw", "s", "this_owu_idx", "s", "last_owu_dx", "s", "raw", "template_idx", "raw_idx", "this_owu_idx", "last_owu_idx", "if", "template_idx", "starts", "1", "yield", "the", "bit", "before", "this", "literal", "we", "yield", "it", "all", "as", "a", "tuple", "because", "if", "we", "could", "do", "any", "better", "we", "would", "have", "done", "it", "by", "now", "can", "we", "identify", "a", "meaningful", "portion", "of", "the", "patch", "to", "recurse", "a", "split", "sub_section", "optional", "list", "rawfileslice", "none", "if", "it", "s", "the", "start", "the", "slicing", "is", "easy", "if", "starts", "1", "int_file_slice", "templated_slice", "stop", "pragma", "no", "cover", "todo", "sub_section", "int_file_slice", "slice_buffer", "this_owu_idx", "if", "we", "are", "after", "the", "previous", "in", "the", "template", "then", "it", "s", "also", "easy", "assuming", "it", "s", "not", "the", "same", "owu", "elif", "raw_idx", "starts", "0", "and", "last_owu_idx", "this_owu_idx", "if", "last_owu_idx", "sub_section", "int_file_slice", "slice_buffer", "last_owu_idx", "1", "this_owu_idx", "else", "sub_section", "int_file_slice", "slice_buffer", "this_owu_idx", "if", "we", "succeeded", "in", "one", "of", "the", "above", "we", "can", "also", "recurse", "and", "be", "more", "intelligent", "with", "the", "other", "sections", "if", "sub_section", "this", "assertion", "makes", "mypy", "happy", "in", "this", "case", "we", "never", "set", "source_slice", "without", "also", "setting", "subsection", "templater_logger", "debug", "attempting", "subsplit", "pre", "s", "r", "sub_section", "templated_str", "slice", "starts", "1", "template_idx", "yield", "from", "cls", "_split_uniques_coalesce_rest", "intermediatefileslice", "compound", "slicing", "is", "easy", "here", "we", "have", "no", "choice", "slice", "starts", "0", "raw_idx", "slice", "starts", "1", "template_idx", "sub_section", "raw_occs", "templ_occs", "templated_str", "otherwise", "it", "s", "the", "tricky", "case", "else", "in", "this", "case", "we", "ve", "found", "a", "literal", "coming", "after", "another", "in", "the", "templated", "version", "but", "before", "or", "the", "same", "in", "the", "raw", "version", "this", "only", "happens", "during", "loops", "but", "it", "means", "that", "identifying", "exactly", "what", "the", "intervening", "bit", "refers", "to", "is", "a", "bit", "arbitrary", "in", "this", "case", "we", "re", "going", "to", "over", "estimate", "and", "refer", "to", "the", "whole", "loop", "segment", "todo", "maybe", "this", "should", "make", "two", "chunks", "instead", "one", "working", "backward", "and", "one", "working", "forward", "but", "that", "s", "a", "job", "for", "another", "day", "first", "find", "where", "we", "are", "starting", "this", "remainder", "in", "the", "template", "as", "an", "index", "in", "the", "buffer", "any", "segments", "after", "cur_idx", "are", "involved", "if", "last_owu_idx", "is", "none", "or", "last_owu_idx", "1", "len", "int_file_slice", "slice_buffer", "cur_idx", "0", "pragma", "no", "cover", "else", "cur_idx", "last_owu_idx", "1", "we", "need", "to", "know", "how", "many", "block_ends", "are", "after", "this", "block_ends", "sum", "slc", "1", "block_end", "for", "slc", "in", "int_file_slice", "slice_buffer", "cur_idx", "we", "can", "allow", "up", "to", "this", "number", "of", "preceding", "block", "starts", "block_start_indices", "idx", "for", "idx", "slc", "in", "enumerate", "int_file_slice", "slice_buffer", "cur_idx", "if", "slc", "1", "block_start", "trim", "anything", "which", "we", "re", "not", "allowed", "to", "use", "if", "len", "block_start_indices", "block_ends", "offset", "block_start_indices", "1", "block_ends", "1", "elem_sub_buffer", "int_file_slice", "slice_buffer", "offset", "cur_idx", "offset", "else", "elem_sub_buffer", "int_file_slice", "slice_buffer", "we", "also", "need", "to", "know", "whether", "any", "of", "the", "starting", "segments", "are", "involved", "anything", "up", "to", "start_idx", "exclusive", "is", "included", "include_start", "raw_idx", "elem_sub_buffer", "0", "2", "the", "ending", "point", "of", "this", "slice", "is", "already", "decided", "end_point", "elem_sub_buffer", "1", "end_source_idx", "if", "start_idx", "is", "none", "we", "re", "in", "luck", "we", "don", "t", "need", "to", "include", "the", "beginning", "if", "include_start", "start_point", "elem_sub_buffer", "0", "source_idx", "otherwise", "we", "know", "it", "s", "looped", "round", "we", "need", "to", "include", "the", "whole", "slice", "else", "start_point", "elem_sub_buffer", "cur_idx", "source_idx", "tricky", "templatedfileslice", "templated", "slice", "start_point", "end_point", "slice", "starts", "1", "template_idx", "templater_logger", "debug", "yielding", "tricky", "case", "s", "tricky", "yield", "tricky", "yield", "the", "literal", "owu_literal_slice", "templatedfileslice", "literal", "slice", "raw_idx", "raw_idx", "raw_len", "slice", "template_idx", "template_idx", "raw_len", "templater_logger", "debug", "yielding", "unique", "r", "s", "raw", "owu_literal_slice", "yield", "owu_literal_slice", "update", "our", "bookmark", "starts", "raw_idx", "raw_len", "template_idx", "raw_len", "if", "starts", "1", "stops", "1", "and", "last_owu_idx", "is", "not", "none", "yield", "the", "end", "bit", "templater_logger", "debug", "attempting", "subsplit", "post", "yield", "from", "cls", "_split_uniques_coalesce_rest", "intermediatefileslice", "compound", "slicing", "is", "easy", "here", "we", "have", "no", "choice", "slice", "raw_idx", "raw_len", "stops", "0", "slice", "starts", "1", "stops", "1", "int_file_slice", "slice_buffer", "last_owu_idx", "1", "raw_occs", "templ_occs", "templated_str", "yield", "anything", "from", "the", "tail", "buffer", "if", "tail_buffer", "templater_logger", "debug", "yielding", "tail", "buffer", "end", "s", "tail_buffer", "yield", "from", "tail_buffer"], "doc_len": 1713}
{"doc_id": "src/sqlfluff/core/templaters/__init__.py::core_templaters", "file_path": "src/sqlfluff/core/templaters/__init__.py", "class_name": null, "func_name": "core_templaters", "text": "文件路径: src/sqlfluff/core/templaters/__init__.py\ndef core_templaters():\n    \"\"\"Returns the templater tuples for the core templaters.\"\"\"\n    yield from [RawTemplater, JinjaTemplater, PythonTemplater]\n", "tokens": ["src", "sqlfluff", "core", "templaters", "__init__", "py", "def", "core_templaters", "returns", "the", "templater", "tuples", "for", "the", "core", "templaters", "yield", "from", "rawtemplater", "jinjatemplater", "pythontemplater"], "doc_len": 21}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment._iter_reference_parts", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "_iter_reference_parts", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def _iter_reference_parts(cls, elem) -> Generator[ObjectReferencePart, None, None]:\n        \"\"\"Extract the elements of a reference and yield.\"\"\"\n        # trim on quotes and split out any dots.\n        for part in elem.raw_trimmed().split(\".\"):\n            yield cls.ObjectReferencePart(part, [elem])\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "_iter_reference_parts", "cls", "elem", "generator", "objectreferencepart", "none", "none", "extract", "the", "elements", "of", "a", "reference", "and", "yield", "trim", "on", "quotes", "and", "split", "out", "any", "dots", "for", "part", "in", "elem", "raw_trimmed", "split", "yield", "cls", "objectreferencepart", "part", "elem"], "doc_len": 41}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment.iter_raw_references", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "iter_raw_references", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def iter_raw_references(self) -> Generator[ObjectReferencePart, None, None]:\n        \"\"\"Generate a list of reference strings and elements.\n\n        Each reference is an ObjectReferencePart. If some are split, then a\n        segment may appear twice, but the substring will only appear once.\n        \"\"\"\n        # Extract the references from those identifiers (because some may be quoted)\n        for elem in self.recursive_crawl(\"identifier\"):\n            yield from self._iter_reference_parts(elem)\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "iter_raw_references", "self", "generator", "objectreferencepart", "none", "none", "generate", "a", "list", "of", "reference", "strings", "and", "elements", "each", "reference", "is", "an", "objectreferencepart", "if", "some", "are", "split", "then", "a", "segment", "may", "appear", "twice", "but", "the", "substring", "will", "only", "appear", "once", "extract", "the", "references", "from", "those", "identifiers", "because", "some", "may", "be", "quoted", "for", "elem", "in", "self", "recursive_crawl", "identifier", "yield", "from", "self", "_iter_reference_parts", "elem"], "doc_len": 65}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment.is_qualified", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "is_qualified", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def is_qualified(self):\n        \"\"\"Return if there is more than one element to the reference.\"\"\"\n        return len(list(self.iter_raw_references())) > 1\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "is_qualified", "self", "return", "if", "there", "is", "more", "than", "one", "element", "to", "the", "reference", "return", "len", "list", "self", "iter_raw_references", "1"], "doc_len": 26}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment.qualification", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "qualification", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def qualification(self):\n        \"\"\"Return the qualification type of this reference.\"\"\"\n        return \"qualified\" if self.is_qualified() else \"unqualified\"\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "qualification", "self", "return", "the", "qualification", "type", "of", "this", "reference", "return", "qualified", "if", "self", "is_qualified", "else", "unqualified"], "doc_len": 23}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment.extract_possible_references", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "extract_possible_references", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def extract_possible_references(\n        self, level: Union[ObjectReferenceLevel, int]\n    ) -> List[ObjectReferencePart]:\n        \"\"\"Extract possible references of a given level.\n\n        \"level\" may be (but is not required to be) a value from the\n        ObjectReferenceLevel enum defined above.\n\n        NOTE: The base implementation here returns at most one part, but\n        dialects such as BigQuery that support nesting (e.g. STRUCT) may return\n        multiple reference parts.\n        \"\"\"\n        level = self._level_to_int(level)\n        refs = list(self.iter_raw_references())\n        if len(refs) >= level:\n            return [refs[-level]]\n        return []\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "extract_possible_references", "self", "level", "union", "objectreferencelevel", "int", "list", "objectreferencepart", "extract", "possible", "references", "of", "a", "given", "level", "level", "may", "be", "but", "is", "not", "required", "to", "be", "a", "value", "from", "the", "objectreferencelevel", "enum", "defined", "above", "note", "the", "base", "implementation", "here", "returns", "at", "most", "one", "part", "but", "dialects", "such", "as", "bigquery", "that", "support", "nesting", "e", "g", "struct", "may", "return", "multiple", "reference", "parts", "level", "self", "_level_to_int", "level", "refs", "list", "self", "iter_raw_references", "if", "len", "refs", "level", "return", "refs", "level", "return"], "doc_len": 81}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::ObjectReferenceSegment._level_to_int", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "ObjectReferenceSegment", "func_name": "_level_to_int", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: ObjectReferenceSegment\n    def _level_to_int(level: Union[ObjectReferenceLevel, int]) -> int:\n        # If it's an ObjectReferenceLevel, get the value. Otherwise, assume it's\n        # an int.\n        level = getattr(level, \"value\", level)\n        assert isinstance(level, int)\n        return level\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "objectreferencesegment", "def", "_level_to_int", "level", "union", "objectreferencelevel", "int", "int", "if", "it", "s", "an", "objectreferencelevel", "get", "the", "value", "otherwise", "assume", "it", "s", "an", "int", "level", "getattr", "level", "value", "level", "assert", "isinstance", "level", "int", "return", "level"], "doc_len": 38}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::FromExpressionElementSegment.get_eventual_alias", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "FromExpressionElementSegment", "func_name": "get_eventual_alias", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: FromExpressionElementSegment\n    def get_eventual_alias(self) -> Optional[AliasInfo]:\n        \"\"\"Return the eventual table name referred to by this table expression.\n\n        Returns:\n            :obj:`tuple` of (:obj:`str`, :obj:`BaseSegment`, :obj:`bool`) containing\n                a string representation of the alias, a reference to the\n                segment containing it, and whether it's an alias.\n\n        \"\"\"\n        alias_expression = self.get_child(\"alias_expression\")\n        tbl_expression = self.get_child(\"table_expression\")\n        if not tbl_expression:  # pragma: no cover\n            tbl_expression = self.get_child(\"bracketed\").get_child(\"table_expression\")\n        ref = tbl_expression.get_child(\"object_reference\")\n        if alias_expression:\n            # If it has an alias, return that\n            segment = alias_expression.get_child(\"identifier\")\n            return AliasInfo(segment.raw, segment, True, self, alias_expression, ref)\n\n        # If not return the object name (or None if there isn't one)\n        # ref = self.get_child(\"object_reference\")\n        if ref:\n            # Return the last element of the reference.\n            penultimate_ref: ObjectReferenceSegment.ObjectReferencePart = list(\n                ref.iter_raw_references()\n            )[-1]\n            return AliasInfo(\n                penultimate_ref.part,\n                penultimate_ref.segments[0],\n                False,\n                self,\n                None,\n                ref,\n            )\n        # No references or alias, return None\n        return None\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "fromexpressionelementsegment", "def", "get_eventual_alias", "self", "optional", "aliasinfo", "return", "the", "eventual", "table", "name", "referred", "to", "by", "this", "table", "expression", "returns", "obj", "tuple", "of", "obj", "str", "obj", "basesegment", "obj", "bool", "containing", "a", "string", "representation", "of", "the", "alias", "a", "reference", "to", "the", "segment", "containing", "it", "and", "whether", "it", "s", "an", "alias", "alias_expression", "self", "get_child", "alias_expression", "tbl_expression", "self", "get_child", "table_expression", "if", "not", "tbl_expression", "pragma", "no", "cover", "tbl_expression", "self", "get_child", "bracketed", "get_child", "table_expression", "ref", "tbl_expression", "get_child", "object_reference", "if", "alias_expression", "if", "it", "has", "an", "alias", "return", "that", "segment", "alias_expression", "get_child", "identifier", "return", "aliasinfo", "segment", "raw", "segment", "true", "self", "alias_expression", "ref", "if", "not", "return", "the", "object", "name", "or", "none", "if", "there", "isn", "t", "one", "ref", "self", "get_child", "object_reference", "if", "ref", "return", "the", "last", "element", "of", "the", "reference", "penultimate_ref", "objectreferencesegment", "objectreferencepart", "list", "ref", "iter_raw_references", "1", "return", "aliasinfo", "penultimate_ref", "part", "penultimate_ref", "segments", "0", "false", "self", "none", "ref", "no", "references", "or", "alias", "return", "none", "return", "none"], "doc_len": 150}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::WildcardIdentifierSegment.iter_raw_references", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "WildcardIdentifierSegment", "func_name": "iter_raw_references", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: WildcardIdentifierSegment\n    def iter_raw_references(self):\n        \"\"\"Generate a list of reference strings and elements.\n\n        Each element is a tuple of (str, segment). If some are\n        split, then a segment may appear twice, but the substring\n        will only appear once.\n        \"\"\"\n        # Extract the references from those identifiers (because some may be quoted)\n        for elem in self.recursive_crawl(\"identifier\", \"star\"):\n            yield from self._iter_reference_parts(elem)\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "wildcardidentifiersegment", "def", "iter_raw_references", "self", "generate", "a", "list", "of", "reference", "strings", "and", "elements", "each", "element", "is", "a", "tuple", "of", "str", "segment", "if", "some", "are", "split", "then", "a", "segment", "may", "appear", "twice", "but", "the", "substring", "will", "only", "appear", "once", "extract", "the", "references", "from", "those", "identifiers", "because", "some", "may", "be", "quoted", "for", "elem", "in", "self", "recursive_crawl", "identifier", "star", "yield", "from", "self", "_iter_reference_parts", "elem"], "doc_len": 65}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::JoinClauseSegment.get_eventual_alias", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "JoinClauseSegment", "func_name": "get_eventual_alias", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: JoinClauseSegment\n    def get_eventual_alias(self) -> AliasInfo:\n        \"\"\"Return the eventual table name referred to by this join clause.\"\"\"\n        from_expression_element = self.get_child(\"from_expression_element\")\n        return from_expression_element.get_eventual_alias()\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "joinclausesegment", "def", "get_eventual_alias", "self", "aliasinfo", "return", "the", "eventual", "table", "name", "referred", "to", "by", "this", "join", "clause", "from_expression_element", "self", "get_child", "from_expression_element", "return", "from_expression_element", "get_eventual_alias"], "doc_len": 28}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::FromClauseSegment.get_eventual_aliases", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "FromClauseSegment", "func_name": "get_eventual_aliases", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: FromClauseSegment\n    def get_eventual_aliases(self) -> List[Tuple[BaseSegment, AliasInfo]]:\n        \"\"\"List the eventual aliases of this from clause.\n\n        Comes as a list of tuples (table expr, tuple (string, segment, bool)).\n        \"\"\"\n        buff = []\n        direct_table_children = []\n        join_clauses = []\n\n        for from_expression in self.get_children(\"from_expression\"):\n            direct_table_children += from_expression.get_children(\n                \"from_expression_element\"\n            )\n            join_clauses += from_expression.get_children(\"join_clause\")\n\n        # Iterate through the potential sources of aliases\n        for clause in (*direct_table_children, *join_clauses):\n            ref: AliasInfo = clause.get_eventual_alias()\n            # Only append if non null. A None reference, may\n            # indicate a generator expression or similar.\n            table_expr = (\n                clause\n                if clause in direct_table_children\n                else clause.get_child(\"from_expression_element\")\n            )\n            if ref:\n                buff.append((table_expr, ref))\n        return buff\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "fromclausesegment", "def", "get_eventual_aliases", "self", "list", "tuple", "basesegment", "aliasinfo", "list", "the", "eventual", "aliases", "of", "this", "from", "clause", "comes", "as", "a", "list", "of", "tuples", "table", "expr", "tuple", "string", "segment", "bool", "buff", "direct_table_children", "join_clauses", "for", "from_expression", "in", "self", "get_children", "from_expression", "direct_table_children", "from_expression", "get_children", "from_expression_element", "join_clauses", "from_expression", "get_children", "join_clause", "iterate", "through", "the", "potential", "sources", "of", "aliases", "for", "clause", "in", "direct_table_children", "join_clauses", "ref", "aliasinfo", "clause", "get_eventual_alias", "only", "append", "if", "non", "null", "a", "none", "reference", "may", "indicate", "a", "generator", "expression", "or", "similar", "table_expr", "clause", "if", "clause", "in", "direct_table_children", "else", "clause", "get_child", "from_expression_element", "if", "ref", "buff", "append", "table_expr", "ref", "return", "buff"], "doc_len": 99}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::CTEDefinitionSegment.get_identifier", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "CTEDefinitionSegment", "func_name": "get_identifier", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: CTEDefinitionSegment\n    def get_identifier(self) -> BaseSegment:\n        \"\"\"Gets the identifier of this CTE.\n\n        Note: it blindly get the first identifier it finds\n        which given the structure of a CTE definition is\n        usually the right one.\n        \"\"\"\n        return self.get_child(\"identifier\")\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "ctedefinitionsegment", "def", "get_identifier", "self", "basesegment", "gets", "the", "identifier", "of", "this", "cte", "note", "it", "blindly", "get", "the", "first", "identifier", "it", "finds", "which", "given", "the", "structure", "of", "a", "cte", "definition", "is", "usually", "the", "right", "one", "return", "self", "get_child", "identifier"], "doc_len": 42}
{"doc_id": "src/sqlfluff/dialects/dialect_ansi.py::StatementSegment.get_table_references", "file_path": "src/sqlfluff/dialects/dialect_ansi.py", "class_name": "StatementSegment", "func_name": "get_table_references", "text": "文件路径: src/sqlfluff/dialects/dialect_ansi.py, 类名: StatementSegment\n    def get_table_references(self):\n        \"\"\"Use parsed tree to extract table references.\"\"\"\n        table_refs = {\n            tbl_ref.raw for tbl_ref in self.recursive_crawl(\"table_reference\")\n        }\n        cte_refs = {\n            cte_def.get_identifier().raw\n            for cte_def in self.recursive_crawl(\"common_table_expression\")\n        }\n        # External references are any table references which aren't\n        # also cte aliases.\n        return table_refs - cte_refs\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_ansi", "py", "statementsegment", "def", "get_table_references", "self", "use", "parsed", "tree", "to", "extract", "table", "references", "table_refs", "tbl_ref", "raw", "for", "tbl_ref", "in", "self", "recursive_crawl", "table_reference", "cte_refs", "cte_def", "get_identifier", "raw", "for", "cte_def", "in", "self", "recursive_crawl", "common_table_expression", "external", "references", "are", "any", "table", "references", "which", "aren", "t", "also", "cte", "aliases", "return", "table_refs", "cte_refs"], "doc_len": 50}
{"doc_id": "src/sqlfluff/dialects/dialect_bigquery.py::ColumnReferenceSegment.extract_possible_references", "file_path": "src/sqlfluff/dialects/dialect_bigquery.py", "class_name": "ColumnReferenceSegment", "func_name": "extract_possible_references", "text": "文件路径: src/sqlfluff/dialects/dialect_bigquery.py, 类名: ColumnReferenceSegment\n    def extract_possible_references(self, level):\n        \"\"\"Extract possible references of a given level.\"\"\"\n        level = self._level_to_int(level)\n        refs = list(self.iter_raw_references())\n        if level == self.ObjectReferenceLevel.SCHEMA.value and len(refs) >= 3:\n            return [refs[0]]  # pragma: no cover\n        if level == self.ObjectReferenceLevel.TABLE.value and len(refs) >= 3:\n            # Ambiguous case: The table could be the first or second part, so\n            # return both.\n            return [refs[0], refs[1]]\n        if level == self.ObjectReferenceLevel.OBJECT.value and len(refs) >= 3:\n            # Ambiguous case: The object (i.e. column) could be the first or\n            # second part, so return both.\n            return [refs[1], refs[2]]  # pragma: no cover\n        return super().extract_possible_references(level)\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_bigquery", "py", "columnreferencesegment", "def", "extract_possible_references", "self", "level", "extract", "possible", "references", "of", "a", "given", "level", "level", "self", "_level_to_int", "level", "refs", "list", "self", "iter_raw_references", "if", "level", "self", "objectreferencelevel", "schema", "value", "and", "len", "refs", "3", "return", "refs", "0", "pragma", "no", "cover", "if", "level", "self", "objectreferencelevel", "table", "value", "and", "len", "refs", "3", "ambiguous", "case", "the", "table", "could", "be", "the", "first", "or", "second", "part", "so", "return", "both", "return", "refs", "0", "refs", "1", "if", "level", "self", "objectreferencelevel", "object", "value", "and", "len", "refs", "3", "ambiguous", "case", "the", "object", "i", "e", "column", "could", "be", "the", "first", "or", "second", "part", "so", "return", "both", "return", "refs", "1", "refs", "2", "pragma", "no", "cover", "return", "super", "extract_possible_references", "level"], "doc_len": 109}
{"doc_id": "src/sqlfluff/dialects/dialect_bigquery.py::HyphenatedObjectReferenceSegment.iter_raw_references", "file_path": "src/sqlfluff/dialects/dialect_bigquery.py", "class_name": "HyphenatedObjectReferenceSegment", "func_name": "iter_raw_references", "text": "文件路径: src/sqlfluff/dialects/dialect_bigquery.py, 类名: HyphenatedObjectReferenceSegment\n    def iter_raw_references(self):\n        \"\"\"Generate a list of reference strings and elements.\n\n        Each reference is an ObjectReferencePart. Overrides the base class\n        because hyphens (MinusSegment) causes one logical part of the name to\n        be split across multiple elements, e.g. \"table-a\" is parsed as three\n        segments.\n        \"\"\"\n        # For each descendant element, group them, using \"dot\" elements as a\n        # delimiter.\n        for is_dot, elems in itertools.groupby(\n            self.recursive_crawl(\"identifier\", \"binary_operator\", \"dot\"),\n            lambda e: e.is_type(\"dot\"),\n        ):\n            if not is_dot:\n                segments = list(elems)\n                parts = [seg.raw_trimmed() for seg in segments]\n                yield self.ObjectReferencePart(\"\".join(parts), segments)\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_bigquery", "py", "hyphenatedobjectreferencesegment", "def", "iter_raw_references", "self", "generate", "a", "list", "of", "reference", "strings", "and", "elements", "each", "reference", "is", "an", "objectreferencepart", "overrides", "the", "base", "class", "because", "hyphens", "minussegment", "causes", "one", "logical", "part", "of", "the", "name", "to", "be", "split", "across", "multiple", "elements", "e", "g", "table", "a", "is", "parsed", "as", "three", "segments", "for", "each", "descendant", "element", "group", "them", "using", "dot", "elements", "as", "a", "delimiter", "for", "is_dot", "elems", "in", "itertools", "groupby", "self", "recursive_crawl", "identifier", "binary_operator", "dot", "lambda", "e", "e", "is_type", "dot", "if", "not", "is_dot", "segments", "list", "elems", "parts", "seg", "raw_trimmed", "for", "seg", "in", "segments", "yield", "self", "objectreferencepart", "join", "parts", "segments"], "doc_len": 98}
{"doc_id": "src/sqlfluff/dialects/dialect_postgres_keywords.py::priority_keyword_merge", "file_path": "src/sqlfluff/dialects/dialect_postgres_keywords.py", "class_name": null, "func_name": "priority_keyword_merge", "text": "文件路径: src/sqlfluff/dialects/dialect_postgres_keywords.py\ndef priority_keyword_merge(*args):\n    \"\"\"Merge keyword lists, giving priority to entries in later lists.\n\n    *args is a list of keyword lists, these lists should be of tuples in the form (keyword, type)\n\n    \"\"\"\n    keyword_lists = [*args]\n    base_list = []\n    if len(keyword_lists) == 1:\n        return keyword_lists[0]\n\n    while len(keyword_lists) > 1:\n        base_list, priority_list = keyword_lists[0], keyword_lists[1]\n        keyword_set = set([x[0] for x in base_list])\n        for item in priority_list:\n            if item[0] in keyword_set:\n                for index, keyword in enumerate(base_list):\n                    if keyword[0] == item[0]:\n                        base_list.pop(index)\n                        break\n            base_list.append(item)\n\n        keyword_lists.pop(1)\n\n    return base_list\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_postgres_keywords", "py", "def", "priority_keyword_merge", "args", "merge", "keyword", "lists", "giving", "priority", "to", "entries", "in", "later", "lists", "args", "is", "a", "list", "of", "keyword", "lists", "these", "lists", "should", "be", "of", "tuples", "in", "the", "form", "keyword", "type", "keyword_lists", "args", "base_list", "if", "len", "keyword_lists", "1", "return", "keyword_lists", "0", "while", "len", "keyword_lists", "1", "base_list", "priority_list", "keyword_lists", "0", "keyword_lists", "1", "keyword_set", "set", "x", "0", "for", "x", "in", "base_list", "for", "item", "in", "priority_list", "if", "item", "0", "in", "keyword_set", "for", "index", "keyword", "in", "enumerate", "base_list", "if", "keyword", "0", "item", "0", "base_list", "pop", "index", "break", "base_list", "append", "item", "keyword_lists", "pop", "1", "return", "base_list"], "doc_len": 96}
{"doc_id": "src/sqlfluff/dialects/dialect_postgres_keywords.py::get_keywords", "file_path": "src/sqlfluff/dialects/dialect_postgres_keywords.py", "class_name": null, "func_name": "get_keywords", "text": "文件路径: src/sqlfluff/dialects/dialect_postgres_keywords.py\ndef get_keywords(keyword_list, keyword_type):\n    \"\"\"Get a list of keywords of the required type.\n\n    keyword_type should be one of \"not-keyword\", \"reserved\", \"non-reserved\"\n    \"\"\"\n    keywords = [x[0] for x in keyword_list if x[1].startswith(keyword_type)]\n\n    return keywords\n", "tokens": ["src", "sqlfluff", "dialects", "dialect_postgres_keywords", "py", "def", "get_keywords", "keyword_list", "keyword_type", "get", "a", "list", "of", "keywords", "of", "the", "required", "type", "keyword_type", "should", "be", "one", "of", "not", "keyword", "reserved", "non", "reserved", "keywords", "x", "0", "for", "x", "in", "keyword_list", "if", "x", "1", "startswith", "keyword_type", "return", "keywords"], "doc_len": 42}
{"doc_id": "src/sqlfluff/rules/L001.py::Rule_L001._eval", "file_path": "src/sqlfluff/rules/L001.py", "class_name": "Rule_L001", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L001.py, 类名: Rule_L001\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Unnecessary trailing whitespace.\n\n        Look for newline segments, and then evaluate what\n        it was preceded by.\n        \"\"\"\n        # We only trigger on newlines\n        if (\n            context.segment.is_type(\"newline\")\n            and len(context.raw_stack) > 0\n            and context.raw_stack[-1].is_type(\"whitespace\")\n        ):\n            # If we find a newline, which is preceded by whitespace, then bad\n            deletions = []\n            idx = -1\n            while abs(idx) <= len(context.raw_stack) and context.raw_stack[idx].is_type(\n                \"whitespace\"\n            ):\n                deletions.append(context.raw_stack[idx])\n                idx -= 1\n            last_deletion_slice = deletions[-1].pos_marker.source_slice\n\n            # Check the raw source (before template expansion) immediately\n            # following the whitespace we want to delete. Often, what looks\n            # like trailing whitespace in rendered SQL is actually a line like:\n            # \"    {% for elem in elements %}\\n\", in which case the code is\n            # fine -- it's not trailing whitespace from a source code\n            # perspective.\n            if context.templated_file:\n                next_raw_slice = (\n                    context.templated_file.raw_slices_spanning_source_slice(\n                        slice(last_deletion_slice.stop, last_deletion_slice.stop)\n                    )\n                )\n                # If the next slice is literal, that means it's regular code, so\n                # it's safe to delete the trailing whitespace. If it's anything\n                # else, it's template code, so don't delete the whitespace because\n                # it's not REALLY trailing whitespace in terms of the raw source\n                # code.\n                if next_raw_slice[0].slice_type != \"literal\":\n                    return LintResult()\n            return LintResult(\n                anchor=deletions[-1],\n                fixes=[LintFix(\"delete\", d) for d in deletions],\n            )\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l001", "py", "rule_l001", "def", "_eval", "self", "context", "rulecontext", "lintresult", "unnecessary", "trailing", "whitespace", "look", "for", "newline", "segments", "and", "then", "evaluate", "what", "it", "was", "preceded", "by", "we", "only", "trigger", "on", "newlines", "if", "context", "segment", "is_type", "newline", "and", "len", "context", "raw_stack", "0", "and", "context", "raw_stack", "1", "is_type", "whitespace", "if", "we", "find", "a", "newline", "which", "is", "preceded", "by", "whitespace", "then", "bad", "deletions", "idx", "1", "while", "abs", "idx", "len", "context", "raw_stack", "and", "context", "raw_stack", "idx", "is_type", "whitespace", "deletions", "append", "context", "raw_stack", "idx", "idx", "1", "last_deletion_slice", "deletions", "1", "pos_marker", "source_slice", "check", "the", "raw", "source", "before", "template", "expansion", "immediately", "following", "the", "whitespace", "we", "want", "to", "delete", "often", "what", "looks", "like", "trailing", "whitespace", "in", "rendered", "sql", "is", "actually", "a", "line", "like", "for", "elem", "in", "elements", "n", "in", "which", "case", "the", "code", "is", "fine", "it", "s", "not", "trailing", "whitespace", "from", "a", "source", "code", "perspective", "if", "context", "templated_file", "next_raw_slice", "context", "templated_file", "raw_slices_spanning_source_slice", "slice", "last_deletion_slice", "stop", "last_deletion_slice", "stop", "if", "the", "next", "slice", "is", "literal", "that", "means", "it", "s", "regular", "code", "so", "it", "s", "safe", "to", "delete", "the", "trailing", "whitespace", "if", "it", "s", "anything", "else", "it", "s", "template", "code", "so", "don", "t", "delete", "the", "whitespace", "because", "it", "s", "not", "really", "trailing", "whitespace", "in", "terms", "of", "the", "raw", "source", "code", "if", "next_raw_slice", "0", "slice_type", "literal", "return", "lintresult", "return", "lintresult", "anchor", "deletions", "1", "fixes", "lintfix", "delete", "d", "for", "d", "in", "deletions", "return", "lintresult"], "doc_len": 222}
{"doc_id": "src/sqlfluff/rules/L002.py::Rule_L002._eval", "file_path": "src/sqlfluff/rules/L002.py", "class_name": "Rule_L002", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L002.py, 类名: Rule_L002\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Mixed Tabs and Spaces in single whitespace.\n\n        Only trigger from whitespace segments if they contain\n        multiple kinds of whitespace.\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n\n        if context.segment.is_type(\"whitespace\"):\n            if \" \" in context.segment.raw and \"\\t\" in context.segment.raw:\n                if len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\n                    \"newline\"\n                ):\n                    # We've got a single whitespace at the beginning of a line.\n                    # It's got a mix of spaces and tabs. Replace each tab with\n                    # a multiple of spaces\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                context.segment,\n                                context.segment.edit(\n                                    context.segment.raw.replace(\n                                        \"\\t\", \" \" * self.tab_space_size\n                                    )\n                                ),\n                            )\n                        ],\n                    )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l002", "py", "rule_l002", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "mixed", "tabs", "and", "spaces", "in", "single", "whitespace", "only", "trigger", "from", "whitespace", "segments", "if", "they", "contain", "multiple", "kinds", "of", "whitespace", "config", "type", "hints", "self", "tab_space_size", "int", "if", "context", "segment", "is_type", "whitespace", "if", "in", "context", "segment", "raw", "and", "t", "in", "context", "segment", "raw", "if", "len", "context", "raw_stack", "0", "or", "context", "raw_stack", "1", "is_type", "newline", "we", "ve", "got", "a", "single", "whitespace", "at", "the", "beginning", "of", "a", "line", "it", "s", "got", "a", "mix", "of", "spaces", "and", "tabs", "replace", "each", "tab", "with", "a", "multiple", "of", "spaces", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "context", "segment", "context", "segment", "edit", "context", "segment", "raw", "replace", "t", "self", "tab_space_size", "return", "none"], "doc_len": 116}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._make_indent", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_make_indent", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _make_indent(\n        num: int = 1, tab_space_size: int = 4, indent_unit: str = \"space\"\n    ) -> str:\n        if indent_unit == \"tab\":\n            base_unit = \"\\t\"\n        elif indent_unit == \"space\":\n            base_unit = \" \" * tab_space_size\n        else:\n            raise ValueError(\n                f\"Parameter indent_unit has unexpected value: `{indent_unit}`. Expected `tab` or `space`.\"\n            )\n        return base_unit * num\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_make_indent", "num", "int", "1", "tab_space_size", "int", "4", "indent_unit", "str", "space", "str", "if", "indent_unit", "tab", "base_unit", "t", "elif", "indent_unit", "space", "base_unit", "tab_space_size", "else", "raise", "valueerror", "f", "parameter", "indent_unit", "has", "unexpected", "value", "indent_unit", "expected", "tab", "or", "space", "return", "base_unit", "num"], "doc_len": 45}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._indent_size", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_indent_size", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _indent_size(segments: Sequence[RawSegment], tab_space_size: int = 4) -> int:\n        indent_size = 0\n        for elem in segments:\n            raw = elem.raw\n            # convert to spaces for convenience (and hanging indents)\n            raw = raw.replace(\"\\t\", \" \" * tab_space_size)\n            indent_size += len(raw)\n        return indent_size\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_indent_size", "segments", "sequence", "rawsegment", "tab_space_size", "int", "4", "int", "indent_size", "0", "for", "elem", "in", "segments", "raw", "elem", "raw", "convert", "to", "spaces", "for", "convenience", "and", "hanging", "indents", "raw", "raw", "replace", "t", "tab_space_size", "indent_size", "len", "raw", "return", "indent_size"], "doc_len": 42}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._reorder_raw_stack", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_reorder_raw_stack", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _reorder_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        templated_file: Optional[TemplatedFile],\n    ) -> Tuple[RawSegment, ...]:\n        \"\"\"Reorder raw_stack to simplify indentation logic.\n\n        Context: The indentation logic was mostly designed to work with normal\n        segment types. Templating introduces additional segments into the parse\n        tree, often in the \"wrong\" place with respect to the indentation logic,\n        for example, where do indent/dedent segments appear with respect to the\n        segments that trigger indent/dedent behavior? This function reorders\n        nodes locally (i.e. only within L003) to get the desired behavior.\n        \"\"\"\n\n        def segment_info(idx: int) -> Tuple[str, Optional[str]]:\n            \"\"\"Helper function for sort_current_line().\"\"\"\n            seg = current_line[idx]\n            return seg.type, cls._get_element_template_info(seg, templated_file)\n\n        def move_indent_before_templated() -> None:\n            \"\"\"Swap position of template and indent segment if code follows.\n\n            This allows for correct indentation of templated table names in\n            \"FROM\", for example:\n\n            SELECT brand\n            FROM\n                {{ product }}\n\n            \"\"\"\n            for idx in range(2, len(current_line)):\n                if (\n                    segment_info(idx - 2)\n                    == (\n                        \"placeholder\",\n                        \"templated\",\n                    )\n                    and segment_info(idx - 1) == (\"indent\", None)\n                    and segment_info(idx) == (\"raw\", None)\n                ):\n                    current_line[idx - 2], current_line[idx - 1] = (\n                        current_line[idx - 1],\n                        current_line[idx - 2],\n                    )\n\n        # Break raw_stack into lines.\n        lines = []\n        current_line = []\n        for elem in raw_stack:\n            if not elem.is_type(\"newline\"):\n                current_line.append(elem)\n            else:\n                move_indent_before_templated()\n                current_line.append(elem)\n                lines.append(current_line)\n                current_line = []\n        if current_line:\n            move_indent_before_templated()\n            lines.append(current_line)\n        new_raw_stack = [s for line in lines for s in line]\n        return tuple(new_raw_stack)\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_reorder_raw_stack", "cls", "raw_stack", "tuple", "rawsegment", "templated_file", "optional", "templatedfile", "tuple", "rawsegment", "reorder", "raw_stack", "to", "simplify", "indentation", "logic", "context", "the", "indentation", "logic", "was", "mostly", "designed", "to", "work", "with", "normal", "segment", "types", "templating", "introduces", "additional", "segments", "into", "the", "parse", "tree", "often", "in", "the", "wrong", "place", "with", "respect", "to", "the", "indentation", "logic", "for", "example", "where", "do", "indent", "dedent", "segments", "appear", "with", "respect", "to", "the", "segments", "that", "trigger", "indent", "dedent", "behavior", "this", "function", "reorders", "nodes", "locally", "i", "e", "only", "within", "l003", "to", "get", "the", "desired", "behavior", "def", "segment_info", "idx", "int", "tuple", "str", "optional", "str", "helper", "function", "for", "sort_current_line", "seg", "current_line", "idx", "return", "seg", "type", "cls", "_get_element_template_info", "seg", "templated_file", "def", "move_indent_before_templated", "none", "swap", "position", "of", "template", "and", "indent", "segment", "if", "code", "follows", "this", "allows", "for", "correct", "indentation", "of", "templated", "table", "names", "in", "from", "for", "example", "select", "brand", "from", "product", "for", "idx", "in", "range", "2", "len", "current_line", "if", "segment_info", "idx", "2", "placeholder", "templated", "and", "segment_info", "idx", "1", "indent", "none", "and", "segment_info", "idx", "raw", "none", "current_line", "idx", "2", "current_line", "idx", "1", "current_line", "idx", "1", "current_line", "idx", "2", "break", "raw_stack", "into", "lines", "lines", "current_line", "for", "elem", "in", "raw_stack", "if", "not", "elem", "is_type", "newline", "current_line", "append", "elem", "else", "move_indent_before_templated", "current_line", "append", "elem", "lines", "append", "current_line", "current_line", "if", "current_line", "move_indent_before_templated", "lines", "append", "current_line", "new_raw_stack", "s", "for", "line", "in", "lines", "for", "s", "in", "line", "return", "tuple", "new_raw_stack"], "doc_len": 222}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._process_raw_stack", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_process_raw_stack", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _process_raw_stack(\n        cls,\n        raw_stack: Tuple[RawSegment, ...],\n        memory: dict = None,\n        tab_space_size: int = 4,\n        templated_file: Optional[TemplatedFile] = None,\n    ) -> dict:\n        \"\"\"Take the raw stack, split into lines and evaluate some stats.\"\"\"\n        raw_stack = cls._reorder_raw_stack(raw_stack, templated_file)\n        indent_balance = 0\n        line_no = 1\n        in_indent = True\n        indent_buffer: List[RawSegment] = []\n        line_buffer: List[RawSegment] = []\n        result_buffer = {}\n        indent_size = 0\n        line_indent_stack: List[int] = []\n        this_indent_balance = 0\n        clean_indent = False\n        hanger_pos = None\n\n        for elem in raw_stack:\n            line_buffer.append(elem)\n            # Pin indent_balance to above zero\n            if indent_balance < 0:\n                indent_balance = 0\n\n            if elem.is_type(\"newline\"):\n                result_buffer[line_no] = {\n                    \"line_no\": line_no,\n                    # Using slicing to copy line_buffer here to be py2 compliant\n                    \"line_buffer\": line_buffer[:],\n                    \"indent_buffer\": indent_buffer,\n                    \"indent_size\": indent_size,\n                    # Indent balance is the indent at the start of the first content\n                    \"indent_balance\": this_indent_balance,\n                    \"hanging_indent\": hanger_pos if line_indent_stack else None,\n                    # Clean indent is true if the line *ends* with an indent\n                    # or has an indent in the initial whitespace.\n                    \"clean_indent\": clean_indent,\n                }\n                line_no += 1\n                indent_buffer = []\n                line_buffer = []\n                indent_size = 0\n                in_indent = True\n                line_indent_stack = []\n                hanger_pos = None\n                # Assume an unclean indent, but if the last line\n                # ended with an indent then we might be ok.\n                clean_indent = False\n                # Was there an indent after the last code element of the previous line?\n                for search_elem in reversed(result_buffer[line_no - 1][\"line_buffer\"]):  # type: ignore\n                    if not search_elem.is_code and not search_elem.is_meta:\n                        continue\n                    elif search_elem.is_meta and search_elem.indent_val > 0:\n                        clean_indent = True\n                    break\n            elif in_indent:\n                if elem.is_type(\"whitespace\"):\n                    indent_buffer.append(elem)\n                elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                    indent_balance += elem.indent_val  # type: ignore\n                    if elem.indent_val > 0:  # type: ignore\n                        # a \"clean\" indent is one where it contains\n                        # an increase in indentation? Can't quite\n                        # remember the logic here. Let's go with that.\n                        clean_indent = True\n                else:\n                    in_indent = False\n                    this_indent_balance = indent_balance\n                    indent_size = cls._indent_size(\n                        indent_buffer, tab_space_size=tab_space_size\n                    )\n            elif elem.is_meta and elem.indent_val != 0:  # type: ignore\n                indent_balance += elem.indent_val  # type: ignore\n                if elem.indent_val > 0:  # type: ignore\n                    # Keep track of the indent at the last ... indent\n                    line_indent_stack.append(\n                        cls._indent_size(line_buffer, tab_space_size=tab_space_size)\n                    )\n                    hanger_pos = None\n                else:\n                    # this is a dedent, we could still have a hanging indent,\n                    # but only if there's enough on the stack\n                    if line_indent_stack:\n                        line_indent_stack.pop()\n            elif elem.is_code:\n                if hanger_pos is None:\n                    hanger_pos = cls._indent_size(\n                        line_buffer[:-1], tab_space_size=tab_space_size\n                    )\n\n            # If we hit the trigger element, stop processing.\n            if memory and elem is memory[\"trigger\"]:\n                break\n\n        # If we get to the end, and still have a buffer, add it on\n        if line_buffer:\n            result_buffer[line_no] = {\n                \"line_no\": line_no,\n                \"line_buffer\": line_buffer,\n                \"indent_buffer\": indent_buffer,\n                \"indent_size\": indent_size,\n                \"indent_balance\": this_indent_balance,\n                \"hanging_indent\": line_indent_stack.pop()\n                if line_indent_stack\n                else None,\n                \"clean_indent\": clean_indent,\n            }\n        return result_buffer\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_process_raw_stack", "cls", "raw_stack", "tuple", "rawsegment", "memory", "dict", "none", "tab_space_size", "int", "4", "templated_file", "optional", "templatedfile", "none", "dict", "take", "the", "raw", "stack", "split", "into", "lines", "and", "evaluate", "some", "stats", "raw_stack", "cls", "_reorder_raw_stack", "raw_stack", "templated_file", "indent_balance", "0", "line_no", "1", "in_indent", "true", "indent_buffer", "list", "rawsegment", "line_buffer", "list", "rawsegment", "result_buffer", "indent_size", "0", "line_indent_stack", "list", "int", "this_indent_balance", "0", "clean_indent", "false", "hanger_pos", "none", "for", "elem", "in", "raw_stack", "line_buffer", "append", "elem", "pin", "indent_balance", "to", "above", "zero", "if", "indent_balance", "0", "indent_balance", "0", "if", "elem", "is_type", "newline", "result_buffer", "line_no", "line_no", "line_no", "using", "slicing", "to", "copy", "line_buffer", "here", "to", "be", "py2", "compliant", "line_buffer", "line_buffer", "indent_buffer", "indent_buffer", "indent_size", "indent_size", "indent", "balance", "is", "the", "indent", "at", "the", "start", "of", "the", "first", "content", "indent_balance", "this_indent_balance", "hanging_indent", "hanger_pos", "if", "line_indent_stack", "else", "none", "clean", "indent", "is", "true", "if", "the", "line", "ends", "with", "an", "indent", "or", "has", "an", "indent", "in", "the", "initial", "whitespace", "clean_indent", "clean_indent", "line_no", "1", "indent_buffer", "line_buffer", "indent_size", "0", "in_indent", "true", "line_indent_stack", "hanger_pos", "none", "assume", "an", "unclean", "indent", "but", "if", "the", "last", "line", "ended", "with", "an", "indent", "then", "we", "might", "be", "ok", "clean_indent", "false", "was", "there", "an", "indent", "after", "the", "last", "code", "element", "of", "the", "previous", "line", "for", "search_elem", "in", "reversed", "result_buffer", "line_no", "1", "line_buffer", "type", "ignore", "if", "not", "search_elem", "is_code", "and", "not", "search_elem", "is_meta", "continue", "elif", "search_elem", "is_meta", "and", "search_elem", "indent_val", "0", "clean_indent", "true", "break", "elif", "in_indent", "if", "elem", "is_type", "whitespace", "indent_buffer", "append", "elem", "elif", "elem", "is_meta", "and", "elem", "indent_val", "0", "type", "ignore", "indent_balance", "elem", "indent_val", "type", "ignore", "if", "elem", "indent_val", "0", "type", "ignore", "a", "clean", "indent", "is", "one", "where", "it", "contains", "an", "increase", "in", "indentation", "can", "t", "quite", "remember", "the", "logic", "here", "let", "s", "go", "with", "that", "clean_indent", "true", "else", "in_indent", "false", "this_indent_balance", "indent_balance", "indent_size", "cls", "_indent_size", "indent_buffer", "tab_space_size", "tab_space_size", "elif", "elem", "is_meta", "and", "elem", "indent_val", "0", "type", "ignore", "indent_balance", "elem", "indent_val", "type", "ignore", "if", "elem", "indent_val", "0", "type", "ignore", "keep", "track", "of", "the", "indent", "at", "the", "last", "indent", "line_indent_stack", "append", "cls", "_indent_size", "line_buffer", "tab_space_size", "tab_space_size", "hanger_pos", "none", "else", "this", "is", "a", "dedent", "we", "could", "still", "have", "a", "hanging", "indent", "but", "only", "if", "there", "s", "enough", "on", "the", "stack", "if", "line_indent_stack", "line_indent_stack", "pop", "elif", "elem", "is_code", "if", "hanger_pos", "is", "none", "hanger_pos", "cls", "_indent_size", "line_buffer", "1", "tab_space_size", "tab_space_size", "if", "we", "hit", "the", "trigger", "element", "stop", "processing", "if", "memory", "and", "elem", "is", "memory", "trigger", "break", "if", "we", "get", "to", "the", "end", "and", "still", "have", "a", "buffer", "add", "it", "on", "if", "line_buffer", "result_buffer", "line_no", "line_no", "line_no", "line_buffer", "line_buffer", "indent_buffer", "indent_buffer", "indent_size", "indent_size", "indent_balance", "this_indent_balance", "hanging_indent", "line_indent_stack", "pop", "if", "line_indent_stack", "else", "none", "clean_indent", "clean_indent", "return", "result_buffer"], "doc_len": 416}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._coerce_indent_to", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_coerce_indent_to", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _coerce_indent_to(\n        self,\n        desired_indent: str,\n        current_indent_buffer: Tuple[RawSegment, ...],\n        current_anchor: BaseSegment,\n    ) -> List[LintFix]:\n        \"\"\"Generate fixes to make an indent a certain size.\"\"\"\n        # If there shouldn't be an indent at all, just delete.\n        if len(desired_indent) == 0:\n            fixes = [LintFix(\"delete\", elem) for elem in current_indent_buffer]\n        # If we don't have any indent and we should, then add a single\n        elif len(\"\".join(elem.raw for elem in current_indent_buffer)) == 0:\n            fixes = [\n                LintFix(\n                    \"create\",\n                    current_anchor,\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        # Otherwise edit the first element to be the right size\n        else:\n            # Edit the first element of this line's indent.\n            fixes = [\n                LintFix(\n                    \"edit\",\n                    current_indent_buffer[0],\n                    WhitespaceSegment(\n                        raw=desired_indent,\n                    ),\n                )\n            ]\n        return fixes\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_coerce_indent_to", "self", "desired_indent", "str", "current_indent_buffer", "tuple", "rawsegment", "current_anchor", "basesegment", "list", "lintfix", "generate", "fixes", "to", "make", "an", "indent", "a", "certain", "size", "if", "there", "shouldn", "t", "be", "an", "indent", "at", "all", "just", "delete", "if", "len", "desired_indent", "0", "fixes", "lintfix", "delete", "elem", "for", "elem", "in", "current_indent_buffer", "if", "we", "don", "t", "have", "any", "indent", "and", "we", "should", "then", "add", "a", "single", "elif", "len", "join", "elem", "raw", "for", "elem", "in", "current_indent_buffer", "0", "fixes", "lintfix", "create", "current_anchor", "whitespacesegment", "raw", "desired_indent", "otherwise", "edit", "the", "first", "element", "to", "be", "the", "right", "size", "else", "edit", "the", "first", "element", "of", "this", "line", "s", "indent", "fixes", "lintfix", "edit", "current_indent_buffer", "0", "whitespacesegment", "raw", "desired_indent", "return", "fixes"], "doc_len": 111}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._strip_buffers", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_strip_buffers", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _strip_buffers(line_dict: dict) -> dict:\n        \"\"\"Strip a line dict of buffers for logging.\"\"\"\n        return {\n            key: line_dict[key]\n            for key in line_dict\n            if key not in (\"line_buffer\", \"indent_buffer\")\n        }\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_strip_buffers", "line_dict", "dict", "dict", "strip", "a", "line", "dict", "of", "buffers", "for", "logging", "return", "key", "line_dict", "key", "for", "key", "in", "line_dict", "if", "key", "not", "in", "line_buffer", "indent_buffer"], "doc_len": 33}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._is_last_segment", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_is_last_segment", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _is_last_segment(\n        cls,\n        segment: BaseSegment,\n        memory: dict,\n        parent_stack: Tuple[BaseSegment, ...],\n        siblings_post: Tuple[BaseSegment, ...],\n    ) -> bool:\n        \"\"\"Returns True if 'segment' is the very last node in the parse tree.\"\"\"\n        if siblings_post:\n            # We have subsequent siblings. Not finished.\n            return False\n        elif parent_stack:\n            # No subsequent siblings. Our parent is finished.\n            memory[\"finished\"].add(parent_stack[-1])\n        if segment.segments:\n            # We have children. Not finished.\n            return False\n\n        # We have no subsequent siblings or children. If all our parents are\n        # finished, the whole parse tree is finished.\n        for parent in parent_stack:\n            if parent not in memory[\"finished\"]:\n                return False\n        return True\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_is_last_segment", "cls", "segment", "basesegment", "memory", "dict", "parent_stack", "tuple", "basesegment", "siblings_post", "tuple", "basesegment", "bool", "returns", "true", "if", "segment", "is", "the", "very", "last", "node", "in", "the", "parse", "tree", "if", "siblings_post", "we", "have", "subsequent", "siblings", "not", "finished", "return", "false", "elif", "parent_stack", "no", "subsequent", "siblings", "our", "parent", "is", "finished", "memory", "finished", "add", "parent_stack", "1", "if", "segment", "segments", "we", "have", "children", "not", "finished", "return", "false", "we", "have", "no", "subsequent", "siblings", "or", "children", "if", "all", "our", "parents", "are", "finished", "the", "whole", "parse", "tree", "is", "finished", "for", "parent", "in", "parent_stack", "if", "parent", "not", "in", "memory", "finished", "return", "false", "return", "true"], "doc_len": 100}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._eval", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Indentation not consistent with previous lines.\n\n        To set the default tab size, set the `tab_space_size` value\n        in the appropriate configuration.\n\n        We compare each line (first non-whitespace element of the\n        line), with the indentation of previous lines. The presence\n        (or lack) of indent or dedent meta-characters indicate whether\n        the indent is appropriate.\n\n        - Any line is assessed by the indent level at the first non\n          whitespace element.\n        - Any increase in indentation may be _up to_ the number of\n          indent characters.\n        - Any line must be in line with the previous line which had\n          the same indent balance at its start.\n        - Apart from \"whole\" indents, a \"hanging\" indent is possible\n          if the line starts in line with either the indent of the\n          previous line or if it starts at the same indent as the *last*\n          indent meta segment in the previous line.\n\n        \"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n\n        raw_stack = context.raw_stack\n\n        # We ignore certain types (e.g. non-SQL scripts in functions)\n        # so check if on ignore list\n        if context.segment.type in self._ignore_types:\n            return LintResult()\n        for parent in context.parent_stack:\n            if parent.type in self._ignore_types:\n                return LintResult()\n\n        # Memory keeps track of what we've seen\n        if not context.memory:\n            memory: dict = {\n                # in_indent keeps track of whether we're in an indent right now\n                \"in_indent\": True,\n                # problem_lines keeps track of lines with problems so that we\n                # don't compare to them.\n                \"problem_lines\": [],\n                # hanging_lines keeps track of hanging lines so that we don't\n                # compare to them when assessing indent.\n                \"hanging_lines\": [],\n                # comment_lines keeps track of lines which are all comment.\n                \"comment_lines\": [],\n                # segments we've seen the last child of\n                \"finished\": set(),\n                # First non-whitespace node on a line.\n                \"trigger\": None,\n            }\n        else:\n            memory = context.memory\n\n        if context.segment.is_type(\"newline\"):\n            memory[\"in_indent\"] = True\n        elif memory[\"in_indent\"]:\n            if context.segment.is_type(\"whitespace\"):\n                # it's whitespace, carry on\n                pass\n            elif context.segment.segments or (context.segment.is_meta and context.segment.indent_val != 0):  # type: ignore\n                # it's not a raw segment or placeholder. Carry on.\n                pass\n            else:\n                memory[\"in_indent\"] = False\n                # we're found a non-whitespace element. This is our trigger,\n                # which we'll handle after this if-statement\n                memory[\"trigger\"] = context.segment\n        else:\n            # Not in indent and not a newline, don't trigger here.\n            pass\n\n        # Is this the last segment? If so, need to \"flush\" any leftovers.\n        is_last = self._is_last_segment(\n            context.segment, memory, context.parent_stack, context.siblings_post\n        )\n\n        if not context.segment.is_type(\"newline\") and not is_last:\n            # We only process complete lines or on the very last segment\n            # (since there may not be a newline on the very last line)..\n            return LintResult(memory=memory)\n\n        if raw_stack and raw_stack[-1] is not context.segment:\n            raw_stack = raw_stack + (context.segment,)\n        res = self._process_raw_stack(\n            raw_stack,\n            memory,\n            tab_space_size=self.tab_space_size,\n            templated_file=context.templated_file,\n        )\n\n        if res:\n            # Saw a newline or end of parse tree. Is the current line empty?\n            trigger_segment = memory[\"trigger\"]\n            if trigger_segment:\n                # Not empty. Process it.\n                result = self._process_current_line(res, memory)\n                if context.segment.is_type(\"newline\"):\n                    memory[\"trigger\"] = None\n                return result\n        return LintResult(memory=memory)\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "indentation", "not", "consistent", "with", "previous", "lines", "to", "set", "the", "default", "tab", "size", "set", "the", "tab_space_size", "value", "in", "the", "appropriate", "configuration", "we", "compare", "each", "line", "first", "non", "whitespace", "element", "of", "the", "line", "with", "the", "indentation", "of", "previous", "lines", "the", "presence", "or", "lack", "of", "indent", "or", "dedent", "meta", "characters", "indicate", "whether", "the", "indent", "is", "appropriate", "any", "line", "is", "assessed", "by", "the", "indent", "level", "at", "the", "first", "non", "whitespace", "element", "any", "increase", "in", "indentation", "may", "be", "_up", "to_", "the", "number", "of", "indent", "characters", "any", "line", "must", "be", "in", "line", "with", "the", "previous", "line", "which", "had", "the", "same", "indent", "balance", "at", "its", "start", "apart", "from", "whole", "indents", "a", "hanging", "indent", "is", "possible", "if", "the", "line", "starts", "in", "line", "with", "either", "the", "indent", "of", "the", "previous", "line", "or", "if", "it", "starts", "at", "the", "same", "indent", "as", "the", "last", "indent", "meta", "segment", "in", "the", "previous", "line", "config", "type", "hints", "self", "tab_space_size", "int", "self", "indent_unit", "str", "raw_stack", "context", "raw_stack", "we", "ignore", "certain", "types", "e", "g", "non", "sql", "scripts", "in", "functions", "so", "check", "if", "on", "ignore", "list", "if", "context", "segment", "type", "in", "self", "_ignore_types", "return", "lintresult", "for", "parent", "in", "context", "parent_stack", "if", "parent", "type", "in", "self", "_ignore_types", "return", "lintresult", "memory", "keeps", "track", "of", "what", "we", "ve", "seen", "if", "not", "context", "memory", "memory", "dict", "in_indent", "keeps", "track", "of", "whether", "we", "re", "in", "an", "indent", "right", "now", "in_indent", "true", "problem_lines", "keeps", "track", "of", "lines", "with", "problems", "so", "that", "we", "don", "t", "compare", "to", "them", "problem_lines", "hanging_lines", "keeps", "track", "of", "hanging", "lines", "so", "that", "we", "don", "t", "compare", "to", "them", "when", "assessing", "indent", "hanging_lines", "comment_lines", "keeps", "track", "of", "lines", "which", "are", "all", "comment", "comment_lines", "segments", "we", "ve", "seen", "the", "last", "child", "of", "finished", "set", "first", "non", "whitespace", "node", "on", "a", "line", "trigger", "none", "else", "memory", "context", "memory", "if", "context", "segment", "is_type", "newline", "memory", "in_indent", "true", "elif", "memory", "in_indent", "if", "context", "segment", "is_type", "whitespace", "it", "s", "whitespace", "carry", "on", "pass", "elif", "context", "segment", "segments", "or", "context", "segment", "is_meta", "and", "context", "segment", "indent_val", "0", "type", "ignore", "it", "s", "not", "a", "raw", "segment", "or", "placeholder", "carry", "on", "pass", "else", "memory", "in_indent", "false", "we", "re", "found", "a", "non", "whitespace", "element", "this", "is", "our", "trigger", "which", "we", "ll", "handle", "after", "this", "if", "statement", "memory", "trigger", "context", "segment", "else", "not", "in", "indent", "and", "not", "a", "newline", "don", "t", "trigger", "here", "pass", "is", "this", "the", "last", "segment", "if", "so", "need", "to", "flush", "any", "leftovers", "is_last", "self", "_is_last_segment", "context", "segment", "memory", "context", "parent_stack", "context", "siblings_post", "if", "not", "context", "segment", "is_type", "newline", "and", "not", "is_last", "we", "only", "process", "complete", "lines", "or", "on", "the", "very", "last", "segment", "since", "there", "may", "not", "be", "a", "newline", "on", "the", "very", "last", "line", "return", "lintresult", "memory", "memory", "if", "raw_stack", "and", "raw_stack", "1", "is", "not", "context", "segment", "raw_stack", "raw_stack", "context", "segment", "res", "self", "_process_raw_stack", "raw_stack", "memory", "tab_space_size", "self", "tab_space_size", "templated_file", "context", "templated_file", "if", "res", "saw", "a", "newline", "or", "end", "of", "parse", "tree", "is", "the", "current", "line", "empty", "trigger_segment", "memory", "trigger", "if", "trigger_segment", "not", "empty", "process", "it", "result", "self", "_process_current_line", "res", "memory", "if", "context", "segment", "is_type", "newline", "memory", "trigger", "none", "return", "result", "return", "lintresult", "memory", "memory"], "doc_len": 512}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._process_current_line", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_process_current_line", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _process_current_line(self, res: dict, memory: dict) -> LintResult:\n        \"\"\"Checks indentation of one line of code, returning a LintResult.\n\n        The _eval() function calls it for the current line of code:\n        - When passed a newline segment (thus ending a line)\n        - When passed the *final* segment in the entire parse tree (which may\n          not be a newline)\n        \"\"\"\n        this_line_no = max(res.keys())\n        this_line = res.pop(this_line_no)\n        self.logger.debug(\n            \"Evaluating line #%s. %s\",\n            this_line_no,\n            # Don't log the line or indent buffer, it's too noisy.\n            self._strip_buffers(this_line),\n        )\n        trigger_segment = memory[\"trigger\"]\n\n        # Is this line just comments? (Disregard trailing newline if present.)\n        check_comment_line = this_line[\"line_buffer\"]\n        if check_comment_line and all(\n            seg.is_type(\n                \"whitespace\", \"comment\", \"indent\"  # dedent is a subtype of indent\n            )\n            for seg in check_comment_line\n        ):\n            # Comment line, deal with it later.\n            memory[\"comment_lines\"].append(this_line_no)\n            self.logger.debug(\"    Comment Line. #%s\", this_line_no)\n            return LintResult(memory=memory)\n\n        # Is it a hanging indent?\n        # Find last meaningful line indent.\n        last_code_line = None\n        for k in sorted(res.keys(), reverse=True):\n            if any(seg.is_code for seg in res[k][\"line_buffer\"]):\n                last_code_line = k\n                break\n\n        if len(res) > 0 and last_code_line:\n            last_line_hanger_indent = res[last_code_line][\"hanging_indent\"]\n            # Let's just deal with hanging indents here.\n            if (\n                # NB: Hangers are only allowed if there was content after the last\n                # indent on the previous line. Otherwise it's just an indent.\n                this_line[\"indent_size\"] == last_line_hanger_indent\n                # Or they're if the indent balance is the same and the indent is the\n                # same AND the previous line was a hanger\n                or (\n                    this_line[\"indent_size\"] == res[last_code_line][\"indent_size\"]\n                    and this_line[\"indent_balance\"]\n                    == res[last_code_line][\"indent_balance\"]\n                    and last_code_line in memory[\"hanging_lines\"]\n                )\n            ) and (\n                # There MUST also be a non-zero indent. Otherwise we're just on the baseline.\n                this_line[\"indent_size\"]\n                > 0\n            ):\n                # This is a HANGER\n                memory[\"hanging_lines\"].append(this_line_no)\n                self.logger.debug(\"    Hanger Line. #%s\", this_line_no)\n                self.logger.debug(\n                    \"    Last Line: %s\", self._strip_buffers(res[last_code_line])\n                )\n                return LintResult(memory=memory)\n\n        # Is this an indented first line?\n        elif len(res) == 0:\n            if this_line[\"indent_size\"] > 0:\n                self.logger.debug(\"    Indented First Line. #%s\", this_line_no)\n                return LintResult(\n                    anchor=trigger_segment,\n                    memory=memory,\n                    description=\"First line has unexpected indent\",\n                    fixes=[\n                        LintFix(\"delete\", elem) for elem in this_line[\"indent_buffer\"]\n                    ],\n                )\n\n        # Assuming it's not a hanger, let's compare it to the other previous\n        # lines. We do it in reverse so that closer lines are more relevant.\n        for k in sorted(res.keys(), reverse=True):\n\n            # Is this a problem line?\n            if k in memory[\"problem_lines\"] + memory[\"hanging_lines\"]:\n                # Skip it if it is\n                continue\n\n            # Is this an empty line?\n            if not any(elem.is_code for elem in res[k][\"line_buffer\"]):\n                # Skip if it is\n                continue\n\n            # Work out the difference in indent\n            indent_diff = this_line[\"indent_balance\"] - res[k][\"indent_balance\"]\n            # If we're comparing to a previous, more deeply indented line, then skip and keep looking.\n            if indent_diff < 0:\n                continue\n            # Is the indent balance the same?\n            elif indent_diff == 0:\n                self.logger.debug(\"    [same indent balance] Comparing to #%s\", k)\n                if this_line[\"indent_size\"] != res[k][\"indent_size\"]:\n                    # Indents don't match even though balance is the same...\n                    memory[\"problem_lines\"].append(this_line_no)\n\n                    # Work out desired indent\n                    if res[k][\"indent_size\"] == 0:\n                        desired_indent = \"\"\n                    elif this_line[\"indent_size\"] == 0:\n                        desired_indent = self._make_indent(\n                            indent_unit=self.indent_unit,\n                            tab_space_size=self.tab_space_size,\n                        )\n                    else:\n                        # The previous indent.\n                        desired_indent = \"\".join(\n                            elem.raw for elem in res[k][\"indent_buffer\"]\n                        )\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n                    self.logger.debug(\n                        \"    !! Indentation does not match #%s. Fixes: %s\", k, fixes\n                    )\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Indentation not consistent with line #{}\".format(\n                            k\n                        ),\n                        # See above for logic\n                        fixes=fixes,\n                    )\n            # Are we at a deeper indent?\n            elif indent_diff > 0:\n                self.logger.debug(\"    [deeper indent balance] Comparing to #%s\", k)\n                # NB: We shouldn't need to deal with correct hanging indents\n                # here, they should already have been dealt with before. We\n                # may still need to deal with *creating* hanging indents if\n                # appropriate.\n                self.logger.debug(\n                    \"    Comparison Line: %s\", self._strip_buffers(res[k])\n                )\n\n                # Check to see if we've got a whole number of multiples. If\n                # we do then record the number for later, otherwise raise\n                # an error. We do the comparison here so we have a reference\n                # point to do the repairs. We need a sensible previous line\n                # to base the repairs off. If there's no indent at all, then\n                # we should also take this route because there SHOULD be one.\n                if this_line[\"indent_size\"] % self.tab_space_size != 0:\n                    memory[\"problem_lines\"].append(this_line_no)\n\n                    # The default indent is the one just reconstructs it from\n                    # the indent size.\n                    default_indent = \"\".join(\n                        elem.raw for elem in res[k][\"indent_buffer\"]\n                    ) + self._make_indent(\n                        indent_unit=self.indent_unit,\n                        tab_space_size=self.tab_space_size,\n                        num=indent_diff,\n                    )\n                    # If we have a clean indent, we can just add steps in line\n                    # with the difference in the indent buffers. simples.\n                    if this_line[\"clean_indent\"]:\n                        self.logger.debug(\"        Use clean indent.\")\n                        desired_indent = default_indent\n                    # If we have the option of a hanging indent then use it.\n                    elif res[k][\"hanging_indent\"]:\n                        self.logger.debug(\"        Use hanging indent.\")\n                        desired_indent = \" \" * res[k][\"hanging_indent\"]\n                    else:  # pragma: no cover\n                        self.logger.debug(\"        Use default indent.\")\n                        desired_indent = default_indent\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=(\n                            \"Indentation not hanging or a multiple of {} spaces\"\n                        ).format(self.tab_space_size),\n                        fixes=fixes,\n                    )\n                else:\n                    # We'll need this value later.\n                    this_indent_num = this_line[\"indent_size\"] // self.tab_space_size\n\n                # We know that the indent balance is higher, what actually is\n                # the difference in indent counts? It should be a whole number\n                # if we're still here.\n                comp_indent_num = res[k][\"indent_size\"] // self.tab_space_size\n\n                # The indent number should be at least 1, and can be UP TO\n                # and including the difference in the indent balance.\n                if comp_indent_num == this_indent_num:\n                    # We have two lines indented the same, but with a different starting\n                    # indent balance. This is either a problem OR a sign that one of the\n                    # opening indents wasn't used. We account for the latter and then\n                    # have a violation if that wasn't the case.\n\n                    # Does the comparison line have enough unused indent to get us back\n                    # to where we need to be? NB: This should only be applied if this is\n                    # a CLOSING bracket.\n\n                    # First work out if we have some closing brackets, and if so, how many.\n                    b_idx = 0\n                    b_num = 0\n                    while True:\n                        if len(this_line[\"line_buffer\"][b_idx:]) == 0:\n                            break\n\n                        elem = this_line[\"line_buffer\"][b_idx]\n                        if not elem.is_code:\n                            b_idx += 1\n                            continue\n                        else:\n                            if elem.is_type(\"end_bracket\", \"end_square_bracket\"):\n                                b_idx += 1\n                                b_num += 1\n                                continue\n                            break  # pragma: no cover\n\n                    if b_num >= indent_diff:\n                        # It does. This line is fine.\n                        pass\n                    else:\n                        # It doesn't. That means we *should* have an indent when compared to\n                        # this line and we DON'T.\n                        memory[\"problem_lines\"].append(this_line_no)\n                        return LintResult(\n                            anchor=trigger_segment,\n                            memory=memory,\n                            description=\"Indent expected and not found compared to line #{}\".format(\n                                k\n                            ),\n                            # Add in an extra bit of whitespace for the indent\n                            fixes=[\n                                LintFix(\n                                    \"create\",\n                                    trigger_segment,\n                                    WhitespaceSegment(\n                                        raw=self._make_indent(\n                                            indent_unit=self.indent_unit,\n                                            tab_space_size=self.tab_space_size,\n                                        ),\n                                    ),\n                                )\n                            ],\n                        )\n                elif this_indent_num < comp_indent_num:\n                    memory[\"problem_lines\"].append(this_line_no)\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Line under-indented compared to line #{}\".format(\n                            k\n                        ),\n                        fixes=[\n                            LintFix(\n                                \"create\",\n                                trigger_segment,\n                                WhitespaceSegment(\n                                    # Make the minimum indent for it to be ok.\n                                    raw=self._make_indent(\n                                        num=comp_indent_num - this_indent_num,\n                                        indent_unit=self.indent_unit,\n                                        tab_space_size=self.tab_space_size,\n                                    ),\n                                ),\n                            )\n                        ],\n                    )\n                elif this_indent_num > comp_indent_num + indent_diff:\n                    # Calculate the lowest ok indent:\n                    desired_indent = self._make_indent(\n                        num=comp_indent_num - this_indent_num,\n                        indent_unit=self.indent_unit,\n                        tab_space_size=self.tab_space_size,\n                    )\n\n                    # Make fixes\n                    fixes = self._coerce_indent_to(\n                        desired_indent=desired_indent,\n                        current_indent_buffer=this_line[\"indent_buffer\"],\n                        current_anchor=trigger_segment,\n                    )\n\n                    memory[\"problem_lines\"].append(this_line_no)\n                    return LintResult(\n                        anchor=trigger_segment,\n                        memory=memory,\n                        description=\"Line over-indented compared to line #{}\".format(k),\n                        fixes=fixes,\n                    )\n\n            # This was a valid comparison, so if it doesn't flag then\n            # we can assume that we're ok.\n            self.logger.debug(\"    Indent deemed ok comparing to #%s\", k)\n\n            # Given that this line is ok, consider if the preceding lines are\n            # comments. If they are, lint the indentation of the comment(s).\n            fixes = []\n            for n in range(this_line_no - 1, -1, -1):\n                if n in memory[\"comment_lines\"]:\n                    # The previous line WAS a comment.\n                    prev_line = res[n]\n                    if this_line[\"indent_size\"] != prev_line[\"indent_size\"]:\n                        # It's not aligned.\n                        # Find the anchor first.\n                        anchor: BaseSegment = None  # type: ignore\n                        for seg in prev_line[\"line_buffer\"]:\n                            if seg.is_type(\"comment\"):\n                                anchor = seg\n                                break\n                        # Make fixes.\n                        fixes += self._coerce_indent_to(\n                            desired_indent=\"\".join(\n                                elem.raw for elem in this_line[\"indent_buffer\"]\n                            ),\n                            current_indent_buffer=prev_line[\"indent_buffer\"],\n                            current_anchor=anchor,\n                        )\n\n                        memory[\"problem_lines\"].append(n)\n                else:\n                    break\n\n            if fixes:\n                return LintResult(\n                    anchor=anchor,\n                    memory=memory,\n                    description=\"Comment not aligned with following line.\",\n                    fixes=fixes,\n                )\n\n            # Otherwise all good.\n            return LintResult(memory=memory)\n\n            # NB: At shallower indents, we don't check, we just check the\n            # previous lines with the same balance. Deeper indents can check\n            # themselves.\n\n        # If we get to here, then we're all good for now.\n        return LintResult(memory=memory)\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_process_current_line", "self", "res", "dict", "memory", "dict", "lintresult", "checks", "indentation", "of", "one", "line", "of", "code", "returning", "a", "lintresult", "the", "_eval", "function", "calls", "it", "for", "the", "current", "line", "of", "code", "when", "passed", "a", "newline", "segment", "thus", "ending", "a", "line", "when", "passed", "the", "final", "segment", "in", "the", "entire", "parse", "tree", "which", "may", "not", "be", "a", "newline", "this_line_no", "max", "res", "keys", "this_line", "res", "pop", "this_line_no", "self", "logger", "debug", "evaluating", "line", "s", "s", "this_line_no", "don", "t", "log", "the", "line", "or", "indent", "buffer", "it", "s", "too", "noisy", "self", "_strip_buffers", "this_line", "trigger_segment", "memory", "trigger", "is", "this", "line", "just", "comments", "disregard", "trailing", "newline", "if", "present", "check_comment_line", "this_line", "line_buffer", "if", "check_comment_line", "and", "all", "seg", "is_type", "whitespace", "comment", "indent", "dedent", "is", "a", "subtype", "of", "indent", "for", "seg", "in", "check_comment_line", "comment", "line", "deal", "with", "it", "later", "memory", "comment_lines", "append", "this_line_no", "self", "logger", "debug", "comment", "line", "s", "this_line_no", "return", "lintresult", "memory", "memory", "is", "it", "a", "hanging", "indent", "find", "last", "meaningful", "line", "indent", "last_code_line", "none", "for", "k", "in", "sorted", "res", "keys", "reverse", "true", "if", "any", "seg", "is_code", "for", "seg", "in", "res", "k", "line_buffer", "last_code_line", "k", "break", "if", "len", "res", "0", "and", "last_code_line", "last_line_hanger_indent", "res", "last_code_line", "hanging_indent", "let", "s", "just", "deal", "with", "hanging", "indents", "here", "if", "nb", "hangers", "are", "only", "allowed", "if", "there", "was", "content", "after", "the", "last", "indent", "on", "the", "previous", "line", "otherwise", "it", "s", "just", "an", "indent", "this_line", "indent_size", "last_line_hanger_indent", "or", "they", "re", "if", "the", "indent", "balance", "is", "the", "same", "and", "the", "indent", "is", "the", "same", "and", "the", "previous", "line", "was", "a", "hanger", "or", "this_line", "indent_size", "res", "last_code_line", "indent_size", "and", "this_line", "indent_balance", "res", "last_code_line", "indent_balance", "and", "last_code_line", "in", "memory", "hanging_lines", "and", "there", "must", "also", "be", "a", "non", "zero", "indent", "otherwise", "we", "re", "just", "on", "the", "baseline", "this_line", "indent_size", "0", "this", "is", "a", "hanger", "memory", "hanging_lines", "append", "this_line_no", "self", "logger", "debug", "hanger", "line", "s", "this_line_no", "self", "logger", "debug", "last", "line", "s", "self", "_strip_buffers", "res", "last_code_line", "return", "lintresult", "memory", "memory", "is", "this", "an", "indented", "first", "line", "elif", "len", "res", "0", "if", "this_line", "indent_size", "0", "self", "logger", "debug", "indented", "first", "line", "s", "this_line_no", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "first", "line", "has", "unexpected", "indent", "fixes", "lintfix", "delete", "elem", "for", "elem", "in", "this_line", "indent_buffer", "assuming", "it", "s", "not", "a", "hanger", "let", "s", "compare", "it", "to", "the", "other", "previous", "lines", "we", "do", "it", "in", "reverse", "so", "that", "closer", "lines", "are", "more", "relevant", "for", "k", "in", "sorted", "res", "keys", "reverse", "true", "is", "this", "a", "problem", "line", "if", "k", "in", "memory", "problem_lines", "memory", "hanging_lines", "skip", "it", "if", "it", "is", "continue", "is", "this", "an", "empty", "line", "if", "not", "any", "elem", "is_code", "for", "elem", "in", "res", "k", "line_buffer", "skip", "if", "it", "is", "continue", "work", "out", "the", "difference", "in", "indent", "indent_diff", "this_line", "indent_balance", "res", "k", "indent_balance", "if", "we", "re", "comparing", "to", "a", "previous", "more", "deeply", "indented", "line", "then", "skip", "and", "keep", "looking", "if", "indent_diff", "0", "continue", "is", "the", "indent", "balance", "the", "same", "elif", "indent_diff", "0", "self", "logger", "debug", "same", "indent", "balance", "comparing", "to", "s", "k", "if", "this_line", "indent_size", "res", "k", "indent_size", "indents", "don", "t", "match", "even", "though", "balance", "is", "the", "same", "memory", "problem_lines", "append", "this_line_no", "work", "out", "desired", "indent", "if", "res", "k", "indent_size", "0", "desired_indent", "elif", "this_line", "indent_size", "0", "desired_indent", "self", "_make_indent", "indent_unit", "self", "indent_unit", "tab_space_size", "self", "tab_space_size", "else", "the", "previous", "indent", "desired_indent", "join", "elem", "raw", "for", "elem", "in", "res", "k", "indent_buffer", "make", "fixes", "fixes", "self", "_coerce_indent_to", "desired_indent", "desired_indent", "current_indent_buffer", "this_line", "indent_buffer", "current_anchor", "trigger_segment", "self", "logger", "debug", "indentation", "does", "not", "match", "s", "fixes", "s", "k", "fixes", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "indentation", "not", "consistent", "with", "line", "format", "k", "see", "above", "for", "logic", "fixes", "fixes", "are", "we", "at", "a", "deeper", "indent", "elif", "indent_diff", "0", "self", "logger", "debug", "deeper", "indent", "balance", "comparing", "to", "s", "k", "nb", "we", "shouldn", "t", "need", "to", "deal", "with", "correct", "hanging", "indents", "here", "they", "should", "already", "have", "been", "dealt", "with", "before", "we", "may", "still", "need", "to", "deal", "with", "creating", "hanging", "indents", "if", "appropriate", "self", "logger", "debug", "comparison", "line", "s", "self", "_strip_buffers", "res", "k", "check", "to", "see", "if", "we", "ve", "got", "a", "whole", "number", "of", "multiples", "if", "we", "do", "then", "record", "the", "number", "for", "later", "otherwise", "raise", "an", "error", "we", "do", "the", "comparison", "here", "so", "we", "have", "a", "reference", "point", "to", "do", "the", "repairs", "we", "need", "a", "sensible", "previous", "line", "to", "base", "the", "repairs", "off", "if", "there", "s", "no", "indent", "at", "all", "then", "we", "should", "also", "take", "this", "route", "because", "there", "should", "be", "one", "if", "this_line", "indent_size", "self", "tab_space_size", "0", "memory", "problem_lines", "append", "this_line_no", "the", "default", "indent", "is", "the", "one", "just", "reconstructs", "it", "from", "the", "indent", "size", "default_indent", "join", "elem", "raw", "for", "elem", "in", "res", "k", "indent_buffer", "self", "_make_indent", "indent_unit", "self", "indent_unit", "tab_space_size", "self", "tab_space_size", "num", "indent_diff", "if", "we", "have", "a", "clean", "indent", "we", "can", "just", "add", "steps", "in", "line", "with", "the", "difference", "in", "the", "indent", "buffers", "simples", "if", "this_line", "clean_indent", "self", "logger", "debug", "use", "clean", "indent", "desired_indent", "default_indent", "if", "we", "have", "the", "option", "of", "a", "hanging", "indent", "then", "use", "it", "elif", "res", "k", "hanging_indent", "self", "logger", "debug", "use", "hanging", "indent", "desired_indent", "res", "k", "hanging_indent", "else", "pragma", "no", "cover", "self", "logger", "debug", "use", "default", "indent", "desired_indent", "default_indent", "make", "fixes", "fixes", "self", "_coerce_indent_to", "desired_indent", "desired_indent", "current_indent_buffer", "this_line", "indent_buffer", "current_anchor", "trigger_segment", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "indentation", "not", "hanging", "or", "a", "multiple", "of", "spaces", "format", "self", "tab_space_size", "fixes", "fixes", "else", "we", "ll", "need", "this", "value", "later", "this_indent_num", "this_line", "indent_size", "self", "tab_space_size", "we", "know", "that", "the", "indent", "balance", "is", "higher", "what", "actually", "is", "the", "difference", "in", "indent", "counts", "it", "should", "be", "a", "whole", "number", "if", "we", "re", "still", "here", "comp_indent_num", "res", "k", "indent_size", "self", "tab_space_size", "the", "indent", "number", "should", "be", "at", "least", "1", "and", "can", "be", "up", "to", "and", "including", "the", "difference", "in", "the", "indent", "balance", "if", "comp_indent_num", "this_indent_num", "we", "have", "two", "lines", "indented", "the", "same", "but", "with", "a", "different", "starting", "indent", "balance", "this", "is", "either", "a", "problem", "or", "a", "sign", "that", "one", "of", "the", "opening", "indents", "wasn", "t", "used", "we", "account", "for", "the", "latter", "and", "then", "have", "a", "violation", "if", "that", "wasn", "t", "the", "case", "does", "the", "comparison", "line", "have", "enough", "unused", "indent", "to", "get", "us", "back", "to", "where", "we", "need", "to", "be", "nb", "this", "should", "only", "be", "applied", "if", "this", "is", "a", "closing", "bracket", "first", "work", "out", "if", "we", "have", "some", "closing", "brackets", "and", "if", "so", "how", "many", "b_idx", "0", "b_num", "0", "while", "true", "if", "len", "this_line", "line_buffer", "b_idx", "0", "break", "elem", "this_line", "line_buffer", "b_idx", "if", "not", "elem", "is_code", "b_idx", "1", "continue", "else", "if", "elem", "is_type", "end_bracket", "end_square_bracket", "b_idx", "1", "b_num", "1", "continue", "break", "pragma", "no", "cover", "if", "b_num", "indent_diff", "it", "does", "this", "line", "is", "fine", "pass", "else", "it", "doesn", "t", "that", "means", "we", "should", "have", "an", "indent", "when", "compared", "to", "this", "line", "and", "we", "don", "t", "memory", "problem_lines", "append", "this_line_no", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "indent", "expected", "and", "not", "found", "compared", "to", "line", "format", "k", "add", "in", "an", "extra", "bit", "of", "whitespace", "for", "the", "indent", "fixes", "lintfix", "create", "trigger_segment", "whitespacesegment", "raw", "self", "_make_indent", "indent_unit", "self", "indent_unit", "tab_space_size", "self", "tab_space_size", "elif", "this_indent_num", "comp_indent_num", "memory", "problem_lines", "append", "this_line_no", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "line", "under", "indented", "compared", "to", "line", "format", "k", "fixes", "lintfix", "create", "trigger_segment", "whitespacesegment", "make", "the", "minimum", "indent", "for", "it", "to", "be", "ok", "raw", "self", "_make_indent", "num", "comp_indent_num", "this_indent_num", "indent_unit", "self", "indent_unit", "tab_space_size", "self", "tab_space_size", "elif", "this_indent_num", "comp_indent_num", "indent_diff", "calculate", "the", "lowest", "ok", "indent", "desired_indent", "self", "_make_indent", "num", "comp_indent_num", "this_indent_num", "indent_unit", "self", "indent_unit", "tab_space_size", "self", "tab_space_size", "make", "fixes", "fixes", "self", "_coerce_indent_to", "desired_indent", "desired_indent", "current_indent_buffer", "this_line", "indent_buffer", "current_anchor", "trigger_segment", "memory", "problem_lines", "append", "this_line_no", "return", "lintresult", "anchor", "trigger_segment", "memory", "memory", "description", "line", "over", "indented", "compared", "to", "line", "format", "k", "fixes", "fixes", "this", "was", "a", "valid", "comparison", "so", "if", "it", "doesn", "t", "flag", "then", "we", "can", "assume", "that", "we", "re", "ok", "self", "logger", "debug", "indent", "deemed", "ok", "comparing", "to", "s", "k", "given", "that", "this", "line", "is", "ok", "consider", "if", "the", "preceding", "lines", "are", "comments", "if", "they", "are", "lint", "the", "indentation", "of", "the", "comment", "s", "fixes", "for", "n", "in", "range", "this_line_no", "1", "1", "1", "if", "n", "in", "memory", "comment_lines", "the", "previous", "line", "was", "a", "comment", "prev_line", "res", "n", "if", "this_line", "indent_size", "prev_line", "indent_size", "it", "s", "not", "aligned", "find", "the", "anchor", "first", "anchor", "basesegment", "none", "type", "ignore", "for", "seg", "in", "prev_line", "line_buffer", "if", "seg", "is_type", "comment", "anchor", "seg", "break", "make", "fixes", "fixes", "self", "_coerce_indent_to", "desired_indent", "join", "elem", "raw", "for", "elem", "in", "this_line", "indent_buffer", "current_indent_buffer", "prev_line", "indent_buffer", "current_anchor", "anchor", "memory", "problem_lines", "append", "n", "else", "break", "if", "fixes", "return", "lintresult", "anchor", "anchor", "memory", "memory", "description", "comment", "not", "aligned", "with", "following", "line", "fixes", "fixes", "otherwise", "all", "good", "return", "lintresult", "memory", "memory", "nb", "at", "shallower", "indents", "we", "don", "t", "check", "we", "just", "check", "the", "previous", "lines", "with", "the", "same", "balance", "deeper", "indents", "can", "check", "themselves", "if", "we", "get", "to", "here", "then", "we", "re", "all", "good", "for", "now", "return", "lintresult", "memory", "memory"], "doc_len": 1427}
{"doc_id": "src/sqlfluff/rules/L003.py::Rule_L003._get_element_template_info", "file_path": "src/sqlfluff/rules/L003.py", "class_name": "Rule_L003", "func_name": "_get_element_template_info", "text": "文件路径: src/sqlfluff/rules/L003.py, 类名: Rule_L003\n    def _get_element_template_info(\n        cls, elem: BaseSegment, templated_file: Optional[TemplatedFile]\n    ) -> Optional[str]:\n        if elem.is_type(\"placeholder\"):\n            if templated_file is None:\n                raise ValueError(\"Parameter templated_file cannot be: None.\")\n            slices = templated_file.raw_slices_spanning_source_slice(\n                elem.pos_marker.source_slice\n            )\n            if slices:\n                return slices[0].slice_type\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l003", "py", "rule_l003", "def", "_get_element_template_info", "cls", "elem", "basesegment", "templated_file", "optional", "templatedfile", "optional", "str", "if", "elem", "is_type", "placeholder", "if", "templated_file", "is", "none", "raise", "valueerror", "parameter", "templated_file", "cannot", "be", "none", "slices", "templated_file", "raw_slices_spanning_source_slice", "elem", "pos_marker", "source_slice", "if", "slices", "return", "slices", "0", "slice_type", "return", "none"], "doc_len": 45}
{"doc_id": "src/sqlfluff/rules/L004.py::Rule_L004._eval", "file_path": "src/sqlfluff/rules/L004.py", "class_name": "Rule_L004", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L004.py, 类名: Rule_L004\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Incorrect indentation found in file.\"\"\"\n        # Config type hints\n        self.tab_space_size: int\n        self.indent_unit: str\n\n        tab = \"\\t\"\n        space = \" \"\n        correct_indent = (\n            space * self.tab_space_size if self.indent_unit == \"space\" else tab\n        )\n        wrong_indent = (\n            tab if self.indent_unit == \"space\" else space * self.tab_space_size\n        )\n        if (\n            context.segment.is_type(\"whitespace\")\n            and wrong_indent in context.segment.raw\n        ):\n            fixes = []\n            description = \"Incorrect indentation type found in file.\"\n            edit_indent = context.segment.raw.replace(wrong_indent, correct_indent)\n            # Ensure that the number of space indents is a multiple of tab_space_size\n            # before attempting to convert spaces to tabs to avoid mixed indents\n            # unless we are converted tabs to spaces (indent_unit = space)\n            if (\n                (\n                    self.indent_unit == \"space\"\n                    or context.segment.raw.count(space) % self.tab_space_size == 0\n                )\n                # Only attempt a fix at the start of a newline for now\n                and (\n                    len(context.raw_stack) == 0\n                    or context.raw_stack[-1].is_type(\"newline\")\n                )\n            ):\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        context.segment,\n                        WhitespaceSegment(raw=edit_indent),\n                    )\n                ]\n            elif not (\n                len(context.raw_stack) == 0 or context.raw_stack[-1].is_type(\"newline\")\n            ):\n                # give a helpful message if the wrong indent has been found and is not at the start of a newline\n                description += (\n                    \" The indent occurs after other text, so a manual fix is needed.\"\n                )\n            else:\n                # If we get here, the indent_unit is tabs, and the number of spaces is not a multiple of tab_space_size\n                description += \" The number of spaces is not a multiple of tab_space_size, so a manual fix is needed.\"\n            return LintResult(\n                anchor=context.segment, fixes=fixes, description=description\n            )\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l004", "py", "rule_l004", "def", "_eval", "self", "context", "rulecontext", "lintresult", "incorrect", "indentation", "found", "in", "file", "config", "type", "hints", "self", "tab_space_size", "int", "self", "indent_unit", "str", "tab", "t", "space", "correct_indent", "space", "self", "tab_space_size", "if", "self", "indent_unit", "space", "else", "tab", "wrong_indent", "tab", "if", "self", "indent_unit", "space", "else", "space", "self", "tab_space_size", "if", "context", "segment", "is_type", "whitespace", "and", "wrong_indent", "in", "context", "segment", "raw", "fixes", "description", "incorrect", "indentation", "type", "found", "in", "file", "edit_indent", "context", "segment", "raw", "replace", "wrong_indent", "correct_indent", "ensure", "that", "the", "number", "of", "space", "indents", "is", "a", "multiple", "of", "tab_space_size", "before", "attempting", "to", "convert", "spaces", "to", "tabs", "to", "avoid", "mixed", "indents", "unless", "we", "are", "converted", "tabs", "to", "spaces", "indent_unit", "space", "if", "self", "indent_unit", "space", "or", "context", "segment", "raw", "count", "space", "self", "tab_space_size", "0", "only", "attempt", "a", "fix", "at", "the", "start", "of", "a", "newline", "for", "now", "and", "len", "context", "raw_stack", "0", "or", "context", "raw_stack", "1", "is_type", "newline", "fixes", "lintfix", "edit", "context", "segment", "whitespacesegment", "raw", "edit_indent", "elif", "not", "len", "context", "raw_stack", "0", "or", "context", "raw_stack", "1", "is_type", "newline", "give", "a", "helpful", "message", "if", "the", "wrong", "indent", "has", "been", "found", "and", "is", "not", "at", "the", "start", "of", "a", "newline", "description", "the", "indent", "occurs", "after", "other", "text", "so", "a", "manual", "fix", "is", "needed", "else", "if", "we", "get", "here", "the", "indent_unit", "is", "tabs", "and", "the", "number", "of", "spaces", "is", "not", "a", "multiple", "of", "tab_space_size", "description", "the", "number", "of", "spaces", "is", "not", "a", "multiple", "of", "tab_space_size", "so", "a", "manual", "fix", "is", "needed", "return", "lintresult", "anchor", "context", "segment", "fixes", "fixes", "description", "description", "return", "lintresult"], "doc_len": 244}
{"doc_id": "src/sqlfluff/rules/L005.py::Rule_L005._eval", "file_path": "src/sqlfluff/rules/L005.py", "class_name": "Rule_L005", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L005.py, 类名: Rule_L005\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Commas should not have whitespace directly before them.\n\n        We need at least one segment behind us for this to work.\n\n        \"\"\"\n        if len(context.raw_stack) >= 1:\n            cm1 = context.raw_stack[-1]\n            if (\n                context.segment.is_type(\"comma\")\n                and cm1.is_type(\"whitespace\")\n                and cm1.pos_marker.line_pos > 1\n            ):\n                anchor = cm1\n                return LintResult(anchor=anchor, fixes=[LintFix(\"delete\", cm1)])\n        # Otherwise fine\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l005", "py", "rule_l005", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "commas", "should", "not", "have", "whitespace", "directly", "before", "them", "we", "need", "at", "least", "one", "segment", "behind", "us", "for", "this", "to", "work", "if", "len", "context", "raw_stack", "1", "cm1", "context", "raw_stack", "1", "if", "context", "segment", "is_type", "comma", "and", "cm1", "is_type", "whitespace", "and", "cm1", "pos_marker", "line_pos", "1", "anchor", "cm1", "return", "lintresult", "anchor", "anchor", "fixes", "lintfix", "delete", "cm1", "otherwise", "fine", "return", "none"], "doc_len": 70}
{"doc_id": "src/sqlfluff/rules/L006.py::Rule_L006._missing_whitespace", "file_path": "src/sqlfluff/rules/L006.py", "class_name": "Rule_L006", "func_name": "_missing_whitespace", "text": "文件路径: src/sqlfluff/rules/L006.py, 类名: Rule_L006\n    def _missing_whitespace(seg, before=True):\n        \"\"\"Check whether we're missing whitespace given an adjoining segment.\"\"\"\n        # There is a segment\n        if not seg:\n            return False\n        # And it's not whitespace\n        if seg.is_whitespace:\n            return False\n        # And it's not an opening/closing bracket\n        if seg.name.endswith(\"_bracket\"):\n            if seg.name.startswith(\"start_\" if before else \"end_\"):\n                return False\n        if seg.is_meta:  # pragma: no cover\n            if before:\n                if seg.source_str.endswith(\" \") or seg.source_str.endswith(\"\\n\"):\n                    return False\n            else:\n                if seg.source_str.startswith(\" \") or seg.source_str.startswith(\"\\n\"):\n                    return False\n        return True\n", "tokens": ["src", "sqlfluff", "rules", "l006", "py", "rule_l006", "def", "_missing_whitespace", "seg", "before", "true", "check", "whether", "we", "re", "missing", "whitespace", "given", "an", "adjoining", "segment", "there", "is", "a", "segment", "if", "not", "seg", "return", "false", "and", "it", "s", "not", "whitespace", "if", "seg", "is_whitespace", "return", "false", "and", "it", "s", "not", "an", "opening", "closing", "bracket", "if", "seg", "name", "endswith", "_bracket", "if", "seg", "name", "startswith", "start_", "if", "before", "else", "end_", "return", "false", "if", "seg", "is_meta", "pragma", "no", "cover", "if", "before", "if", "seg", "source_str", "endswith", "or", "seg", "source_str", "endswith", "n", "return", "false", "else", "if", "seg", "source_str", "startswith", "or", "seg", "source_str", "startswith", "n", "return", "false", "return", "true"], "doc_len": 97}
{"doc_id": "src/sqlfluff/rules/L006.py::Rule_L006._find_segment", "file_path": "src/sqlfluff/rules/L006.py", "class_name": "Rule_L006", "func_name": "_find_segment", "text": "文件路径: src/sqlfluff/rules/L006.py, 类名: Rule_L006\n    def _find_segment(idx, segments, before=True):\n        \"\"\"Go back or forward to find the next relevant segment.\"\"\"\n        step = -1 if before else 1\n        j = idx + step\n        while (j >= 0) if before else (j < len(segments)):\n            # Don't trigger on indents, but placeholders are allowed.\n            if segments[j].is_type(\"indent\"):\n                j += step\n            else:\n                return segments[j]\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l006", "py", "rule_l006", "def", "_find_segment", "idx", "segments", "before", "true", "go", "back", "or", "forward", "to", "find", "the", "next", "relevant", "segment", "step", "1", "if", "before", "else", "1", "j", "idx", "step", "while", "j", "0", "if", "before", "else", "j", "len", "segments", "don", "t", "trigger", "on", "indents", "but", "placeholders", "are", "allowed", "if", "segments", "j", "is_type", "indent", "j", "step", "else", "return", "segments", "j", "return", "none"], "doc_len": 62}
{"doc_id": "src/sqlfluff/rules/L006.py::Rule_L006._eval", "file_path": "src/sqlfluff/rules/L006.py", "class_name": "Rule_L006", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L006.py, 类名: Rule_L006\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Operators should be surrounded by a single whitespace.\n\n        Rewritten to assess direct children of a segment to make\n        whitespace insertion more sensible.\n\n        We only need to handle *missing* whitespace because excess\n        whitespace is handled by L039.\n\n        NOTE: We also allow bracket characters either side.\n        \"\"\"\n        # Iterate through children of this segment looking for any of the\n        # target types. We also check for whether any of the children start\n        # or end with the targets.\n\n        # We ignore any targets which start or finish this segment. They'll\n        # be dealt with by the parent segment. That also means that we need\n        # to have at least three children.\n\n        if len(context.segment.segments) <= 2:\n            return LintResult()\n\n        violations = []\n\n        for idx, sub_seg in enumerate(context.segment.segments):\n            check_before = False\n            check_after = False\n            before_anchor = sub_seg\n            after_anchor = sub_seg\n            # Skip anything which is whitespace\n            if sub_seg.is_whitespace:\n                continue\n            # Skip any non-code elements\n            if not sub_seg.is_code:\n                continue\n\n            # Is it a target in itself?\n            if self.matches_target_tuples(sub_seg, self._target_elems):\n                self.logger.debug(\n                    \"Found Target [main] @%s: %r\", sub_seg.pos_marker, sub_seg.raw\n                )\n                check_before = True\n                check_after = True\n            # Is it a compound segment ending or starting with the target?\n            elif sub_seg.segments:\n                # Get first and last raw segments.\n                raw_list = list(sub_seg.iter_raw_seg())\n                if len(raw_list) > 1:\n                    leading = raw_list[0]\n                    trailing = raw_list[-1]\n                    if self.matches_target_tuples(leading, self._target_elems):\n                        before_anchor = leading\n                        self.logger.debug(\n                            \"Found Target [leading] @%s: %r\",\n                            before_anchor.pos_marker,\n                            before_anchor.raw,\n                        )\n                        check_before = True\n                    if self.matches_target_tuples(trailing, self._target_elems):\n                        after_anchor = trailing\n                        self.logger.debug(\n                            \"Found Target [trailing] @%s: %r\",\n                            after_anchor.pos_marker,\n                            after_anchor.raw,\n                        )\n                        check_after = True\n\n            if check_before:\n                prev_seg = self._find_segment(\n                    idx, context.segment.segments, before=True\n                )\n                if self._missing_whitespace(prev_seg, before=True):\n                    self.logger.debug(\n                        \"Missing Whitespace Before %r. Found %r instead.\",\n                        before_anchor.raw,\n                        prev_seg.raw,\n                    )\n                    violations.append(\n                        LintResult(\n                            anchor=before_anchor,\n                            description=\"Missing whitespace before {}\".format(\n                                before_anchor.raw[:10]\n                            ),\n                            fixes=[\n                                LintFix(\n                                    \"create\",\n                                    # NB the anchor here is always in the parent and not anchor\n                                    anchor=sub_seg,\n                                    edit=WhitespaceSegment(raw=\" \"),\n                                )\n                            ],\n                        )\n                    )\n\n            if check_after:\n                next_seg = self._find_segment(\n                    idx, context.segment.segments, before=False\n                )\n                if self._missing_whitespace(next_seg, before=False):\n                    self.logger.debug(\n                        \"Missing Whitespace After %r. Found %r instead.\",\n                        after_anchor.raw,\n                        next_seg.raw,\n                    )\n                    violations.append(\n                        LintResult(\n                            anchor=after_anchor,\n                            description=\"Missing whitespace after {}\".format(\n                                after_anchor.raw[-10:]\n                            ),\n                            fixes=[\n                                LintFix(\n                                    \"create\",\n                                    # NB the anchor here is always in the parent and not anchor\n                                    anchor=next_seg,\n                                    edit=WhitespaceSegment(raw=\" \"),\n                                )\n                            ],\n                        )\n                    )\n\n        if violations:\n            return violations\n\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l006", "py", "rule_l006", "def", "_eval", "self", "context", "rulecontext", "evalresulttype", "operators", "should", "be", "surrounded", "by", "a", "single", "whitespace", "rewritten", "to", "assess", "direct", "children", "of", "a", "segment", "to", "make", "whitespace", "insertion", "more", "sensible", "we", "only", "need", "to", "handle", "missing", "whitespace", "because", "excess", "whitespace", "is", "handled", "by", "l039", "note", "we", "also", "allow", "bracket", "characters", "either", "side", "iterate", "through", "children", "of", "this", "segment", "looking", "for", "any", "of", "the", "target", "types", "we", "also", "check", "for", "whether", "any", "of", "the", "children", "start", "or", "end", "with", "the", "targets", "we", "ignore", "any", "targets", "which", "start", "or", "finish", "this", "segment", "they", "ll", "be", "dealt", "with", "by", "the", "parent", "segment", "that", "also", "means", "that", "we", "need", "to", "have", "at", "least", "three", "children", "if", "len", "context", "segment", "segments", "2", "return", "lintresult", "violations", "for", "idx", "sub_seg", "in", "enumerate", "context", "segment", "segments", "check_before", "false", "check_after", "false", "before_anchor", "sub_seg", "after_anchor", "sub_seg", "skip", "anything", "which", "is", "whitespace", "if", "sub_seg", "is_whitespace", "continue", "skip", "any", "non", "code", "elements", "if", "not", "sub_seg", "is_code", "continue", "is", "it", "a", "target", "in", "itself", "if", "self", "matches_target_tuples", "sub_seg", "self", "_target_elems", "self", "logger", "debug", "found", "target", "main", "s", "r", "sub_seg", "pos_marker", "sub_seg", "raw", "check_before", "true", "check_after", "true", "is", "it", "a", "compound", "segment", "ending", "or", "starting", "with", "the", "target", "elif", "sub_seg", "segments", "get", "first", "and", "last", "raw", "segments", "raw_list", "list", "sub_seg", "iter_raw_seg", "if", "len", "raw_list", "1", "leading", "raw_list", "0", "trailing", "raw_list", "1", "if", "self", "matches_target_tuples", "leading", "self", "_target_elems", "before_anchor", "leading", "self", "logger", "debug", "found", "target", "leading", "s", "r", "before_anchor", "pos_marker", "before_anchor", "raw", "check_before", "true", "if", "self", "matches_target_tuples", "trailing", "self", "_target_elems", "after_anchor", "trailing", "self", "logger", "debug", "found", "target", "trailing", "s", "r", "after_anchor", "pos_marker", "after_anchor", "raw", "check_after", "true", "if", "check_before", "prev_seg", "self", "_find_segment", "idx", "context", "segment", "segments", "before", "true", "if", "self", "_missing_whitespace", "prev_seg", "before", "true", "self", "logger", "debug", "missing", "whitespace", "before", "r", "found", "r", "instead", "before_anchor", "raw", "prev_seg", "raw", "violations", "append", "lintresult", "anchor", "before_anchor", "description", "missing", "whitespace", "before", "format", "before_anchor", "raw", "10", "fixes", "lintfix", "create", "nb", "the", "anchor", "here", "is", "always", "in", "the", "parent", "and", "not", "anchor", "anchor", "sub_seg", "edit", "whitespacesegment", "raw", "if", "check_after", "next_seg", "self", "_find_segment", "idx", "context", "segment", "segments", "before", "false", "if", "self", "_missing_whitespace", "next_seg", "before", "false", "self", "logger", "debug", "missing", "whitespace", "after", "r", "found", "r", "instead", "after_anchor", "raw", "next_seg", "raw", "violations", "append", "lintresult", "anchor", "after_anchor", "description", "missing", "whitespace", "after", "format", "after_anchor", "raw", "10", "fixes", "lintfix", "create", "nb", "the", "anchor", "here", "is", "always", "in", "the", "parent", "and", "not", "anchor", "anchor", "next_seg", "edit", "whitespacesegment", "raw", "if", "violations", "return", "violations", "return", "lintresult"], "doc_len": 399}
{"doc_id": "src/sqlfluff/rules/L007.py::Rule_L007._eval", "file_path": "src/sqlfluff/rules/L007.py", "class_name": "Rule_L007", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L007.py, 类名: Rule_L007\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Operators should follow a standard for being before/after newlines.\n\n        We use the memory to keep track of whitespace up to now, and\n        whether the last code segment was an operator or not.\n        Anchor is our signal as to whether there's a problem.\n\n        We only trigger if we have an operator FOLLOWED BY a newline\n        before the next meaningful code segment.\n\n        \"\"\"\n        anchor = None\n        memory = context.memory\n        description = after_description\n        if self.operator_new_lines == \"before\":  # type: ignore\n            description = before_description\n\n        # The parent stack tells us whether we're in an expression or not.\n        if context.parent_stack and context.parent_stack[-1].is_type(\"expression\"):\n            if context.segment.is_code:\n                # This is code, what kind?\n                if context.segment.is_type(\"binary_operator\", \"comparison_operator\"):\n                    # If it's an operator, then check if in \"before\" mode\n                    if self.operator_new_lines == \"before\":  # type: ignore\n                        # If we're in \"before\" mode, then check if newline since last code\n                        for s in memory[\"since_code\"]:\n                            if s.name == \"newline\":\n                                # Had a newline - so mark this operator as a fail\n                                anchor = context.segment\n                                # TODO: Work out a nice fix for this failure.\n                elif memory[\"last_code\"] and memory[\"last_code\"].is_type(\n                    \"binary_operator\", \"comparison_operator\"\n                ):\n                    # It's not an operator, but the last code was.\n                    if self.operator_new_lines != \"before\":  # type: ignore\n                        # If in \"after\" mode, then check to see\n                        # there is a newline between us and the last operator.\n                        for s in memory[\"since_code\"]:\n                            if s.name == \"newline\":\n                                # Had a newline - so mark last operator as a fail\n                                anchor = memory[\"last_code\"]\n                                # TODO: Work out a nice fix for this failure.\n                # Prepare memory for later\n                memory[\"last_code\"] = context.segment\n                memory[\"since_code\"] = []\n            else:\n                # This isn't a code segment...\n                # Prepare memory for later\n                memory[\"since_code\"].append(context.segment)\n        else:\n            # Reset the memory if we're not in an expression\n            memory = {\"last_code\": None, \"since_code\": []}\n\n        # Anchor is our signal as to whether there's a problem\n        if anchor:\n            return LintResult(anchor=anchor, memory=memory, description=description)\n        else:\n            return LintResult(memory=memory)\n", "tokens": ["src", "sqlfluff", "rules", "l007", "py", "rule_l007", "def", "_eval", "self", "context", "rulecontext", "lintresult", "operators", "should", "follow", "a", "standard", "for", "being", "before", "after", "newlines", "we", "use", "the", "memory", "to", "keep", "track", "of", "whitespace", "up", "to", "now", "and", "whether", "the", "last", "code", "segment", "was", "an", "operator", "or", "not", "anchor", "is", "our", "signal", "as", "to", "whether", "there", "s", "a", "problem", "we", "only", "trigger", "if", "we", "have", "an", "operator", "followed", "by", "a", "newline", "before", "the", "next", "meaningful", "code", "segment", "anchor", "none", "memory", "context", "memory", "description", "after_description", "if", "self", "operator_new_lines", "before", "type", "ignore", "description", "before_description", "the", "parent", "stack", "tells", "us", "whether", "we", "re", "in", "an", "expression", "or", "not", "if", "context", "parent_stack", "and", "context", "parent_stack", "1", "is_type", "expression", "if", "context", "segment", "is_code", "this", "is", "code", "what", "kind", "if", "context", "segment", "is_type", "binary_operator", "comparison_operator", "if", "it", "s", "an", "operator", "then", "check", "if", "in", "before", "mode", "if", "self", "operator_new_lines", "before", "type", "ignore", "if", "we", "re", "in", "before", "mode", "then", "check", "if", "newline", "since", "last", "code", "for", "s", "in", "memory", "since_code", "if", "s", "name", "newline", "had", "a", "newline", "so", "mark", "this", "operator", "as", "a", "fail", "anchor", "context", "segment", "todo", "work", "out", "a", "nice", "fix", "for", "this", "failure", "elif", "memory", "last_code", "and", "memory", "last_code", "is_type", "binary_operator", "comparison_operator", "it", "s", "not", "an", "operator", "but", "the", "last", "code", "was", "if", "self", "operator_new_lines", "before", "type", "ignore", "if", "in", "after", "mode", "then", "check", "to", "see", "there", "is", "a", "newline", "between", "us", "and", "the", "last", "operator", "for", "s", "in", "memory", "since_code", "if", "s", "name", "newline", "had", "a", "newline", "so", "mark", "last", "operator", "as", "a", "fail", "anchor", "memory", "last_code", "todo", "work", "out", "a", "nice", "fix", "for", "this", "failure", "prepare", "memory", "for", "later", "memory", "last_code", "context", "segment", "memory", "since_code", "else", "this", "isn", "t", "a", "code", "segment", "prepare", "memory", "for", "later", "memory", "since_code", "append", "context", "segment", "else", "reset", "the", "memory", "if", "we", "re", "not", "in", "an", "expression", "memory", "last_code", "none", "since_code", "anchor", "is", "our", "signal", "as", "to", "whether", "there", "s", "a", "problem", "if", "anchor", "return", "lintresult", "anchor", "anchor", "memory", "memory", "description", "description", "else", "return", "lintresult", "memory", "memory"], "doc_len": 328}
{"doc_id": "src/sqlfluff/rules/L008.py::Rule_L008._eval", "file_path": "src/sqlfluff/rules/L008.py", "class_name": "Rule_L008", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L008.py, 类名: Rule_L008\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Commas should be followed by a single whitespace unless followed by a comment.\n\n        This is a slightly odd one, because we'll almost always evaluate from a point a few places\n        after the problem site. NB: We need at least two segments behind us for this to work.\n        \"\"\"\n        if len(context.raw_stack) < 1:\n            return None\n\n        # Get the first element of this segment.\n        first_elem = next(context.segment.iter_raw_seg())\n\n        cm1 = context.raw_stack[-1]\n        if cm1.name == \"comma\":\n            # comma followed by something that isn't whitespace?\n            if first_elem.name not in [\"whitespace\", \"newline\", \"Dedent\"]:\n                self.logger.debug(\n                    \"Comma followed by something other than whitespace: %s\", first_elem\n                )\n                ins = WhitespaceSegment(raw=\" \")\n                return LintResult(\n                    anchor=cm1,\n                    fixes=[LintFix(\"edit\", context.segment, [ins, context.segment])],\n                )\n\n        if len(context.raw_stack) < 2:\n            return None\n\n        cm2 = context.raw_stack[-2]\n        if cm2.name == \"comma\":\n            # comma followed by too much whitespace?\n            if (\n                cm1.is_whitespace  # Must be whitespace\n                and cm1.raw != \" \"  # ...and not a single one\n                and cm1.name != \"newline\"  # ...and not a newline\n                and not first_elem.is_comment  # ...and not followed by a comment\n            ):\n                self.logger.debug(\"Comma followed by too much whitespace: %s\", cm1)\n                repl = WhitespaceSegment(raw=\" \")\n                return LintResult(anchor=cm1, fixes=[LintFix(\"edit\", cm1, repl)])\n        # Otherwise we're fine\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l008", "py", "rule_l008", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "commas", "should", "be", "followed", "by", "a", "single", "whitespace", "unless", "followed", "by", "a", "comment", "this", "is", "a", "slightly", "odd", "one", "because", "we", "ll", "almost", "always", "evaluate", "from", "a", "point", "a", "few", "places", "after", "the", "problem", "site", "nb", "we", "need", "at", "least", "two", "segments", "behind", "us", "for", "this", "to", "work", "if", "len", "context", "raw_stack", "1", "return", "none", "get", "the", "first", "element", "of", "this", "segment", "first_elem", "next", "context", "segment", "iter_raw_seg", "cm1", "context", "raw_stack", "1", "if", "cm1", "name", "comma", "comma", "followed", "by", "something", "that", "isn", "t", "whitespace", "if", "first_elem", "name", "not", "in", "whitespace", "newline", "dedent", "self", "logger", "debug", "comma", "followed", "by", "something", "other", "than", "whitespace", "s", "first_elem", "ins", "whitespacesegment", "raw", "return", "lintresult", "anchor", "cm1", "fixes", "lintfix", "edit", "context", "segment", "ins", "context", "segment", "if", "len", "context", "raw_stack", "2", "return", "none", "cm2", "context", "raw_stack", "2", "if", "cm2", "name", "comma", "comma", "followed", "by", "too", "much", "whitespace", "if", "cm1", "is_whitespace", "must", "be", "whitespace", "and", "cm1", "raw", "and", "not", "a", "single", "one", "and", "cm1", "name", "newline", "and", "not", "a", "newline", "and", "not", "first_elem", "is_comment", "and", "not", "followed", "by", "a", "comment", "self", "logger", "debug", "comma", "followed", "by", "too", "much", "whitespace", "s", "cm1", "repl", "whitespacesegment", "raw", "return", "lintresult", "anchor", "cm1", "fixes", "lintfix", "edit", "cm1", "repl", "otherwise", "we", "re", "fine", "return", "none"], "doc_len": 213}
{"doc_id": "src/sqlfluff/rules/L009.py::Rule_L009._eval", "file_path": "src/sqlfluff/rules/L009.py", "class_name": "Rule_L009", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L009.py, 类名: Rule_L009\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Files must end with a trailing newline.\n\n        We only care about the segment and the siblings which come after it\n        for this rule, we discard the others into the kwargs argument.\n\n        \"\"\"\n        if len(self.filter_meta(context.siblings_post)) > 0:\n            # This can only fail on the last segment\n            return None\n        elif len(context.segment.segments) > 0:\n            # This can only fail on the last base segment\n            return None\n        elif context.segment.name == \"newline\":\n            # If this is the last segment, and it's a newline then we're good\n            return None\n        elif context.segment.is_meta:\n            # We can't fail on a meta segment\n            return None\n        else:\n            # So this looks like the end of the file, but we\n            # need to check that each parent segment is also the last.\n            # We do this with reference to the templated file, because it's\n            # the best we can do given the information available.\n            file_len = len(context.segment.pos_marker.templated_file.templated_str)\n            pos = context.segment.pos_marker.templated_slice.stop\n            # Does the length of the file equal the end of the templated position?\n            if file_len != pos:\n                return None\n\n        # We're going to make an edit because we're appending to the end and there's\n        # no segment after it to match on. Otherwise we would never get a match!\n        return LintResult(\n            anchor=context.segment,\n            fixes=[\n                LintFix(\"edit\", context.segment, [context.segment, NewlineSegment()])\n            ],\n        )\n", "tokens": ["src", "sqlfluff", "rules", "l009", "py", "rule_l009", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "files", "must", "end", "with", "a", "trailing", "newline", "we", "only", "care", "about", "the", "segment", "and", "the", "siblings", "which", "come", "after", "it", "for", "this", "rule", "we", "discard", "the", "others", "into", "the", "kwargs", "argument", "if", "len", "self", "filter_meta", "context", "siblings_post", "0", "this", "can", "only", "fail", "on", "the", "last", "segment", "return", "none", "elif", "len", "context", "segment", "segments", "0", "this", "can", "only", "fail", "on", "the", "last", "base", "segment", "return", "none", "elif", "context", "segment", "name", "newline", "if", "this", "is", "the", "last", "segment", "and", "it", "s", "a", "newline", "then", "we", "re", "good", "return", "none", "elif", "context", "segment", "is_meta", "we", "can", "t", "fail", "on", "a", "meta", "segment", "return", "none", "else", "so", "this", "looks", "like", "the", "end", "of", "the", "file", "but", "we", "need", "to", "check", "that", "each", "parent", "segment", "is", "also", "the", "last", "we", "do", "this", "with", "reference", "to", "the", "templated", "file", "because", "it", "s", "the", "best", "we", "can", "do", "given", "the", "information", "available", "file_len", "len", "context", "segment", "pos_marker", "templated_file", "templated_str", "pos", "context", "segment", "pos_marker", "templated_slice", "stop", "does", "the", "length", "of", "the", "file", "equal", "the", "end", "of", "the", "templated", "position", "if", "file_len", "pos", "return", "none", "we", "re", "going", "to", "make", "an", "edit", "because", "we", "re", "appending", "to", "the", "end", "and", "there", "s", "no", "segment", "after", "it", "to", "match", "on", "otherwise", "we", "would", "never", "get", "a", "match", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "context", "segment", "context", "segment", "newlinesegment"], "doc_len": 233}
{"doc_id": "src/sqlfluff/rules/L010.py::Rule_L010._eval", "file_path": "src/sqlfluff/rules/L010.py", "class_name": "Rule_L010", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L010.py, 类名: Rule_L010\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Inconsistent capitalisation of keywords.\n\n        We use the `memory` feature here to keep track of cases known to be\n        INconsistent with what we've seen so far as well as the top choice\n        for what the possible case is.\n\n        \"\"\"\n        # Skip if not an element of the specified type/name\n        if not self.matches_target_tuples(context.segment, self._target_elems):\n            return LintResult(memory=context.memory)\n\n        # Get the capitalisation policy configuration.\n        try:\n            cap_policy = self.cap_policy\n            cap_policy_opts = self.cap_policy_opts\n        except AttributeError:\n            # First-time only, read the settings from configuration. This is\n            # very slow.\n            cap_policy, cap_policy_opts = self._init_capitalisation_policy()\n\n        memory = context.memory\n        refuted_cases = memory.get(\"refuted_cases\", set())\n\n        # Which cases are definitely inconsistent with the segment?\n        if context.segment.raw[0] != context.segment.raw[0].upper():\n            refuted_cases.update([\"upper\", \"capitalise\", \"pascal\"])\n            if context.segment.raw != context.segment.raw.lower():\n                refuted_cases.update([\"lower\"])\n        else:\n            refuted_cases.update([\"lower\"])\n            if context.segment.raw != context.segment.raw.upper():\n                refuted_cases.update([\"upper\"])\n            if context.segment.raw != context.segment.raw.capitalize():\n                refuted_cases.update([\"capitalise\"])\n            if not context.segment.raw.isalnum():\n                refuted_cases.update([\"pascal\"])\n\n        # Update the memory\n        memory[\"refuted_cases\"] = refuted_cases\n\n        self.logger.debug(\n            f\"Refuted cases after segment '{context.segment.raw}': {refuted_cases}\"\n        )\n\n        # Skip if no inconsistencies, otherwise compute a concrete policy\n        # to convert to.\n        if cap_policy == \"consistent\":\n            possible_cases = [c for c in cap_policy_opts if c not in refuted_cases]\n            self.logger.debug(\n                f\"Possible cases after segment '{context.segment.raw}': {possible_cases}\"\n            )\n            if possible_cases:\n                # Save the latest possible case and skip\n                memory[\"latest_possible_case\"] = possible_cases[0]\n                self.logger.debug(\n                    f\"Consistent capitalization, returning with memory: {memory}\"\n                )\n                return LintResult(memory=memory)\n            else:\n                concrete_policy = memory.get(\"latest_possible_case\", \"upper\")\n                self.logger.debug(\n                    f\"Getting concrete policy '{concrete_policy}' from memory\"\n                )\n        else:\n            if cap_policy not in refuted_cases:\n                # Skip\n                self.logger.debug(\n                    f\"Consistent capitalization {cap_policy}, returning with \"\n                    f\"memory: {memory}\"\n                )\n                return LintResult(memory=memory)\n            else:\n                concrete_policy = cap_policy\n                self.logger.debug(\n                    f\"Setting concrete policy '{concrete_policy}' from cap_policy\"\n                )\n\n        # Set the fixed to same as initial in case any of below don't match\n        fixed_raw = context.segment.raw\n        # We need to change the segment to match the concrete policy\n        if concrete_policy in [\"upper\", \"lower\", \"capitalise\"]:\n            if concrete_policy == \"upper\":\n                fixed_raw = fixed_raw.upper()\n            elif concrete_policy == \"lower\":\n                fixed_raw = fixed_raw.lower()\n            elif concrete_policy == \"capitalise\":\n                fixed_raw = fixed_raw.capitalize()\n        elif concrete_policy == \"pascal\":\n            # For Pascal we set the first letter in each \"word\" to uppercase\n            # We do not lowercase other letters to allow for PascalCase style\n            # words. This does mean we allow all UPPERCASE and also don't\n            # correct Pascalcase to PascalCase, but there's only so much we can\n            # do. We do correct underscore_words to Underscore_Words.\n            fixed_raw = re.sub(\n                \"([^a-zA-Z0-9]+|^)([a-zA-Z0-9])([a-zA-Z0-9]*)\",\n                lambda match: match.group(1) + match.group(2).upper() + match.group(3),\n                context.segment.raw,\n            )\n\n        if fixed_raw == context.segment.raw:\n            # No need to fix\n            self.logger.debug(\n                f\"Capitalisation of segment '{context.segment.raw}' already OK with policy \"\n                f\"'{concrete_policy}', returning with memory {memory}\"\n            )\n            return LintResult(memory=memory)\n        else:\n            # build description based on the policy in use\n            consistency = \"consistently \" if cap_policy == \"consistent\" else \"\"\n\n            if concrete_policy in [\"upper\", \"lower\"]:\n                policy = f\"{concrete_policy} case.\"\n            elif concrete_policy == \"capitalise\":\n                policy = \"capitalised.\"\n            elif concrete_policy == \"pascal\":\n                policy = \"pascal case.\"\n\n            # Return the fixed segment\n            self.logger.debug(\n                f\"INCONSISTENT Capitalisation of segment '{context.segment.raw}', fixing to \"\n                f\"'{fixed_raw}' and returning with memory {memory}\"\n            )\n\n            return LintResult(\n                anchor=context.segment,\n                fixes=[self._get_fix(context.segment, fixed_raw)],\n                memory=memory,\n                description=f\"{self._description_elem} must be {consistency}{policy}\",\n            )\n", "tokens": ["src", "sqlfluff", "rules", "l010", "py", "rule_l010", "def", "_eval", "self", "context", "rulecontext", "lintresult", "inconsistent", "capitalisation", "of", "keywords", "we", "use", "the", "memory", "feature", "here", "to", "keep", "track", "of", "cases", "known", "to", "be", "inconsistent", "with", "what", "we", "ve", "seen", "so", "far", "as", "well", "as", "the", "top", "choice", "for", "what", "the", "possible", "case", "is", "skip", "if", "not", "an", "element", "of", "the", "specified", "type", "name", "if", "not", "self", "matches_target_tuples", "context", "segment", "self", "_target_elems", "return", "lintresult", "memory", "context", "memory", "get", "the", "capitalisation", "policy", "configuration", "try", "cap_policy", "self", "cap_policy", "cap_policy_opts", "self", "cap_policy_opts", "except", "attributeerror", "first", "time", "only", "read", "the", "settings", "from", "configuration", "this", "is", "very", "slow", "cap_policy", "cap_policy_opts", "self", "_init_capitalisation_policy", "memory", "context", "memory", "refuted_cases", "memory", "get", "refuted_cases", "set", "which", "cases", "are", "definitely", "inconsistent", "with", "the", "segment", "if", "context", "segment", "raw", "0", "context", "segment", "raw", "0", "upper", "refuted_cases", "update", "upper", "capitalise", "pascal", "if", "context", "segment", "raw", "context", "segment", "raw", "lower", "refuted_cases", "update", "lower", "else", "refuted_cases", "update", "lower", "if", "context", "segment", "raw", "context", "segment", "raw", "upper", "refuted_cases", "update", "upper", "if", "context", "segment", "raw", "context", "segment", "raw", "capitalize", "refuted_cases", "update", "capitalise", "if", "not", "context", "segment", "raw", "isalnum", "refuted_cases", "update", "pascal", "update", "the", "memory", "memory", "refuted_cases", "refuted_cases", "self", "logger", "debug", "f", "refuted", "cases", "after", "segment", "context", "segment", "raw", "refuted_cases", "skip", "if", "no", "inconsistencies", "otherwise", "compute", "a", "concrete", "policy", "to", "convert", "to", "if", "cap_policy", "consistent", "possible_cases", "c", "for", "c", "in", "cap_policy_opts", "if", "c", "not", "in", "refuted_cases", "self", "logger", "debug", "f", "possible", "cases", "after", "segment", "context", "segment", "raw", "possible_cases", "if", "possible_cases", "save", "the", "latest", "possible", "case", "and", "skip", "memory", "latest_possible_case", "possible_cases", "0", "self", "logger", "debug", "f", "consistent", "capitalization", "returning", "with", "memory", "memory", "return", "lintresult", "memory", "memory", "else", "concrete_policy", "memory", "get", "latest_possible_case", "upper", "self", "logger", "debug", "f", "getting", "concrete", "policy", "concrete_policy", "from", "memory", "else", "if", "cap_policy", "not", "in", "refuted_cases", "skip", "self", "logger", "debug", "f", "consistent", "capitalization", "cap_policy", "returning", "with", "f", "memory", "memory", "return", "lintresult", "memory", "memory", "else", "concrete_policy", "cap_policy", "self", "logger", "debug", "f", "setting", "concrete", "policy", "concrete_policy", "from", "cap_policy", "set", "the", "fixed", "to", "same", "as", "initial", "in", "case", "any", "of", "below", "don", "t", "match", "fixed_raw", "context", "segment", "raw", "we", "need", "to", "change", "the", "segment", "to", "match", "the", "concrete", "policy", "if", "concrete_policy", "in", "upper", "lower", "capitalise", "if", "concrete_policy", "upper", "fixed_raw", "fixed_raw", "upper", "elif", "concrete_policy", "lower", "fixed_raw", "fixed_raw", "lower", "elif", "concrete_policy", "capitalise", "fixed_raw", "fixed_raw", "capitalize", "elif", "concrete_policy", "pascal", "for", "pascal", "we", "set", "the", "first", "letter", "in", "each", "word", "to", "uppercase", "we", "do", "not", "lowercase", "other", "letters", "to", "allow", "for", "pascalcase", "style", "words", "this", "does", "mean", "we", "allow", "all", "uppercase", "and", "also", "don", "t", "correct", "pascalcase", "to", "pascalcase", "but", "there", "s", "only", "so", "much", "we", "can", "do", "we", "do", "correct", "underscore_words", "to", "underscore_words", "fixed_raw", "re", "sub", "a", "za", "z0", "9", "a", "za", "z0", "9", "a", "za", "z0", "9", "lambda", "match", "match", "group", "1", "match", "group", "2", "upper", "match", "group", "3", "context", "segment", "raw", "if", "fixed_raw", "context", "segment", "raw", "no", "need", "to", "fix", "self", "logger", "debug", "f", "capitalisation", "of", "segment", "context", "segment", "raw", "already", "ok", "with", "policy", "f", "concrete_policy", "returning", "with", "memory", "memory", "return", "lintresult", "memory", "memory", "else", "build", "description", "based", "on", "the", "policy", "in", "use", "consistency", "consistently", "if", "cap_policy", "consistent", "else", "if", "concrete_policy", "in", "upper", "lower", "policy", "f", "concrete_policy", "case", "elif", "concrete_policy", "capitalise", "policy", "capitalised", "elif", "concrete_policy", "pascal", "policy", "pascal", "case", "return", "the", "fixed", "segment", "self", "logger", "debug", "f", "inconsistent", "capitalisation", "of", "segment", "context", "segment", "raw", "fixing", "to", "f", "fixed_raw", "and", "returning", "with", "memory", "memory", "return", "lintresult", "anchor", "context", "segment", "fixes", "self", "_get_fix", "context", "segment", "fixed_raw", "memory", "memory", "description", "f", "self", "_description_elem", "must", "be", "consistency", "policy"], "doc_len": 569}
{"doc_id": "src/sqlfluff/rules/L010.py::Rule_L010._get_fix", "file_path": "src/sqlfluff/rules/L010.py", "class_name": "Rule_L010", "func_name": "_get_fix", "text": "文件路径: src/sqlfluff/rules/L010.py, 类名: Rule_L010\n    def _get_fix(self, segment, fixed_raw):\n        \"\"\"Given a segment found to have a fix, returns a LintFix for it.\n\n        May be overridden by subclasses, which is useful when the parse tree\n        structure varies from this simple base case.\n        \"\"\"\n        return LintFix(\"edit\", segment, segment.edit(fixed_raw))\n", "tokens": ["src", "sqlfluff", "rules", "l010", "py", "rule_l010", "def", "_get_fix", "self", "segment", "fixed_raw", "given", "a", "segment", "found", "to", "have", "a", "fix", "returns", "a", "lintfix", "for", "it", "may", "be", "overridden", "by", "subclasses", "which", "is", "useful", "when", "the", "parse", "tree", "structure", "varies", "from", "this", "simple", "base", "case", "return", "lintfix", "edit", "segment", "segment", "edit", "fixed_raw"], "doc_len": 50}
{"doc_id": "src/sqlfluff/rules/L010.py::Rule_L010._init_capitalisation_policy", "file_path": "src/sqlfluff/rules/L010.py", "class_name": "Rule_L010", "func_name": "_init_capitalisation_policy", "text": "文件路径: src/sqlfluff/rules/L010.py, 类名: Rule_L010\n    def _init_capitalisation_policy(self):\n        \"\"\"Called first time rule is evaluated to fetch & cache the policy.\"\"\"\n        cap_policy_name = next(\n            k for k in self.config_keywords if k.endswith(\"capitalisation_policy\")\n        )\n        self.cap_policy = getattr(self, cap_policy_name)\n        self.cap_policy_opts = [\n            opt\n            for opt in get_config_info()[cap_policy_name][\"validation\"]\n            if opt != \"consistent\"\n        ]\n        self.logger.debug(\n            f\"Selected '{cap_policy_name}': '{self.cap_policy}' from options \"\n            f\"{self.cap_policy_opts}\"\n        )\n        cap_policy = self.cap_policy\n        cap_policy_opts = self.cap_policy_opts\n        return cap_policy, cap_policy_opts\n", "tokens": ["src", "sqlfluff", "rules", "l010", "py", "rule_l010", "def", "_init_capitalisation_policy", "self", "called", "first", "time", "rule", "is", "evaluated", "to", "fetch", "cache", "the", "policy", "cap_policy_name", "next", "k", "for", "k", "in", "self", "config_keywords", "if", "k", "endswith", "capitalisation_policy", "self", "cap_policy", "getattr", "self", "cap_policy_name", "self", "cap_policy_opts", "opt", "for", "opt", "in", "get_config_info", "cap_policy_name", "validation", "if", "opt", "consistent", "self", "logger", "debug", "f", "selected", "cap_policy_name", "self", "cap_policy", "from", "options", "f", "self", "cap_policy_opts", "cap_policy", "self", "cap_policy", "cap_policy_opts", "self", "cap_policy_opts", "return", "cap_policy", "cap_policy_opts"], "doc_len": 71}
{"doc_id": "src/sqlfluff/rules/L011.py::Rule_L011._eval", "file_path": "src/sqlfluff/rules/L011.py", "class_name": "Rule_L011", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L011.py, 类名: Rule_L011\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Implicit aliasing of table/column not allowed. Use explicit `AS` clause.\n\n        We look for the alias segment, and then evaluate its parent and whether\n        it contains an AS keyword. This is the _eval function for both L011 and L012.\n\n        The use of `raw_stack` is just for working out how much whitespace to add.\n\n        \"\"\"\n        fixes = []\n\n        if context.segment.is_type(\"alias_expression\"):\n            if context.parent_stack[-1].is_type(*self._target_elems):\n                if any(e.name.lower() == \"as\" for e in context.segment.segments):\n                    if self.aliasing == \"implicit\":  # type: ignore\n                        if context.segment.segments[0].name.lower() == \"as\":\n\n                            # Remove the AS as we're using implict aliasing\n                            fixes.append(LintFix(\"delete\", context.segment.segments[0]))\n                            anchor = context.raw_stack[-1]\n\n                            # Remove whitespace before (if exists) or after (if not)\n                            if (\n                                len(context.raw_stack) > 0\n                                and context.raw_stack[-1].type == \"whitespace\"\n                            ):\n                                fixes.append(LintFix(\"delete\", context.raw_stack[-1]))\n                            elif (\n                                len(context.segment.segments) > 0\n                                and context.segment.segments[1].type == \"whitespace\"\n                            ):\n                                fixes.append(\n                                    LintFix(\"delete\", context.segment.segments[1])\n                                )\n\n                            return LintResult(anchor=anchor, fixes=fixes)\n\n                else:\n                    insert_buff: List[Union[WhitespaceSegment, KeywordSegment]] = []\n\n                    # Add initial whitespace if we need to...\n                    if context.raw_stack[-1].name not in [\"whitespace\", \"newline\"]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    # Add an AS (Uppercase for now, but could be corrected later)\n                    insert_buff.append(KeywordSegment(\"AS\"))\n\n                    # Add a trailing whitespace if we need to\n                    if context.segment.segments[0].name not in [\n                        \"whitespace\",\n                        \"newline\",\n                    ]:\n                        insert_buff.append(WhitespaceSegment())\n\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\"create\", context.segment.segments[0], insert_buff)\n                        ],\n                    )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l011", "py", "rule_l011", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "implicit", "aliasing", "of", "table", "column", "not", "allowed", "use", "explicit", "as", "clause", "we", "look", "for", "the", "alias", "segment", "and", "then", "evaluate", "its", "parent", "and", "whether", "it", "contains", "an", "as", "keyword", "this", "is", "the", "_eval", "function", "for", "both", "l011", "and", "l012", "the", "use", "of", "raw_stack", "is", "just", "for", "working", "out", "how", "much", "whitespace", "to", "add", "fixes", "if", "context", "segment", "is_type", "alias_expression", "if", "context", "parent_stack", "1", "is_type", "self", "_target_elems", "if", "any", "e", "name", "lower", "as", "for", "e", "in", "context", "segment", "segments", "if", "self", "aliasing", "implicit", "type", "ignore", "if", "context", "segment", "segments", "0", "name", "lower", "as", "remove", "the", "as", "as", "we", "re", "using", "implict", "aliasing", "fixes", "append", "lintfix", "delete", "context", "segment", "segments", "0", "anchor", "context", "raw_stack", "1", "remove", "whitespace", "before", "if", "exists", "or", "after", "if", "not", "if", "len", "context", "raw_stack", "0", "and", "context", "raw_stack", "1", "type", "whitespace", "fixes", "append", "lintfix", "delete", "context", "raw_stack", "1", "elif", "len", "context", "segment", "segments", "0", "and", "context", "segment", "segments", "1", "type", "whitespace", "fixes", "append", "lintfix", "delete", "context", "segment", "segments", "1", "return", "lintresult", "anchor", "anchor", "fixes", "fixes", "else", "insert_buff", "list", "union", "whitespacesegment", "keywordsegment", "add", "initial", "whitespace", "if", "we", "need", "to", "if", "context", "raw_stack", "1", "name", "not", "in", "whitespace", "newline", "insert_buff", "append", "whitespacesegment", "add", "an", "as", "uppercase", "for", "now", "but", "could", "be", "corrected", "later", "insert_buff", "append", "keywordsegment", "as", "add", "a", "trailing", "whitespace", "if", "we", "need", "to", "if", "context", "segment", "segments", "0", "name", "not", "in", "whitespace", "newline", "insert_buff", "append", "whitespacesegment", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "create", "context", "segment", "segments", "0", "insert_buff", "return", "none"], "doc_len": 256}
{"doc_id": "src/sqlfluff/rules/L013.py::Rule_L013._eval", "file_path": "src/sqlfluff/rules/L013.py", "class_name": "Rule_L013", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L013.py, 类名: Rule_L013\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Column expression without alias. Use explicit `AS` clause.\n\n        We look for the select_clause_element segment, and then evaluate\n        whether it has an alias segment or not and whether the expression\n        is complicated enough. `parent_stack` is to assess how many other\n        elements there are.\n\n        \"\"\"\n        if context.segment.is_type(\"select_clause_element\"):\n            if not any(e.is_type(\"alias_expression\") for e in context.segment.segments):\n                types = {\n                    e.get_type() for e in context.segment.segments if e.name != \"star\"\n                }\n                unallowed_types = types - {\n                    \"whitespace\",\n                    \"newline\",\n                    \"column_reference\",\n                    \"wildcard_expression\",\n                }\n                if len(unallowed_types) > 0:\n                    # No fixes, because we don't know what the alias should be,\n                    # the user should document it themselves.\n                    if self.allow_scalar:  # type: ignore\n                        # Check *how many* elements there are in the select\n                        # statement. If this is the only one, then we won't\n                        # report an error.\n                        num_elements = sum(\n                            e.is_type(\"select_clause_element\")\n                            for e in context.parent_stack[-1].segments\n                        )\n                        if num_elements > 1:\n                            return LintResult(anchor=context.segment)\n                        else:\n                            return None\n                    else:\n                        # Just error if we don't care.\n                        return LintResult(anchor=context.segment)\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l013", "py", "rule_l013", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "column", "expression", "without", "alias", "use", "explicit", "as", "clause", "we", "look", "for", "the", "select_clause_element", "segment", "and", "then", "evaluate", "whether", "it", "has", "an", "alias", "segment", "or", "not", "and", "whether", "the", "expression", "is", "complicated", "enough", "parent_stack", "is", "to", "assess", "how", "many", "other", "elements", "there", "are", "if", "context", "segment", "is_type", "select_clause_element", "if", "not", "any", "e", "is_type", "alias_expression", "for", "e", "in", "context", "segment", "segments", "types", "e", "get_type", "for", "e", "in", "context", "segment", "segments", "if", "e", "name", "star", "unallowed_types", "types", "whitespace", "newline", "column_reference", "wildcard_expression", "if", "len", "unallowed_types", "0", "no", "fixes", "because", "we", "don", "t", "know", "what", "the", "alias", "should", "be", "the", "user", "should", "document", "it", "themselves", "if", "self", "allow_scalar", "type", "ignore", "check", "how", "many", "elements", "there", "are", "in", "the", "select", "statement", "if", "this", "is", "the", "only", "one", "then", "we", "won", "t", "report", "an", "error", "num_elements", "sum", "e", "is_type", "select_clause_element", "for", "e", "in", "context", "parent_stack", "1", "segments", "if", "num_elements", "1", "return", "lintresult", "anchor", "context", "segment", "else", "return", "none", "else", "just", "error", "if", "we", "don", "t", "care", "return", "lintresult", "anchor", "context", "segment", "return", "none"], "doc_len": 179}
{"doc_id": "src/sqlfluff/rules/L014.py::unquoted_ids_policy_applicable", "file_path": "src/sqlfluff/rules/L014.py", "class_name": null, "func_name": "unquoted_ids_policy_applicable", "text": "文件路径: src/sqlfluff/rules/L014.py\ndef unquoted_ids_policy_applicable(\n    policy: str, parent_stack: Tuple[BaseSegment, ...]\n) -> bool:\n    \"\"\"Does `unquoted_identifiers_policy` apply to this segment?\"\"\"\n    if policy == \"all\":\n        return True\n    is_alias = parent_stack and parent_stack[-1].is_type(\n        \"alias_expression\", \"column_definition\", \"with_compound_statement\"\n    )\n    if policy == \"aliases\" and is_alias:\n        return True\n    is_inside_from = any(p.is_type(\"from_clause\") for p in parent_stack)\n    if policy == \"column_aliases\" and is_alias and not is_inside_from:\n        return True\n    return False\n", "tokens": ["src", "sqlfluff", "rules", "l014", "py", "def", "unquoted_ids_policy_applicable", "policy", "str", "parent_stack", "tuple", "basesegment", "bool", "does", "unquoted_identifiers_policy", "apply", "to", "this", "segment", "if", "policy", "all", "return", "true", "is_alias", "parent_stack", "and", "parent_stack", "1", "is_type", "alias_expression", "column_definition", "with_compound_statement", "if", "policy", "aliases", "and", "is_alias", "return", "true", "is_inside_from", "any", "p", "is_type", "from_clause", "for", "p", "in", "parent_stack", "if", "policy", "column_aliases", "and", "is_alias", "and", "not", "is_inside_from", "return", "true", "return", "false"], "doc_len": 61}
{"doc_id": "src/sqlfluff/rules/L014.py::Rule_L014._eval", "file_path": "src/sqlfluff/rules/L014.py", "class_name": "Rule_L014", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L014.py, 类名: Rule_L014\n    def _eval(self, context: RuleContext) -> LintResult:\n        if unquoted_ids_policy_applicable(\n            self.unquoted_identifiers_policy, context.parent_stack  # type: ignore\n        ):\n            return super()._eval(context=context)\n        else:\n            return LintResult(memory=context.memory)\n", "tokens": ["src", "sqlfluff", "rules", "l014", "py", "rule_l014", "def", "_eval", "self", "context", "rulecontext", "lintresult", "if", "unquoted_ids_policy_applicable", "self", "unquoted_identifiers_policy", "context", "parent_stack", "type", "ignore", "return", "super", "_eval", "context", "context", "else", "return", "lintresult", "memory", "context", "memory"], "doc_len": 31}
{"doc_id": "src/sqlfluff/rules/L015.py::Rule_L015._eval", "file_path": "src/sqlfluff/rules/L015.py", "class_name": "Rule_L015", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L015.py, 类名: Rule_L015\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Looking for DISTINCT before a bracket.\n\n        Look for DISTINCT keyword immediately followed by open parenthesis.\n        \"\"\"\n        # We trigger on `select_clause` and look for `select_clause_modifier`\n        if context.segment.is_type(\"select_clause\"):\n            modifier = context.segment.get_child(\"select_clause_modifier\")\n            if not modifier:\n                return None\n            first_element = context.segment.get_child(\"select_clause_element\")\n            if not first_element:\n                return None  # pragma: no cover\n            # is the first element only an expression with only brackets?\n            expression = first_element.get_child(\"expression\")\n            if not expression:\n                expression = first_element\n            bracketed = expression.get_child(\"bracketed\")\n            if not bracketed:\n                return None\n            fixes = []\n            # If there's nothing else in the expression, remove the brackets.\n            if len(expression.segments) == 1:\n                # Remove the brackets, and strip any meta segments.\n                fixes = [\n                    LintFix(\n                        \"edit\", bracketed, self.filter_meta(bracketed.segments)[1:-1]\n                    ),\n                ]\n            # Is there any whitespace between DISTINCT and the expression?\n            distinct_idx = context.segment.segments.index(modifier)\n            elem_idx = context.segment.segments.index(first_element)\n            if not any(\n                seg.is_whitespace\n                for seg in context.segment.segments[distinct_idx:elem_idx]\n            ):\n                fixes.append(\n                    LintFix(\n                        \"create\",\n                        first_element,\n                        WhitespaceSegment(),\n                    )\n                )\n            # If no fixes, no problem.\n            if fixes:\n                return LintResult(anchor=modifier, fixes=fixes)\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l015", "py", "rule_l015", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "looking", "for", "distinct", "before", "a", "bracket", "look", "for", "distinct", "keyword", "immediately", "followed", "by", "open", "parenthesis", "we", "trigger", "on", "select_clause", "and", "look", "for", "select_clause_modifier", "if", "context", "segment", "is_type", "select_clause", "modifier", "context", "segment", "get_child", "select_clause_modifier", "if", "not", "modifier", "return", "none", "first_element", "context", "segment", "get_child", "select_clause_element", "if", "not", "first_element", "return", "none", "pragma", "no", "cover", "is", "the", "first", "element", "only", "an", "expression", "with", "only", "brackets", "expression", "first_element", "get_child", "expression", "if", "not", "expression", "expression", "first_element", "bracketed", "expression", "get_child", "bracketed", "if", "not", "bracketed", "return", "none", "fixes", "if", "there", "s", "nothing", "else", "in", "the", "expression", "remove", "the", "brackets", "if", "len", "expression", "segments", "1", "remove", "the", "brackets", "and", "strip", "any", "meta", "segments", "fixes", "lintfix", "edit", "bracketed", "self", "filter_meta", "bracketed", "segments", "1", "1", "is", "there", "any", "whitespace", "between", "distinct", "and", "the", "expression", "distinct_idx", "context", "segment", "segments", "index", "modifier", "elem_idx", "context", "segment", "segments", "index", "first_element", "if", "not", "any", "seg", "is_whitespace", "for", "seg", "in", "context", "segment", "segments", "distinct_idx", "elem_idx", "fixes", "append", "lintfix", "create", "first_element", "whitespacesegment", "if", "no", "fixes", "no", "problem", "if", "fixes", "return", "lintresult", "anchor", "modifier", "fixes", "fixes", "return", "none"], "doc_len": 182}
{"doc_id": "src/sqlfluff/rules/L016.py::Rule_L016._eval_line_for_breaks", "file_path": "src/sqlfluff/rules/L016.py", "class_name": "Rule_L016", "func_name": "_eval_line_for_breaks", "text": "文件路径: src/sqlfluff/rules/L016.py, 类名: Rule_L016\n    def _eval_line_for_breaks(self, segments):\n        \"\"\"Evaluate the line for break points.\n\n        We split the line into a few particular sections:\n        - The indent (all the whitespace up to this point)\n        - Content (which doesn't have whitespace at the start or end)\n        - Breakpoint (which contains Indent/Dedent and potential\n          whitespace). NB: If multiple indent/dedent sections share\n          a breakpoint, then they will occupy the SAME one, so that\n          dealing with whitespace post-split is easier.\n        - Pausepoint (which is a comma, potentially surrounded by\n          whitespace). This is for potential list splitting.\n\n        Once split, we'll use a separate method to work out what\n        combinations make most sense for reflow.\n        \"\"\"\n        chunk_buff = []\n        indent_section = None\n\n        class Section:\n            def __init__(self, segments, role, indent_balance, indent_impulse=0):\n                self.segments = segments\n                self.role = role\n                self.indent_balance = indent_balance\n                self.indent_impulse = indent_impulse\n\n            def __repr__(self):\n                return \"<Section @ {pos}: {role} [{indent_balance}:{indent_impulse}]. {segments!r}>\".format(\n                    role=self.role,\n                    indent_balance=self.indent_balance,\n                    indent_impulse=self.indent_impulse,\n                    segments=\"\".join(elem.raw for elem in self.segments),\n                    pos=self.segments[0].get_start_point_marker()\n                    if self.segments\n                    else \"\",\n                )\n\n            @property\n            def raw(self):\n                return \"\".join(seg.raw for seg in self.segments)\n\n            @staticmethod\n            def find_segment_at(segments, loc: Tuple[int, int]):\n                for seg in segments:\n                    if not seg.is_meta and seg.pos_marker.working_loc == loc:\n                        return seg\n\n            def generate_fixes_to_coerce(\n                self, segments, indent_section, crawler, indent\n            ):\n                \"\"\"Generate a list of fixes to create a break at this point.\n\n                The `segments` argument is necessary to extract anchors\n                from the existing segments.\n                \"\"\"\n                fixes = []\n\n                # Generate some sample indents:\n                unit_indent = crawler._make_indent(\n                    indent_unit=crawler.indent_unit,\n                    tab_space_size=crawler.tab_space_size,\n                )\n                indent_p1 = indent_section.raw + unit_indent\n                if unit_indent in indent_section.raw:\n                    indent_m1 = indent_section.raw.replace(unit_indent, \"\", 1)\n                else:\n                    indent_m1 = indent_section.raw\n\n                if indent > 0:\n                    new_indent = indent_p1\n                elif indent < 0:\n                    new_indent = indent_m1\n                else:\n                    new_indent = indent_section.raw\n\n                create_anchor = self.find_segment_at(\n                    segments, self.segments[-1].get_end_loc()\n                )\n\n                if self.role == \"pausepoint\":\n                    # Assume that this means there isn't a breakpoint\n                    # and that we'll break with the same indent as the\n                    # existing line.\n\n                    # NOTE: Deal with commas and binary operators differently here.\n                    # Maybe only deal with commas to start with?\n                    if any(\n                        seg.is_type(\"binary_operator\") for seg in self.segments\n                    ):  # pragma: no cover\n                        raise NotImplementedError(\n                            \"Don't know how to deal with binary operators here yet!!\"\n                        )\n\n                    # Remove any existing whitespace\n                    for elem in self.segments:\n                        if not elem.is_meta and elem.is_type(\"whitespace\"):\n                            fixes.append(LintFix(\"delete\", elem))\n\n                    # Create a newline and a similar indent\n                    fixes.append(\n                        LintFix(\n                            \"create\",\n                            create_anchor,\n                            [\n                                NewlineSegment(),\n                                WhitespaceSegment(new_indent),\n                            ],\n                        )\n                    )\n                    return fixes\n\n                if self.role == \"breakpoint\":\n                    # Can we determine the required indent just from\n                    # the info in this segment only?\n\n                    # Remove anything which is already here\n                    for elem in self.segments:\n                        if not elem.is_meta:\n                            fixes.append(LintFix(\"delete\", elem))\n                    # Create a newline, create an indent of the relevant size\n                    fixes.append(\n                        LintFix(\n                            \"create\",\n                            create_anchor,\n                            [\n                                NewlineSegment(),\n                                WhitespaceSegment(new_indent),\n                            ],\n                        )\n                    )\n                    return fixes\n                raise ValueError(\n                    f\"Unexpected break generated at {self}\"\n                )  # pragma: no cover\n\n        segment_buff = ()\n        whitespace_buff = ()\n        indent_impulse = 0\n        indent_balance = 0\n        is_pause = False\n\n        for seg in segments:\n            if indent_section is None:\n                if seg.is_type(\"whitespace\") or seg.is_meta:\n                    whitespace_buff += (seg,)\n                else:\n                    indent_section = Section(\n                        segments=whitespace_buff,\n                        role=\"indent\",\n                        indent_balance=indent_balance,\n                    )\n                    whitespace_buff = ()\n                    segment_buff = (seg,)\n            else:\n                if seg.is_type(\"whitespace\") or seg.is_meta:\n                    whitespace_buff += (seg,)\n                    if seg.is_meta:\n                        indent_impulse += seg.indent_val\n                else:\n                    # We got something other than whitespace or a meta.\n                    # Have we passed an indent?\n                    if indent_impulse != 0:\n                        # Yes. Bank the section, perhaps also with a content\n                        # section.\n                        if segment_buff:\n                            chunk_buff.append(\n                                Section(\n                                    segments=segment_buff,\n                                    role=\"content\",\n                                    indent_balance=indent_balance,\n                                )\n                            )\n                            segment_buff = ()\n                        # Deal with the whitespace\n                        chunk_buff.append(\n                            Section(\n                                segments=whitespace_buff,\n                                role=\"breakpoint\",\n                                indent_balance=indent_balance,\n                                indent_impulse=indent_impulse,\n                            )\n                        )\n                        whitespace_buff = ()\n                        indent_balance += indent_impulse\n                        indent_impulse = 0\n\n                    # Did we think we were in a pause?\n                    # TODO: Renable binary operator breaks some time in future.\n                    if is_pause:\n                        # We need to end the comma/operator\n                        # (taking any whitespace with it).\n                        chunk_buff.append(\n                            Section(\n                                segments=segment_buff + whitespace_buff,\n                                role=\"pausepoint\",\n                                indent_balance=indent_balance,\n                            )\n                        )\n                        # Start the segment buffer off with this section.\n                        whitespace_buff = ()\n                        segment_buff = (seg,)\n                        is_pause = False\n                    else:\n                        # We're not in a pause (or not in a pause yet)\n                        if seg.name == \"comma\":  # or seg.is_type('binary_operator')\n                            if segment_buff:\n                                # End the previous section, start a comma/operator.\n                                # Any whitespace is added to the segment\n                                # buff to go with the comma.\n                                chunk_buff.append(\n                                    Section(\n                                        segments=segment_buff,\n                                        role=\"content\",\n                                        indent_balance=indent_balance,\n                                    )\n                                )\n                                segment_buff = ()\n\n                            # Having a double comma should be impossible\n                            # but let's deal with that case regardless.\n                            segment_buff += whitespace_buff + (seg,)\n                            whitespace_buff = ()\n                            is_pause = True\n                        else:\n                            # Not in a pause, it's not a comma, were in\n                            # some content.\n                            segment_buff += whitespace_buff + (seg,)\n                            whitespace_buff = ()\n\n        # We're at the end, do we have anything left?\n        if is_pause:\n            role = \"pausepoint\"\n        elif segment_buff:\n            role = \"content\"\n        elif indent_impulse:  # pragma: no cover\n            role = \"breakpoint\"\n        else:\n            raise ValueError(\"Is this possible?\")  # pragma: no cover\n\n        chunk_buff.append(\n            Section(\n                segments=segment_buff + whitespace_buff,\n                role=role,\n                indent_balance=indent_balance,\n            )\n        )\n\n        self.logger.info(\"Sections:\")\n        for idx, sec in enumerate(chunk_buff):\n            self.logger.info(f\"    {idx}: {sec!r}\")\n\n        # How do we prioritise where to work?\n        # First, do we ever go through a negative breakpoint?\n        lowest_bal = min(sec.indent_balance for sec in chunk_buff)\n        split_at = []  # split_at is probably going to be a list.\n        fixes = []\n        if lowest_bal < 0:\n            for sec in chunk_buff:\n                if sec.indent_balance == 0 and sec.indent_impulse < 0:\n                    split_at = [(sec, -1)]\n                    break\n        # Assuming we never go negative, we'll either use a pause\n        # point in the base indent balance, or we'll split out\n        # a section or two using the lowest breakpoints.\n        else:\n            # Look for low level pauses. Additionally, ignore\n            # them if they're a comma at the end of the line,\n            # they're useless for splitting\n            pauses = [\n                sec\n                for sec in chunk_buff\n                if sec.role == \"pausepoint\" and sec.indent_balance == 0\n                # Not the last chunk\n                and sec is not chunk_buff[-1]\n            ]\n            if any(pauses):\n                split_at = [(pause, 0) for pause in pauses]\n            else:\n                # No pauses and no negatives. We should extract\n                # a subsection using the breakpoints.\n\n                # We'll definitely have an up. It's possible that the *down*\n                # might not be on this line, so we have to allow for that case.\n                upbreaks = [\n                    sec\n                    for sec in chunk_buff\n                    if sec.role == \"breakpoint\"\n                    and sec.indent_balance == 0\n                    and sec.indent_impulse > 0\n                ]\n                if not upbreaks:\n                    # No upbreaks?!\n                    # abort\n                    return []\n                # First up break\n                split_at = [(upbreaks[0], 1)]\n                downbreaks = [\n                    sec\n                    for sec in chunk_buff\n                    if sec.role == \"breakpoint\"\n                    and sec.indent_balance + sec.indent_impulse == 0\n                    and sec.indent_impulse < 0\n                ]\n                # First down break where we reach the base\n                if downbreaks:\n                    split_at.append((downbreaks[0], 0))\n                # If no downbreaks then the corresponding downbreak isn't on this line.\n\n        self.logger.info(\"Split at: %s\", split_at)\n\n        fixes = []\n        for split, indent in split_at:\n            if split.segments:\n                fixes += split.generate_fixes_to_coerce(\n                    segments, indent_section, self, indent\n                )\n\n        self.logger.info(\"Fixes: %s\", fixes)\n\n        return fixes\n", "tokens": ["src", "sqlfluff", "rules", "l016", "py", "rule_l016", "def", "_eval_line_for_breaks", "self", "segments", "evaluate", "the", "line", "for", "break", "points", "we", "split", "the", "line", "into", "a", "few", "particular", "sections", "the", "indent", "all", "the", "whitespace", "up", "to", "this", "point", "content", "which", "doesn", "t", "have", "whitespace", "at", "the", "start", "or", "end", "breakpoint", "which", "contains", "indent", "dedent", "and", "potential", "whitespace", "nb", "if", "multiple", "indent", "dedent", "sections", "share", "a", "breakpoint", "then", "they", "will", "occupy", "the", "same", "one", "so", "that", "dealing", "with", "whitespace", "post", "split", "is", "easier", "pausepoint", "which", "is", "a", "comma", "potentially", "surrounded", "by", "whitespace", "this", "is", "for", "potential", "list", "splitting", "once", "split", "we", "ll", "use", "a", "separate", "method", "to", "work", "out", "what", "combinations", "make", "most", "sense", "for", "reflow", "chunk_buff", "indent_section", "none", "class", "section", "def", "__init__", "self", "segments", "role", "indent_balance", "indent_impulse", "0", "self", "segments", "segments", "self", "role", "role", "self", "indent_balance", "indent_balance", "self", "indent_impulse", "indent_impulse", "def", "__repr__", "self", "return", "section", "pos", "role", "indent_balance", "indent_impulse", "segments", "r", "format", "role", "self", "role", "indent_balance", "self", "indent_balance", "indent_impulse", "self", "indent_impulse", "segments", "join", "elem", "raw", "for", "elem", "in", "self", "segments", "pos", "self", "segments", "0", "get_start_point_marker", "if", "self", "segments", "else", "property", "def", "raw", "self", "return", "join", "seg", "raw", "for", "seg", "in", "self", "segments", "staticmethod", "def", "find_segment_at", "segments", "loc", "tuple", "int", "int", "for", "seg", "in", "segments", "if", "not", "seg", "is_meta", "and", "seg", "pos_marker", "working_loc", "loc", "return", "seg", "def", "generate_fixes_to_coerce", "self", "segments", "indent_section", "crawler", "indent", "generate", "a", "list", "of", "fixes", "to", "create", "a", "break", "at", "this", "point", "the", "segments", "argument", "is", "necessary", "to", "extract", "anchors", "from", "the", "existing", "segments", "fixes", "generate", "some", "sample", "indents", "unit_indent", "crawler", "_make_indent", "indent_unit", "crawler", "indent_unit", "tab_space_size", "crawler", "tab_space_size", "indent_p1", "indent_section", "raw", "unit_indent", "if", "unit_indent", "in", "indent_section", "raw", "indent_m1", "indent_section", "raw", "replace", "unit_indent", "1", "else", "indent_m1", "indent_section", "raw", "if", "indent", "0", "new_indent", "indent_p1", "elif", "indent", "0", "new_indent", "indent_m1", "else", "new_indent", "indent_section", "raw", "create_anchor", "self", "find_segment_at", "segments", "self", "segments", "1", "get_end_loc", "if", "self", "role", "pausepoint", "assume", "that", "this", "means", "there", "isn", "t", "a", "breakpoint", "and", "that", "we", "ll", "break", "with", "the", "same", "indent", "as", "the", "existing", "line", "note", "deal", "with", "commas", "and", "binary", "operators", "differently", "here", "maybe", "only", "deal", "with", "commas", "to", "start", "with", "if", "any", "seg", "is_type", "binary_operator", "for", "seg", "in", "self", "segments", "pragma", "no", "cover", "raise", "notimplementederror", "don", "t", "know", "how", "to", "deal", "with", "binary", "operators", "here", "yet", "remove", "any", "existing", "whitespace", "for", "elem", "in", "self", "segments", "if", "not", "elem", "is_meta", "and", "elem", "is_type", "whitespace", "fixes", "append", "lintfix", "delete", "elem", "create", "a", "newline", "and", "a", "similar", "indent", "fixes", "append", "lintfix", "create", "create_anchor", "newlinesegment", "whitespacesegment", "new_indent", "return", "fixes", "if", "self", "role", "breakpoint", "can", "we", "determine", "the", "required", "indent", "just", "from", "the", "info", "in", "this", "segment", "only", "remove", "anything", "which", "is", "already", "here", "for", "elem", "in", "self", "segments", "if", "not", "elem", "is_meta", "fixes", "append", "lintfix", "delete", "elem", "create", "a", "newline", "create", "an", "indent", "of", "the", "relevant", "size", "fixes", "append", "lintfix", "create", "create_anchor", "newlinesegment", "whitespacesegment", "new_indent", "return", "fixes", "raise", "valueerror", "f", "unexpected", "break", "generated", "at", "self", "pragma", "no", "cover", "segment_buff", "whitespace_buff", "indent_impulse", "0", "indent_balance", "0", "is_pause", "false", "for", "seg", "in", "segments", "if", "indent_section", "is", "none", "if", "seg", "is_type", "whitespace", "or", "seg", "is_meta", "whitespace_buff", "seg", "else", "indent_section", "section", "segments", "whitespace_buff", "role", "indent", "indent_balance", "indent_balance", "whitespace_buff", "segment_buff", "seg", "else", "if", "seg", "is_type", "whitespace", "or", "seg", "is_meta", "whitespace_buff", "seg", "if", "seg", "is_meta", "indent_impulse", "seg", "indent_val", "else", "we", "got", "something", "other", "than", "whitespace", "or", "a", "meta", "have", "we", "passed", "an", "indent", "if", "indent_impulse", "0", "yes", "bank", "the", "section", "perhaps", "also", "with", "a", "content", "section", "if", "segment_buff", "chunk_buff", "append", "section", "segments", "segment_buff", "role", "content", "indent_balance", "indent_balance", "segment_buff", "deal", "with", "the", "whitespace", "chunk_buff", "append", "section", "segments", "whitespace_buff", "role", "breakpoint", "indent_balance", "indent_balance", "indent_impulse", "indent_impulse", "whitespace_buff", "indent_balance", "indent_impulse", "indent_impulse", "0", "did", "we", "think", "we", "were", "in", "a", "pause", "todo", "renable", "binary", "operator", "breaks", "some", "time", "in", "future", "if", "is_pause", "we", "need", "to", "end", "the", "comma", "operator", "taking", "any", "whitespace", "with", "it", "chunk_buff", "append", "section", "segments", "segment_buff", "whitespace_buff", "role", "pausepoint", "indent_balance", "indent_balance", "start", "the", "segment", "buffer", "off", "with", "this", "section", "whitespace_buff", "segment_buff", "seg", "is_pause", "false", "else", "we", "re", "not", "in", "a", "pause", "or", "not", "in", "a", "pause", "yet", "if", "seg", "name", "comma", "or", "seg", "is_type", "binary_operator", "if", "segment_buff", "end", "the", "previous", "section", "start", "a", "comma", "operator", "any", "whitespace", "is", "added", "to", "the", "segment", "buff", "to", "go", "with", "the", "comma", "chunk_buff", "append", "section", "segments", "segment_buff", "role", "content", "indent_balance", "indent_balance", "segment_buff", "having", "a", "double", "comma", "should", "be", "impossible", "but", "let", "s", "deal", "with", "that", "case", "regardless", "segment_buff", "whitespace_buff", "seg", "whitespace_buff", "is_pause", "true", "else", "not", "in", "a", "pause", "it", "s", "not", "a", "comma", "were", "in", "some", "content", "segment_buff", "whitespace_buff", "seg", "whitespace_buff", "we", "re", "at", "the", "end", "do", "we", "have", "anything", "left", "if", "is_pause", "role", "pausepoint", "elif", "segment_buff", "role", "content", "elif", "indent_impulse", "pragma", "no", "cover", "role", "breakpoint", "else", "raise", "valueerror", "is", "this", "possible", "pragma", "no", "cover", "chunk_buff", "append", "section", "segments", "segment_buff", "whitespace_buff", "role", "role", "indent_balance", "indent_balance", "self", "logger", "info", "sections", "for", "idx", "sec", "in", "enumerate", "chunk_buff", "self", "logger", "info", "f", "idx", "sec", "r", "how", "do", "we", "prioritise", "where", "to", "work", "first", "do", "we", "ever", "go", "through", "a", "negative", "breakpoint", "lowest_bal", "min", "sec", "indent_balance", "for", "sec", "in", "chunk_buff", "split_at", "split_at", "is", "probably", "going", "to", "be", "a", "list", "fixes", "if", "lowest_bal", "0", "for", "sec", "in", "chunk_buff", "if", "sec", "indent_balance", "0", "and", "sec", "indent_impulse", "0", "split_at", "sec", "1", "break", "assuming", "we", "never", "go", "negative", "we", "ll", "either", "use", "a", "pause", "point", "in", "the", "base", "indent", "balance", "or", "we", "ll", "split", "out", "a", "section", "or", "two", "using", "the", "lowest", "breakpoints", "else", "look", "for", "low", "level", "pauses", "additionally", "ignore", "them", "if", "they", "re", "a", "comma", "at", "the", "end", "of", "the", "line", "they", "re", "useless", "for", "splitting", "pauses", "sec", "for", "sec", "in", "chunk_buff", "if", "sec", "role", "pausepoint", "and", "sec", "indent_balance", "0", "not", "the", "last", "chunk", "and", "sec", "is", "not", "chunk_buff", "1", "if", "any", "pauses", "split_at", "pause", "0", "for", "pause", "in", "pauses", "else", "no", "pauses", "and", "no", "negatives", "we", "should", "extract", "a", "subsection", "using", "the", "breakpoints", "we", "ll", "definitely", "have", "an", "up", "it", "s", "possible", "that", "the", "down", "might", "not", "be", "on", "this", "line", "so", "we", "have", "to", "allow", "for", "that", "case", "upbreaks", "sec", "for", "sec", "in", "chunk_buff", "if", "sec", "role", "breakpoint", "and", "sec", "indent_balance", "0", "and", "sec", "indent_impulse", "0", "if", "not", "upbreaks", "no", "upbreaks", "abort", "return", "first", "up", "break", "split_at", "upbreaks", "0", "1", "downbreaks", "sec", "for", "sec", "in", "chunk_buff", "if", "sec", "role", "breakpoint", "and", "sec", "indent_balance", "sec", "indent_impulse", "0", "and", "sec", "indent_impulse", "0", "first", "down", "break", "where", "we", "reach", "the", "base", "if", "downbreaks", "split_at", "append", "downbreaks", "0", "0", "if", "no", "downbreaks", "then", "the", "corresponding", "downbreak", "isn", "t", "on", "this", "line", "self", "logger", "info", "split", "at", "s", "split_at", "fixes", "for", "split", "indent", "in", "split_at", "if", "split", "segments", "fixes", "split", "generate_fixes_to_coerce", "segments", "indent_section", "self", "indent", "self", "logger", "info", "fixes", "s", "fixes", "return", "fixes"], "doc_len": 1087}
{"doc_id": "src/sqlfluff/rules/L016.py::Rule_L016._gen_line_so_far", "file_path": "src/sqlfluff/rules/L016.py", "class_name": "Rule_L016", "func_name": "_gen_line_so_far", "text": "文件路径: src/sqlfluff/rules/L016.py, 类名: Rule_L016\n    def _gen_line_so_far(raw_stack, initial_buff=None):\n        \"\"\"Work out from the raw stack what the elements on this line are.\n\n        Returns:\n            :obj:`list` of segments\n\n        \"\"\"\n        working_buff = initial_buff or []\n        idx = -1\n        while True:\n            if len(raw_stack) >= abs(idx):\n                s = raw_stack[idx]\n                if s.name == \"newline\":\n                    break\n                else:\n                    working_buff.insert(0, s)\n                    idx -= 1\n            else:\n                break  # pragma: no cover\n        return working_buff\n", "tokens": ["src", "sqlfluff", "rules", "l016", "py", "rule_l016", "def", "_gen_line_so_far", "raw_stack", "initial_buff", "none", "work", "out", "from", "the", "raw", "stack", "what", "the", "elements", "on", "this", "line", "are", "returns", "obj", "list", "of", "segments", "working_buff", "initial_buff", "or", "idx", "1", "while", "true", "if", "len", "raw_stack", "abs", "idx", "s", "raw_stack", "idx", "if", "s", "name", "newline", "break", "else", "working_buff", "insert", "0", "s", "idx", "1", "else", "break", "pragma", "no", "cover", "return", "working_buff"], "doc_len": 63}
{"doc_id": "src/sqlfluff/rules/L016.py::Rule_L016._compute_segment_length", "file_path": "src/sqlfluff/rules/L016.py", "class_name": "Rule_L016", "func_name": "_compute_segment_length", "text": "文件路径: src/sqlfluff/rules/L016.py, 类名: Rule_L016\n    def _compute_segment_length(cls, segment):\n        if segment.is_type(\"newline\"):\n            # Generally, we won't see newlines, but if we do, simply ignore\n            # them. Rationale: The intent of this rule is to enforce maximum\n            # line length, and newlines don't make lines longer.\n            return 0\n\n        if \"\\n\" in segment.pos_marker.source_str():\n            # Similarly we shouldn't see newlines in source segments\n            # However for templated loops it's often not possible to\n            # accurately calculate the segments. These will be caught by\n            # the first iteration of the loop (which is non-templated)\n            # so doesn't suffer from the same bug, so we can ignore these\n            return 0\n\n        # Compute the length of this segments in SOURCE space (before template\n        # expansion).\n        slice_length = (\n            segment.pos_marker.source_slice.stop - segment.pos_marker.source_slice.start\n        )\n        if slice_length:\n            return slice_length\n        else:\n            # If a segment did not originate from the original source, its slice\n            # length slice length will be zero. This occurs, for example, when\n            # other lint rules add indentation or other whitespace. In that\n            # case, compute the length of its contents.\n            return len(segment.raw)\n", "tokens": ["src", "sqlfluff", "rules", "l016", "py", "rule_l016", "def", "_compute_segment_length", "cls", "segment", "if", "segment", "is_type", "newline", "generally", "we", "won", "t", "see", "newlines", "but", "if", "we", "do", "simply", "ignore", "them", "rationale", "the", "intent", "of", "this", "rule", "is", "to", "enforce", "maximum", "line", "length", "and", "newlines", "don", "t", "make", "lines", "longer", "return", "0", "if", "n", "in", "segment", "pos_marker", "source_str", "similarly", "we", "shouldn", "t", "see", "newlines", "in", "source", "segments", "however", "for", "templated", "loops", "it", "s", "often", "not", "possible", "to", "accurately", "calculate", "the", "segments", "these", "will", "be", "caught", "by", "the", "first", "iteration", "of", "the", "loop", "which", "is", "non", "templated", "so", "doesn", "t", "suffer", "from", "the", "same", "bug", "so", "we", "can", "ignore", "these", "return", "0", "compute", "the", "length", "of", "this", "segments", "in", "source", "space", "before", "template", "expansion", "slice_length", "segment", "pos_marker", "source_slice", "stop", "segment", "pos_marker", "source_slice", "start", "if", "slice_length", "return", "slice_length", "else", "if", "a", "segment", "did", "not", "originate", "from", "the", "original", "source", "its", "slice", "length", "slice", "length", "will", "be", "zero", "this", "occurs", "for", "example", "when", "other", "lint", "rules", "add", "indentation", "or", "other", "whitespace", "in", "that", "case", "compute", "the", "length", "of", "its", "contents", "return", "len", "segment", "raw"], "doc_len": 177}
{"doc_id": "src/sqlfluff/rules/L016.py::Rule_L016._compute_source_length", "file_path": "src/sqlfluff/rules/L016.py", "class_name": "Rule_L016", "func_name": "_compute_source_length", "text": "文件路径: src/sqlfluff/rules/L016.py, 类名: Rule_L016\n    def _compute_source_length(cls, segments):\n        line_len = 0\n        seen_slices = set()\n        for segment in segments:\n            slice = (\n                segment.pos_marker.source_slice.start,\n                segment.pos_marker.source_slice.stop,\n            )\n            # Often, a single templated area of a source file will expand to\n            # multiple SQL tokens. Here, we use a set to avoid double counting\n            # the length of that text. For example, in BigQuery, we might\n            # see this source query:\n            #\n            # SELECT user_id\n            # FROM `{{bi_ecommerce_orders}}` {{table_at_job_start}}\n            #\n            # where 'table_at_job_start' is defined as:\n            # \"FOR SYSTEM_TIME AS OF CAST('2021-03-02T01:22:59+00:00' AS TIMESTAMP)\"\n            #\n            # So this one substitution results in roughly 10 segments (one per\n            # word or bit of punctuation). Each of these would have the same\n            # source slice, and if we didn't correct for this, we'd count the\n            # length of {{bi_ecommerce_orders}} roughly 10 times, resulting in\n            # vast overcount of the source length.\n            if slice not in seen_slices:\n                seen_slices.add(slice)\n                line_len += cls._compute_segment_length(segment)\n        return line_len\n", "tokens": ["src", "sqlfluff", "rules", "l016", "py", "rule_l016", "def", "_compute_source_length", "cls", "segments", "line_len", "0", "seen_slices", "set", "for", "segment", "in", "segments", "slice", "segment", "pos_marker", "source_slice", "start", "segment", "pos_marker", "source_slice", "stop", "often", "a", "single", "templated", "area", "of", "a", "source", "file", "will", "expand", "to", "multiple", "sql", "tokens", "here", "we", "use", "a", "set", "to", "avoid", "double", "counting", "the", "length", "of", "that", "text", "for", "example", "in", "bigquery", "we", "might", "see", "this", "source", "query", "select", "user_id", "from", "bi_ecommerce_orders", "table_at_job_start", "where", "table_at_job_start", "is", "defined", "as", "for", "system_time", "as", "of", "cast", "2021", "03", "02t01", "22", "59", "00", "00", "as", "timestamp", "so", "this", "one", "substitution", "results", "in", "roughly", "10", "segments", "one", "per", "word", "or", "bit", "of", "punctuation", "each", "of", "these", "would", "have", "the", "same", "source", "slice", "and", "if", "we", "didn", "t", "correct", "for", "this", "we", "d", "count", "the", "length", "of", "bi_ecommerce_orders", "roughly", "10", "times", "resulting", "in", "vast", "overcount", "of", "the", "source", "length", "if", "slice", "not", "in", "seen_slices", "seen_slices", "add", "slice", "line_len", "cls", "_compute_segment_length", "segment", "return", "line_len"], "doc_len": 155}
{"doc_id": "src/sqlfluff/rules/L016.py::Rule_L016._eval", "file_path": "src/sqlfluff/rules/L016.py", "class_name": "Rule_L016", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L016.py, 类名: Rule_L016\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Line is too long.\n\n        This only triggers on newline segments, evaluating the whole line.\n        The detection is simple, the fixing is much trickier.\n\n        \"\"\"\n        # Config type hints\n        self.max_line_length: int\n        self.ignore_comment_lines: bool\n\n        if context.segment.name == \"newline\":\n            # iterate to buffer the whole line up to this point\n            this_line = self._gen_line_so_far(context.raw_stack, [])\n        else:\n            # Otherwise we're all good\n            return None\n\n        # Now we can work out the line length and deal with the content\n        line_len = self._compute_source_length(this_line)\n        if line_len > self.max_line_length:\n            # Problem, we'll be reporting a violation. The\n            # question is, can we fix it?\n\n            # We'll need the indent, so let's get it for fixing.\n            line_indent = []\n            for s in this_line:\n                if s.name == \"whitespace\":\n                    line_indent.append(s)\n                else:\n                    break\n\n            # Don't even attempt to handle template placeholders as gets\n            # complicated if logic changes (e.g. moving for loops). Most of\n            # these long lines will likely be single line Jinja comments.\n            # They will remain as unfixable.\n            if this_line[-1].type == \"placeholder\":\n                self.logger.info(\"Unfixable template segment: %s\", this_line[-1])\n                return LintResult(anchor=context.segment)\n\n            # Does the line end in an inline comment that we can move back?\n            if this_line[-1].name == \"inline_comment\":\n                # Is this line JUST COMMENT (with optional preceding whitespace) if\n                # so, user will have to fix themselves.\n                if len(this_line) == 1 or all(\n                    elem.name == \"whitespace\" or elem.is_meta for elem in this_line[:-1]\n                ):\n                    self.logger.info(\n                        \"Unfixable inline comment, alone on line: %s\", this_line[-1]\n                    )\n                    if self.ignore_comment_lines:\n                        return LintResult()\n                    else:\n                        return LintResult(anchor=context.segment)\n\n                self.logger.info(\n                    \"Attempting move of inline comment at end of line: %s\",\n                    this_line[-1],\n                )\n                # Set up to delete the original comment and the preceding whitespace\n                delete_buffer = [LintFix(\"delete\", this_line[-1])]\n                idx = -2\n                while True:\n                    if (\n                        len(this_line) >= abs(idx)\n                        and this_line[idx].name == \"whitespace\"\n                    ):\n                        delete_buffer.append(LintFix(\"delete\", this_line[idx]))\n                        idx -= 1\n                    else:\n                        break  # pragma: no cover\n                create_elements = line_indent + [this_line[-1], context.segment]\n                if self._compute_source_length(create_elements) > self.max_line_length:\n                    # The inline comment is NOT on a line by itself, but even if\n                    # we move it onto a line by itself, it's still too long. In\n                    # this case, the rule should do nothing, otherwise it\n                    # triggers an endless cycle of \"fixes\" that simply keeps\n                    # adding blank lines.\n                    self.logger.info(\n                        \"Unfixable inline comment, too long even on a line by itself: %s\",\n                        this_line[-1],\n                    )\n                    if self.ignore_comment_lines:\n                        return LintResult()\n                    else:\n                        return LintResult(anchor=context.segment)\n                # Create a newline before this one with the existing comment, an\n                # identical indent AND a terminating newline, copied from the current\n                # target segment.\n                create_buffer = [LintFix(\"create\", this_line[0], create_elements)]\n                return LintResult(\n                    anchor=context.segment, fixes=delete_buffer + create_buffer\n                )\n\n            fixes = self._eval_line_for_breaks(this_line)\n            if fixes:\n                return LintResult(anchor=context.segment, fixes=fixes)\n            return LintResult(anchor=context.segment)\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l016", "py", "rule_l016", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "line", "is", "too", "long", "this", "only", "triggers", "on", "newline", "segments", "evaluating", "the", "whole", "line", "the", "detection", "is", "simple", "the", "fixing", "is", "much", "trickier", "config", "type", "hints", "self", "max_line_length", "int", "self", "ignore_comment_lines", "bool", "if", "context", "segment", "name", "newline", "iterate", "to", "buffer", "the", "whole", "line", "up", "to", "this", "point", "this_line", "self", "_gen_line_so_far", "context", "raw_stack", "else", "otherwise", "we", "re", "all", "good", "return", "none", "now", "we", "can", "work", "out", "the", "line", "length", "and", "deal", "with", "the", "content", "line_len", "self", "_compute_source_length", "this_line", "if", "line_len", "self", "max_line_length", "problem", "we", "ll", "be", "reporting", "a", "violation", "the", "question", "is", "can", "we", "fix", "it", "we", "ll", "need", "the", "indent", "so", "let", "s", "get", "it", "for", "fixing", "line_indent", "for", "s", "in", "this_line", "if", "s", "name", "whitespace", "line_indent", "append", "s", "else", "break", "don", "t", "even", "attempt", "to", "handle", "template", "placeholders", "as", "gets", "complicated", "if", "logic", "changes", "e", "g", "moving", "for", "loops", "most", "of", "these", "long", "lines", "will", "likely", "be", "single", "line", "jinja", "comments", "they", "will", "remain", "as", "unfixable", "if", "this_line", "1", "type", "placeholder", "self", "logger", "info", "unfixable", "template", "segment", "s", "this_line", "1", "return", "lintresult", "anchor", "context", "segment", "does", "the", "line", "end", "in", "an", "inline", "comment", "that", "we", "can", "move", "back", "if", "this_line", "1", "name", "inline_comment", "is", "this", "line", "just", "comment", "with", "optional", "preceding", "whitespace", "if", "so", "user", "will", "have", "to", "fix", "themselves", "if", "len", "this_line", "1", "or", "all", "elem", "name", "whitespace", "or", "elem", "is_meta", "for", "elem", "in", "this_line", "1", "self", "logger", "info", "unfixable", "inline", "comment", "alone", "on", "line", "s", "this_line", "1", "if", "self", "ignore_comment_lines", "return", "lintresult", "else", "return", "lintresult", "anchor", "context", "segment", "self", "logger", "info", "attempting", "move", "of", "inline", "comment", "at", "end", "of", "line", "s", "this_line", "1", "set", "up", "to", "delete", "the", "original", "comment", "and", "the", "preceding", "whitespace", "delete_buffer", "lintfix", "delete", "this_line", "1", "idx", "2", "while", "true", "if", "len", "this_line", "abs", "idx", "and", "this_line", "idx", "name", "whitespace", "delete_buffer", "append", "lintfix", "delete", "this_line", "idx", "idx", "1", "else", "break", "pragma", "no", "cover", "create_elements", "line_indent", "this_line", "1", "context", "segment", "if", "self", "_compute_source_length", "create_elements", "self", "max_line_length", "the", "inline", "comment", "is", "not", "on", "a", "line", "by", "itself", "but", "even", "if", "we", "move", "it", "onto", "a", "line", "by", "itself", "it", "s", "still", "too", "long", "in", "this", "case", "the", "rule", "should", "do", "nothing", "otherwise", "it", "triggers", "an", "endless", "cycle", "of", "fixes", "that", "simply", "keeps", "adding", "blank", "lines", "self", "logger", "info", "unfixable", "inline", "comment", "too", "long", "even", "on", "a", "line", "by", "itself", "s", "this_line", "1", "if", "self", "ignore_comment_lines", "return", "lintresult", "else", "return", "lintresult", "anchor", "context", "segment", "create", "a", "newline", "before", "this", "one", "with", "the", "existing", "comment", "an", "identical", "indent", "and", "a", "terminating", "newline", "copied", "from", "the", "current", "target", "segment", "create_buffer", "lintfix", "create", "this_line", "0", "create_elements", "return", "lintresult", "anchor", "context", "segment", "fixes", "delete_buffer", "create_buffer", "fixes", "self", "_eval_line_for_breaks", "this_line", "if", "fixes", "return", "lintresult", "anchor", "context", "segment", "fixes", "fixes", "return", "lintresult", "anchor", "context", "segment", "return", "lintresult"], "doc_len": 467}
{"doc_id": "src/sqlfluff/rules/L017.py::Rule_L017._eval", "file_path": "src/sqlfluff/rules/L017.py", "class_name": "Rule_L017", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L017.py, 类名: Rule_L017\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Function name not immediately followed by bracket.\n\n        Look for Function Segment with anything other than the\n        function name before brackets\n        \"\"\"\n        # We only trigger on start_bracket (open parenthesis)\n        if context.segment.is_type(\"function\"):\n            # Look for the function name\n            for fname_idx, seg in enumerate(context.segment.segments):\n                if seg.is_type(\"function_name\"):\n                    break\n\n            # Look for the start bracket\n            for bracket_idx, seg in enumerate(context.segment.segments):\n                if seg.is_type(\"bracketed\"):\n                    break\n\n            if bracket_idx != fname_idx + 1:\n                # It's only safe to fix if there is only whitespace\n                # or newlines in the intervening section.\n                intermediate_segments = context.segment.segments[\n                    fname_idx + 1 : bracket_idx\n                ]\n                if all(\n                    seg.is_type(\"whitespace\", \"newline\")\n                    for seg in intermediate_segments\n                ):\n                    return LintResult(\n                        anchor=intermediate_segments[0],\n                        fixes=[LintFix(\"delete\", seg) for seg in intermediate_segments],\n                    )\n                else:\n                    # It's not all whitespace, just report the error.\n                    return LintResult(\n                        anchor=intermediate_segments[0],\n                    )\n\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l017", "py", "rule_l017", "def", "_eval", "self", "context", "rulecontext", "lintresult", "function", "name", "not", "immediately", "followed", "by", "bracket", "look", "for", "function", "segment", "with", "anything", "other", "than", "the", "function", "name", "before", "brackets", "we", "only", "trigger", "on", "start_bracket", "open", "parenthesis", "if", "context", "segment", "is_type", "function", "look", "for", "the", "function", "name", "for", "fname_idx", "seg", "in", "enumerate", "context", "segment", "segments", "if", "seg", "is_type", "function_name", "break", "look", "for", "the", "start", "bracket", "for", "bracket_idx", "seg", "in", "enumerate", "context", "segment", "segments", "if", "seg", "is_type", "bracketed", "break", "if", "bracket_idx", "fname_idx", "1", "it", "s", "only", "safe", "to", "fix", "if", "there", "is", "only", "whitespace", "or", "newlines", "in", "the", "intervening", "section", "intermediate_segments", "context", "segment", "segments", "fname_idx", "1", "bracket_idx", "if", "all", "seg", "is_type", "whitespace", "newline", "for", "seg", "in", "intermediate_segments", "return", "lintresult", "anchor", "intermediate_segments", "0", "fixes", "lintfix", "delete", "seg", "for", "seg", "in", "intermediate_segments", "else", "it", "s", "not", "all", "whitespace", "just", "report", "the", "error", "return", "lintresult", "anchor", "intermediate_segments", "0", "return", "lintresult"], "doc_len": 148}
{"doc_id": "src/sqlfluff/rules/L018.py::Rule_L018._eval", "file_path": "src/sqlfluff/rules/L018.py", "class_name": "Rule_L018", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L018.py, 类名: Rule_L018\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"WITH clause closing bracket should be aligned with WITH keyword.\n\n        Look for a with clause and evaluate the position of closing brackets.\n        \"\"\"\n        # We only trigger on start_bracket (open parenthesis)\n        if context.segment.is_type(\"with_compound_statement\"):\n            raw_stack_buff = list(context.raw_stack)\n            # Look for the with keyword\n            for seg in context.segment.segments:\n                if seg.name.lower() == \"with\":\n                    seg_line_no = seg.pos_marker.line_no\n                    break\n            else:  # pragma: no cover\n                # This *could* happen if the with statement is unparsable,\n                # in which case then the user will have to fix that first.\n                if any(s.is_type(\"unparsable\") for s in context.segment.segments):\n                    return LintResult()\n                # If it's parsable but we still didn't find a with, then\n                # we should raise that.\n                raise RuntimeError(\"Didn't find WITH keyword!\")\n\n            def indent_size_up_to(segs):\n                seg_buff = []\n                # Get any segments running up to the WITH\n                for elem in reversed(segs):\n                    if elem.is_type(\"newline\"):\n                        break\n                    elif elem.is_meta:\n                        continue\n                    else:\n                        seg_buff.append(elem)\n                # reverse the indent if we have one\n                if seg_buff:\n                    seg_buff = list(reversed(seg_buff))\n                indent_str = \"\".join(seg.raw for seg in seg_buff).replace(\n                    \"\\t\", \" \" * self.tab_space_size\n                )\n                indent_size = len(indent_str)\n                return indent_size, indent_str\n\n            balance = 0\n            with_indent, with_indent_str = indent_size_up_to(raw_stack_buff)\n            for seg in context.segment.iter_segments(\n                expanding=[\"common_table_expression\", \"bracketed\"], pass_through=True\n            ):\n                if seg.name == \"start_bracket\":\n                    balance += 1\n                elif seg.name == \"end_bracket\":\n                    balance -= 1\n                    if balance == 0:\n                        closing_bracket_indent, _ = indent_size_up_to(raw_stack_buff)\n                        indent_diff = closing_bracket_indent - with_indent\n                        # Is indent of closing bracket not the same as\n                        # indent of WITH keyword.\n                        if seg.pos_marker.line_no == seg_line_no:\n                            # Skip if it's the one-line version. That's ok\n                            pass\n                        elif indent_diff < 0:\n                            return LintResult(\n                                anchor=seg,\n                                fixes=[\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        WhitespaceSegment(\" \" * (-indent_diff)),\n                                    )\n                                ],\n                            )\n                        elif indent_diff > 0:\n                            # Is it all whitespace before the bracket on this line?\n                            prev_segs_on_line = [\n                                elem\n                                for elem in context.segment.iter_segments(\n                                    expanding=[\"common_table_expression\", \"bracketed\"],\n                                    pass_through=True,\n                                )\n                                if elem.pos_marker.line_no == seg.pos_marker.line_no\n                                and elem.pos_marker.line_pos < seg.pos_marker.line_pos\n                            ]\n                            if all(\n                                elem.is_type(\"whitespace\") for elem in prev_segs_on_line\n                            ):\n                                # We can move it back, it's all whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment(with_indent_str)],\n                                    )\n                                ] + [\n                                    LintFix(\"delete\", elem)\n                                    for elem in prev_segs_on_line\n                                ]\n                            else:\n                                # We have to move it to a newline\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [\n                                            NewlineSegment(),\n                                            WhitespaceSegment(with_indent_str),\n                                        ],\n                                    )\n                                ]\n                            return LintResult(anchor=seg, fixes=fixes)\n                else:\n                    raw_stack_buff.append(seg)\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l018", "py", "rule_l018", "def", "_eval", "self", "context", "rulecontext", "lintresult", "with", "clause", "closing", "bracket", "should", "be", "aligned", "with", "with", "keyword", "look", "for", "a", "with", "clause", "and", "evaluate", "the", "position", "of", "closing", "brackets", "we", "only", "trigger", "on", "start_bracket", "open", "parenthesis", "if", "context", "segment", "is_type", "with_compound_statement", "raw_stack_buff", "list", "context", "raw_stack", "look", "for", "the", "with", "keyword", "for", "seg", "in", "context", "segment", "segments", "if", "seg", "name", "lower", "with", "seg_line_no", "seg", "pos_marker", "line_no", "break", "else", "pragma", "no", "cover", "this", "could", "happen", "if", "the", "with", "statement", "is", "unparsable", "in", "which", "case", "then", "the", "user", "will", "have", "to", "fix", "that", "first", "if", "any", "s", "is_type", "unparsable", "for", "s", "in", "context", "segment", "segments", "return", "lintresult", "if", "it", "s", "parsable", "but", "we", "still", "didn", "t", "find", "a", "with", "then", "we", "should", "raise", "that", "raise", "runtimeerror", "didn", "t", "find", "with", "keyword", "def", "indent_size_up_to", "segs", "seg_buff", "get", "any", "segments", "running", "up", "to", "the", "with", "for", "elem", "in", "reversed", "segs", "if", "elem", "is_type", "newline", "break", "elif", "elem", "is_meta", "continue", "else", "seg_buff", "append", "elem", "reverse", "the", "indent", "if", "we", "have", "one", "if", "seg_buff", "seg_buff", "list", "reversed", "seg_buff", "indent_str", "join", "seg", "raw", "for", "seg", "in", "seg_buff", "replace", "t", "self", "tab_space_size", "indent_size", "len", "indent_str", "return", "indent_size", "indent_str", "balance", "0", "with_indent", "with_indent_str", "indent_size_up_to", "raw_stack_buff", "for", "seg", "in", "context", "segment", "iter_segments", "expanding", "common_table_expression", "bracketed", "pass_through", "true", "if", "seg", "name", "start_bracket", "balance", "1", "elif", "seg", "name", "end_bracket", "balance", "1", "if", "balance", "0", "closing_bracket_indent", "_", "indent_size_up_to", "raw_stack_buff", "indent_diff", "closing_bracket_indent", "with_indent", "is", "indent", "of", "closing", "bracket", "not", "the", "same", "as", "indent", "of", "with", "keyword", "if", "seg", "pos_marker", "line_no", "seg_line_no", "skip", "if", "it", "s", "the", "one", "line", "version", "that", "s", "ok", "pass", "elif", "indent_diff", "0", "return", "lintresult", "anchor", "seg", "fixes", "lintfix", "create", "seg", "whitespacesegment", "indent_diff", "elif", "indent_diff", "0", "is", "it", "all", "whitespace", "before", "the", "bracket", "on", "this", "line", "prev_segs_on_line", "elem", "for", "elem", "in", "context", "segment", "iter_segments", "expanding", "common_table_expression", "bracketed", "pass_through", "true", "if", "elem", "pos_marker", "line_no", "seg", "pos_marker", "line_no", "and", "elem", "pos_marker", "line_pos", "seg", "pos_marker", "line_pos", "if", "all", "elem", "is_type", "whitespace", "for", "elem", "in", "prev_segs_on_line", "we", "can", "move", "it", "back", "it", "s", "all", "whitespace", "fixes", "lintfix", "create", "seg", "whitespacesegment", "with_indent_str", "lintfix", "delete", "elem", "for", "elem", "in", "prev_segs_on_line", "else", "we", "have", "to", "move", "it", "to", "a", "newline", "fixes", "lintfix", "create", "seg", "newlinesegment", "whitespacesegment", "with_indent_str", "return", "lintresult", "anchor", "seg", "fixes", "fixes", "else", "raw_stack_buff", "append", "seg", "return", "lintresult"], "doc_len": 375}
{"doc_id": "src/sqlfluff/rules/L019.py::Rule_L019._last_comment_seg", "file_path": "src/sqlfluff/rules/L019.py", "class_name": "Rule_L019", "func_name": "_last_comment_seg", "text": "文件路径: src/sqlfluff/rules/L019.py, 类名: Rule_L019\n    def _last_comment_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent comment segment.\n\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_comment:\n                return segment\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l019", "py", "rule_l019", "def", "_last_comment_seg", "raw_stack", "trace", "the", "raw", "stack", "back", "to", "the", "most", "recent", "comment", "segment", "a", "return", "value", "of", "none", "indicates", "no", "code", "segments", "preceding", "the", "current", "position", "for", "segment", "in", "raw_stack", "1", "if", "segment", "is_comment", "return", "segment", "return", "none"], "doc_len": 45}
{"doc_id": "src/sqlfluff/rules/L019.py::Rule_L019._last_code_seg", "file_path": "src/sqlfluff/rules/L019.py", "class_name": "Rule_L019", "func_name": "_last_code_seg", "text": "文件路径: src/sqlfluff/rules/L019.py, 类名: Rule_L019\n    def _last_code_seg(raw_stack):\n        \"\"\"Trace the raw stack back to the most recent code segment.\n\n        A return value of `None` indicates no code segments preceding the current position.\n        \"\"\"\n        for segment in raw_stack[::-1]:\n            if segment.is_code or segment.is_type(\"newline\"):\n                return segment\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l019", "py", "rule_l019", "def", "_last_code_seg", "raw_stack", "trace", "the", "raw", "stack", "back", "to", "the", "most", "recent", "code", "segment", "a", "return", "value", "of", "none", "indicates", "no", "code", "segments", "preceding", "the", "current", "position", "for", "segment", "in", "raw_stack", "1", "if", "segment", "is_code", "or", "segment", "is_type", "newline", "return", "segment", "return", "none"], "doc_len": 49}
{"doc_id": "src/sqlfluff/rules/L019.py::Rule_L019._eval", "file_path": "src/sqlfluff/rules/L019.py", "class_name": "Rule_L019", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L019.py, 类名: Rule_L019\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Enforce comma placement.\n\n        For leading commas we're looking for trailing commas, so\n        we look for newline segments. For trailing commas we're\n        looking for leading commas, so we look for the comma itself.\n\n        We also want to handle proper whitespace removal/addition. We remove\n        any trailing whitespace after the leading comma, when converting a\n        leading comma to a trailing comma. We add whitespace after the leading\n        comma when converting a trailing comma to a leading comma.\n        \"\"\"\n        # Config type hints\n        self.comma_style: str\n\n        if not context.memory:\n            memory: Dict[str, Any] = {\n                # Trailing comma keys\n                #\n                # Do we have a fix in place for removing a leading\n                # comma violation, and inserting a new trailing comma?\n                \"insert_trailing_comma\": False,\n                # A list of whitespace segments that come after a\n                # leading comma violation, to be removed during fixing.\n                \"whitespace_deletions\": None,\n                # The leading comma violation segment to be removed during fixing\n                \"last_leading_comma_seg\": None,\n                # The newline segment where we're going to insert our new trailing\n                # comma during fixing\n                \"anchor_for_new_trailing_comma_seg\": None,\n                #\n                # Leading comma keys\n                #\n                # Do we have a fix in place for removing a trailing\n                # comma violation, and inserting a new leading comma?\n                \"insert_leading_comma\": False,\n                # The trailing comma violation segment to be removed during fixing\n                \"last_trailing_comma_segment\": None,\n            }\n        else:\n            memory = context.memory\n\n        if self.comma_style == \"trailing\":\n            # A comma preceded by a new line == a leading comma\n            if context.segment.is_type(\"comma\"):\n                last_seg = self._last_code_seg(context.raw_stack)\n                if last_seg.is_type(\"newline\"):\n                    # Recorded where the fix should be applied\n                    memory[\"last_leading_comma_seg\"] = context.segment\n                    last_comment_seg = self._last_comment_seg(context.raw_stack)\n                    inline_comment = (\n                        last_comment_seg.pos_marker.line_no\n                        == last_seg.pos_marker.line_no\n                        if last_comment_seg\n                        else False\n                    )\n                    # If we have a comment right before the newline, then anchor\n                    # the fix at the comment instead\n                    memory[\"anchor_for_new_trailing_comma_seg\"] = (\n                        last_seg if not inline_comment else last_comment_seg\n                    )\n                    # Trigger fix routine\n                    memory[\"insert_trailing_comma\"] = True\n                    memory[\"whitespace_deletions\"] = []\n                    return LintResult(memory=memory)\n            # Have we found a leading comma violation?\n            if memory[\"insert_trailing_comma\"]:\n                # Search for trailing whitespace to delete after the leading\n                # comma violation\n                if context.segment.is_type(\"whitespace\"):\n                    memory[\"whitespace_deletions\"] += [context.segment]\n                    return LintResult(memory=memory)\n                else:\n                    # We've run out of whitespace to delete, time to fix\n                    last_leading_comma_seg = memory[\"last_leading_comma_seg\"]\n                    # Scan backwards to find the last code segment, skipping\n                    # over lines that are either entirely blank or just a\n                    # comment. We want to place the comma immediately after it.\n                    last_code_seg = None\n                    while last_code_seg is None or last_code_seg.is_type(\"newline\"):\n                        last_code_seg = self._last_code_seg(\n                            context.raw_stack[\n                                : context.raw_stack.index(\n                                    last_code_seg\n                                    if last_code_seg\n                                    else memory[\"last_leading_comma_seg\"]\n                                )\n                            ]\n                        )\n                    return LintResult(\n                        anchor=last_leading_comma_seg,\n                        description=\"Found leading comma. Expected only trailing.\",\n                        fixes=[\n                            LintFix(\"delete\", last_leading_comma_seg),\n                            *[\n                                LintFix(\"delete\", d)\n                                for d in memory[\"whitespace_deletions\"]\n                            ],\n                            LintFix(\n                                \"edit\",\n                                last_code_seg,\n                                # Reuse the previous leading comma violation to\n                                # create a new trailing comma\n                                [last_code_seg, last_leading_comma_seg],\n                            ),\n                        ],\n                    )\n\n        elif self.comma_style == \"leading\":\n            # A new line preceded by a comma == a trailing comma\n            if context.segment.is_type(\"newline\"):\n                last_seg = self._last_code_seg(context.raw_stack)\n                # no code precedes the current position: no issue\n                if last_seg is None:\n                    return None\n                if last_seg.is_type(\"comma\"):\n                    # Trigger fix routine\n                    memory[\"insert_leading_comma\"] = True\n                    # Record where the fix should be applied\n                    memory[\"last_trailing_comma_segment\"] = last_seg\n                    return LintResult(memory=memory)\n            # Have we found a trailing comma violation?\n            if memory[\"insert_leading_comma\"]:\n                # Only insert the comma here if this isn't a comment/whitespace segment\n                if context.segment.is_code:\n                    last_comma_seg = memory[\"last_trailing_comma_segment\"]\n                    # Create whitespace to insert after the new leading comma\n                    new_whitespace_seg = WhitespaceSegment()\n                    return LintResult(\n                        anchor=last_comma_seg,\n                        description=\"Found trailing comma. Expected only leading.\",\n                        fixes=[\n                            LintFix(\"delete\", anchor=last_comma_seg),\n                            LintFix(\n                                \"create\",\n                                anchor=context.segment,\n                                edit=[last_comma_seg, new_whitespace_seg],\n                            ),\n                        ],\n                    )\n        # Otherwise, no issue\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l019", "py", "rule_l019", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "enforce", "comma", "placement", "for", "leading", "commas", "we", "re", "looking", "for", "trailing", "commas", "so", "we", "look", "for", "newline", "segments", "for", "trailing", "commas", "we", "re", "looking", "for", "leading", "commas", "so", "we", "look", "for", "the", "comma", "itself", "we", "also", "want", "to", "handle", "proper", "whitespace", "removal", "addition", "we", "remove", "any", "trailing", "whitespace", "after", "the", "leading", "comma", "when", "converting", "a", "leading", "comma", "to", "a", "trailing", "comma", "we", "add", "whitespace", "after", "the", "leading", "comma", "when", "converting", "a", "trailing", "comma", "to", "a", "leading", "comma", "config", "type", "hints", "self", "comma_style", "str", "if", "not", "context", "memory", "memory", "dict", "str", "any", "trailing", "comma", "keys", "do", "we", "have", "a", "fix", "in", "place", "for", "removing", "a", "leading", "comma", "violation", "and", "inserting", "a", "new", "trailing", "comma", "insert_trailing_comma", "false", "a", "list", "of", "whitespace", "segments", "that", "come", "after", "a", "leading", "comma", "violation", "to", "be", "removed", "during", "fixing", "whitespace_deletions", "none", "the", "leading", "comma", "violation", "segment", "to", "be", "removed", "during", "fixing", "last_leading_comma_seg", "none", "the", "newline", "segment", "where", "we", "re", "going", "to", "insert", "our", "new", "trailing", "comma", "during", "fixing", "anchor_for_new_trailing_comma_seg", "none", "leading", "comma", "keys", "do", "we", "have", "a", "fix", "in", "place", "for", "removing", "a", "trailing", "comma", "violation", "and", "inserting", "a", "new", "leading", "comma", "insert_leading_comma", "false", "the", "trailing", "comma", "violation", "segment", "to", "be", "removed", "during", "fixing", "last_trailing_comma_segment", "none", "else", "memory", "context", "memory", "if", "self", "comma_style", "trailing", "a", "comma", "preceded", "by", "a", "new", "line", "a", "leading", "comma", "if", "context", "segment", "is_type", "comma", "last_seg", "self", "_last_code_seg", "context", "raw_stack", "if", "last_seg", "is_type", "newline", "recorded", "where", "the", "fix", "should", "be", "applied", "memory", "last_leading_comma_seg", "context", "segment", "last_comment_seg", "self", "_last_comment_seg", "context", "raw_stack", "inline_comment", "last_comment_seg", "pos_marker", "line_no", "last_seg", "pos_marker", "line_no", "if", "last_comment_seg", "else", "false", "if", "we", "have", "a", "comment", "right", "before", "the", "newline", "then", "anchor", "the", "fix", "at", "the", "comment", "instead", "memory", "anchor_for_new_trailing_comma_seg", "last_seg", "if", "not", "inline_comment", "else", "last_comment_seg", "trigger", "fix", "routine", "memory", "insert_trailing_comma", "true", "memory", "whitespace_deletions", "return", "lintresult", "memory", "memory", "have", "we", "found", "a", "leading", "comma", "violation", "if", "memory", "insert_trailing_comma", "search", "for", "trailing", "whitespace", "to", "delete", "after", "the", "leading", "comma", "violation", "if", "context", "segment", "is_type", "whitespace", "memory", "whitespace_deletions", "context", "segment", "return", "lintresult", "memory", "memory", "else", "we", "ve", "run", "out", "of", "whitespace", "to", "delete", "time", "to", "fix", "last_leading_comma_seg", "memory", "last_leading_comma_seg", "scan", "backwards", "to", "find", "the", "last", "code", "segment", "skipping", "over", "lines", "that", "are", "either", "entirely", "blank", "or", "just", "a", "comment", "we", "want", "to", "place", "the", "comma", "immediately", "after", "it", "last_code_seg", "none", "while", "last_code_seg", "is", "none", "or", "last_code_seg", "is_type", "newline", "last_code_seg", "self", "_last_code_seg", "context", "raw_stack", "context", "raw_stack", "index", "last_code_seg", "if", "last_code_seg", "else", "memory", "last_leading_comma_seg", "return", "lintresult", "anchor", "last_leading_comma_seg", "description", "found", "leading", "comma", "expected", "only", "trailing", "fixes", "lintfix", "delete", "last_leading_comma_seg", "lintfix", "delete", "d", "for", "d", "in", "memory", "whitespace_deletions", "lintfix", "edit", "last_code_seg", "reuse", "the", "previous", "leading", "comma", "violation", "to", "create", "a", "new", "trailing", "comma", "last_code_seg", "last_leading_comma_seg", "elif", "self", "comma_style", "leading", "a", "new", "line", "preceded", "by", "a", "comma", "a", "trailing", "comma", "if", "context", "segment", "is_type", "newline", "last_seg", "self", "_last_code_seg", "context", "raw_stack", "no", "code", "precedes", "the", "current", "position", "no", "issue", "if", "last_seg", "is", "none", "return", "none", "if", "last_seg", "is_type", "comma", "trigger", "fix", "routine", "memory", "insert_leading_comma", "true", "record", "where", "the", "fix", "should", "be", "applied", "memory", "last_trailing_comma_segment", "last_seg", "return", "lintresult", "memory", "memory", "have", "we", "found", "a", "trailing", "comma", "violation", "if", "memory", "insert_leading_comma", "only", "insert", "the", "comma", "here", "if", "this", "isn", "t", "a", "comment", "whitespace", "segment", "if", "context", "segment", "is_code", "last_comma_seg", "memory", "last_trailing_comma_segment", "create", "whitespace", "to", "insert", "after", "the", "new", "leading", "comma", "new_whitespace_seg", "whitespacesegment", "return", "lintresult", "anchor", "last_comma_seg", "description", "found", "trailing", "comma", "expected", "only", "leading", "fixes", "lintfix", "delete", "anchor", "last_comma_seg", "lintfix", "create", "anchor", "context", "segment", "edit", "last_comma_seg", "new_whitespace_seg", "otherwise", "no", "issue", "return", "none"], "doc_len": 582}
{"doc_id": "src/sqlfluff/rules/L020.py::Rule_L020._lint_references_and_aliases", "file_path": "src/sqlfluff/rules/L020.py", "class_name": "Rule_L020", "func_name": "_lint_references_and_aliases", "text": "文件路径: src/sqlfluff/rules/L020.py, 类名: Rule_L020\n    def _lint_references_and_aliases(\n        self,\n        table_aliases: List[AliasInfo],\n        standalone_aliases: List[str],\n        references: List[BaseSegment],\n        col_aliases: List[str],\n        using_cols: List[str],\n        parent_select: Optional[BaseSegment],\n    ) -> Optional[List[LintResult]]:\n        \"\"\"Check whether any aliases are duplicates.\n\n        NB: Subclasses of this error should override this function.\n\n        \"\"\"\n        # Are any of the aliases the same?\n        duplicate = set()\n        for a1, a2 in itertools.combinations(table_aliases, 2):\n            # Compare the strings\n            if a1.ref_str == a2.ref_str and a1.ref_str:\n                duplicate.add(a2)\n        if duplicate:\n            return [\n                LintResult(\n                    # Reference the element, not the string.\n                    anchor=aliases.segment,\n                    description=(\n                        \"Duplicate table alias {!r}. Table \" \"aliases should be unique.\"\n                    ).format(aliases.ref_str),\n                )\n                for aliases in duplicate\n            ]\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "rules", "l020", "py", "rule_l020", "def", "_lint_references_and_aliases", "self", "table_aliases", "list", "aliasinfo", "standalone_aliases", "list", "str", "references", "list", "basesegment", "col_aliases", "list", "str", "using_cols", "list", "str", "parent_select", "optional", "basesegment", "optional", "list", "lintresult", "check", "whether", "any", "aliases", "are", "duplicates", "nb", "subclasses", "of", "this", "error", "should", "override", "this", "function", "are", "any", "of", "the", "aliases", "the", "same", "duplicate", "set", "for", "a1", "a2", "in", "itertools", "combinations", "table_aliases", "2", "compare", "the", "strings", "if", "a1", "ref_str", "a2", "ref_str", "and", "a1", "ref_str", "duplicate", "add", "a2", "if", "duplicate", "return", "lintresult", "reference", "the", "element", "not", "the", "string", "anchor", "aliases", "segment", "description", "duplicate", "table", "alias", "r", "table", "aliases", "should", "be", "unique", "format", "aliases", "ref_str", "for", "aliases", "in", "duplicate", "else", "return", "none"], "doc_len": 109}
{"doc_id": "src/sqlfluff/rules/L020.py::Rule_L020._eval", "file_path": "src/sqlfluff/rules/L020.py", "class_name": "Rule_L020", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L020.py, 类名: Rule_L020\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Get References and Aliases and allow linting.\n\n        This rule covers a lot of potential cases of odd usages of\n        references, see the code for each of the potential cases.\n\n        Subclasses of this rule should override the\n        `_lint_references_and_aliases` method.\n        \"\"\"\n        if context.segment.is_type(\"select_statement\"):\n            select_info = get_select_statement_info(context.segment, context.dialect)\n            if not select_info:\n                return None\n\n            # Work out if we have a parent select function\n            parent_select = None\n            for seg in reversed(context.parent_stack):\n                if seg.is_type(\"select_statement\"):\n                    parent_select = seg\n                    break\n\n            # Pass them all to the function that does all the work.\n            # NB: Subclasses of this rules should override the function below\n            return self._lint_references_and_aliases(\n                select_info.table_aliases,\n                select_info.standalone_aliases,\n                select_info.reference_buffer,\n                select_info.col_aliases,\n                select_info.using_cols,\n                parent_select,\n            )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l020", "py", "rule_l020", "def", "_eval", "self", "context", "rulecontext", "evalresulttype", "get", "references", "and", "aliases", "and", "allow", "linting", "this", "rule", "covers", "a", "lot", "of", "potential", "cases", "of", "odd", "usages", "of", "references", "see", "the", "code", "for", "each", "of", "the", "potential", "cases", "subclasses", "of", "this", "rule", "should", "override", "the", "_lint_references_and_aliases", "method", "if", "context", "segment", "is_type", "select_statement", "select_info", "get_select_statement_info", "context", "segment", "context", "dialect", "if", "not", "select_info", "return", "none", "work", "out", "if", "we", "have", "a", "parent", "select", "function", "parent_select", "none", "for", "seg", "in", "reversed", "context", "parent_stack", "if", "seg", "is_type", "select_statement", "parent_select", "seg", "break", "pass", "them", "all", "to", "the", "function", "that", "does", "all", "the", "work", "nb", "subclasses", "of", "this", "rules", "should", "override", "the", "function", "below", "return", "self", "_lint_references_and_aliases", "select_info", "table_aliases", "select_info", "standalone_aliases", "select_info", "reference_buffer", "select_info", "col_aliases", "select_info", "using_cols", "parent_select", "return", "none"], "doc_len": 127}
{"doc_id": "src/sqlfluff/rules/L021.py::Rule_L021._eval", "file_path": "src/sqlfluff/rules/L021.py", "class_name": "Rule_L021", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L021.py, 类名: Rule_L021\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Ambiguous use of DISTINCT in select statement with GROUP BY.\"\"\"\n        if context.segment.is_type(\"select_statement\"):\n            # Do we have a group by clause\n            group_clause = context.segment.get_child(\"groupby_clause\")\n            if not group_clause:\n                return None\n\n            # Do we have the \"DISTINCT\" keyword in the select clause\n            select_clause = context.segment.get_child(\"select_clause\")\n            select_modifier = select_clause.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None\n            select_keywords = select_modifier.get_children(\"keyword\")\n            for kw in select_keywords:\n                if kw.name == \"distinct\":\n                    return LintResult(anchor=kw)\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l021", "py", "rule_l021", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "ambiguous", "use", "of", "distinct", "in", "select", "statement", "with", "group", "by", "if", "context", "segment", "is_type", "select_statement", "do", "we", "have", "a", "group", "by", "clause", "group_clause", "context", "segment", "get_child", "groupby_clause", "if", "not", "group_clause", "return", "none", "do", "we", "have", "the", "distinct", "keyword", "in", "the", "select", "clause", "select_clause", "context", "segment", "get_child", "select_clause", "select_modifier", "select_clause", "get_child", "select_clause_modifier", "if", "not", "select_modifier", "return", "none", "select_keywords", "select_modifier", "get_children", "keyword", "for", "kw", "in", "select_keywords", "if", "kw", "name", "distinct", "return", "lintresult", "anchor", "kw", "return", "none"], "doc_len": 87}
{"doc_id": "src/sqlfluff/rules/L022.py::Rule_L022._eval", "file_path": "src/sqlfluff/rules/L022.py", "class_name": "Rule_L022", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L022.py, 类名: Rule_L022\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Blank line expected but not found after CTE definition.\"\"\"\n        # Config type hints\n        self.comma_style: str\n\n        error_buffer = []\n        if context.segment.is_type(\"with_compound_statement\"):\n            # First we need to find all the commas, the end brackets, the\n            # things that come after that and the blank lines in between.\n\n            # Find all the closing brackets. They are our anchor points.\n            bracket_indices = []\n            expanded_segments = list(\n                context.segment.iter_segments(expanding=[\"common_table_expression\"])\n            )\n            for idx, seg in enumerate(expanded_segments):\n                if seg.is_type(\"bracketed\"):\n                    bracket_indices.append(idx)\n\n            # Work through each point and deal with it individually\n            for bracket_idx in bracket_indices:\n                forward_slice = expanded_segments[bracket_idx:]\n                seg_idx = 1\n                line_idx = 0\n                comma_seg_idx = 0\n                blank_lines = 0\n                comma_line_idx = None\n                line_blank = False\n                comma_style = None\n                line_starts = {}\n                comment_lines = []\n\n                self.logger.info(\n                    \"## CTE closing bracket found at %s, idx: %s. Forward slice: %.20r\",\n                    forward_slice[0].pos_marker,\n                    bracket_idx,\n                    \"\".join(elem.raw for elem in forward_slice),\n                )\n\n                # Work forward to map out the following segments.\n                while (\n                    forward_slice[seg_idx].is_type(\"comma\")\n                    or not forward_slice[seg_idx].is_code\n                ):\n                    if forward_slice[seg_idx].is_type(\"newline\"):\n                        if line_blank:\n                            # It's a blank line!\n                            blank_lines += 1\n                        line_blank = True\n                        line_idx += 1\n                        line_starts[line_idx] = seg_idx + 1\n                    elif forward_slice[seg_idx].is_type(\"comment\"):\n                        # Lines with comments aren't blank\n                        line_blank = False\n                        comment_lines.append(line_idx)\n                    elif forward_slice[seg_idx].is_type(\"comma\"):\n                        # Keep track of where the comma is.\n                        # We'll evaluate it later.\n                        comma_line_idx = line_idx\n                        comma_seg_idx = seg_idx\n                    seg_idx += 1\n\n                # Infer the comma style (NB this could be different for each case!)\n                if comma_line_idx is None:\n                    comma_style = \"final\"\n                elif line_idx == 0:\n                    comma_style = \"oneline\"\n                elif comma_line_idx == 0:\n                    comma_style = \"trailing\"\n                elif comma_line_idx == line_idx:\n                    comma_style = \"leading\"\n                else:\n                    comma_style = \"floating\"\n\n                # Readout of findings\n                self.logger.info(\n                    \"blank_lines: %s, comma_line_idx: %s. final_line_idx: %s, final_seg_idx: %s\",\n                    blank_lines,\n                    comma_line_idx,\n                    line_idx,\n                    seg_idx,\n                )\n                self.logger.info(\n                    \"comma_style: %r, line_starts: %r, comment_lines: %r\",\n                    comma_style,\n                    line_starts,\n                    comment_lines,\n                )\n\n                if blank_lines < 1:\n                    # We've got an issue\n                    self.logger.info(\"!! Found CTE without enough blank lines.\")\n\n                    # Based on the current location of the comma we insert newlines\n                    # to correct the issue.\n                    fix_type = \"create\"  # In most cases we just insert newlines.\n                    if comma_style == \"oneline\":\n                        # Here we respect the target comma style to insert at the relevant point.\n                        if self.comma_style == \"trailing\":\n                            # Add a blank line after the comma\n                            fix_point = forward_slice[comma_seg_idx + 1]\n                            # Optionally here, if the segment we've landed on is\n                            # whitespace then we REPLACE it rather than inserting.\n                            if forward_slice[comma_seg_idx + 1].is_type(\"whitespace\"):\n                                fix_type = \"edit\"\n                        elif self.comma_style == \"leading\":\n                            # Add a blank line before the comma\n                            fix_point = forward_slice[comma_seg_idx]\n                        # In both cases it's a double newline.\n                        num_newlines = 2\n                    else:\n                        # In the following cases we only care which one we're in\n                        # when comments don't get in the way. If they *do*, then\n                        # we just work around them.\n                        if not comment_lines or line_idx - 1 not in comment_lines:\n                            self.logger.info(\"Comment routines not applicable\")\n                            if comma_style in (\"trailing\", \"final\", \"floating\"):\n                                # Detected an existing trailing comma or it's a final CTE,\n                                # OR the comma isn't leading or trailing.\n                                # If the preceding segment is whitespace, replace it\n                                if forward_slice[seg_idx - 1].is_type(\"whitespace\"):\n                                    fix_point = forward_slice[seg_idx - 1]\n                                    fix_type = \"edit\"\n                                else:\n                                    # Otherwise add a single newline before the end content.\n                                    fix_point = forward_slice[seg_idx]\n                            elif comma_style == \"leading\":\n                                # Detected an existing leading comma.\n                                fix_point = forward_slice[comma_seg_idx]\n                        else:\n                            self.logger.info(\"Handling preceding comments\")\n                            offset = 1\n                            while line_idx - offset in comment_lines:\n                                offset += 1\n                            fix_point = forward_slice[\n                                line_starts[line_idx - (offset - 1)]\n                            ]\n                        # Note: There is an edge case where this isn't enough, if\n                        # comments are in strange places, but we'll catch them on\n                        # the next iteration.\n                        num_newlines = 1\n\n                    fixes = [\n                        LintFix(\n                            fix_type,\n                            fix_point,\n                            [NewlineSegment()] * num_newlines,\n                        )\n                    ]\n                    # Create a result, anchored on the start of the next content.\n                    error_buffer.append(\n                        LintResult(anchor=forward_slice[seg_idx], fixes=fixes)\n                    )\n        # Return the buffer if we have one.\n        return error_buffer or None\n", "tokens": ["src", "sqlfluff", "rules", "l022", "py", "rule_l022", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "blank", "line", "expected", "but", "not", "found", "after", "cte", "definition", "config", "type", "hints", "self", "comma_style", "str", "error_buffer", "if", "context", "segment", "is_type", "with_compound_statement", "first", "we", "need", "to", "find", "all", "the", "commas", "the", "end", "brackets", "the", "things", "that", "come", "after", "that", "and", "the", "blank", "lines", "in", "between", "find", "all", "the", "closing", "brackets", "they", "are", "our", "anchor", "points", "bracket_indices", "expanded_segments", "list", "context", "segment", "iter_segments", "expanding", "common_table_expression", "for", "idx", "seg", "in", "enumerate", "expanded_segments", "if", "seg", "is_type", "bracketed", "bracket_indices", "append", "idx", "work", "through", "each", "point", "and", "deal", "with", "it", "individually", "for", "bracket_idx", "in", "bracket_indices", "forward_slice", "expanded_segments", "bracket_idx", "seg_idx", "1", "line_idx", "0", "comma_seg_idx", "0", "blank_lines", "0", "comma_line_idx", "none", "line_blank", "false", "comma_style", "none", "line_starts", "comment_lines", "self", "logger", "info", "cte", "closing", "bracket", "found", "at", "s", "idx", "s", "forward", "slice", "20r", "forward_slice", "0", "pos_marker", "bracket_idx", "join", "elem", "raw", "for", "elem", "in", "forward_slice", "work", "forward", "to", "map", "out", "the", "following", "segments", "while", "forward_slice", "seg_idx", "is_type", "comma", "or", "not", "forward_slice", "seg_idx", "is_code", "if", "forward_slice", "seg_idx", "is_type", "newline", "if", "line_blank", "it", "s", "a", "blank", "line", "blank_lines", "1", "line_blank", "true", "line_idx", "1", "line_starts", "line_idx", "seg_idx", "1", "elif", "forward_slice", "seg_idx", "is_type", "comment", "lines", "with", "comments", "aren", "t", "blank", "line_blank", "false", "comment_lines", "append", "line_idx", "elif", "forward_slice", "seg_idx", "is_type", "comma", "keep", "track", "of", "where", "the", "comma", "is", "we", "ll", "evaluate", "it", "later", "comma_line_idx", "line_idx", "comma_seg_idx", "seg_idx", "seg_idx", "1", "infer", "the", "comma", "style", "nb", "this", "could", "be", "different", "for", "each", "case", "if", "comma_line_idx", "is", "none", "comma_style", "final", "elif", "line_idx", "0", "comma_style", "oneline", "elif", "comma_line_idx", "0", "comma_style", "trailing", "elif", "comma_line_idx", "line_idx", "comma_style", "leading", "else", "comma_style", "floating", "readout", "of", "findings", "self", "logger", "info", "blank_lines", "s", "comma_line_idx", "s", "final_line_idx", "s", "final_seg_idx", "s", "blank_lines", "comma_line_idx", "line_idx", "seg_idx", "self", "logger", "info", "comma_style", "r", "line_starts", "r", "comment_lines", "r", "comma_style", "line_starts", "comment_lines", "if", "blank_lines", "1", "we", "ve", "got", "an", "issue", "self", "logger", "info", "found", "cte", "without", "enough", "blank", "lines", "based", "on", "the", "current", "location", "of", "the", "comma", "we", "insert", "newlines", "to", "correct", "the", "issue", "fix_type", "create", "in", "most", "cases", "we", "just", "insert", "newlines", "if", "comma_style", "oneline", "here", "we", "respect", "the", "target", "comma", "style", "to", "insert", "at", "the", "relevant", "point", "if", "self", "comma_style", "trailing", "add", "a", "blank", "line", "after", "the", "comma", "fix_point", "forward_slice", "comma_seg_idx", "1", "optionally", "here", "if", "the", "segment", "we", "ve", "landed", "on", "is", "whitespace", "then", "we", "replace", "it", "rather", "than", "inserting", "if", "forward_slice", "comma_seg_idx", "1", "is_type", "whitespace", "fix_type", "edit", "elif", "self", "comma_style", "leading", "add", "a", "blank", "line", "before", "the", "comma", "fix_point", "forward_slice", "comma_seg_idx", "in", "both", "cases", "it", "s", "a", "double", "newline", "num_newlines", "2", "else", "in", "the", "following", "cases", "we", "only", "care", "which", "one", "we", "re", "in", "when", "comments", "don", "t", "get", "in", "the", "way", "if", "they", "do", "then", "we", "just", "work", "around", "them", "if", "not", "comment_lines", "or", "line_idx", "1", "not", "in", "comment_lines", "self", "logger", "info", "comment", "routines", "not", "applicable", "if", "comma_style", "in", "trailing", "final", "floating", "detected", "an", "existing", "trailing", "comma", "or", "it", "s", "a", "final", "cte", "or", "the", "comma", "isn", "t", "leading", "or", "trailing", "if", "the", "preceding", "segment", "is", "whitespace", "replace", "it", "if", "forward_slice", "seg_idx", "1", "is_type", "whitespace", "fix_point", "forward_slice", "seg_idx", "1", "fix_type", "edit", "else", "otherwise", "add", "a", "single", "newline", "before", "the", "end", "content", "fix_point", "forward_slice", "seg_idx", "elif", "comma_style", "leading", "detected", "an", "existing", "leading", "comma", "fix_point", "forward_slice", "comma_seg_idx", "else", "self", "logger", "info", "handling", "preceding", "comments", "offset", "1", "while", "line_idx", "offset", "in", "comment_lines", "offset", "1", "fix_point", "forward_slice", "line_starts", "line_idx", "offset", "1", "note", "there", "is", "an", "edge", "case", "where", "this", "isn", "t", "enough", "if", "comments", "are", "in", "strange", "places", "but", "we", "ll", "catch", "them", "on", "the", "next", "iteration", "num_newlines", "1", "fixes", "lintfix", "fix_type", "fix_point", "newlinesegment", "num_newlines", "create", "a", "result", "anchored", "on", "the", "start", "of", "the", "next", "content", "error_buffer", "append", "lintresult", "anchor", "forward_slice", "seg_idx", "fixes", "fixes", "return", "the", "buffer", "if", "we", "have", "one", "return", "error_buffer", "or", "none"], "doc_len": 614}
{"doc_id": "src/sqlfluff/rules/L023.py::Rule_L023._eval", "file_path": "src/sqlfluff/rules/L023.py", "class_name": "Rule_L023", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L023.py, 类名: Rule_L023\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Single whitespace expected in mother segment between pre and post segments.\"\"\"\n        error_buffer: List[LintResult] = []\n        if context.segment.is_type(self.expected_mother_segment_type):\n            last_code = None\n            mid_segs: List[BaseSegment] = []\n            for seg in context.segment.iter_segments(expanding=self.expand_children):\n                if seg.is_code:\n                    if (\n                        last_code\n                        and self.matches_target_tuples(\n                            last_code, [self.pre_segment_identifier]\n                        )\n                        and self.matches_target_tuples(\n                            seg, [self.post_segment_identifier]\n                        )\n                    ):\n                        # Do we actually have the right amount of whitespace?\n                        raw_inner = \"\".join(s.raw for s in mid_segs)\n                        if raw_inner != \" \" and not (\n                            self.allow_newline\n                            and any(s.name == \"newline\" for s in mid_segs)\n                        ):\n                            if not raw_inner:\n                                # There's nothing between. Just add a whitespace\n                                fixes = [\n                                    LintFix(\n                                        \"create\",\n                                        seg,\n                                        [WhitespaceSegment()],\n                                    )\n                                ]\n                            else:\n                                # Don't otherwise suggest a fix for now.\n                                # TODO: Enable more complex fixing here.\n                                fixes = None  # pragma: no cover\n                            error_buffer.append(\n                                LintResult(anchor=last_code, fixes=fixes)\n                            )\n                    mid_segs = []\n                    if not seg.is_meta:\n                        last_code = seg\n                else:\n                    mid_segs.append(seg)\n        return error_buffer or None\n", "tokens": ["src", "sqlfluff", "rules", "l023", "py", "rule_l023", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "single", "whitespace", "expected", "in", "mother", "segment", "between", "pre", "and", "post", "segments", "error_buffer", "list", "lintresult", "if", "context", "segment", "is_type", "self", "expected_mother_segment_type", "last_code", "none", "mid_segs", "list", "basesegment", "for", "seg", "in", "context", "segment", "iter_segments", "expanding", "self", "expand_children", "if", "seg", "is_code", "if", "last_code", "and", "self", "matches_target_tuples", "last_code", "self", "pre_segment_identifier", "and", "self", "matches_target_tuples", "seg", "self", "post_segment_identifier", "do", "we", "actually", "have", "the", "right", "amount", "of", "whitespace", "raw_inner", "join", "s", "raw", "for", "s", "in", "mid_segs", "if", "raw_inner", "and", "not", "self", "allow_newline", "and", "any", "s", "name", "newline", "for", "s", "in", "mid_segs", "if", "not", "raw_inner", "there", "s", "nothing", "between", "just", "add", "a", "whitespace", "fixes", "lintfix", "create", "seg", "whitespacesegment", "else", "don", "t", "otherwise", "suggest", "a", "fix", "for", "now", "todo", "enable", "more", "complex", "fixing", "here", "fixes", "none", "pragma", "no", "cover", "error_buffer", "append", "lintresult", "anchor", "last_code", "fixes", "fixes", "mid_segs", "if", "not", "seg", "is_meta", "last_code", "seg", "else", "mid_segs", "append", "seg", "return", "error_buffer", "or", "none"], "doc_len": 155}
{"doc_id": "src/sqlfluff/rules/L025.py::Rule_L025._lint_references_and_aliases", "file_path": "src/sqlfluff/rules/L025.py", "class_name": "Rule_L025", "func_name": "_lint_references_and_aliases", "text": "文件路径: src/sqlfluff/rules/L025.py, 类名: Rule_L025\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Check all aliased references against tables referenced in the query.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have, keep track of which aliases we refer to.\n        tbl_refs = set()\n        for r in references:\n            tbl_refs.update(\n                tr.part\n                for tr in r.extract_possible_references(\n                    level=r.ObjectReferenceLevel.TABLE\n                )\n            )\n\n        alias: AliasInfo\n        for alias in table_aliases:\n            if alias.aliased and alias.ref_str not in tbl_refs:\n                fixes = [LintFix(\"delete\", alias.alias_expression)]\n                found_alias_segment = False\n                # Walk back to remove indents/whitespaces\n                for segment in reversed(alias.from_expression_element.segments):\n                    if not found_alias_segment:\n                        if segment is alias.alias_expression:\n                            found_alias_segment = True\n                    else:\n                        if (\n                            segment.name == \"whitespace\"\n                            or segment.name == \"newline\"\n                            or segment.is_meta\n                        ):\n                            fixes.append(LintFix(\"delete\", segment))\n                        else:\n                            # Stop once we reach an other, \"regular\" segment.\n                            break\n                violation_buff.append(\n                    LintResult(\n                        anchor=alias.segment,\n                        description=\"Alias {!r} is never used in SELECT statement.\".format(\n                            alias.ref_str\n                        ),\n                        fixes=fixes,\n                    )\n                )\n        return violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l025", "py", "rule_l025", "def", "_lint_references_and_aliases", "self", "table_aliases", "standalone_aliases", "references", "col_aliases", "using_cols", "parent_select", "check", "all", "aliased", "references", "against", "tables", "referenced", "in", "the", "query", "a", "buffer", "to", "keep", "any", "violations", "violation_buff", "check", "all", "the", "references", "that", "we", "have", "keep", "track", "of", "which", "aliases", "we", "refer", "to", "tbl_refs", "set", "for", "r", "in", "references", "tbl_refs", "update", "tr", "part", "for", "tr", "in", "r", "extract_possible_references", "level", "r", "objectreferencelevel", "table", "alias", "aliasinfo", "for", "alias", "in", "table_aliases", "if", "alias", "aliased", "and", "alias", "ref_str", "not", "in", "tbl_refs", "fixes", "lintfix", "delete", "alias", "alias_expression", "found_alias_segment", "false", "walk", "back", "to", "remove", "indents", "whitespaces", "for", "segment", "in", "reversed", "alias", "from_expression_element", "segments", "if", "not", "found_alias_segment", "if", "segment", "is", "alias", "alias_expression", "found_alias_segment", "true", "else", "if", "segment", "name", "whitespace", "or", "segment", "name", "newline", "or", "segment", "is_meta", "fixes", "append", "lintfix", "delete", "segment", "else", "stop", "once", "we", "reach", "an", "other", "regular", "segment", "break", "violation_buff", "append", "lintresult", "anchor", "alias", "segment", "description", "alias", "r", "is", "never", "used", "in", "select", "statement", "format", "alias", "ref_str", "fixes", "fixes", "return", "violation_buff", "or", "none"], "doc_len": 162}
{"doc_id": "src/sqlfluff/rules/L026.py::Rule_L026._is_bad_tbl_ref", "file_path": "src/sqlfluff/rules/L026.py", "class_name": "Rule_L026", "func_name": "_is_bad_tbl_ref", "text": "文件路径: src/sqlfluff/rules/L026.py, 类名: Rule_L026\n    def _is_bad_tbl_ref(table_aliases, parent_select, tbl_ref):\n        \"\"\"Given a table reference, try to find what it's referring to.\"\"\"\n        # Is it referring to one of the table aliases?\n        if tbl_ref[0] in [a.ref_str for a in table_aliases]:\n            # Yes. Therefore okay.\n            return False\n\n        # Not a table alias. It it referring to a correlated subquery?\n        if parent_select:\n            parent_aliases, _ = get_aliases_from_select(parent_select)\n            if parent_aliases and tbl_ref[0] in [a[0] for a in parent_aliases]:\n                # Yes. Therefore okay.\n                return False\n\n        # It's not referring to an alias or a correlated subquery. Looks like a\n        # bad reference (i.e. referring to something unknown.)\n        return True\n", "tokens": ["src", "sqlfluff", "rules", "l026", "py", "rule_l026", "def", "_is_bad_tbl_ref", "table_aliases", "parent_select", "tbl_ref", "given", "a", "table", "reference", "try", "to", "find", "what", "it", "s", "referring", "to", "is", "it", "referring", "to", "one", "of", "the", "table", "aliases", "if", "tbl_ref", "0", "in", "a", "ref_str", "for", "a", "in", "table_aliases", "yes", "therefore", "okay", "return", "false", "not", "a", "table", "alias", "it", "it", "referring", "to", "a", "correlated", "subquery", "if", "parent_select", "parent_aliases", "_", "get_aliases_from_select", "parent_select", "if", "parent_aliases", "and", "tbl_ref", "0", "in", "a", "0", "for", "a", "in", "parent_aliases", "yes", "therefore", "okay", "return", "false", "it", "s", "not", "referring", "to", "an", "alias", "or", "a", "correlated", "subquery", "looks", "like", "a", "bad", "reference", "i", "e", "referring", "to", "something", "unknown", "return", "true"], "doc_len": 105}
{"doc_id": "src/sqlfluff/rules/L026.py::Rule_L026._lint_references_and_aliases", "file_path": "src/sqlfluff/rules/L026.py", "class_name": "Rule_L026", "func_name": "_lint_references_and_aliases", "text": "文件路径: src/sqlfluff/rules/L026.py, 类名: Rule_L026\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        # Check all the references that we have, do they reference present aliases?\n        for r in references:\n            tbl_refs = r.extract_possible_references(level=r.ObjectReferenceLevel.TABLE)\n            if tbl_refs and all(\n                self._is_bad_tbl_ref(table_aliases, parent_select, tbl_ref)\n                for tbl_ref in tbl_refs\n            ):\n                violation_buff.append(\n                    LintResult(\n                        # Return the first segment rather than the string\n                        anchor=tbl_refs[0].segments[0],\n                        description=f\"Reference {r.raw!r} refers to table/view \"\n                        \"not found in the FROM clause or found in parent \"\n                        \"subquery.\",\n                    )\n                )\n        return violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l026", "py", "rule_l026", "def", "_lint_references_and_aliases", "self", "table_aliases", "standalone_aliases", "references", "col_aliases", "using_cols", "parent_select", "a", "buffer", "to", "keep", "any", "violations", "violation_buff", "check", "all", "the", "references", "that", "we", "have", "do", "they", "reference", "present", "aliases", "for", "r", "in", "references", "tbl_refs", "r", "extract_possible_references", "level", "r", "objectreferencelevel", "table", "if", "tbl_refs", "and", "all", "self", "_is_bad_tbl_ref", "table_aliases", "parent_select", "tbl_ref", "for", "tbl_ref", "in", "tbl_refs", "violation_buff", "append", "lintresult", "return", "the", "first", "segment", "rather", "than", "the", "string", "anchor", "tbl_refs", "0", "segments", "0", "description", "f", "reference", "r", "raw", "r", "refers", "to", "table", "view", "not", "found", "in", "the", "from", "clause", "or", "found", "in", "parent", "subquery", "return", "violation_buff", "or", "none"], "doc_len": 99}
{"doc_id": "src/sqlfluff/rules/L026.py::Rule_L026._eval", "file_path": "src/sqlfluff/rules/L026.py", "class_name": "Rule_L026", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L026.py, 类名: Rule_L026\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Override Rule L020 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        # Config type hints\n        self.force_enable: bool\n\n        if context.dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(context=context)\n", "tokens": ["src", "sqlfluff", "rules", "l026", "py", "rule_l026", "def", "_eval", "self", "context", "rulecontext", "evalresulttype", "override", "rule", "l020", "for", "dialects", "that", "use", "structs", "some", "dialects", "use", "structs", "e", "g", "column", "field", "which", "look", "like", "table", "references", "and", "so", "incorrectly", "trigger", "this", "rule", "config", "type", "hints", "self", "force_enable", "bool", "if", "context", "dialect", "name", "in", "bigquery", "and", "not", "self", "force_enable", "return", "lintresult", "return", "super", "_eval", "context", "context"], "doc_len": 62}
{"doc_id": "src/sqlfluff/rules/L027.py::Rule_L027._lint_references_and_aliases", "file_path": "src/sqlfluff/rules/L027.py", "class_name": "Rule_L027", "func_name": "_lint_references_and_aliases", "text": "文件路径: src/sqlfluff/rules/L027.py, 类名: Rule_L027\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        # Do we have more than one? If so, all references should be qualified.\n        if len(table_aliases) <= 1:\n            return None\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        for r in references:\n            this_ref_type = r.qualification()\n            if (\n                this_ref_type == \"unqualified\"\n                and r.raw not in col_aliases\n                and r.raw not in using_cols\n            ):\n                violation_buff.append(\n                    LintResult(\n                        anchor=r,\n                        description=f\"Unqualified reference {r.raw!r} found in \"\n                        \"select with more than one referenced table/view.\",\n                    )\n                )\n\n        return violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l027", "py", "rule_l027", "def", "_lint_references_and_aliases", "self", "table_aliases", "standalone_aliases", "references", "col_aliases", "using_cols", "parent_select", "do", "we", "have", "more", "than", "one", "if", "so", "all", "references", "should", "be", "qualified", "if", "len", "table_aliases", "1", "return", "none", "a", "buffer", "to", "keep", "any", "violations", "violation_buff", "check", "all", "the", "references", "that", "we", "have", "for", "r", "in", "references", "this_ref_type", "r", "qualification", "if", "this_ref_type", "unqualified", "and", "r", "raw", "not", "in", "col_aliases", "and", "r", "raw", "not", "in", "using_cols", "violation_buff", "append", "lintresult", "anchor", "r", "description", "f", "unqualified", "reference", "r", "raw", "r", "found", "in", "select", "with", "more", "than", "one", "referenced", "table", "view", "return", "violation_buff", "or", "none"], "doc_len": 96}
{"doc_id": "src/sqlfluff/rules/L028.py::Rule_L028._lint_references_and_aliases", "file_path": "src/sqlfluff/rules/L028.py", "class_name": "Rule_L028", "func_name": "_lint_references_and_aliases", "text": "文件路径: src/sqlfluff/rules/L028.py, 类名: Rule_L028\n    def _lint_references_and_aliases(\n        self,\n        table_aliases,\n        standalone_aliases,\n        references,\n        col_aliases,\n        using_cols,\n        parent_select,\n    ):\n        \"\"\"Iterate through references and check consistency.\"\"\"\n        # How many aliases are there? If more than one then abort.\n        if len(table_aliases) > 1:\n            return None\n        # A buffer to keep any violations.\n        violation_buff = []\n        # Check all the references that we have.\n        seen_ref_types = set()\n        for ref in references:\n            # We skip any unqualified wildcard references (i.e. *). They shouldn't count.\n            if not ref.is_qualified() and ref.is_type(\"wildcard_identifier\"):\n                continue\n            # Oddball case: Column aliases provided via function calls in by\n            # FROM or JOIN. References to these don't need to be qualified.\n            # Note there could be a table with a column by the same name as\n            # this alias, so avoid bogus warnings by just skipping them\n            # entirely rather than trying to enforce anything.\n            if ref.raw in standalone_aliases:\n                continue\n            this_ref_type = ref.qualification()\n            if self.single_table_references == \"consistent\":\n                if seen_ref_types and this_ref_type not in seen_ref_types:\n                    violation_buff.append(\n                        LintResult(\n                            anchor=ref,\n                            description=f\"{this_ref_type.capitalize()} reference \"\n                            f\"{ref.raw!r} found in single table select which is \"\n                            \"inconsistent with previous references.\",\n                        )\n                    )\n            elif self.single_table_references != this_ref_type:\n                violation_buff.append(\n                    LintResult(\n                        anchor=ref,\n                        description=\"{} reference {!r} found in single table select.\".format(\n                            this_ref_type.capitalize(), ref.raw\n                        ),\n                    )\n                )\n            seen_ref_types.add(this_ref_type)\n\n        return violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l028", "py", "rule_l028", "def", "_lint_references_and_aliases", "self", "table_aliases", "standalone_aliases", "references", "col_aliases", "using_cols", "parent_select", "iterate", "through", "references", "and", "check", "consistency", "how", "many", "aliases", "are", "there", "if", "more", "than", "one", "then", "abort", "if", "len", "table_aliases", "1", "return", "none", "a", "buffer", "to", "keep", "any", "violations", "violation_buff", "check", "all", "the", "references", "that", "we", "have", "seen_ref_types", "set", "for", "ref", "in", "references", "we", "skip", "any", "unqualified", "wildcard", "references", "i", "e", "they", "shouldn", "t", "count", "if", "not", "ref", "is_qualified", "and", "ref", "is_type", "wildcard_identifier", "continue", "oddball", "case", "column", "aliases", "provided", "via", "function", "calls", "in", "by", "from", "or", "join", "references", "to", "these", "don", "t", "need", "to", "be", "qualified", "note", "there", "could", "be", "a", "table", "with", "a", "column", "by", "the", "same", "name", "as", "this", "alias", "so", "avoid", "bogus", "warnings", "by", "just", "skipping", "them", "entirely", "rather", "than", "trying", "to", "enforce", "anything", "if", "ref", "raw", "in", "standalone_aliases", "continue", "this_ref_type", "ref", "qualification", "if", "self", "single_table_references", "consistent", "if", "seen_ref_types", "and", "this_ref_type", "not", "in", "seen_ref_types", "violation_buff", "append", "lintresult", "anchor", "ref", "description", "f", "this_ref_type", "capitalize", "reference", "f", "ref", "raw", "r", "found", "in", "single", "table", "select", "which", "is", "inconsistent", "with", "previous", "references", "elif", "self", "single_table_references", "this_ref_type", "violation_buff", "append", "lintresult", "anchor", "ref", "description", "reference", "r", "found", "in", "single", "table", "select", "format", "this_ref_type", "capitalize", "ref", "raw", "seen_ref_types", "add", "this_ref_type", "return", "violation_buff", "or", "none"], "doc_len": 206}
{"doc_id": "src/sqlfluff/rules/L028.py::Rule_L028._eval", "file_path": "src/sqlfluff/rules/L028.py", "class_name": "Rule_L028", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L028.py, 类名: Rule_L028\n    def _eval(self, context: RuleContext) -> EvalResultType:\n        \"\"\"Override Rule L025 for dialects that use structs.\n\n        Some dialects use structs (e.g. column.field) which look like\n        table references and so incorrectly trigger this rule.\n        \"\"\"\n        # Config type hints\n        self.force_enable: bool\n\n        if context.dialect.name in [\"bigquery\"] and not self.force_enable:\n            return LintResult()\n\n        return super()._eval(context=context)\n", "tokens": ["src", "sqlfluff", "rules", "l028", "py", "rule_l028", "def", "_eval", "self", "context", "rulecontext", "evalresulttype", "override", "rule", "l025", "for", "dialects", "that", "use", "structs", "some", "dialects", "use", "structs", "e", "g", "column", "field", "which", "look", "like", "table", "references", "and", "so", "incorrectly", "trigger", "this", "rule", "config", "type", "hints", "self", "force_enable", "bool", "if", "context", "dialect", "name", "in", "bigquery", "and", "not", "self", "force_enable", "return", "lintresult", "return", "super", "_eval", "context", "context"], "doc_len": 62}
{"doc_id": "src/sqlfluff/rules/L029.py::Rule_L029._eval", "file_path": "src/sqlfluff/rules/L029.py", "class_name": "Rule_L029", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L029.py, 类名: Rule_L029\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Keywords should not be used as identifiers.\"\"\"\n        if (\n            context.segment.name == \"naked_identifier\"\n            and unquoted_ids_policy_applicable(\n                self.unquoted_identifiers_policy, context.parent_stack  # type: ignore\n            )\n            and (\n                context.segment.raw.upper()\n                in context.dialect.sets(\"unreserved_keywords\")\n            )\n        ):\n            return LintResult(anchor=context.segment)\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "rules", "l029", "py", "rule_l029", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "keywords", "should", "not", "be", "used", "as", "identifiers", "if", "context", "segment", "name", "naked_identifier", "and", "unquoted_ids_policy_applicable", "self", "unquoted_identifiers_policy", "context", "parent_stack", "type", "ignore", "and", "context", "segment", "raw", "upper", "in", "context", "dialect", "sets", "unreserved_keywords", "return", "lintresult", "anchor", "context", "segment", "else", "return", "none"], "doc_len": 51}
{"doc_id": "src/sqlfluff/rules/L030.py::Rule_L030._get_fix", "file_path": "src/sqlfluff/rules/L030.py", "class_name": "Rule_L030", "func_name": "_get_fix", "text": "文件路径: src/sqlfluff/rules/L030.py, 类名: Rule_L030\n    def _get_fix(self, segment, fixed_raw):\n        return super()._get_fix(segment, fixed_raw)\n", "tokens": ["src", "sqlfluff", "rules", "l030", "py", "rule_l030", "def", "_get_fix", "self", "segment", "fixed_raw", "return", "super", "_get_fix", "segment", "fixed_raw"], "doc_len": 16}
{"doc_id": "src/sqlfluff/rules/L031.py::Rule_L031._eval", "file_path": "src/sqlfluff/rules/L031.py", "class_name": "Rule_L031", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L031.py, 类名: Rule_L031\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Identify aliases in from clause and join conditions.\n\n        Find base table, table expressions in join, and other expressions in select clause\n        and decide if it's needed to report them.\n        \"\"\"\n        if context.segment.is_type(\"select_statement\"):\n            # A buffer for all table expressions in join conditions\n            from_expression_elements = []\n            column_reference_segments = []\n\n            from_clause_segment = context.segment.get_child(\"from_clause\")\n\n            if not from_clause_segment:\n                return None\n\n            from_expression = from_clause_segment.get_child(\"from_expression\")\n            from_expression_element = None\n            if from_expression:\n                from_expression_element = from_expression.get_child(\n                    \"from_expression_element\"\n                )\n\n            if not from_expression_element:\n                return None\n            from_expression_element = from_expression_element.get_child(\n                \"table_expression\"\n            )\n\n            # Find base table\n            base_table = None\n            if from_expression_element:\n                base_table = from_expression_element.get_child(\"object_reference\")\n\n            from_clause_index = context.segment.segments.index(from_clause_segment)\n            from_clause_and_after = context.segment.segments[from_clause_index:]\n\n            for clause in from_clause_and_after:\n                for from_expression_element in clause.recursive_crawl(\n                    \"from_expression_element\"\n                ):\n                    from_expression_elements.append(from_expression_element)\n                for column_reference in clause.recursive_crawl(\"column_reference\"):\n                    column_reference_segments.append(column_reference)\n\n            return (\n                self._lint_aliases_in_join(\n                    base_table,\n                    from_expression_elements,\n                    column_reference_segments,\n                    context.segment,\n                )\n                or None\n            )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l031", "py", "rule_l031", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "identify", "aliases", "in", "from", "clause", "and", "join", "conditions", "find", "base", "table", "table", "expressions", "in", "join", "and", "other", "expressions", "in", "select", "clause", "and", "decide", "if", "it", "s", "needed", "to", "report", "them", "if", "context", "segment", "is_type", "select_statement", "a", "buffer", "for", "all", "table", "expressions", "in", "join", "conditions", "from_expression_elements", "column_reference_segments", "from_clause_segment", "context", "segment", "get_child", "from_clause", "if", "not", "from_clause_segment", "return", "none", "from_expression", "from_clause_segment", "get_child", "from_expression", "from_expression_element", "none", "if", "from_expression", "from_expression_element", "from_expression", "get_child", "from_expression_element", "if", "not", "from_expression_element", "return", "none", "from_expression_element", "from_expression_element", "get_child", "table_expression", "find", "base", "table", "base_table", "none", "if", "from_expression_element", "base_table", "from_expression_element", "get_child", "object_reference", "from_clause_index", "context", "segment", "segments", "index", "from_clause_segment", "from_clause_and_after", "context", "segment", "segments", "from_clause_index", "for", "clause", "in", "from_clause_and_after", "for", "from_expression_element", "in", "clause", "recursive_crawl", "from_expression_element", "from_expression_elements", "append", "from_expression_element", "for", "column_reference", "in", "clause", "recursive_crawl", "column_reference", "column_reference_segments", "append", "column_reference", "return", "self", "_lint_aliases_in_join", "base_table", "from_expression_elements", "column_reference_segments", "context", "segment", "or", "none", "return", "none"], "doc_len": 146}
{"doc_id": "src/sqlfluff/rules/L031.py::Rule_L031._filter_table_expressions", "file_path": "src/sqlfluff/rules/L031.py", "class_name": "Rule_L031", "func_name": "_filter_table_expressions", "text": "文件路径: src/sqlfluff/rules/L031.py, 类名: Rule_L031\n    def _filter_table_expressions(\n        cls, base_table, from_expression_elements\n    ) -> Generator[TableAliasInfo, None, None]:\n        for from_expression in from_expression_elements:\n            table_expression = from_expression.get_child(\"table_expression\")\n            if not table_expression:\n                continue\n            table_ref = table_expression.get_child(\"object_reference\")\n\n            # If the from_expression_element has no object_references - skip it\n            # An example case is a lateral flatten, where we have a function segment\n            # instead of a table_reference segment.\n            if not table_ref:\n                continue\n\n            # If this is self-join - skip it\n            if (\n                base_table\n                and base_table.raw == table_ref.raw\n                and base_table != table_ref\n            ):\n                continue\n\n            whitespace_ref = from_expression.get_child(\"whitespace\")\n\n            # If there's no alias expression - skip it\n            alias_exp_ref = from_expression.get_child(\"alias_expression\")\n            if alias_exp_ref is None:\n                continue\n\n            alias_identifier_ref = alias_exp_ref.get_child(\"identifier\")\n            yield cls.TableAliasInfo(\n                table_ref, whitespace_ref, alias_exp_ref, alias_identifier_ref\n            )\n", "tokens": ["src", "sqlfluff", "rules", "l031", "py", "rule_l031", "def", "_filter_table_expressions", "cls", "base_table", "from_expression_elements", "generator", "tablealiasinfo", "none", "none", "for", "from_expression", "in", "from_expression_elements", "table_expression", "from_expression", "get_child", "table_expression", "if", "not", "table_expression", "continue", "table_ref", "table_expression", "get_child", "object_reference", "if", "the", "from_expression_element", "has", "no", "object_references", "skip", "it", "an", "example", "case", "is", "a", "lateral", "flatten", "where", "we", "have", "a", "function", "segment", "instead", "of", "a", "table_reference", "segment", "if", "not", "table_ref", "continue", "if", "this", "is", "self", "join", "skip", "it", "if", "base_table", "and", "base_table", "raw", "table_ref", "raw", "and", "base_table", "table_ref", "continue", "whitespace_ref", "from_expression", "get_child", "whitespace", "if", "there", "s", "no", "alias", "expression", "skip", "it", "alias_exp_ref", "from_expression", "get_child", "alias_expression", "if", "alias_exp_ref", "is", "none", "continue", "alias_identifier_ref", "alias_exp_ref", "get_child", "identifier", "yield", "cls", "tablealiasinfo", "table_ref", "whitespace_ref", "alias_exp_ref", "alias_identifier_ref"], "doc_len": 111}
{"doc_id": "src/sqlfluff/rules/L031.py::Rule_L031._lint_aliases_in_join", "file_path": "src/sqlfluff/rules/L031.py", "class_name": "Rule_L031", "func_name": "_lint_aliases_in_join", "text": "文件路径: src/sqlfluff/rules/L031.py, 类名: Rule_L031\n    def _lint_aliases_in_join(\n        self, base_table, from_expression_elements, column_reference_segments, segment\n    ):\n        \"\"\"Lint and fix all aliases in joins - except for self-joins.\"\"\"\n        # A buffer to keep any violations.\n        violation_buff = []\n\n        to_check = list(\n            self._filter_table_expressions(base_table, from_expression_elements)\n        )\n\n        # How many times does each table appear in the FROM clause?\n        table_counts = Counter(ai.table_ref.raw for ai in to_check)\n\n        # What is the set of aliases used for each table? (We are mainly\n        # interested in the NUMBER of different aliases used.)\n        table_aliases = defaultdict(set)\n        for ai in to_check:\n            table_aliases[ai.table_ref.raw].add(ai.alias_identifier_ref.raw)\n\n        # For each aliased table, check whether to keep or remove it.\n        for alias_info in to_check:\n            # If the same table appears more than once in the FROM clause with\n            # different alias names, do not consider removing its aliases.\n            # The aliases may have been introduced simply to make each\n            # occurrence of the table independent within the query.\n            if (\n                table_counts[alias_info.table_ref.raw] > 1\n                and len(table_aliases[alias_info.table_ref.raw]) > 1\n            ):\n                continue\n\n            select_clause = segment.get_child(\"select_clause\")\n\n            ids_refs = []\n\n            # Find all references to alias in select clause\n            alias_name = alias_info.alias_identifier_ref.raw\n            for alias_with_column in select_clause.recursive_crawl(\"object_reference\"):\n                used_alias_ref = alias_with_column.get_child(\"identifier\")\n                if used_alias_ref and used_alias_ref.raw == alias_name:\n                    ids_refs.append(used_alias_ref)\n\n            # Find all references to alias in column references\n            for exp_ref in column_reference_segments:\n                used_alias_ref = exp_ref.get_child(\"identifier\")\n                # exp_ref.get_child('dot') ensures that the column reference includes a table reference\n                if (\n                    used_alias_ref\n                    and used_alias_ref.raw == alias_name\n                    and exp_ref.get_child(\"dot\")\n                ):\n                    ids_refs.append(used_alias_ref)\n\n            # Fixes for deleting ` as sth` and for editing references to aliased tables\n            fixes = [\n                *[\n                    LintFix(\"delete\", d)\n                    for d in [alias_info.alias_exp_ref, alias_info.whitespace_ref]\n                ],\n                *[\n                    LintFix(\"edit\", alias, alias.edit(alias_info.table_ref.raw))\n                    for alias in [alias_info.alias_identifier_ref, *ids_refs]\n                ],\n            ]\n\n            violation_buff.append(\n                LintResult(\n                    anchor=alias_info.alias_identifier_ref,\n                    description=\"Avoid aliases in from clauses and join conditions.\",\n                    fixes=fixes,\n                )\n            )\n\n        return violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l031", "py", "rule_l031", "def", "_lint_aliases_in_join", "self", "base_table", "from_expression_elements", "column_reference_segments", "segment", "lint", "and", "fix", "all", "aliases", "in", "joins", "except", "for", "self", "joins", "a", "buffer", "to", "keep", "any", "violations", "violation_buff", "to_check", "list", "self", "_filter_table_expressions", "base_table", "from_expression_elements", "how", "many", "times", "does", "each", "table", "appear", "in", "the", "from", "clause", "table_counts", "counter", "ai", "table_ref", "raw", "for", "ai", "in", "to_check", "what", "is", "the", "set", "of", "aliases", "used", "for", "each", "table", "we", "are", "mainly", "interested", "in", "the", "number", "of", "different", "aliases", "used", "table_aliases", "defaultdict", "set", "for", "ai", "in", "to_check", "table_aliases", "ai", "table_ref", "raw", "add", "ai", "alias_identifier_ref", "raw", "for", "each", "aliased", "table", "check", "whether", "to", "keep", "or", "remove", "it", "for", "alias_info", "in", "to_check", "if", "the", "same", "table", "appears", "more", "than", "once", "in", "the", "from", "clause", "with", "different", "alias", "names", "do", "not", "consider", "removing", "its", "aliases", "the", "aliases", "may", "have", "been", "introduced", "simply", "to", "make", "each", "occurrence", "of", "the", "table", "independent", "within", "the", "query", "if", "table_counts", "alias_info", "table_ref", "raw", "1", "and", "len", "table_aliases", "alias_info", "table_ref", "raw", "1", "continue", "select_clause", "segment", "get_child", "select_clause", "ids_refs", "find", "all", "references", "to", "alias", "in", "select", "clause", "alias_name", "alias_info", "alias_identifier_ref", "raw", "for", "alias_with_column", "in", "select_clause", "recursive_crawl", "object_reference", "used_alias_ref", "alias_with_column", "get_child", "identifier", "if", "used_alias_ref", "and", "used_alias_ref", "raw", "alias_name", "ids_refs", "append", "used_alias_ref", "find", "all", "references", "to", "alias", "in", "column", "references", "for", "exp_ref", "in", "column_reference_segments", "used_alias_ref", "exp_ref", "get_child", "identifier", "exp_ref", "get_child", "dot", "ensures", "that", "the", "column", "reference", "includes", "a", "table", "reference", "if", "used_alias_ref", "and", "used_alias_ref", "raw", "alias_name", "and", "exp_ref", "get_child", "dot", "ids_refs", "append", "used_alias_ref", "fixes", "for", "deleting", "as", "sth", "and", "for", "editing", "references", "to", "aliased", "tables", "fixes", "lintfix", "delete", "d", "for", "d", "in", "alias_info", "alias_exp_ref", "alias_info", "whitespace_ref", "lintfix", "edit", "alias", "alias", "edit", "alias_info", "table_ref", "raw", "for", "alias", "in", "alias_info", "alias_identifier_ref", "ids_refs", "violation_buff", "append", "lintresult", "anchor", "alias_info", "alias_identifier_ref", "description", "avoid", "aliases", "in", "from", "clauses", "and", "join", "conditions", "fixes", "fixes", "return", "violation_buff", "or", "none"], "doc_len": 297}
{"doc_id": "src/sqlfluff/rules/L032.py::Rule_L032._eval", "file_path": "src/sqlfluff/rules/L032.py", "class_name": "Rule_L032", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L032.py, 类名: Rule_L032\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Look for USING in a join clause.\"\"\"\n        if context.segment.is_type(\"join_clause\"):\n            for seg in context.segment.segments:\n                if seg.is_type(\"keyword\") and seg.name == \"using\":\n                    return [\n                        LintResult(\n                            # Reference the element, not the string.\n                            anchor=seg,\n                            description=(\n                                \"Found USING statement. Expected only ON statements.\"\n                            ),\n                        )\n                    ]\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l032", "py", "rule_l032", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "look", "for", "using", "in", "a", "join", "clause", "if", "context", "segment", "is_type", "join_clause", "for", "seg", "in", "context", "segment", "segments", "if", "seg", "is_type", "keyword", "and", "seg", "name", "using", "return", "lintresult", "reference", "the", "element", "not", "the", "string", "anchor", "seg", "description", "found", "using", "statement", "expected", "only", "on", "statements", "return", "none"], "doc_len": 60}
{"doc_id": "src/sqlfluff/rules/L033.py::Rule_L033._eval", "file_path": "src/sqlfluff/rules/L033.py", "class_name": "Rule_L033", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L033.py, 类名: Rule_L033\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Look for UNION keyword not immediately followed by DISTINCT or ALL.\n\n        Note that UNION DISTINCT is valid, rule only applies to bare UNION.\n        The function does this by looking for a segment of type set_operator\n        which has a UNION but no DISTINCT or ALL.\n\n        Note only some dialects have concept of UNION DISTINCT, so rule is only\n        applied to dialects that are known to support this syntax.\n        \"\"\"\n        if context.dialect.name not in [\"ansi\", \"bigquery\", \"hive\", \"mysql\"]:\n            return LintResult()\n\n        if context.segment.is_type(\"set_operator\"):\n            if \"union\" in context.segment.raw and not (\n                \"ALL\" in context.segment.raw.upper()\n                or \"DISTINCT\" in context.segment.raw.upper()\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[\n                        LintFix(\n                            \"edit\",\n                            context.segment.segments[0],\n                            [\n                                KeywordSegment(\"union\"),\n                                WhitespaceSegment(),\n                                KeywordSegment(\"distinct\"),\n                            ],\n                        )\n                    ],\n                )\n            elif \"UNION\" in context.segment.raw.upper() and not (\n                \"ALL\" in context.segment.raw.upper()\n                or \"DISTINCT\" in context.segment.raw.upper()\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[\n                        LintFix(\n                            \"edit\",\n                            context.segment.segments[0],\n                            [\n                                KeywordSegment(\"UNION\"),\n                                WhitespaceSegment(),\n                                KeywordSegment(\"DISTINCT\"),\n                            ],\n                        )\n                    ],\n                )\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l033", "py", "rule_l033", "def", "_eval", "self", "context", "rulecontext", "lintresult", "look", "for", "union", "keyword", "not", "immediately", "followed", "by", "distinct", "or", "all", "note", "that", "union", "distinct", "is", "valid", "rule", "only", "applies", "to", "bare", "union", "the", "function", "does", "this", "by", "looking", "for", "a", "segment", "of", "type", "set_operator", "which", "has", "a", "union", "but", "no", "distinct", "or", "all", "note", "only", "some", "dialects", "have", "concept", "of", "union", "distinct", "so", "rule", "is", "only", "applied", "to", "dialects", "that", "are", "known", "to", "support", "this", "syntax", "if", "context", "dialect", "name", "not", "in", "ansi", "bigquery", "hive", "mysql", "return", "lintresult", "if", "context", "segment", "is_type", "set_operator", "if", "union", "in", "context", "segment", "raw", "and", "not", "all", "in", "context", "segment", "raw", "upper", "or", "distinct", "in", "context", "segment", "raw", "upper", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "context", "segment", "segments", "0", "keywordsegment", "union", "whitespacesegment", "keywordsegment", "distinct", "elif", "union", "in", "context", "segment", "raw", "upper", "and", "not", "all", "in", "context", "segment", "raw", "upper", "or", "distinct", "in", "context", "segment", "raw", "upper", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "context", "segment", "segments", "0", "keywordsegment", "union", "whitespacesegment", "keywordsegment", "distinct", "return", "lintresult"], "doc_len": 175}
{"doc_id": "src/sqlfluff/rules/L034.py::Rule_L034._validate", "file_path": "src/sqlfluff/rules/L034.py", "class_name": "Rule_L034", "func_name": "_validate", "text": "文件路径: src/sqlfluff/rules/L034.py, 类名: Rule_L034\n    def _validate(self, i: int, segment: BaseSegment) -> None:\n        # Check if we've seen a more complex select target element already\n        if self.seen_band_elements[i + 1 : :] != [[]] * len(\n            self.seen_band_elements[i + 1 : :]\n        ):\n            # Found a violation (i.e. a simpler element that *follows* a more\n            # complex element.\n            self.violation_exists = True\n        self.current_element_band: Optional[int] = i\n        self.seen_band_elements[i].append(segment)\n", "tokens": ["src", "sqlfluff", "rules", "l034", "py", "rule_l034", "def", "_validate", "self", "i", "int", "segment", "basesegment", "none", "check", "if", "we", "ve", "seen", "a", "more", "complex", "select", "target", "element", "already", "if", "self", "seen_band_elements", "i", "1", "len", "self", "seen_band_elements", "i", "1", "found", "a", "violation", "i", "e", "a", "simpler", "element", "that", "follows", "a", "more", "complex", "element", "self", "violation_exists", "true", "self", "current_element_band", "optional", "int", "i", "self", "seen_band_elements", "i", "append", "segment"], "doc_len": 63}
{"doc_id": "src/sqlfluff/rules/L034.py::Rule_L034._eval", "file_path": "src/sqlfluff/rules/L034.py", "class_name": "Rule_L034", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L034.py, 类名: Rule_L034\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        self.violation_buff = []\n        self.violation_exists = False\n        # Bands of select targets in order to be enforced\n        select_element_order_preference = (\n            (\"wildcard_expression\",),\n            (\n                \"object_reference\",\n                \"literal\",\n                \"cast_expression\",\n                (\"function\", \"cast\"),\n                (\"expression\", \"cast_expression\"),\n            ),\n        )\n\n        # Track which bands have been seen, with additional empty list for the non-matching elements\n        # If we find a matching target element, we append the element to the corresponding index\n        self.seen_band_elements: List[List[BaseSegment]] = [[] for _ in select_element_order_preference] + [[]]  # type: ignore\n\n        if context.segment.is_type(\"select_clause\"):\n            # Ignore select clauses which belong to:\n            # - set expression, which is most commonly a union\n            # - insert_statement\n            # - create table statement\n            #\n            # In each of these contexts, the order of columns in a select should\n            # be preserved.\n            if len(context.parent_stack) >= 2 and context.parent_stack[-2].is_type(\n                \"insert_statement\", \"set_expression\"\n            ):\n                return None\n            if len(context.parent_stack) >= 3 and context.parent_stack[-3].is_type(\n                \"create_table_statement\"\n            ):\n                return None\n\n            select_clause_segment = context.segment\n            select_target_elements = context.segment.get_children(\n                \"select_clause_element\"\n            )\n            if not select_target_elements:\n                return None\n\n            # Iterate through all the select targets to find any order violations\n            for segment in select_target_elements:\n                # The band index of the current segment in select_element_order_preference\n                self.current_element_band = None\n\n                # Compare the segment to the bands in select_element_order_preference\n                for i, band in enumerate(select_element_order_preference):\n                    for e in band:\n                        # Identify simple select target\n                        if segment.get_child(e):\n                            self._validate(i, segment)\n\n                        # Identify function\n                        elif type(e) == tuple and e[0] == \"function\":\n                            try:\n                                if (\n                                    segment.get_child(\"function\")\n                                    .get_child(\"function_name\")\n                                    .raw\n                                    == e[1]\n                                ):\n                                    self._validate(i, segment)\n                            except AttributeError:\n                                # If the segment doesn't match\n                                pass\n\n                        # Identify simple expression\n                        elif type(e) == tuple and e[0] == \"expression\":\n                            try:\n                                if (\n                                    segment.get_child(\"expression\").get_child(e[1])\n                                    and segment.get_child(\"expression\").segments[0].type\n                                    in (\n                                        \"column_reference\",\n                                        \"object_reference\",\n                                        \"literal\",\n                                    )\n                                    # len == 2 to ensure the expression is 'simple'\n                                    and len(segment.get_child(\"expression\").segments)\n                                    == 2\n                                ):\n                                    self._validate(i, segment)\n                            except AttributeError:\n                                # If the segment doesn't match\n                                pass\n\n                # If the target doesn't exist in select_element_order_preference then it is 'complex' and must go last\n                if self.current_element_band is None:\n                    self.seen_band_elements[-1].append(segment)\n\n            if self.violation_exists:\n                # Create a list of all the edit fixes\n                # We have to do this at the end of iterating through all the select_target_elements to get the order correct\n                # This means we can't add a lint fix to each individual LintResult as we go\n                ordered_select_target_elements = [\n                    segment for band in self.seen_band_elements for segment in band\n                ]\n                # TODO: The \"if\" in the loop below compares corresponding items\n                # to avoid creating \"do-nothing\" edits. A potentially better\n                # approach would leverage difflib.SequenceMatcher.get_opcodes(),\n                # which generates a list of edit actions (similar to the\n                # command-line \"diff\" tool in Linux). This is more complex to\n                # implement, but minimizing the number of LintFixes makes the\n                # final application of patches (in \"sqlfluff fix\") more robust.\n                fixes = [\n                    LintFix(\n                        \"edit\",\n                        initial_select_target_element,\n                        replace_select_target_element,\n                    )\n                    for initial_select_target_element, replace_select_target_element in zip(\n                        select_target_elements, ordered_select_target_elements\n                    )\n                    if initial_select_target_element\n                    is not replace_select_target_element\n                ]\n                # Anchoring on the select statement segment ensures that\n                # select statements which include macro targets are ignored\n                # when ignore_templated_areas is set\n                lint_result = LintResult(anchor=select_clause_segment, fixes=fixes)\n                self.violation_buff = [lint_result]\n\n        return self.violation_buff or None\n", "tokens": ["src", "sqlfluff", "rules", "l034", "py", "rule_l034", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "self", "violation_buff", "self", "violation_exists", "false", "bands", "of", "select", "targets", "in", "order", "to", "be", "enforced", "select_element_order_preference", "wildcard_expression", "object_reference", "literal", "cast_expression", "function", "cast", "expression", "cast_expression", "track", "which", "bands", "have", "been", "seen", "with", "additional", "empty", "list", "for", "the", "non", "matching", "elements", "if", "we", "find", "a", "matching", "target", "element", "we", "append", "the", "element", "to", "the", "corresponding", "index", "self", "seen_band_elements", "list", "list", "basesegment", "for", "_", "in", "select_element_order_preference", "type", "ignore", "if", "context", "segment", "is_type", "select_clause", "ignore", "select", "clauses", "which", "belong", "to", "set", "expression", "which", "is", "most", "commonly", "a", "union", "insert_statement", "create", "table", "statement", "in", "each", "of", "these", "contexts", "the", "order", "of", "columns", "in", "a", "select", "should", "be", "preserved", "if", "len", "context", "parent_stack", "2", "and", "context", "parent_stack", "2", "is_type", "insert_statement", "set_expression", "return", "none", "if", "len", "context", "parent_stack", "3", "and", "context", "parent_stack", "3", "is_type", "create_table_statement", "return", "none", "select_clause_segment", "context", "segment", "select_target_elements", "context", "segment", "get_children", "select_clause_element", "if", "not", "select_target_elements", "return", "none", "iterate", "through", "all", "the", "select", "targets", "to", "find", "any", "order", "violations", "for", "segment", "in", "select_target_elements", "the", "band", "index", "of", "the", "current", "segment", "in", "select_element_order_preference", "self", "current_element_band", "none", "compare", "the", "segment", "to", "the", "bands", "in", "select_element_order_preference", "for", "i", "band", "in", "enumerate", "select_element_order_preference", "for", "e", "in", "band", "identify", "simple", "select", "target", "if", "segment", "get_child", "e", "self", "_validate", "i", "segment", "identify", "function", "elif", "type", "e", "tuple", "and", "e", "0", "function", "try", "if", "segment", "get_child", "function", "get_child", "function_name", "raw", "e", "1", "self", "_validate", "i", "segment", "except", "attributeerror", "if", "the", "segment", "doesn", "t", "match", "pass", "identify", "simple", "expression", "elif", "type", "e", "tuple", "and", "e", "0", "expression", "try", "if", "segment", "get_child", "expression", "get_child", "e", "1", "and", "segment", "get_child", "expression", "segments", "0", "type", "in", "column_reference", "object_reference", "literal", "len", "2", "to", "ensure", "the", "expression", "is", "simple", "and", "len", "segment", "get_child", "expression", "segments", "2", "self", "_validate", "i", "segment", "except", "attributeerror", "if", "the", "segment", "doesn", "t", "match", "pass", "if", "the", "target", "doesn", "t", "exist", "in", "select_element_order_preference", "then", "it", "is", "complex", "and", "must", "go", "last", "if", "self", "current_element_band", "is", "none", "self", "seen_band_elements", "1", "append", "segment", "if", "self", "violation_exists", "create", "a", "list", "of", "all", "the", "edit", "fixes", "we", "have", "to", "do", "this", "at", "the", "end", "of", "iterating", "through", "all", "the", "select_target_elements", "to", "get", "the", "order", "correct", "this", "means", "we", "can", "t", "add", "a", "lint", "fix", "to", "each", "individual", "lintresult", "as", "we", "go", "ordered_select_target_elements", "segment", "for", "band", "in", "self", "seen_band_elements", "for", "segment", "in", "band", "todo", "the", "if", "in", "the", "loop", "below", "compares", "corresponding", "items", "to", "avoid", "creating", "do", "nothing", "edits", "a", "potentially", "better", "approach", "would", "leverage", "difflib", "sequencematcher", "get_opcodes", "which", "generates", "a", "list", "of", "edit", "actions", "similar", "to", "the", "command", "line", "diff", "tool", "in", "linux", "this", "is", "more", "complex", "to", "implement", "but", "minimizing", "the", "number", "of", "lintfixes", "makes", "the", "final", "application", "of", "patches", "in", "sqlfluff", "fix", "more", "robust", "fixes", "lintfix", "edit", "initial_select_target_element", "replace_select_target_element", "for", "initial_select_target_element", "replace_select_target_element", "in", "zip", "select_target_elements", "ordered_select_target_elements", "if", "initial_select_target_element", "is", "not", "replace_select_target_element", "anchoring", "on", "the", "select", "statement", "segment", "ensures", "that", "select", "statements", "which", "include", "macro", "targets", "are", "ignored", "when", "ignore_templated_areas", "is", "set", "lint_result", "lintresult", "anchor", "select_clause_segment", "fixes", "fixes", "self", "violation_buff", "lint_result", "return", "self", "violation_buff", "or", "none"], "doc_len": 502}
{"doc_id": "src/sqlfluff/rules/L035.py::Rule_L035._eval", "file_path": "src/sqlfluff/rules/L035.py", "class_name": "Rule_L035", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L035.py, 类名: Rule_L035\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Look for a case expression\n        1. Look for \"ELSE\"\n        2. Mark \"ELSE\" for deletion (populate \"fixes\")\n        3. Backtrack and mark all newlines/whitespaces for deletion\n        4. Look for a raw \"NULL\" segment\n        5.a. The raw \"NULL\" segment is found, we mark it for deletion and return\n        5.b. We reach the end of case when without matching \"NULL\": the rule passes\n        \"\"\"\n        if context.segment.is_type(\"case_expression\"):\n            fixes: List[LintFix] = []\n            for idx, seg in enumerate(context.segment.segments):\n                # When we find ELSE we delete\n                # everything up to NULL\n                if fixes:\n                    fixes.append(LintFix(\"delete\", seg))\n                    # Safe to look for NULL, as an expression\n                    # would contain NULL but not be == NULL\n                    if seg.raw_upper == \"NULL\":\n                        return LintResult(anchor=context.segment, fixes=fixes)\n\n                if not fixes and seg.name == \"else\":\n                    fixes.append(LintFix(\"delete\", seg))\n                    # Walk back to remove indents/whitespaces\n                    walk_idx = idx - 1\n                    while (\n                        context.segment.segments[walk_idx].name == \"whitespace\"\n                        or context.segment.segments[walk_idx].name == \"newline\"\n                        or context.segment.segments[walk_idx].is_meta\n                    ):\n                        fixes.append(\n                            LintFix(\"delete\", context.segment.segments[walk_idx])\n                        )\n                        walk_idx = walk_idx - 1\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l035", "py", "rule_l035", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "find", "rule", "violations", "and", "provide", "fixes", "0", "look", "for", "a", "case", "expression", "1", "look", "for", "else", "2", "mark", "else", "for", "deletion", "populate", "fixes", "3", "backtrack", "and", "mark", "all", "newlines", "whitespaces", "for", "deletion", "4", "look", "for", "a", "raw", "null", "segment", "5", "a", "the", "raw", "null", "segment", "is", "found", "we", "mark", "it", "for", "deletion", "and", "return", "5", "b", "we", "reach", "the", "end", "of", "case", "when", "without", "matching", "null", "the", "rule", "passes", "if", "context", "segment", "is_type", "case_expression", "fixes", "list", "lintfix", "for", "idx", "seg", "in", "enumerate", "context", "segment", "segments", "when", "we", "find", "else", "we", "delete", "everything", "up", "to", "null", "if", "fixes", "fixes", "append", "lintfix", "delete", "seg", "safe", "to", "look", "for", "null", "as", "an", "expression", "would", "contain", "null", "but", "not", "be", "null", "if", "seg", "raw_upper", "null", "return", "lintresult", "anchor", "context", "segment", "fixes", "fixes", "if", "not", "fixes", "and", "seg", "name", "else", "fixes", "append", "lintfix", "delete", "seg", "walk", "back", "to", "remove", "indents", "whitespaces", "walk_idx", "idx", "1", "while", "context", "segment", "segments", "walk_idx", "name", "whitespace", "or", "context", "segment", "segments", "walk_idx", "name", "newline", "or", "context", "segment", "segments", "walk_idx", "is_meta", "fixes", "append", "lintfix", "delete", "context", "segment", "segments", "walk_idx", "walk_idx", "walk_idx", "1", "return", "none"], "doc_len": 195}
{"doc_id": "src/sqlfluff/rules/L036.py::Rule_L036._eval", "file_path": "src/sqlfluff/rules/L036.py", "class_name": "Rule_L036", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L036.py, 类名: Rule_L036\n    def _eval(self, context: RuleContext):\n        if context.segment.is_type(\"select_clause\"):\n            select_targets_info = self._get_indexes(context.segment)\n            if len(select_targets_info.select_targets) == 1:\n                return self._eval_single_select_target_element(\n                    select_targets_info, context.segment, context.parent_stack\n                )\n            elif len(select_targets_info.select_targets) > 1:\n                return self._eval_multiple_select_target_elements(\n                    select_targets_info, context.segment\n                )\n", "tokens": ["src", "sqlfluff", "rules", "l036", "py", "rule_l036", "def", "_eval", "self", "context", "rulecontext", "if", "context", "segment", "is_type", "select_clause", "select_targets_info", "self", "_get_indexes", "context", "segment", "if", "len", "select_targets_info", "select_targets", "1", "return", "self", "_eval_single_select_target_element", "select_targets_info", "context", "segment", "context", "parent_stack", "elif", "len", "select_targets_info", "select_targets", "1", "return", "self", "_eval_multiple_select_target_elements", "select_targets_info", "context", "segment"], "doc_len": 45}
{"doc_id": "src/sqlfluff/rules/L036.py::Rule_L036._get_indexes", "file_path": "src/sqlfluff/rules/L036.py", "class_name": "Rule_L036", "func_name": "_get_indexes", "text": "文件路径: src/sqlfluff/rules/L036.py, 类名: Rule_L036\n    def _get_indexes(segment):\n        select_idx = -1\n        first_new_line_idx = -1\n        first_select_target_idx = -1\n        first_whitespace_idx = -1\n        select_targets = []\n        for fname_idx, seg in enumerate(segment.segments):\n            if seg.is_type(\"select_clause_element\"):\n                select_targets.append(seg)\n                if first_select_target_idx == -1:\n                    first_select_target_idx = fname_idx\n            if seg.is_type(\"keyword\") and seg.name == \"select\" and select_idx == -1:\n                select_idx = fname_idx\n            if seg.is_type(\"newline\") and first_new_line_idx == -1:\n                first_new_line_idx = fname_idx\n            # TRICKY: Ignore whitespace prior to the first newline, e.g. if\n            # the line with \"SELECT\" (before any select targets) has trailing\n            # whitespace.\n            if (\n                seg.is_type(\"whitespace\")\n                and first_new_line_idx != -1\n                and first_whitespace_idx == -1\n            ):\n                first_whitespace_idx = fname_idx\n\n        return SelectTargetsInfo(\n            select_idx,\n            first_new_line_idx,\n            first_select_target_idx,\n            first_whitespace_idx,\n            select_targets,\n        )\n", "tokens": ["src", "sqlfluff", "rules", "l036", "py", "rule_l036", "def", "_get_indexes", "segment", "select_idx", "1", "first_new_line_idx", "1", "first_select_target_idx", "1", "first_whitespace_idx", "1", "select_targets", "for", "fname_idx", "seg", "in", "enumerate", "segment", "segments", "if", "seg", "is_type", "select_clause_element", "select_targets", "append", "seg", "if", "first_select_target_idx", "1", "first_select_target_idx", "fname_idx", "if", "seg", "is_type", "keyword", "and", "seg", "name", "select", "and", "select_idx", "1", "select_idx", "fname_idx", "if", "seg", "is_type", "newline", "and", "first_new_line_idx", "1", "first_new_line_idx", "fname_idx", "tricky", "ignore", "whitespace", "prior", "to", "the", "first", "newline", "e", "g", "if", "the", "line", "with", "select", "before", "any", "select", "targets", "has", "trailing", "whitespace", "if", "seg", "is_type", "whitespace", "and", "first_new_line_idx", "1", "and", "first_whitespace_idx", "1", "first_whitespace_idx", "fname_idx", "return", "selecttargetsinfo", "select_idx", "first_new_line_idx", "first_select_target_idx", "first_whitespace_idx", "select_targets"], "doc_len": 100}
{"doc_id": "src/sqlfluff/rules/L036.py::Rule_L036._eval_multiple_select_target_elements", "file_path": "src/sqlfluff/rules/L036.py", "class_name": "Rule_L036", "func_name": "_eval_multiple_select_target_elements", "text": "文件路径: src/sqlfluff/rules/L036.py, 类名: Rule_L036\n    def _eval_multiple_select_target_elements(self, select_targets_info, segment):\n        \"\"\"Multiple select targets. Ensure each is on a separate line.\"\"\"\n        # Insert newline before every select target.\n        fixes = []\n        for i, select_target in enumerate(select_targets_info.select_targets):\n            base_segment = (\n                segment if not i else select_targets_info.select_targets[i - 1]\n            )\n            if (\n                base_segment.pos_marker.working_line_no\n                == select_target.pos_marker.working_line_no\n            ):\n                # Find and delete any whitespace before the select target.\n                start_seg = select_targets_info.select_idx\n                # If any select modifier (e.g. distinct ) is present, start\n                # there rather than at the beginning.\n                modifier = segment.get_child(\"select_clause_modifier\")\n                if modifier:\n                    start_seg = segment.segments.index(modifier)\n\n                ws_to_delete = segment.select_children(\n                    start_seg=segment.segments[start_seg]\n                    if not i\n                    else select_targets_info.select_targets[i - 1],\n                    select_if=lambda s: s.is_type(\"whitespace\"),\n                    loop_while=lambda s: s.is_type(\"whitespace\", \"comma\") or s.is_meta,\n                )\n                fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n                fixes.append(LintFix(\"create\", select_target, NewlineSegment()))\n        if fixes:\n            return LintResult(anchor=segment, fixes=fixes)\n", "tokens": ["src", "sqlfluff", "rules", "l036", "py", "rule_l036", "def", "_eval_multiple_select_target_elements", "self", "select_targets_info", "segment", "multiple", "select", "targets", "ensure", "each", "is", "on", "a", "separate", "line", "insert", "newline", "before", "every", "select", "target", "fixes", "for", "i", "select_target", "in", "enumerate", "select_targets_info", "select_targets", "base_segment", "segment", "if", "not", "i", "else", "select_targets_info", "select_targets", "i", "1", "if", "base_segment", "pos_marker", "working_line_no", "select_target", "pos_marker", "working_line_no", "find", "and", "delete", "any", "whitespace", "before", "the", "select", "target", "start_seg", "select_targets_info", "select_idx", "if", "any", "select", "modifier", "e", "g", "distinct", "is", "present", "start", "there", "rather", "than", "at", "the", "beginning", "modifier", "segment", "get_child", "select_clause_modifier", "if", "modifier", "start_seg", "segment", "segments", "index", "modifier", "ws_to_delete", "segment", "select_children", "start_seg", "segment", "segments", "start_seg", "if", "not", "i", "else", "select_targets_info", "select_targets", "i", "1", "select_if", "lambda", "s", "s", "is_type", "whitespace", "loop_while", "lambda", "s", "s", "is_type", "whitespace", "comma", "or", "s", "is_meta", "fixes", "lintfix", "delete", "ws", "for", "ws", "in", "ws_to_delete", "fixes", "append", "lintfix", "create", "select_target", "newlinesegment", "if", "fixes", "return", "lintresult", "anchor", "segment", "fixes", "fixes"], "doc_len": 144}
{"doc_id": "src/sqlfluff/rules/L036.py::Rule_L036._eval_single_select_target_element", "file_path": "src/sqlfluff/rules/L036.py", "class_name": "Rule_L036", "func_name": "_eval_single_select_target_element", "text": "文件路径: src/sqlfluff/rules/L036.py, 类名: Rule_L036\n    def _eval_single_select_target_element(\n        self, select_targets_info, select_clause, parent_stack\n    ):\n        is_wildcard = False\n        for segment in select_clause.segments:\n            if segment.is_type(\"select_clause_element\"):\n                for sub_segment in segment.segments:\n                    if sub_segment.is_type(\"wildcard_expression\"):\n                        is_wildcard = True\n\n        if is_wildcard:\n            return None\n        elif (\n            select_targets_info.select_idx\n            < select_targets_info.first_new_line_idx\n            < select_targets_info.first_select_target_idx\n        ):\n            # Do we have a modifier?\n            modifier = select_clause.get_child(\"select_clause_modifier\")\n\n            # Prepare the select clause which will be inserted\n            # In most (but not all) case we'll want to replace the newline with\n            # the statement and a newline, but in some cases however (see #1424)\n            # we don't need the final newline.\n            copy_with_newline = True\n            insert_buff = [\n                WhitespaceSegment(),\n                select_clause.segments[select_targets_info.first_select_target_idx],\n            ]\n\n            # Check if the modifier is one we care about\n            if modifier:\n                # If it's already on the first line, ignore it.\n                if (\n                    select_clause.segments.index(modifier)\n                    < select_targets_info.first_new_line_idx\n                ):\n                    modifier = None\n            fixes = [\n                # Delete the first select target from its original location.\n                # We'll add it to the right section at the end, once we know\n                # what to add.\n                LintFix(\n                    \"delete\",\n                    select_clause.segments[select_targets_info.first_select_target_idx],\n                ),\n            ]\n\n            start_idx = 0\n\n            # If we have a modifier to move:\n            if modifier:\n\n                # Add it to the insert\n                insert_buff = [WhitespaceSegment(), modifier] + insert_buff\n\n                modifier_idx = select_clause.segments.index(modifier)\n                # Delete the whitespace after it (which is two after, thanks to indent)\n                if (\n                    len(select_clause.segments) > modifier_idx + 1\n                    and select_clause.segments[modifier_idx + 2].is_whitespace\n                ):\n                    fixes += [\n                        LintFix(\n                            \"delete\",\n                            select_clause.segments[modifier_idx + 2],\n                        ),\n                    ]\n\n                # Delete the modifier itself\n                fixes += [\n                    LintFix(\n                        \"delete\",\n                        modifier,\n                    ),\n                ]\n\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = modifier_idx\n            else:\n                # Set the position marker for removing the preceding\n                # whitespace and newline, which we'll use below.\n                start_idx = select_targets_info.first_select_target_idx\n\n            if parent_stack and parent_stack[-1].is_type(\"select_statement\"):\n                select_stmt = parent_stack[-1]\n                select_clause_idx = select_stmt.segments.index(select_clause)\n                after_select_clause_idx = select_clause_idx + 1\n                if len(select_stmt.segments) > after_select_clause_idx:\n                    if select_stmt.segments[after_select_clause_idx].is_type(\"newline\"):\n                        # The select_clause is immediately followed by a\n                        # newline. Delete the newline in order to avoid leaving\n                        # behind an empty line after fix.\n                        delete_last_newline = True\n\n                        # Since, we're deleting the newline, we should also delete all\n                        # whitespace before it or it will add random whitespace to\n                        # following statements. So walk back through the segment\n                        # deleting whitespace until you get the previous newline, or\n                        # something else.\n                        idx = 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\",\n                            ):\n                                break\n\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                delete_last_newline = False\n                                break\n\n                            idx += 1\n\n                        # Finally delete the newline, unless we've decided not to\n                        if delete_last_newline:\n                            fixes.append(\n                                LintFix(\n                                    \"delete\",\n                                    select_stmt.segments[after_select_clause_idx],\n                                )\n                            )\n\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"whitespace\"\n                    ):\n                        # The select_clause has stuff after (most likely a comment)\n                        # Delete the whitespace immeadiately after the select clause\n                        # so the other stuff aligns nicely based on where the select\n                        # clause started\n                        fixes += [\n                            LintFix(\n                                \"delete\",\n                                select_stmt.segments[after_select_clause_idx],\n                            ),\n                        ]\n                    elif select_stmt.segments[after_select_clause_idx].is_type(\n                        \"dedent\"\n                    ):\n                        # The end of the select statement, so this is the one\n                        # case we don't want the newline added to end of\n                        # select_clause (see #1424)\n                        copy_with_newline = False\n\n                        # Again let's strip back the whitespace, bnut simpler\n                        # as don't need to worry about new line so just break\n                        # if see non-whitespace\n                        idx = 1\n                        start_idx = select_clause_idx - 1\n                        while start_idx - idx < len(select_clause.segments):\n                            # Delete any whitespace\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\"\n                            ):\n                                fixes += [\n                                    LintFix(\n                                        \"delete\",\n                                        select_clause.segments[start_idx - idx],\n                                    ),\n                                ]\n\n                            # Once we see a newline, then we're done\n                            if select_clause.segments[start_idx - idx].is_type(\n                                \"newline\"\n                            ):\n                                break\n\n                            # If we see anything other than whitespace,\n                            # then we're done, but in this case we want to\n                            # keep the final newline.\n                            if not select_clause.segments[start_idx - idx].is_type(\n                                \"whitespace\", \"newline\"\n                            ):\n                                copy_with_newline = True\n                                break\n                            idx += 1\n\n            if copy_with_newline:\n                insert_buff = insert_buff + [NewlineSegment()]\n\n            fixes += [\n                # Insert the select_clause in place of the first newlin in the\n                # Select statement\n                LintFix(\n                    \"edit\",\n                    select_clause.segments[select_targets_info.first_new_line_idx],\n                    insert_buff,\n                ),\n            ]\n\n            return LintResult(\n                anchor=select_clause,\n                fixes=fixes,\n            )\n        else:\n            return None\n", "tokens": ["src", "sqlfluff", "rules", "l036", "py", "rule_l036", "def", "_eval_single_select_target_element", "self", "select_targets_info", "select_clause", "parent_stack", "is_wildcard", "false", "for", "segment", "in", "select_clause", "segments", "if", "segment", "is_type", "select_clause_element", "for", "sub_segment", "in", "segment", "segments", "if", "sub_segment", "is_type", "wildcard_expression", "is_wildcard", "true", "if", "is_wildcard", "return", "none", "elif", "select_targets_info", "select_idx", "select_targets_info", "first_new_line_idx", "select_targets_info", "first_select_target_idx", "do", "we", "have", "a", "modifier", "modifier", "select_clause", "get_child", "select_clause_modifier", "prepare", "the", "select", "clause", "which", "will", "be", "inserted", "in", "most", "but", "not", "all", "case", "we", "ll", "want", "to", "replace", "the", "newline", "with", "the", "statement", "and", "a", "newline", "but", "in", "some", "cases", "however", "see", "1424", "we", "don", "t", "need", "the", "final", "newline", "copy_with_newline", "true", "insert_buff", "whitespacesegment", "select_clause", "segments", "select_targets_info", "first_select_target_idx", "check", "if", "the", "modifier", "is", "one", "we", "care", "about", "if", "modifier", "if", "it", "s", "already", "on", "the", "first", "line", "ignore", "it", "if", "select_clause", "segments", "index", "modifier", "select_targets_info", "first_new_line_idx", "modifier", "none", "fixes", "delete", "the", "first", "select", "target", "from", "its", "original", "location", "we", "ll", "add", "it", "to", "the", "right", "section", "at", "the", "end", "once", "we", "know", "what", "to", "add", "lintfix", "delete", "select_clause", "segments", "select_targets_info", "first_select_target_idx", "start_idx", "0", "if", "we", "have", "a", "modifier", "to", "move", "if", "modifier", "add", "it", "to", "the", "insert", "insert_buff", "whitespacesegment", "modifier", "insert_buff", "modifier_idx", "select_clause", "segments", "index", "modifier", "delete", "the", "whitespace", "after", "it", "which", "is", "two", "after", "thanks", "to", "indent", "if", "len", "select_clause", "segments", "modifier_idx", "1", "and", "select_clause", "segments", "modifier_idx", "2", "is_whitespace", "fixes", "lintfix", "delete", "select_clause", "segments", "modifier_idx", "2", "delete", "the", "modifier", "itself", "fixes", "lintfix", "delete", "modifier", "set", "the", "position", "marker", "for", "removing", "the", "preceding", "whitespace", "and", "newline", "which", "we", "ll", "use", "below", "start_idx", "modifier_idx", "else", "set", "the", "position", "marker", "for", "removing", "the", "preceding", "whitespace", "and", "newline", "which", "we", "ll", "use", "below", "start_idx", "select_targets_info", "first_select_target_idx", "if", "parent_stack", "and", "parent_stack", "1", "is_type", "select_statement", "select_stmt", "parent_stack", "1", "select_clause_idx", "select_stmt", "segments", "index", "select_clause", "after_select_clause_idx", "select_clause_idx", "1", "if", "len", "select_stmt", "segments", "after_select_clause_idx", "if", "select_stmt", "segments", "after_select_clause_idx", "is_type", "newline", "the", "select_clause", "is", "immediately", "followed", "by", "a", "newline", "delete", "the", "newline", "in", "order", "to", "avoid", "leaving", "behind", "an", "empty", "line", "after", "fix", "delete_last_newline", "true", "since", "we", "re", "deleting", "the", "newline", "we", "should", "also", "delete", "all", "whitespace", "before", "it", "or", "it", "will", "add", "random", "whitespace", "to", "following", "statements", "so", "walk", "back", "through", "the", "segment", "deleting", "whitespace", "until", "you", "get", "the", "previous", "newline", "or", "something", "else", "idx", "1", "while", "start_idx", "idx", "len", "select_clause", "segments", "delete", "any", "whitespace", "if", "select_clause", "segments", "start_idx", "idx", "is_type", "whitespace", "fixes", "lintfix", "delete", "select_clause", "segments", "start_idx", "idx", "once", "we", "see", "a", "newline", "then", "we", "re", "done", "if", "select_clause", "segments", "start_idx", "idx", "is_type", "newline", "break", "if", "we", "see", "anything", "other", "than", "whitespace", "then", "we", "re", "done", "but", "in", "this", "case", "we", "want", "to", "keep", "the", "final", "newline", "if", "not", "select_clause", "segments", "start_idx", "idx", "is_type", "whitespace", "newline", "delete_last_newline", "false", "break", "idx", "1", "finally", "delete", "the", "newline", "unless", "we", "ve", "decided", "not", "to", "if", "delete_last_newline", "fixes", "append", "lintfix", "delete", "select_stmt", "segments", "after_select_clause_idx", "elif", "select_stmt", "segments", "after_select_clause_idx", "is_type", "whitespace", "the", "select_clause", "has", "stuff", "after", "most", "likely", "a", "comment", "delete", "the", "whitespace", "immeadiately", "after", "the", "select", "clause", "so", "the", "other", "stuff", "aligns", "nicely", "based", "on", "where", "the", "select", "clause", "started", "fixes", "lintfix", "delete", "select_stmt", "segments", "after_select_clause_idx", "elif", "select_stmt", "segments", "after_select_clause_idx", "is_type", "dedent", "the", "end", "of", "the", "select", "statement", "so", "this", "is", "the", "one", "case", "we", "don", "t", "want", "the", "newline", "added", "to", "end", "of", "select_clause", "see", "1424", "copy_with_newline", "false", "again", "let", "s", "strip", "back", "the", "whitespace", "bnut", "simpler", "as", "don", "t", "need", "to", "worry", "about", "new", "line", "so", "just", "break", "if", "see", "non", "whitespace", "idx", "1", "start_idx", "select_clause_idx", "1", "while", "start_idx", "idx", "len", "select_clause", "segments", "delete", "any", "whitespace", "if", "select_clause", "segments", "start_idx", "idx", "is_type", "whitespace", "fixes", "lintfix", "delete", "select_clause", "segments", "start_idx", "idx", "once", "we", "see", "a", "newline", "then", "we", "re", "done", "if", "select_clause", "segments", "start_idx", "idx", "is_type", "newline", "break", "if", "we", "see", "anything", "other", "than", "whitespace", "then", "we", "re", "done", "but", "in", "this", "case", "we", "want", "to", "keep", "the", "final", "newline", "if", "not", "select_clause", "segments", "start_idx", "idx", "is_type", "whitespace", "newline", "copy_with_newline", "true", "break", "idx", "1", "if", "copy_with_newline", "insert_buff", "insert_buff", "newlinesegment", "fixes", "insert", "the", "select_clause", "in", "place", "of", "the", "first", "newlin", "in", "the", "select", "statement", "lintfix", "edit", "select_clause", "segments", "select_targets_info", "first_new_line_idx", "insert_buff", "return", "lintresult", "anchor", "select_clause", "fixes", "fixes", "else", "return", "none"], "doc_len": 674}
{"doc_id": "src/sqlfluff/rules/L037.py::Rule_L037._get_orderby_info", "file_path": "src/sqlfluff/rules/L037.py", "class_name": "Rule_L037", "func_name": "_get_orderby_info", "text": "文件路径: src/sqlfluff/rules/L037.py, 类名: Rule_L037\n    def _get_orderby_info(segment: BaseSegment) -> List[OrderByColumnInfo]:\n        assert segment.is_type(\"orderby_clause\")\n\n        result = []\n        found_column_reference = False\n        ordering_reference = None\n        for child_segment in segment.segments:\n            if child_segment.is_type(\"column_reference\"):\n                found_column_reference = True\n            elif child_segment.is_type(\"keyword\") and child_segment.name in (\n                \"asc\",\n                \"desc\",\n            ):\n                ordering_reference = child_segment.name\n            elif found_column_reference and child_segment.type not in [\n                \"keyword\",\n                \"whitespace\",\n                \"indent\",\n                \"dedent\",\n            ]:\n                result.append(\n                    OrderByColumnInfo(separator=child_segment, order=ordering_reference)\n                )\n\n                # Reset findings\n                found_column_reference = False\n                ordering_reference = None\n\n        # Special handling for last column\n        if found_column_reference:\n            result.append(\n                OrderByColumnInfo(\n                    separator=segment.segments[-1], order=ordering_reference\n                )\n            )\n        return result\n", "tokens": ["src", "sqlfluff", "rules", "l037", "py", "rule_l037", "def", "_get_orderby_info", "segment", "basesegment", "list", "orderbycolumninfo", "assert", "segment", "is_type", "orderby_clause", "result", "found_column_reference", "false", "ordering_reference", "none", "for", "child_segment", "in", "segment", "segments", "if", "child_segment", "is_type", "column_reference", "found_column_reference", "true", "elif", "child_segment", "is_type", "keyword", "and", "child_segment", "name", "in", "asc", "desc", "ordering_reference", "child_segment", "name", "elif", "found_column_reference", "and", "child_segment", "type", "not", "in", "keyword", "whitespace", "indent", "dedent", "result", "append", "orderbycolumninfo", "separator", "child_segment", "order", "ordering_reference", "reset", "findings", "found_column_reference", "false", "ordering_reference", "none", "special", "handling", "for", "last", "column", "if", "found_column_reference", "result", "append", "orderbycolumninfo", "separator", "segment", "segments", "1", "order", "ordering_reference", "return", "result"], "doc_len": 87}
{"doc_id": "src/sqlfluff/rules/L037.py::Rule_L037._eval", "file_path": "src/sqlfluff/rules/L037.py", "class_name": "Rule_L037", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L037.py, 类名: Rule_L037\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Ambiguous ordering directions for columns in order by clause.\n\n        This rule checks if some ORDER BY columns explicitly specify ASC or\n        DESC and some don't.\n        \"\"\"\n        # We only trigger on orderby_clause\n        if context.segment.is_type(\"orderby_clause\"):\n            lint_fixes = []\n            orderby_spec = self._get_orderby_info(context.segment)\n            order_types = {o.order for o in orderby_spec}\n            # If ALL columns or NO columns explicitly specify ASC/DESC, all is\n            # well.\n            if None not in order_types or order_types == {None}:\n                return None\n\n            # There's a mix of explicit and default sort order. Make everything\n            # explicit.\n            for col_info in orderby_spec:\n                if not col_info.order:\n                    # Since ASC is default in SQL, add in ASC for fix\n                    lint_fixes.append(\n                        LintFix(\n                            \"create\",\n                            col_info.separator,\n                            [WhitespaceSegment(), KeywordSegment(\"ASC\")],\n                        )\n                    )\n\n            return [\n                LintResult(\n                    anchor=context.segment,\n                    fixes=lint_fixes,\n                    description=(\n                        \"Ambiguous order by clause. Order by \"\n                        \"clauses should specify order direction for ALL columns or NO columns.\"\n                    ),\n                )\n            ]\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l037", "py", "rule_l037", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "ambiguous", "ordering", "directions", "for", "columns", "in", "order", "by", "clause", "this", "rule", "checks", "if", "some", "order", "by", "columns", "explicitly", "specify", "asc", "or", "desc", "and", "some", "don", "t", "we", "only", "trigger", "on", "orderby_clause", "if", "context", "segment", "is_type", "orderby_clause", "lint_fixes", "orderby_spec", "self", "_get_orderby_info", "context", "segment", "order_types", "o", "order", "for", "o", "in", "orderby_spec", "if", "all", "columns", "or", "no", "columns", "explicitly", "specify", "asc", "desc", "all", "is", "well", "if", "none", "not", "in", "order_types", "or", "order_types", "none", "return", "none", "there", "s", "a", "mix", "of", "explicit", "and", "default", "sort", "order", "make", "everything", "explicit", "for", "col_info", "in", "orderby_spec", "if", "not", "col_info", "order", "since", "asc", "is", "default", "in", "sql", "add", "in", "asc", "for", "fix", "lint_fixes", "append", "lintfix", "create", "col_info", "separator", "whitespacesegment", "keywordsegment", "asc", "return", "lintresult", "anchor", "context", "segment", "fixes", "lint_fixes", "description", "ambiguous", "order", "by", "clause", "order", "by", "clauses", "should", "specify", "order", "direction", "for", "all", "columns", "or", "no", "columns", "return", "none"], "doc_len": 154}
{"doc_id": "src/sqlfluff/rules/L038.py::Rule_L038._eval", "file_path": "src/sqlfluff/rules/L038.py", "class_name": "Rule_L038", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L038.py, 类名: Rule_L038\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Trailing commas within select clause.\"\"\"\n        # Config type hints\n        self.select_clause_trailing_comma: str\n\n        if context.segment.is_type(\"select_clause\"):\n            # Iterate content to find last element\n            last_content: BaseSegment = None  # type: ignore\n            for seg in context.segment.segments:\n                if seg.is_code:\n                    last_content = seg\n\n            # What mode are we in?\n            if self.select_clause_trailing_comma == \"forbid\":\n                # Is it a comma?\n                if last_content.is_type(\"comma\"):\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[LintFix(\"delete\", last_content)],\n                        description=\"Trailing comma in select statement forbidden\",\n                    )\n            elif self.select_clause_trailing_comma == \"require\":\n                if not last_content.is_type(\"comma\"):\n                    new_comma = SymbolSegment(\",\", name=\"comma\", type=\"comma\")\n                    return LintResult(\n                        anchor=last_content,\n                        fixes=[\n                            LintFix(\"edit\", last_content, [last_content, new_comma])\n                        ],\n                        description=\"Trailing comma in select statement required\",\n                    )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l038", "py", "rule_l038", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "trailing", "commas", "within", "select", "clause", "config", "type", "hints", "self", "select_clause_trailing_comma", "str", "if", "context", "segment", "is_type", "select_clause", "iterate", "content", "to", "find", "last", "element", "last_content", "basesegment", "none", "type", "ignore", "for", "seg", "in", "context", "segment", "segments", "if", "seg", "is_code", "last_content", "seg", "what", "mode", "are", "we", "in", "if", "self", "select_clause_trailing_comma", "forbid", "is", "it", "a", "comma", "if", "last_content", "is_type", "comma", "return", "lintresult", "anchor", "last_content", "fixes", "lintfix", "delete", "last_content", "description", "trailing", "comma", "in", "select", "statement", "forbidden", "elif", "self", "select_clause_trailing_comma", "require", "if", "not", "last_content", "is_type", "comma", "new_comma", "symbolsegment", "name", "comma", "type", "comma", "return", "lintresult", "anchor", "last_content", "fixes", "lintfix", "edit", "last_content", "last_content", "new_comma", "description", "trailing", "comma", "in", "select", "statement", "required", "return", "none"], "doc_len": 117}
{"doc_id": "src/sqlfluff/rules/L039.py::Rule_L039._eval", "file_path": "src/sqlfluff/rules/L039.py", "class_name": "Rule_L039", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L039.py, 类名: Rule_L039\n    def _eval(self, context: RuleContext) -> Optional[List[LintResult]]:\n        \"\"\"Unnecessary whitespace.\"\"\"\n        # For the given segment, lint whitespace directly within it.\n        prev_newline = True\n        prev_whitespace = None\n        violations = []\n        for seg in context.segment.segments:\n            if seg.is_type(\"newline\"):\n                prev_newline = True\n                prev_whitespace = None\n            elif seg.is_type(\"whitespace\"):\n                # This is to avoid indents\n                if not prev_newline:\n                    prev_whitespace = seg\n                prev_newline = False\n            elif seg.is_type(\"comment\"):\n                prev_newline = False\n                prev_whitespace = None\n            else:\n                if prev_whitespace:\n                    if prev_whitespace.raw != \" \":\n                        violations.append(\n                            LintResult(\n                                anchor=prev_whitespace,\n                                fixes=[\n                                    LintFix(\n                                        \"edit\",\n                                        prev_whitespace,\n                                        WhitespaceSegment(),\n                                    )\n                                ],\n                            )\n                        )\n                prev_newline = False\n                prev_whitespace = None\n        return violations or None\n", "tokens": ["src", "sqlfluff", "rules", "l039", "py", "rule_l039", "def", "_eval", "self", "context", "rulecontext", "optional", "list", "lintresult", "unnecessary", "whitespace", "for", "the", "given", "segment", "lint", "whitespace", "directly", "within", "it", "prev_newline", "true", "prev_whitespace", "none", "violations", "for", "seg", "in", "context", "segment", "segments", "if", "seg", "is_type", "newline", "prev_newline", "true", "prev_whitespace", "none", "elif", "seg", "is_type", "whitespace", "this", "is", "to", "avoid", "indents", "if", "not", "prev_newline", "prev_whitespace", "seg", "prev_newline", "false", "elif", "seg", "is_type", "comment", "prev_newline", "false", "prev_whitespace", "none", "else", "if", "prev_whitespace", "if", "prev_whitespace", "raw", "violations", "append", "lintresult", "anchor", "prev_whitespace", "fixes", "lintfix", "edit", "prev_whitespace", "whitespacesegment", "prev_newline", "false", "prev_whitespace", "none", "return", "violations", "or", "none"], "doc_len": 92}
{"doc_id": "src/sqlfluff/rules/L041.py::Rule_L041._eval", "file_path": "src/sqlfluff/rules/L041.py", "class_name": "Rule_L041", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L041.py, 类名: Rule_L041\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Select clause modifiers must appear on same line as SELECT.\"\"\"\n        if context.segment.is_type(\"select_clause\"):\n            # Does the select clause have modifiers?\n            select_modifier = context.segment.get_child(\"select_clause_modifier\")\n            if not select_modifier:\n                return None  # No. We're done.\n            select_modifier_idx = context.segment.segments.index(select_modifier)\n\n            # Does the select clause contain a newline?\n            newline = context.segment.get_child(\"newline\")\n            if not newline:\n                return None  # No. We're done.\n            newline_idx = context.segment.segments.index(newline)\n\n            # Is there a newline before the select modifier?\n            if newline_idx > select_modifier_idx:\n                return None  # No, we're done.\n\n            # Yes to all the above. We found an issue.\n\n            # E.g.: \" DISTINCT\\n\"\n            replace_newline_with = [\n                WhitespaceSegment(),\n                select_modifier,\n                NewlineSegment(),\n            ]\n            fixes = [\n                # E.g. \"\\n\" -> \" DISTINCT\\n.\n                LintFix(\"edit\", newline, replace_newline_with),\n                # E.g. \"DISTINCT\" -> X\n                LintFix(\"delete\", select_modifier),\n            ]\n\n            # E.g. \" \" after \"DISTINCT\"\n            ws_to_delete = context.segment.select_children(\n                start_seg=select_modifier,\n                select_if=lambda s: s.is_type(\"whitespace\"),\n                loop_while=lambda s: s.is_type(\"whitespace\") or s.is_meta,\n            )\n\n            # E.g. \" \" -> X\n            fixes += [LintFix(\"delete\", ws) for ws in ws_to_delete]\n            return LintResult(\n                anchor=context.segment,\n                fixes=fixes,\n            )\n\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l041", "py", "rule_l041", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "select", "clause", "modifiers", "must", "appear", "on", "same", "line", "as", "select", "if", "context", "segment", "is_type", "select_clause", "does", "the", "select", "clause", "have", "modifiers", "select_modifier", "context", "segment", "get_child", "select_clause_modifier", "if", "not", "select_modifier", "return", "none", "no", "we", "re", "done", "select_modifier_idx", "context", "segment", "segments", "index", "select_modifier", "does", "the", "select", "clause", "contain", "a", "newline", "newline", "context", "segment", "get_child", "newline", "if", "not", "newline", "return", "none", "no", "we", "re", "done", "newline_idx", "context", "segment", "segments", "index", "newline", "is", "there", "a", "newline", "before", "the", "select", "modifier", "if", "newline_idx", "select_modifier_idx", "return", "none", "no", "we", "re", "done", "yes", "to", "all", "the", "above", "we", "found", "an", "issue", "e", "g", "distinct", "n", "replace_newline_with", "whitespacesegment", "select_modifier", "newlinesegment", "fixes", "e", "g", "n", "distinct", "n", "lintfix", "edit", "newline", "replace_newline_with", "e", "g", "distinct", "x", "lintfix", "delete", "select_modifier", "e", "g", "after", "distinct", "ws_to_delete", "context", "segment", "select_children", "start_seg", "select_modifier", "select_if", "lambda", "s", "s", "is_type", "whitespace", "loop_while", "lambda", "s", "s", "is_type", "whitespace", "or", "s", "is_meta", "e", "g", "x", "fixes", "lintfix", "delete", "ws", "for", "ws", "in", "ws_to_delete", "return", "lintresult", "anchor", "context", "segment", "fixes", "fixes", "return", "none"], "doc_len": 177}
{"doc_id": "src/sqlfluff/rules/L042.py::Rule_L042._eval", "file_path": "src/sqlfluff/rules/L042.py", "class_name": "Rule_L042", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L042.py, 类名: Rule_L042\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Join/From clauses should not contain subqueries. Use CTEs instead.\n\n        NB: No fix for this routine because it would be very complex to\n        implement reliably.\n        \"\"\"\n        parent_types = self._config_mapping[self.forbid_subquery_in]  # type: ignore\n        for parent_type in parent_types:\n            if context.segment.is_type(parent_type):\n                # Get the referenced table segment\n                from_expression_element = context.segment.get_child(\n                    \"from_expression_element\"\n                )\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Get the main bit\n                from_expression_element = from_expression_element.get_child(\n                    \"table_expression\"\n                )\n                if not from_expression_element:  # pragma: no cover\n                    return None  # There isn't one. We're done.\n                # Is it bracketed?\n                bracketed_expression = from_expression_element.get_child(\"bracketed\")\n                # If it is, lint that instead\n                if bracketed_expression:\n                    from_expression_element = bracketed_expression\n                # If any of the following are found, raise an issue.\n                # If not, we're fine.\n                problem_children = [\n                    \"with_compound_statement\",\n                    \"set_expression\",\n                    \"select_statement\",\n                ]\n                for seg_type in problem_children:\n                    seg = from_expression_element.get_child(seg_type)\n                    if seg:\n                        return LintResult(\n                            anchor=seg,\n                            description=f\"{parent_type} clauses should not contain subqueries. Use CTEs instead\",\n                        )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l042", "py", "rule_l042", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "join", "from", "clauses", "should", "not", "contain", "subqueries", "use", "ctes", "instead", "nb", "no", "fix", "for", "this", "routine", "because", "it", "would", "be", "very", "complex", "to", "implement", "reliably", "parent_types", "self", "_config_mapping", "self", "forbid_subquery_in", "type", "ignore", "for", "parent_type", "in", "parent_types", "if", "context", "segment", "is_type", "parent_type", "get", "the", "referenced", "table", "segment", "from_expression_element", "context", "segment", "get_child", "from_expression_element", "if", "not", "from_expression_element", "pragma", "no", "cover", "return", "none", "there", "isn", "t", "one", "we", "re", "done", "get", "the", "main", "bit", "from_expression_element", "from_expression_element", "get_child", "table_expression", "if", "not", "from_expression_element", "pragma", "no", "cover", "return", "none", "there", "isn", "t", "one", "we", "re", "done", "is", "it", "bracketed", "bracketed_expression", "from_expression_element", "get_child", "bracketed", "if", "it", "is", "lint", "that", "instead", "if", "bracketed_expression", "from_expression_element", "bracketed_expression", "if", "any", "of", "the", "following", "are", "found", "raise", "an", "issue", "if", "not", "we", "re", "fine", "problem_children", "with_compound_statement", "set_expression", "select_statement", "for", "seg_type", "in", "problem_children", "seg", "from_expression_element", "get_child", "seg_type", "if", "seg", "return", "lintresult", "anchor", "seg", "description", "f", "parent_type", "clauses", "should", "not", "contain", "subqueries", "use", "ctes", "instead", "return", "none"], "doc_len": 165}
{"doc_id": "src/sqlfluff/rules/L043.py::Rule_L043._eval", "file_path": "src/sqlfluff/rules/L043.py", "class_name": "Rule_L043", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L043.py, 类名: Rule_L043\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\n\n        0. Look for a case expression\n        1. Find the first expression and \"then\"\n        2. Determine if the \"then\" is followed by a boolean\n        3. If so, determine if the first then-bool is followed by an else-bool\n        4. If so, delete everything but the first expression\n        5a. If then-true-else-false\n            * return deletions\n            * wrap with coalesce\n        5b. If then-false-else-true\n            * return deletions\n            * add a not condition\n            * wrap with parenthesis and coalesce\n        \"\"\"\n        # Look for a case expression\n        if (\n            context.segment.is_type(\"case_expression\")\n            and context.segment.segments[0].name == \"case\"\n        ):\n            # Find the first expression and \"then\"\n            idx = 0\n            while context.segment.segments[idx].name != \"then\":\n                if context.segment.segments[idx].is_type(\"expression\"):\n                    expression_idx = idx\n                idx += 1\n            # Determine if \"then\" is followed by a boolean\n            then_bool_type = None\n            while context.segment.segments[idx].name not in [\"when\", \"else\", \"end\"]:\n                if context.segment.segments[idx].raw_upper in [\"TRUE\", \"FALSE\"]:\n                    then_bool_type = context.segment.segments[idx].raw_upper\n                idx += 1\n            if then_bool_type:\n                # Determine if the first then-bool is followed by an else-bool\n                while context.segment.segments[idx].name != \"else\":\n                    # If the first then-bool is followed by a \"WHEN\" or \"END\", exit\n                    if context.segment.segments[idx].name in [\"when\", \"end\"]:\n                        return None\n                    idx += 1  # pragma: no cover\n                # Determine if \"else\" is followed by a boolean\n                else_bool_type = None\n                while context.segment.segments[idx].name != \"end\":\n                    if context.segment.segments[idx].raw_upper in [\"TRUE\", \"FALSE\"]:\n                        else_bool_type = context.segment.segments[idx].raw_upper\n                    idx += 1\n            # If then-bool-else-bool, return fixes\n            if (\n                then_bool_type is not None\n                and else_bool_type is not None\n                and then_bool_type != else_bool_type\n            ):\n                # Generate list of segments to delete -- everything but the\n                # first expression.\n                delete_segments = []\n                for s in context.segment.segments:\n                    if s != context.segment.segments[expression_idx]:\n                        delete_segments.append(s)\n                # If then-false, add \"not\" and space\n                edits = []\n                if then_bool_type == \"FALSE\":\n                    edits.extend(\n                        [\n                            KeywordSegment(\"not\"),\n                            WhitespaceSegment(),\n                        ]\n                    )\n                # Add coalesce and parenthesis\n                edits.extend(\n                    [\n                        KeywordSegment(\"coalesce\"),\n                        SymbolSegment(\"(\", name=\"start_bracket\", type=\"start_bracket\"),\n                    ]\n                )\n                edit_coalesce_target = context.segment.segments[0]\n                fixes = []\n                fixes.append(\n                    LintFix(\n                        \"edit\",\n                        edit_coalesce_target,\n                        edits,\n                    )\n                )\n                # Add comma, bool, closing parenthesis\n                expression = context.segment.segments[expression_idx + 1]\n                closing_parenthesis = [\n                    SymbolSegment(\",\", name=\"comma\", type=\"comma\"),\n                    WhitespaceSegment(),\n                    KeywordSegment(\"false\"),\n                    SymbolSegment(\")\", name=\"end_bracket\", type=\"end_bracket\"),\n                ]\n                fixes.append(\n                    LintFix(\n                        \"edit\",\n                        expression,\n                        closing_parenthesis,\n                    )\n                )\n                # Generate a \"delete\" action for each segment in\n                # delete_segments EXCEPT the one being edited to become a call\n                # to \"coalesce(\". Deleting and editing the same segment has\n                # unpredictable behavior.\n                fixes += [\n                    LintFix(\"delete\", s)\n                    for s in delete_segments\n                    if s is not edit_coalesce_target\n                ]\n                return LintResult(\n                    anchor=context.segment.segments[expression_idx],\n                    fixes=fixes,\n                    description=\"Case when returns booleans.\",\n                )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l043", "py", "rule_l043", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "find", "rule", "violations", "and", "provide", "fixes", "0", "look", "for", "a", "case", "expression", "1", "find", "the", "first", "expression", "and", "then", "2", "determine", "if", "the", "then", "is", "followed", "by", "a", "boolean", "3", "if", "so", "determine", "if", "the", "first", "then", "bool", "is", "followed", "by", "an", "else", "bool", "4", "if", "so", "delete", "everything", "but", "the", "first", "expression", "5a", "if", "then", "true", "else", "false", "return", "deletions", "wrap", "with", "coalesce", "5b", "if", "then", "false", "else", "true", "return", "deletions", "add", "a", "not", "condition", "wrap", "with", "parenthesis", "and", "coalesce", "look", "for", "a", "case", "expression", "if", "context", "segment", "is_type", "case_expression", "and", "context", "segment", "segments", "0", "name", "case", "find", "the", "first", "expression", "and", "then", "idx", "0", "while", "context", "segment", "segments", "idx", "name", "then", "if", "context", "segment", "segments", "idx", "is_type", "expression", "expression_idx", "idx", "idx", "1", "determine", "if", "then", "is", "followed", "by", "a", "boolean", "then_bool_type", "none", "while", "context", "segment", "segments", "idx", "name", "not", "in", "when", "else", "end", "if", "context", "segment", "segments", "idx", "raw_upper", "in", "true", "false", "then_bool_type", "context", "segment", "segments", "idx", "raw_upper", "idx", "1", "if", "then_bool_type", "determine", "if", "the", "first", "then", "bool", "is", "followed", "by", "an", "else", "bool", "while", "context", "segment", "segments", "idx", "name", "else", "if", "the", "first", "then", "bool", "is", "followed", "by", "a", "when", "or", "end", "exit", "if", "context", "segment", "segments", "idx", "name", "in", "when", "end", "return", "none", "idx", "1", "pragma", "no", "cover", "determine", "if", "else", "is", "followed", "by", "a", "boolean", "else_bool_type", "none", "while", "context", "segment", "segments", "idx", "name", "end", "if", "context", "segment", "segments", "idx", "raw_upper", "in", "true", "false", "else_bool_type", "context", "segment", "segments", "idx", "raw_upper", "idx", "1", "if", "then", "bool", "else", "bool", "return", "fixes", "if", "then_bool_type", "is", "not", "none", "and", "else_bool_type", "is", "not", "none", "and", "then_bool_type", "else_bool_type", "generate", "list", "of", "segments", "to", "delete", "everything", "but", "the", "first", "expression", "delete_segments", "for", "s", "in", "context", "segment", "segments", "if", "s", "context", "segment", "segments", "expression_idx", "delete_segments", "append", "s", "if", "then", "false", "add", "not", "and", "space", "edits", "if", "then_bool_type", "false", "edits", "extend", "keywordsegment", "not", "whitespacesegment", "add", "coalesce", "and", "parenthesis", "edits", "extend", "keywordsegment", "coalesce", "symbolsegment", "name", "start_bracket", "type", "start_bracket", "edit_coalesce_target", "context", "segment", "segments", "0", "fixes", "fixes", "append", "lintfix", "edit", "edit_coalesce_target", "edits", "add", "comma", "bool", "closing", "parenthesis", "expression", "context", "segment", "segments", "expression_idx", "1", "closing_parenthesis", "symbolsegment", "name", "comma", "type", "comma", "whitespacesegment", "keywordsegment", "false", "symbolsegment", "name", "end_bracket", "type", "end_bracket", "fixes", "append", "lintfix", "edit", "expression", "closing_parenthesis", "generate", "a", "delete", "action", "for", "each", "segment", "in", "delete_segments", "except", "the", "one", "being", "edited", "to", "become", "a", "call", "to", "coalesce", "deleting", "and", "editing", "the", "same", "segment", "has", "unpredictable", "behavior", "fixes", "lintfix", "delete", "s", "for", "s", "in", "delete_segments", "if", "s", "is", "not", "edit_coalesce_target", "return", "lintresult", "anchor", "context", "segment", "segments", "expression_idx", "fixes", "fixes", "description", "case", "when", "returns", "booleans", "return", "none"], "doc_len": 436}
{"doc_id": "src/sqlfluff/rules/L044.py::Rule_L044._handle_alias", "file_path": "src/sqlfluff/rules/L044.py", "class_name": "Rule_L044", "func_name": "_handle_alias", "text": "文件路径: src/sqlfluff/rules/L044.py, 类名: Rule_L044\n    def _handle_alias(self, alias_info, dialect, queries):\n        select_info_target = SelectCrawler.get(\n            alias_info.from_expression_element, queries, dialect\n        )\n        if isinstance(select_info_target, str):\n            # It's an alias to an external table whose\n            # number of columns could vary without our\n            # knowledge. Thus, warn.\n            self.logger.debug(\n                f\"Query target {select_info_target} is external. Generating warning.\"\n            )\n            raise RuleFailure()\n        else:\n            # Handle nested SELECT.\n            self._analyze_result_columns(select_info_target, dialect, queries)\n", "tokens": ["src", "sqlfluff", "rules", "l044", "py", "rule_l044", "def", "_handle_alias", "self", "alias_info", "dialect", "queries", "select_info_target", "selectcrawler", "get", "alias_info", "from_expression_element", "queries", "dialect", "if", "isinstance", "select_info_target", "str", "it", "s", "an", "alias", "to", "an", "external", "table", "whose", "number", "of", "columns", "could", "vary", "without", "our", "knowledge", "thus", "warn", "self", "logger", "debug", "f", "query", "target", "select_info_target", "is", "external", "generating", "warning", "raise", "rulefailure", "else", "handle", "nested", "select", "self", "_analyze_result_columns", "select_info_target", "dialect", "queries"], "doc_len": 64}
{"doc_id": "src/sqlfluff/rules/L044.py::Rule_L044._analyze_result_columns", "file_path": "src/sqlfluff/rules/L044.py", "class_name": "Rule_L044", "func_name": "_analyze_result_columns", "text": "文件路径: src/sqlfluff/rules/L044.py, 类名: Rule_L044\n    def _analyze_result_columns(\n        self,\n        select_info_list: List[SelectCrawler],\n        dialect: Dialect,\n        queries: Dict[Optional[str], List[SelectCrawler]],\n    ):\n        \"\"\"Given info on a list of SELECTs, determine whether to warn.\"\"\"\n        # Recursively walk from the given query (select_info_list) to any\n        # wildcard columns in the select targets. If every wildcard evdentually\n        # resolves to a query without wildcards, all is well. Otherwise, warn.\n        for select_info in select_info_list:\n            self.logger.debug(f\"Analyzing query: {select_info.select_statement.raw}\")\n            for wildcard in select_info.get_wildcard_info():\n                if wildcard.tables:\n                    for wildcard_table in wildcard.tables:\n                        self.logger.debug(\n                            f\"Wildcard: {wildcard.segment.raw} has target {wildcard_table}\"\n                        )\n                        # Is it an alias?\n                        alias_info = select_info.find_alias(wildcard_table)\n                        if alias_info:\n                            # Found the alias matching the wildcard. Recurse,\n                            # analyzing the query associated with that alias.\n                            self._handle_alias(alias_info, dialect, queries)\n                        else:\n                            # Not an alias. Is it a CTE?\n                            if wildcard_table in queries:\n                                # Wildcard refers to a CTE. Analyze it.\n                                self._analyze_result_columns(\n                                    queries.pop(wildcard_table), dialect, queries\n                                )\n                            else:\n                                # Not CTE, not table alias. Presumably an\n                                # external table. Warn.\n                                self.logger.debug(\n                                    f\"Query target {wildcard_table} is external. Generating warning.\"\n                                )\n                                raise RuleFailure()\n                else:\n                    # No table was specified with the wildcard. Assume we're\n                    # querying from a nested select in FROM.\n                    select_info_target = SelectCrawler.get(\n                        select_info.select_statement, queries, dialect\n                    )\n                    assert isinstance(select_info_target, list)\n                    self._analyze_result_columns(\n                        select_info_target,\n                        dialect,\n                        queries,\n                    )\n", "tokens": ["src", "sqlfluff", "rules", "l044", "py", "rule_l044", "def", "_analyze_result_columns", "self", "select_info_list", "list", "selectcrawler", "dialect", "dialect", "queries", "dict", "optional", "str", "list", "selectcrawler", "given", "info", "on", "a", "list", "of", "selects", "determine", "whether", "to", "warn", "recursively", "walk", "from", "the", "given", "query", "select_info_list", "to", "any", "wildcard", "columns", "in", "the", "select", "targets", "if", "every", "wildcard", "evdentually", "resolves", "to", "a", "query", "without", "wildcards", "all", "is", "well", "otherwise", "warn", "for", "select_info", "in", "select_info_list", "self", "logger", "debug", "f", "analyzing", "query", "select_info", "select_statement", "raw", "for", "wildcard", "in", "select_info", "get_wildcard_info", "if", "wildcard", "tables", "for", "wildcard_table", "in", "wildcard", "tables", "self", "logger", "debug", "f", "wildcard", "wildcard", "segment", "raw", "has", "target", "wildcard_table", "is", "it", "an", "alias", "alias_info", "select_info", "find_alias", "wildcard_table", "if", "alias_info", "found", "the", "alias", "matching", "the", "wildcard", "recurse", "analyzing", "the", "query", "associated", "with", "that", "alias", "self", "_handle_alias", "alias_info", "dialect", "queries", "else", "not", "an", "alias", "is", "it", "a", "cte", "if", "wildcard_table", "in", "queries", "wildcard", "refers", "to", "a", "cte", "analyze", "it", "self", "_analyze_result_columns", "queries", "pop", "wildcard_table", "dialect", "queries", "else", "not", "cte", "not", "table", "alias", "presumably", "an", "external", "table", "warn", "self", "logger", "debug", "f", "query", "target", "wildcard_table", "is", "external", "generating", "warning", "raise", "rulefailure", "else", "no", "table", "was", "specified", "with", "the", "wildcard", "assume", "we", "re", "querying", "from", "a", "nested", "select", "in", "from", "select_info_target", "selectcrawler", "get", "select_info", "select_statement", "queries", "dialect", "assert", "isinstance", "select_info_target", "list", "self", "_analyze_result_columns", "select_info_target", "dialect", "queries"], "doc_len": 211}
{"doc_id": "src/sqlfluff/rules/L044.py::Rule_L044._eval", "file_path": "src/sqlfluff/rules/L044.py", "class_name": "Rule_L044", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L044.py, 类名: Rule_L044\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Outermost query should produce known number of columns.\"\"\"\n        if context.segment.is_type(\"statement\"):\n            queries = SelectCrawler.gather(context.segment, context.dialect)\n\n            # Begin analysis at the final, outer query (key=None).\n            if None in queries:\n                select_info = queries[None]\n                try:\n                    return self._analyze_result_columns(\n                        select_info, context.dialect, queries\n                    )\n                except RuleFailure:\n                    return LintResult(\n                        anchor=queries[None][0].select_info.select_statement\n                    )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l044", "py", "rule_l044", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "outermost", "query", "should", "produce", "known", "number", "of", "columns", "if", "context", "segment", "is_type", "statement", "queries", "selectcrawler", "gather", "context", "segment", "context", "dialect", "begin", "analysis", "at", "the", "final", "outer", "query", "key", "none", "if", "none", "in", "queries", "select_info", "queries", "none", "try", "return", "self", "_analyze_result_columns", "select_info", "context", "dialect", "queries", "except", "rulefailure", "return", "lintresult", "anchor", "queries", "none", "0", "select_info", "select_statement", "return", "none"], "doc_len": 69}
{"doc_id": "src/sqlfluff/rules/L045.py::Rule_L045._visit_sources", "file_path": "src/sqlfluff/rules/L045.py", "class_name": "Rule_L045", "func_name": "_visit_sources", "text": "文件路径: src/sqlfluff/rules/L045.py, 类名: Rule_L045\n    def _visit_sources(\n        cls,\n        select_info_list: List[SelectCrawler],\n        dialect: Dialect,\n        queries: Dict[Optional[str], List[SelectCrawler]],\n    ):\n        for select_info in select_info_list:\n            for source in SelectCrawler.crawl(\n                select_info.select_statement, queries, dialect\n            ):\n                if isinstance(source, list):\n                    cls._visit_sources(source, dialect, queries)\n", "tokens": ["src", "sqlfluff", "rules", "l045", "py", "rule_l045", "def", "_visit_sources", "cls", "select_info_list", "list", "selectcrawler", "dialect", "dialect", "queries", "dict", "optional", "str", "list", "selectcrawler", "for", "select_info", "in", "select_info_list", "for", "source", "in", "selectcrawler", "crawl", "select_info", "select_statement", "queries", "dialect", "if", "isinstance", "source", "list", "cls", "_visit_sources", "source", "dialect", "queries"], "doc_len": 42}
{"doc_id": "src/sqlfluff/rules/L045.py::Rule_L045._eval", "file_path": "src/sqlfluff/rules/L045.py", "class_name": "Rule_L045", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L045.py, 类名: Rule_L045\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        if context.segment.is_type(\"statement\"):\n            queries = SelectCrawler.gather(context.segment, context.dialect)\n            if None in queries:\n                # Begin analysis at the final, outer query (key=None).\n                self._visit_sources(queries.pop(None), context.dialect, queries)\n                if queries:\n                    return LintResult(anchor=context.segment)\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l045", "py", "rule_l045", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "if", "context", "segment", "is_type", "statement", "queries", "selectcrawler", "gather", "context", "segment", "context", "dialect", "if", "none", "in", "queries", "begin", "analysis", "at", "the", "final", "outer", "query", "key", "none", "self", "_visit_sources", "queries", "pop", "none", "context", "dialect", "queries", "if", "queries", "return", "lintresult", "anchor", "context", "segment", "return", "none"], "doc_len": 55}
{"doc_id": "src/sqlfluff/rules/L046.py::Rule_L046._get_whitespace_ends", "file_path": "src/sqlfluff/rules/L046.py", "class_name": "Rule_L046", "func_name": "_get_whitespace_ends", "text": "文件路径: src/sqlfluff/rules/L046.py, 类名: Rule_L046\n    def _get_whitespace_ends(s: str) -> Tuple[str, str, str]:\n        \"\"\"Remove tag ends and partition off any whitespace ends.\"\"\"\n        # Jinja tags all have a length of two. We can use slicing\n        # to remove them easily.\n        main = s[2:-2]\n        # Optionally Jinja tags may also have plus of minus notation\n        # https://jinja2docs.readthedocs.io/en/stable/templates.html#whitespace-control\n        modifier_chars = [\"+\", \"-\"]\n        if main and main[0] in modifier_chars:\n            main = main[1:]\n        if main and main[-1] in modifier_chars:\n            main = main[:-1]\n        inner = main.strip()\n        pos = main.find(inner)\n        return main[:pos], inner, main[pos + len(inner) :]\n", "tokens": ["src", "sqlfluff", "rules", "l046", "py", "rule_l046", "def", "_get_whitespace_ends", "s", "str", "tuple", "str", "str", "str", "remove", "tag", "ends", "and", "partition", "off", "any", "whitespace", "ends", "jinja", "tags", "all", "have", "a", "length", "of", "two", "we", "can", "use", "slicing", "to", "remove", "them", "easily", "main", "s", "2", "2", "optionally", "jinja", "tags", "may", "also", "have", "plus", "of", "minus", "notation", "https", "jinja2docs", "readthedocs", "io", "en", "stable", "templates", "html", "whitespace", "control", "modifier_chars", "if", "main", "and", "main", "0", "in", "modifier_chars", "main", "main", "1", "if", "main", "and", "main", "1", "in", "modifier_chars", "main", "main", "1", "inner", "main", "strip", "pos", "main", "find", "inner", "return", "main", "pos", "inner", "main", "pos", "len", "inner"], "doc_len": 99}
{"doc_id": "src/sqlfluff/rules/L046.py::Rule_L046._eval", "file_path": "src/sqlfluff/rules/L046.py", "class_name": "Rule_L046", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L046.py, 类名: Rule_L046\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Look for non-literal segments.\"\"\"\n        if not context.segment.pos_marker.is_literal():\n            # Does it actually look like a tag?\n            src_raw = context.segment.pos_marker.source_str()\n            if not src_raw or src_raw[0] != \"{\" or src_raw[-1] != \"}\":\n                return LintResult(memory=context.memory)\n\n            # Dedupe using a memory of source indexes.\n            # This is important because several positions in the\n            # templated file may refer to the same position in the\n            # source file and we only want to get one violation.\n            src_idx = context.segment.pos_marker.source_slice.start\n            if context.memory and src_idx in context.memory:\n                return LintResult(memory=context.memory)\n            if not context.memory:\n                memory = set()\n            else:\n                memory = context.memory\n            memory.add(src_idx)\n\n            # Get the inner section\n            ws_pre, inner, ws_post = self._get_whitespace_ends(src_raw)\n\n            # For the following section, whitespace should be a single\n            # whitespace OR it should contain a newline.\n\n            # Check the initial whitespace.\n            if not ws_pre or (ws_pre != \" \" and \"\\n\" not in ws_pre):\n                return LintResult(memory=memory, anchor=context.segment)\n            # Check latter whitespace.\n            if not ws_post or (ws_post != \" \" and \"\\n\" not in ws_post):\n                return LintResult(memory=memory, anchor=context.segment)\n\n            return LintResult(memory=memory)\n        return LintResult(memory=context.memory)\n", "tokens": ["src", "sqlfluff", "rules", "l046", "py", "rule_l046", "def", "_eval", "self", "context", "rulecontext", "lintresult", "look", "for", "non", "literal", "segments", "if", "not", "context", "segment", "pos_marker", "is_literal", "does", "it", "actually", "look", "like", "a", "tag", "src_raw", "context", "segment", "pos_marker", "source_str", "if", "not", "src_raw", "or", "src_raw", "0", "or", "src_raw", "1", "return", "lintresult", "memory", "context", "memory", "dedupe", "using", "a", "memory", "of", "source", "indexes", "this", "is", "important", "because", "several", "positions", "in", "the", "templated", "file", "may", "refer", "to", "the", "same", "position", "in", "the", "source", "file", "and", "we", "only", "want", "to", "get", "one", "violation", "src_idx", "context", "segment", "pos_marker", "source_slice", "start", "if", "context", "memory", "and", "src_idx", "in", "context", "memory", "return", "lintresult", "memory", "context", "memory", "if", "not", "context", "memory", "memory", "set", "else", "memory", "context", "memory", "memory", "add", "src_idx", "get", "the", "inner", "section", "ws_pre", "inner", "ws_post", "self", "_get_whitespace_ends", "src_raw", "for", "the", "following", "section", "whitespace", "should", "be", "a", "single", "whitespace", "or", "it", "should", "contain", "a", "newline", "check", "the", "initial", "whitespace", "if", "not", "ws_pre", "or", "ws_pre", "and", "n", "not", "in", "ws_pre", "return", "lintresult", "memory", "memory", "anchor", "context", "segment", "check", "latter", "whitespace", "if", "not", "ws_post", "or", "ws_post", "and", "n", "not", "in", "ws_post", "return", "lintresult", "memory", "memory", "anchor", "context", "segment", "return", "lintresult", "memory", "memory", "return", "lintresult", "memory", "context", "memory"], "doc_len": 192}
{"doc_id": "src/sqlfluff/rules/L047.py::Rule_L047._eval", "file_path": "src/sqlfluff/rules/L047.py", "class_name": "Rule_L047", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L047.py, 类名: Rule_L047\n    def _eval(self, context: RuleContext) -> Optional[LintResult]:\n        \"\"\"Find rule violations and provide fixes.\"\"\"\n        # Config type hints\n        self.prefer_count_0: bool\n        self.prefer_count_1: bool\n\n        if (\n            context.segment.is_type(\"function\")\n            and context.segment.get_child(\"function_name\").raw_upper == \"COUNT\"\n        ):\n            # Get bracketed content\n            bracketed = context.segment.get_child(\"bracketed\")\n\n            if not bracketed:  # pragma: no cover\n                return None\n\n            f_content = [\n                seg\n                for seg in bracketed.segments\n                if not seg.is_meta\n                and not seg.is_type(\n                    \"start_bracket\",\n                    \"end_bracket\",\n                    \"whitespace\",\n                    \"newline\",\n                )\n            ]\n            if len(f_content) != 1:  # pragma: no cover\n                return None\n\n            preferred = \"*\"\n            if self.prefer_count_1:\n                preferred = \"1\"\n            elif self.prefer_count_0:\n                preferred = \"0\"\n\n            if f_content[0].is_type(\"star\") and (\n                self.prefer_count_1 or self.prefer_count_0\n            ):\n                return LintResult(\n                    anchor=context.segment,\n                    fixes=[\n                        LintFix(\n                            \"edit\",\n                            f_content[0],\n                            f_content[0].edit(f_content[0].raw.replace(\"*\", preferred)),\n                        ),\n                    ],\n                )\n\n            if f_content[0].is_type(\"expression\"):\n                expression_content = [\n                    seg for seg in f_content[0].segments if not seg.is_meta\n                ]\n\n                if (\n                    len(expression_content) == 1\n                    and expression_content[0].is_type(\"literal\")\n                    and expression_content[0].raw in [\"0\", \"1\"]\n                    and expression_content[0].raw != preferred\n                ):\n                    return LintResult(\n                        anchor=context.segment,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                expression_content[0],\n                                expression_content[0].edit(\n                                    expression_content[0].raw.replace(\n                                        expression_content[0].raw, preferred\n                                    )\n                                ),\n                            ),\n                        ],\n                    )\n        return None\n", "tokens": ["src", "sqlfluff", "rules", "l047", "py", "rule_l047", "def", "_eval", "self", "context", "rulecontext", "optional", "lintresult", "find", "rule", "violations", "and", "provide", "fixes", "config", "type", "hints", "self", "prefer_count_0", "bool", "self", "prefer_count_1", "bool", "if", "context", "segment", "is_type", "function", "and", "context", "segment", "get_child", "function_name", "raw_upper", "count", "get", "bracketed", "content", "bracketed", "context", "segment", "get_child", "bracketed", "if", "not", "bracketed", "pragma", "no", "cover", "return", "none", "f_content", "seg", "for", "seg", "in", "bracketed", "segments", "if", "not", "seg", "is_meta", "and", "not", "seg", "is_type", "start_bracket", "end_bracket", "whitespace", "newline", "if", "len", "f_content", "1", "pragma", "no", "cover", "return", "none", "preferred", "if", "self", "prefer_count_1", "preferred", "1", "elif", "self", "prefer_count_0", "preferred", "0", "if", "f_content", "0", "is_type", "star", "and", "self", "prefer_count_1", "or", "self", "prefer_count_0", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "f_content", "0", "f_content", "0", "edit", "f_content", "0", "raw", "replace", "preferred", "if", "f_content", "0", "is_type", "expression", "expression_content", "seg", "for", "seg", "in", "f_content", "0", "segments", "if", "not", "seg", "is_meta", "if", "len", "expression_content", "1", "and", "expression_content", "0", "is_type", "literal", "and", "expression_content", "0", "raw", "in", "0", "1", "and", "expression_content", "0", "raw", "preferred", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "edit", "expression_content", "0", "expression_content", "0", "edit", "expression_content", "0", "raw", "replace", "expression_content", "0", "raw", "preferred", "return", "none"], "doc_len": 185}
{"doc_id": "src/sqlfluff/rules/L048.py::Rule_L048._missing_whitespace", "file_path": "src/sqlfluff/rules/L048.py", "class_name": "Rule_L048", "func_name": "_missing_whitespace", "text": "文件路径: src/sqlfluff/rules/L048.py, 类名: Rule_L048\n    def _missing_whitespace(seg: BaseSegment, before=True) -> bool:\n        \"\"\"Check whether we're missing whitespace given an adjoining segment.\n\n        This avoids flagging for commas after quoted strings.\n        https://github.com/sqlfluff/sqlfluff/issues/943\n        \"\"\"\n        simple_res = Rule_L006._missing_whitespace(seg, before=before)\n        if not before and seg and seg.is_type(\"comma\", \"statement_terminator\"):\n            return False\n        return simple_res\n", "tokens": ["src", "sqlfluff", "rules", "l048", "py", "rule_l048", "def", "_missing_whitespace", "seg", "basesegment", "before", "true", "bool", "check", "whether", "we", "re", "missing", "whitespace", "given", "an", "adjoining", "segment", "this", "avoids", "flagging", "for", "commas", "after", "quoted", "strings", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "943", "simple_res", "rule_l006", "_missing_whitespace", "seg", "before", "before", "if", "not", "before", "and", "seg", "and", "seg", "is_type", "comma", "statement_terminator", "return", "false", "return", "simple_res"], "doc_len": 58}
{"doc_id": "src/sqlfluff/rules/L049.py::Rule_L049._eval", "file_path": "src/sqlfluff/rules/L049.py", "class_name": "Rule_L049", "func_name": "_eval", "text": "文件路径: src/sqlfluff/rules/L049.py, 类名: Rule_L049\n    def _eval(self, context: RuleContext) -> LintResult:\n        \"\"\"Relational operators should not be used to check for NULL values.\"\"\"\n        # Context/motivation for this rule:\n        # https://news.ycombinator.com/item?id=28772289\n        # https://stackoverflow.com/questions/9581745/sql-is-null-and-null\n        if len(context.segment.segments) <= 2:\n            return LintResult()\n\n        # Iterate through children of this segment looking for equals or \"not\n        # equals\". Once found, check if the next code segment is a NULL literal.\n        idx_operator = None\n        operator = None\n        for idx, sub_seg in enumerate(context.segment.segments):\n            # Skip anything which is whitespace or non-code.\n            if sub_seg.is_whitespace or not sub_seg.is_code:\n                continue\n\n            # Look for \"=\" or \"<>\".\n            if not operator and sub_seg.name in (\"equals\", \"not_equal_to\"):\n                self.logger.debug(\n                    \"Found equals/not equals @%s: %r\", sub_seg.pos_marker, sub_seg.raw\n                )\n                idx_operator = idx\n                operator = sub_seg\n            elif operator:\n                # Look for a \"NULL\" literal.\n                if sub_seg.name == \"null_literal\":\n                    self.logger.debug(\n                        \"Found NULL literal following equals/not equals @%s: %r\",\n                        sub_seg.pos_marker,\n                        sub_seg.raw,\n                    )\n                    if sub_seg.raw[0] == \"N\":\n                        is_seg = KeywordSegment(\"IS\")\n                        not_seg = KeywordSegment(\"NOT\")\n                    else:\n                        is_seg = KeywordSegment(\"is\")\n                        not_seg = KeywordSegment(\"not\")\n\n                    edit: List[Union[WhitespaceSegment, KeywordSegment]] = (\n                        [is_seg]\n                        if operator.name == \"equals\"\n                        else [\n                            is_seg,\n                            WhitespaceSegment(),\n                            not_seg,\n                        ]\n                    )\n                    prev_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=True\n                    )\n                    next_seg = self._find_segment(\n                        idx_operator, context.segment.segments, before=False\n                    )\n                    if self._missing_whitespace(prev_seg, before=True):\n                        whitespace_segment: List[\n                            Union[WhitespaceSegment, KeywordSegment]\n                        ] = [WhitespaceSegment()]\n                        edit = whitespace_segment + edit\n                    if self._missing_whitespace(next_seg, before=False):\n                        edit = edit + [WhitespaceSegment()]\n                    return LintResult(\n                        anchor=operator,\n                        fixes=[\n                            LintFix(\n                                \"edit\",\n                                operator,\n                                edit,\n                            )\n                        ],\n                    )\n        # If we get to here, it's not a violation\n        return LintResult()\n", "tokens": ["src", "sqlfluff", "rules", "l049", "py", "rule_l049", "def", "_eval", "self", "context", "rulecontext", "lintresult", "relational", "operators", "should", "not", "be", "used", "to", "check", "for", "null", "values", "context", "motivation", "for", "this", "rule", "https", "news", "ycombinator", "com", "item", "id", "28772289", "https", "stackoverflow", "com", "questions", "9581745", "sql", "is", "null", "and", "null", "if", "len", "context", "segment", "segments", "2", "return", "lintresult", "iterate", "through", "children", "of", "this", "segment", "looking", "for", "equals", "or", "not", "equals", "once", "found", "check", "if", "the", "next", "code", "segment", "is", "a", "null", "literal", "idx_operator", "none", "operator", "none", "for", "idx", "sub_seg", "in", "enumerate", "context", "segment", "segments", "skip", "anything", "which", "is", "whitespace", "or", "non", "code", "if", "sub_seg", "is_whitespace", "or", "not", "sub_seg", "is_code", "continue", "look", "for", "or", "if", "not", "operator", "and", "sub_seg", "name", "in", "equals", "not_equal_to", "self", "logger", "debug", "found", "equals", "not", "equals", "s", "r", "sub_seg", "pos_marker", "sub_seg", "raw", "idx_operator", "idx", "operator", "sub_seg", "elif", "operator", "look", "for", "a", "null", "literal", "if", "sub_seg", "name", "null_literal", "self", "logger", "debug", "found", "null", "literal", "following", "equals", "not", "equals", "s", "r", "sub_seg", "pos_marker", "sub_seg", "raw", "if", "sub_seg", "raw", "0", "n", "is_seg", "keywordsegment", "is", "not_seg", "keywordsegment", "not", "else", "is_seg", "keywordsegment", "is", "not_seg", "keywordsegment", "not", "edit", "list", "union", "whitespacesegment", "keywordsegment", "is_seg", "if", "operator", "name", "equals", "else", "is_seg", "whitespacesegment", "not_seg", "prev_seg", "self", "_find_segment", "idx_operator", "context", "segment", "segments", "before", "true", "next_seg", "self", "_find_segment", "idx_operator", "context", "segment", "segments", "before", "false", "if", "self", "_missing_whitespace", "prev_seg", "before", "true", "whitespace_segment", "list", "union", "whitespacesegment", "keywordsegment", "whitespacesegment", "edit", "whitespace_segment", "edit", "if", "self", "_missing_whitespace", "next_seg", "before", "false", "edit", "edit", "whitespacesegment", "return", "lintresult", "anchor", "operator", "fixes", "lintfix", "edit", "operator", "edit", "if", "we", "get", "to", "here", "it", "s", "not", "a", "violation", "return", "lintresult"], "doc_len": 256}
{"doc_id": "src/sqlfluff/testing/rules.py::load_test_cases", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "load_test_cases", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef load_test_cases(\n    test_cases_path: str,\n) -> Tuple[List[str], List[RuleTestCase]]:\n    \"\"\"Load rule test cases from YAML files.\"\"\"\n    ids = []\n    test_cases = []\n\n    for path in sorted(glob(test_cases_path)):\n        with open(path) as f:\n            raw = f.read()\n\n        y = yaml.safe_load(raw)\n\n        rule = y.pop(\"rule\")\n        global_config = y.pop(\"configs\", None)\n        if global_config:\n            for i in y:\n                if not (\"configs\" in y[i].keys()):\n                    y[i].update({\"configs\": global_config})\n        ids.extend([rule + \"_\" + t for t in y])\n        test_cases.extend([RuleTestCase(rule=rule, **v) for k, v in y.items()])\n\n    return ids, test_cases\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "load_test_cases", "test_cases_path", "str", "tuple", "list", "str", "list", "ruletestcase", "load", "rule", "test", "cases", "from", "yaml", "files", "ids", "test_cases", "for", "path", "in", "sorted", "glob", "test_cases_path", "with", "open", "path", "as", "f", "raw", "f", "read", "y", "yaml", "safe_load", "raw", "rule", "y", "pop", "rule", "global_config", "y", "pop", "configs", "none", "if", "global_config", "for", "i", "in", "y", "if", "not", "configs", "in", "y", "i", "keys", "y", "i", "update", "configs", "global_config", "ids", "extend", "rule", "_", "t", "for", "t", "in", "y", "test_cases", "extend", "ruletestcase", "rule", "rule", "v", "for", "k", "v", "in", "y", "items", "return", "ids", "test_cases"], "doc_len": 92}
{"doc_id": "src/sqlfluff/testing/rules.py::get_rule_from_set", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "get_rule_from_set", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef get_rule_from_set(code, config):\n    \"\"\"Fetch a rule from the rule set.\"\"\"\n    for r in get_ruleset().get_rulelist(config=config):\n        if r.code == code:\n            return r\n    raise ValueError(f\"{code!r} not in {get_ruleset()!r}\")\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "get_rule_from_set", "code", "config", "fetch", "a", "rule", "from", "the", "rule", "set", "for", "r", "in", "get_ruleset", "get_rulelist", "config", "config", "if", "r", "code", "code", "return", "r", "raise", "valueerror", "f", "code", "r", "not", "in", "get_ruleset", "r"], "doc_len": 38}
{"doc_id": "src/sqlfluff/testing/rules.py::assert_rule_fail_in_sql", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "assert_rule_fail_in_sql", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef assert_rule_fail_in_sql(code, sql, configs=None, line_numbers=None):\n    \"\"\"Assert that a given rule does fail on the given sql.\"\"\"\n    # Set up the config to only use the rule we are testing.\n    cfg = FluffConfig(configs=configs, overrides={\"rules\": code})\n    # Lint it using the current config (while in fix mode)\n    linted = Linter(config=cfg).lint_string(sql, fix=True)\n    lerrs = linted.get_violations()\n    print(f\"Errors Found: {lerrs}\")\n    for e in lerrs:\n        if e.desc().startswith(\"Unexpected exception\"):\n            pytest.fail(f\"Linter failed with {e.desc()}\")  # pragma: no cover\n    parse_errors = list(\n        filter(lambda v: isinstance(v, (SQLParseError, SQLTemplaterError)), lerrs)\n    )\n    if parse_errors:\n        pytest.fail(f\"Found the following parse errors in test case: {parse_errors}\")\n    if not any(v.rule.code == code for v in lerrs):\n        pytest.fail(\n            f\"No {code} failures found in query which should fail.\",\n            pytrace=False,\n        )\n    if line_numbers:\n        actual_line_numbers = [e.line_no for e in lerrs]\n        if line_numbers != actual_line_numbers:  # pragma: no cover\n            pytest.fail(\n                \"Expected errors on lines {}, but got errors on lines {}\".format(\n                    line_numbers, actual_line_numbers\n                )\n            )\n    # The query should already have been fixed if possible so just return the raw.\n    if linted.num_violations(fixable=True) > 0:\n        fixed, _ = linted.fix_string()\n        return fixed\n    else:\n        return linted.tree.raw\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "assert_rule_fail_in_sql", "code", "sql", "configs", "none", "line_numbers", "none", "assert", "that", "a", "given", "rule", "does", "fail", "on", "the", "given", "sql", "set", "up", "the", "config", "to", "only", "use", "the", "rule", "we", "are", "testing", "cfg", "fluffconfig", "configs", "configs", "overrides", "rules", "code", "lint", "it", "using", "the", "current", "config", "while", "in", "fix", "mode", "linted", "linter", "config", "cfg", "lint_string", "sql", "fix", "true", "lerrs", "linted", "get_violations", "print", "f", "errors", "found", "lerrs", "for", "e", "in", "lerrs", "if", "e", "desc", "startswith", "unexpected", "exception", "pytest", "fail", "f", "linter", "failed", "with", "e", "desc", "pragma", "no", "cover", "parse_errors", "list", "filter", "lambda", "v", "isinstance", "v", "sqlparseerror", "sqltemplatererror", "lerrs", "if", "parse_errors", "pytest", "fail", "f", "found", "the", "following", "parse", "errors", "in", "test", "case", "parse_errors", "if", "not", "any", "v", "rule", "code", "code", "for", "v", "in", "lerrs", "pytest", "fail", "f", "no", "code", "failures", "found", "in", "query", "which", "should", "fail", "pytrace", "false", "if", "line_numbers", "actual_line_numbers", "e", "line_no", "for", "e", "in", "lerrs", "if", "line_numbers", "actual_line_numbers", "pragma", "no", "cover", "pytest", "fail", "expected", "errors", "on", "lines", "but", "got", "errors", "on", "lines", "format", "line_numbers", "actual_line_numbers", "the", "query", "should", "already", "have", "been", "fixed", "if", "possible", "so", "just", "return", "the", "raw", "if", "linted", "num_violations", "fixable", "true", "0", "fixed", "_", "linted", "fix_string", "return", "fixed", "else", "return", "linted", "tree", "raw"], "doc_len": 199}
{"doc_id": "src/sqlfluff/testing/rules.py::assert_rule_pass_in_sql", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "assert_rule_pass_in_sql", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef assert_rule_pass_in_sql(code, sql, configs=None):\n    \"\"\"Assert that a given rule doesn't fail on the given sql.\"\"\"\n    # Configs allows overrides if we want to use them.\n    cfg = FluffConfig(configs=configs)\n    r = get_rule_from_set(code, config=cfg)\n    linter = Linter(config=cfg)\n    rendered = linter.render_string(sql, fname=\"<STR>\", config=cfg, encoding=\"utf-8\")\n    parsed = linter.parse_rendered(rendered, recurse=True)\n    if parsed.violations:\n        pytest.fail(parsed.violations[0].desc() + \"\\n\" + parsed.tree.stringify())\n    print(f\"Parsed:\\n {parsed.tree.stringify()}\")\n    lerrs, _, _, _ = r.crawl(\n        parsed.tree, [], dialect=cfg.get(\"dialect_obj\"), templated_file=rendered[0]\n    )\n    print(f\"Errors Found: {lerrs}\")\n    if any(v.rule.code == code for v in lerrs):\n        pytest.fail(f\"Found {code} failures in query which should pass.\", pytrace=False)\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "assert_rule_pass_in_sql", "code", "sql", "configs", "none", "assert", "that", "a", "given", "rule", "doesn", "t", "fail", "on", "the", "given", "sql", "configs", "allows", "overrides", "if", "we", "want", "to", "use", "them", "cfg", "fluffconfig", "configs", "configs", "r", "get_rule_from_set", "code", "config", "cfg", "linter", "linter", "config", "cfg", "rendered", "linter", "render_string", "sql", "fname", "str", "config", "cfg", "encoding", "utf", "8", "parsed", "linter", "parse_rendered", "rendered", "recurse", "true", "if", "parsed", "violations", "pytest", "fail", "parsed", "violations", "0", "desc", "n", "parsed", "tree", "stringify", "print", "f", "parsed", "n", "parsed", "tree", "stringify", "lerrs", "_", "_", "_", "r", "crawl", "parsed", "tree", "dialect", "cfg", "get", "dialect_obj", "templated_file", "rendered", "0", "print", "f", "errors", "found", "lerrs", "if", "any", "v", "rule", "code", "code", "for", "v", "in", "lerrs", "pytest", "fail", "f", "found", "code", "failures", "in", "query", "which", "should", "pass", "pytrace", "false"], "doc_len": 125}
{"doc_id": "src/sqlfluff/testing/rules.py::assert_rule_raises_violations_in_file", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "assert_rule_raises_violations_in_file", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef assert_rule_raises_violations_in_file(rule, fpath, violations, fluff_config):\n    \"\"\"Assert that a given rule raises given errors in specific positions of a file.\"\"\"\n    lntr = Linter(config=fluff_config)\n    lnt = lntr.lint_path(fpath)\n    # Reformat the test data to match the format we're expecting. We use\n    # sets because we really don't care about order and if one is missing,\n    # we don't care about the orders of the correct ones.\n    assert set(lnt.check_tuples()) == {(rule, v[0], v[1]) for v in violations}\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "assert_rule_raises_violations_in_file", "rule", "fpath", "violations", "fluff_config", "assert", "that", "a", "given", "rule", "raises", "given", "errors", "in", "specific", "positions", "of", "a", "file", "lntr", "linter", "config", "fluff_config", "lnt", "lntr", "lint_path", "fpath", "reformat", "the", "test", "data", "to", "match", "the", "format", "we", "re", "expecting", "we", "use", "sets", "because", "we", "really", "don", "t", "care", "about", "order", "and", "if", "one", "is", "missing", "we", "don", "t", "care", "about", "the", "orders", "of", "the", "correct", "ones", "assert", "set", "lnt", "check_tuples", "rule", "v", "0", "v", "1", "for", "v", "in", "violations"], "doc_len": 84}
{"doc_id": "src/sqlfluff/testing/rules.py::rules__test_helper", "file_path": "src/sqlfluff/testing/rules.py", "class_name": null, "func_name": "rules__test_helper", "text": "文件路径: src/sqlfluff/testing/rules.py\ndef rules__test_helper(test_case):\n    \"\"\"Test that a rule passes/fails on a set of test_cases.\n\n    Optionally, also test the fixed string if provided in the test case.\n    \"\"\"\n    if test_case.skip:\n        pytest.skip(test_case.skip)\n\n    if test_case.pass_str:\n        assert_rule_pass_in_sql(\n            test_case.rule,\n            test_case.pass_str,\n            configs=test_case.configs,\n        )\n    if test_case.fail_str:\n        res = assert_rule_fail_in_sql(\n            test_case.rule,\n            test_case.fail_str,\n            configs=test_case.configs,\n            line_numbers=test_case.line_numbers,\n        )\n        # If a `fixed` value is provided then check it matches\n        if test_case.fix_str:\n            assert res == test_case.fix_str\n", "tokens": ["src", "sqlfluff", "testing", "rules", "py", "def", "rules__test_helper", "test_case", "test", "that", "a", "rule", "passes", "fails", "on", "a", "set", "of", "test_cases", "optionally", "also", "test", "the", "fixed", "string", "if", "provided", "in", "the", "test", "case", "if", "test_case", "skip", "pytest", "skip", "test_case", "skip", "if", "test_case", "pass_str", "assert_rule_pass_in_sql", "test_case", "rule", "test_case", "pass_str", "configs", "test_case", "configs", "if", "test_case", "fail_str", "res", "assert_rule_fail_in_sql", "test_case", "rule", "test_case", "fail_str", "configs", "test_case", "configs", "line_numbers", "test_case", "line_numbers", "if", "a", "fixed", "value", "is", "provided", "then", "check", "it", "matches", "if", "test_case", "fix_str", "assert", "res", "test_case", "fix_str"], "doc_len": 81}
{"doc_id": "test/conftest.py::get_parse_fixtures", "file_path": "test/conftest.py", "class_name": null, "func_name": "get_parse_fixtures", "text": "文件路径: test/conftest.py\ndef get_parse_fixtures(fail_on_missing_yml=False):\n    \"\"\"Search for all parsing fixtures.\"\"\"\n    parse_success_examples = []\n    parse_structure_examples = []\n    # Generate the filenames for each dialect from the parser test directory\n    for d in os.listdir(os.path.join(\"test\", \"fixtures\", \"dialects\")):\n        # Ignore documentation\n        if d.endswith(\".md\"):\n            continue\n        # assume that d is now the name of a dialect\n        dirlist = os.listdir(os.path.join(\"test\", \"fixtures\", \"dialects\", d))\n        for f in dirlist:\n            has_yml = False\n            if f.endswith(\".sql\"):\n                root = f[:-4]\n                # only look for sql files\n                parse_success_examples.append((d, f))\n                # Look for the code_only version of the structure\n                y = root + \".yml\"\n                if y in dirlist:\n                    parse_structure_examples.append((d, f, True, y))\n                    has_yml = True\n                # Look for the non-code included version of the structure\n                y = root + \"_nc.yml\"\n                if y in dirlist:\n                    parse_structure_examples.append((d, f, False, y))\n                    has_yml = True\n                if not has_yml and fail_on_missing_yml:\n                    raise (\n                        Exception(\n                            f\"Missing .yml file for {os.path.join(d, f)}. Run the test/generate_parse_fixture_yml.py script!\"\n                        )\n                    )\n    return parse_success_examples, parse_structure_examples\n", "tokens": ["test", "conftest", "py", "def", "get_parse_fixtures", "fail_on_missing_yml", "false", "search", "for", "all", "parsing", "fixtures", "parse_success_examples", "parse_structure_examples", "generate", "the", "filenames", "for", "each", "dialect", "from", "the", "parser", "test", "directory", "for", "d", "in", "os", "listdir", "os", "path", "join", "test", "fixtures", "dialects", "ignore", "documentation", "if", "d", "endswith", "md", "continue", "assume", "that", "d", "is", "now", "the", "name", "of", "a", "dialect", "dirlist", "os", "listdir", "os", "path", "join", "test", "fixtures", "dialects", "d", "for", "f", "in", "dirlist", "has_yml", "false", "if", "f", "endswith", "sql", "root", "f", "4", "only", "look", "for", "sql", "files", "parse_success_examples", "append", "d", "f", "look", "for", "the", "code_only", "version", "of", "the", "structure", "y", "root", "yml", "if", "y", "in", "dirlist", "parse_structure_examples", "append", "d", "f", "true", "y", "has_yml", "true", "look", "for", "the", "non", "code", "included", "version", "of", "the", "structure", "y", "root", "_nc", "yml", "if", "y", "in", "dirlist", "parse_structure_examples", "append", "d", "f", "false", "y", "has_yml", "true", "if", "not", "has_yml", "and", "fail_on_missing_yml", "raise", "exception", "f", "missing", "yml", "file", "for", "os", "path", "join", "d", "f", "run", "the", "test", "generate_parse_fixture_yml", "py", "script", "return", "parse_success_examples", "parse_structure_examples"], "doc_len": 160}
{"doc_id": "test/conftest.py::make_dialect_path", "file_path": "test/conftest.py", "class_name": null, "func_name": "make_dialect_path", "text": "文件路径: test/conftest.py\ndef make_dialect_path(dialect, fname):\n    \"\"\"Work out how to find paths given a dialect and a file name.\"\"\"\n    return os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, fname)\n", "tokens": ["test", "conftest", "py", "def", "make_dialect_path", "dialect", "fname", "work", "out", "how", "to", "find", "paths", "given", "a", "dialect", "and", "a", "file", "name", "return", "os", "path", "join", "test", "fixtures", "dialects", "dialect", "fname"], "doc_len": 29}
{"doc_id": "test/conftest.py::load_file", "file_path": "test/conftest.py", "class_name": null, "func_name": "load_file", "text": "文件路径: test/conftest.py\ndef load_file(dialect, fname):\n    \"\"\"Load a file.\"\"\"\n    with open(make_dialect_path(dialect, fname)) as f:\n        raw = f.read()\n    return raw\n", "tokens": ["test", "conftest", "py", "def", "load_file", "dialect", "fname", "load", "a", "file", "with", "open", "make_dialect_path", "dialect", "fname", "as", "f", "raw", "f", "read", "return", "raw"], "doc_len": 22}
{"doc_id": "test/conftest.py::process_struct", "file_path": "test/conftest.py", "class_name": null, "func_name": "process_struct", "text": "文件路径: test/conftest.py\ndef process_struct(obj):\n    \"\"\"Process a nested dict or dict-like into a check tuple.\"\"\"\n    if isinstance(obj, dict):\n        return tuple((k, process_struct(obj[k])) for k in obj)\n    elif isinstance(obj, list):\n        # We'll assume that it's a list of dicts\n        if isinstance(obj[0], dict):\n            buff = [process_struct(elem) for elem in obj]\n            if any(len(elem) > 1 for elem in buff):\n                raise ValueError(f\"Not sure how to deal with multi key dict: {buff!r}\")\n            return tuple(elem[0] for elem in buff)\n        else:\n            raise TypeError(f\"Did not expect a list of {type(obj[0])}: {obj[0]!r}\")\n    elif isinstance(obj, (str, int, float)):\n        return str(obj)\n    elif obj is None:\n        return None\n    else:\n        raise TypeError(f\"Not sure how to deal with type {type(obj)}: {obj!r}\")\n", "tokens": ["test", "conftest", "py", "def", "process_struct", "obj", "process", "a", "nested", "dict", "or", "dict", "like", "into", "a", "check", "tuple", "if", "isinstance", "obj", "dict", "return", "tuple", "k", "process_struct", "obj", "k", "for", "k", "in", "obj", "elif", "isinstance", "obj", "list", "we", "ll", "assume", "that", "it", "s", "a", "list", "of", "dicts", "if", "isinstance", "obj", "0", "dict", "buff", "process_struct", "elem", "for", "elem", "in", "obj", "if", "any", "len", "elem", "1", "for", "elem", "in", "buff", "raise", "valueerror", "f", "not", "sure", "how", "to", "deal", "with", "multi", "key", "dict", "buff", "r", "return", "tuple", "elem", "0", "for", "elem", "in", "buff", "else", "raise", "typeerror", "f", "did", "not", "expect", "a", "list", "of", "type", "obj", "0", "obj", "0", "r", "elif", "isinstance", "obj", "str", "int", "float", "return", "str", "obj", "elif", "obj", "is", "none", "return", "none", "else", "raise", "typeerror", "f", "not", "sure", "how", "to", "deal", "with", "type", "type", "obj", "obj", "r"], "doc_len": 134}
{"doc_id": "test/conftest.py::parse_example_file", "file_path": "test/conftest.py", "class_name": null, "func_name": "parse_example_file", "text": "文件路径: test/conftest.py\ndef parse_example_file(dialect, sqlfile):\n    \"\"\"Parse example SQL file, return parse tree.\"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    # Load the SQL\n    raw = load_file(dialect, sqlfile)\n    # Lex and parse the file\n    tokens, _ = Lexer(config=config).lex(raw)\n    tree = Parser(config=config).parse(tokens)\n    return tree\n", "tokens": ["test", "conftest", "py", "def", "parse_example_file", "dialect", "sqlfile", "parse", "example", "sql", "file", "return", "parse", "tree", "config", "fluffconfig", "overrides", "dict", "dialect", "dialect", "load", "the", "sql", "raw", "load_file", "dialect", "sqlfile", "lex", "and", "parse", "the", "file", "tokens", "_", "lexer", "config", "config", "lex", "raw", "tree", "parser", "config", "config", "parse", "tokens", "return", "tree"], "doc_len": 47}
{"doc_id": "test/conftest.py::compute_parse_tree_hash", "file_path": "test/conftest.py", "class_name": null, "func_name": "compute_parse_tree_hash", "text": "文件路径: test/conftest.py\ndef compute_parse_tree_hash(tree):\n    \"\"\"Given a parse tree, compute a consistent hash value for it.\"\"\"\n    if tree:\n        r = tree.as_record(code_only=True, show_raw=True)\n        if r:\n            r_io = io.StringIO()\n            oyaml.dump(r, r_io)\n            result = hashlib.blake2s(r_io.getvalue().encode(\"utf-8\")).hexdigest()\n            return result\n    return None\n", "tokens": ["test", "conftest", "py", "def", "compute_parse_tree_hash", "tree", "given", "a", "parse", "tree", "compute", "a", "consistent", "hash", "value", "for", "it", "if", "tree", "r", "tree", "as_record", "code_only", "true", "show_raw", "true", "if", "r", "r_io", "io", "stringio", "oyaml", "dump", "r", "r_io", "result", "hashlib", "blake2s", "r_io", "getvalue", "encode", "utf", "8", "hexdigest", "return", "result", "return", "none"], "doc_len": 48}
{"doc_id": "test/conftest.py::load_yaml", "file_path": "test/conftest.py", "class_name": null, "func_name": "load_yaml", "text": "文件路径: test/conftest.py\ndef load_yaml(fpath):\n    \"\"\"Load a yaml structure and process it into a tuple.\"\"\"\n    # Load raw file\n    with open(fpath) as f:\n        raw = f.read()\n    # Parse the yaml\n    obj = oyaml.safe_load(raw)\n    # Return the parsed and structured object\n    _hash = None\n    if obj:\n        _hash = obj.pop(\"_hash\", None)\n    processed = process_struct(obj)\n    if processed:\n        return _hash, process_struct(obj)[0]\n    else:\n        return None, None\n", "tokens": ["test", "conftest", "py", "def", "load_yaml", "fpath", "load", "a", "yaml", "structure", "and", "process", "it", "into", "a", "tuple", "load", "raw", "file", "with", "open", "fpath", "as", "f", "raw", "f", "read", "parse", "the", "yaml", "obj", "oyaml", "safe_load", "raw", "return", "the", "parsed", "and", "structured", "object", "_hash", "none", "if", "obj", "_hash", "obj", "pop", "_hash", "none", "processed", "process_struct", "obj", "if", "processed", "return", "_hash", "process_struct", "obj", "0", "else", "return", "none", "none"], "doc_len": 63}
{"doc_id": "test/conftest.py::yaml_loader", "file_path": "test/conftest.py", "class_name": null, "func_name": "yaml_loader", "text": "文件路径: test/conftest.py\ndef yaml_loader():\n    \"\"\"Return a yaml loading function.\"\"\"\n    # Return a function\n    return load_yaml\n", "tokens": ["test", "conftest", "py", "def", "yaml_loader", "return", "a", "yaml", "loading", "function", "return", "a", "function", "return", "load_yaml"], "doc_len": 15}
{"doc_id": "test/conftest.py::generate_test_segments", "file_path": "test/conftest.py", "class_name": null, "func_name": "generate_test_segments", "text": "文件路径: test/conftest.py\ndef generate_test_segments():\n    \"\"\"Roughly generate test segments.\n\n    This is a factory function so that it works as a fixture,\n    but when actually used, this will return the inner function\n    which is what you actually need.\n    \"\"\"\n\n    def generate_test_segments_func(elems):\n        \"\"\"Roughly generate test segments.\n\n        This function isn't totally robust, but good enough\n        for testing. Use with caution.\n        \"\"\"\n        buff = []\n        raw_file = \"\".join(elems)\n        templated_file = TemplatedFile.from_string(raw_file)\n        idx = 0\n\n        for elem in elems:\n            if elem == \"<indent>\":\n                buff.append(\n                    Indent(\n                        pos_marker=PositionMarker.from_point(idx, idx, templated_file)\n                    )\n                )\n                continue\n            elif elem == \"<dedent>\":\n                buff.append(\n                    Dedent(\n                        pos_marker=PositionMarker.from_point(idx, idx, templated_file)\n                    )\n                )\n                continue\n\n            seg_kwargs = {}\n\n            if set(elem) <= {\" \", \"\\t\"}:\n                SegClass = WhitespaceSegment\n            elif set(elem) <= {\"\\n\"}:\n                SegClass = NewlineSegment\n            elif elem == \"(\":\n                SegClass = SymbolSegment\n                seg_kwargs = {\"name\": \"bracket_open\"}\n            elif elem == \")\":\n                SegClass = SymbolSegment\n                seg_kwargs = {\"name\": \"bracket_close\"}\n            elif elem.startswith(\"--\"):\n                SegClass = CommentSegment\n                seg_kwargs = {\"name\": \"inline_comment\"}\n            elif elem.startswith('\"'):\n                SegClass = CodeSegment\n                seg_kwargs = {\"name\": \"double_quote\"}\n            elif elem.startswith(\"'\"):\n                SegClass = CodeSegment\n                seg_kwargs = {\"name\": \"single_quote\"}\n            else:\n                SegClass = CodeSegment\n\n            # Set a none position marker which we'll realign at the end.\n            buff.append(\n                SegClass(\n                    raw=elem,\n                    pos_marker=PositionMarker(\n                        slice(idx, idx + len(elem)),\n                        slice(idx, idx + len(elem)),\n                        templated_file,\n                    ),\n                    **seg_kwargs,\n                )\n            )\n            idx += len(elem)\n\n        return tuple(buff)\n\n    # Return the function\n    return generate_test_segments_func\n", "tokens": ["test", "conftest", "py", "def", "generate_test_segments", "roughly", "generate", "test", "segments", "this", "is", "a", "factory", "function", "so", "that", "it", "works", "as", "a", "fixture", "but", "when", "actually", "used", "this", "will", "return", "the", "inner", "function", "which", "is", "what", "you", "actually", "need", "def", "generate_test_segments_func", "elems", "roughly", "generate", "test", "segments", "this", "function", "isn", "t", "totally", "robust", "but", "good", "enough", "for", "testing", "use", "with", "caution", "buff", "raw_file", "join", "elems", "templated_file", "templatedfile", "from_string", "raw_file", "idx", "0", "for", "elem", "in", "elems", "if", "elem", "indent", "buff", "append", "indent", "pos_marker", "positionmarker", "from_point", "idx", "idx", "templated_file", "continue", "elif", "elem", "dedent", "buff", "append", "dedent", "pos_marker", "positionmarker", "from_point", "idx", "idx", "templated_file", "continue", "seg_kwargs", "if", "set", "elem", "t", "segclass", "whitespacesegment", "elif", "set", "elem", "n", "segclass", "newlinesegment", "elif", "elem", "segclass", "symbolsegment", "seg_kwargs", "name", "bracket_open", "elif", "elem", "segclass", "symbolsegment", "seg_kwargs", "name", "bracket_close", "elif", "elem", "startswith", "segclass", "commentsegment", "seg_kwargs", "name", "inline_comment", "elif", "elem", "startswith", "segclass", "codesegment", "seg_kwargs", "name", "double_quote", "elif", "elem", "startswith", "segclass", "codesegment", "seg_kwargs", "name", "single_quote", "else", "segclass", "codesegment", "set", "a", "none", "position", "marker", "which", "we", "ll", "realign", "at", "the", "end", "buff", "append", "segclass", "raw", "elem", "pos_marker", "positionmarker", "slice", "idx", "idx", "len", "elem", "slice", "idx", "idx", "len", "elem", "templated_file", "seg_kwargs", "idx", "len", "elem", "return", "tuple", "buff", "return", "the", "function", "return", "generate_test_segments_func"], "doc_len": 194}
{"doc_id": "test/diff_quality_plugin_test.py::test_diff_quality_plugin", "file_path": "test/diff_quality_plugin_test.py", "class_name": null, "func_name": "test_diff_quality_plugin", "text": "文件路径: test/diff_quality_plugin_test.py\ndef test_diff_quality_plugin(sql_path, expected_violations_lines):\n    \"\"\"Test the plugin at least finds errors on the expected lines.\"\"\"\n    violation_reporter = diff_quality_plugin.diff_cover_report_quality()\n    violations = violation_reporter.violations(sql_path)\n    assert isinstance(violations, list)\n    if expected_violations_lines:\n        assert len(violations) > 0\n        violations_lines = {v.line for v in violations}\n        for expected_line in expected_violations_lines:\n            assert expected_line in violations_lines\n    else:\n        assert len(violations) == 0\n", "tokens": ["test", "diff_quality_plugin_test", "py", "def", "test_diff_quality_plugin", "sql_path", "expected_violations_lines", "test", "the", "plugin", "at", "least", "finds", "errors", "on", "the", "expected", "lines", "violation_reporter", "diff_quality_plugin", "diff_cover_report_quality", "violations", "violation_reporter", "violations", "sql_path", "assert", "isinstance", "violations", "list", "if", "expected_violations_lines", "assert", "len", "violations", "0", "violations_lines", "v", "line", "for", "v", "in", "violations", "for", "expected_line", "in", "expected_violations_lines", "assert", "expected_line", "in", "violations_lines", "else", "assert", "len", "violations", "0"], "doc_len": 55}
{"doc_id": "test/generate_parse_fixture_yml.py::generate_parse_fixture", "file_path": "test/generate_parse_fixture_yml.py", "class_name": null, "func_name": "generate_parse_fixture", "text": "文件路径: test/generate_parse_fixture_yml.py\ndef generate_parse_fixture(example):\n    \"\"\"Parse example SQL file, write parse tree to YAML file.\"\"\"\n    dialect, sqlfile = example\n    tree = parse_example_file(dialect, sqlfile)\n    _hash = compute_parse_tree_hash(tree)\n    # Remove the .sql file extension\n    root = sqlfile[:-4]\n    path = os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, root + \".yml\")\n    with open(path, \"w\", newline=\"\\n\") as f:\n        r = None\n        if tree:\n            r = dict(\n                [(\"_hash\", _hash)]\n                + list(tree.as_record(code_only=True, show_raw=True).items())\n            )\n            print(\n                \"# YML test files are auto-generated from SQL files and should not be edited by\",\n                '# hand. To help enforce this, the \"hash\" field in the file must match a hash',\n                \"# computed by SQLFluff when running the tests. Please run\",\n                \"# `python test/generate_parse_fixture_yml.py`  to generate them after adding or\",\n                \"# altering SQL files.\",\n                file=f,\n                sep=\"\\n\",\n            )\n            yaml.dump(r, f, default_flow_style=False)\n        else:\n            f.write(\"\")\n", "tokens": ["test", "generate_parse_fixture_yml", "py", "def", "generate_parse_fixture", "example", "parse", "example", "sql", "file", "write", "parse", "tree", "to", "yaml", "file", "dialect", "sqlfile", "example", "tree", "parse_example_file", "dialect", "sqlfile", "_hash", "compute_parse_tree_hash", "tree", "remove", "the", "sql", "file", "extension", "root", "sqlfile", "4", "path", "os", "path", "join", "test", "fixtures", "dialects", "dialect", "root", "yml", "with", "open", "path", "w", "newline", "n", "as", "f", "r", "none", "if", "tree", "r", "dict", "_hash", "_hash", "list", "tree", "as_record", "code_only", "true", "show_raw", "true", "items", "print", "yml", "test", "files", "are", "auto", "generated", "from", "sql", "files", "and", "should", "not", "be", "edited", "by", "hand", "to", "help", "enforce", "this", "the", "hash", "field", "in", "the", "file", "must", "match", "a", "hash", "computed", "by", "sqlfluff", "when", "running", "the", "tests", "please", "run", "python", "test", "generate_parse_fixture_yml", "py", "to", "generate", "them", "after", "adding", "or", "altering", "sql", "files", "file", "f", "sep", "n", "yaml", "dump", "r", "f", "default_flow_style", "false", "else", "f", "write"], "doc_len": 134}
{"doc_id": "test/generate_parse_fixture_yml.py::main", "file_path": "test/generate_parse_fixture_yml.py", "class_name": null, "func_name": "main", "text": "文件路径: test/generate_parse_fixture_yml.py\ndef main():\n    \"\"\"Find all example SQL files, parse and create YAML files.\"\"\"\n    parse_success_examples, _ = get_parse_fixtures()\n    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n        for _ in pool.imap_unordered(generate_parse_fixture, parse_success_examples):\n            pass\n", "tokens": ["test", "generate_parse_fixture_yml", "py", "def", "main", "find", "all", "example", "sql", "files", "parse", "and", "create", "yaml", "files", "parse_success_examples", "_", "get_parse_fixtures", "with", "multiprocessing", "pool", "multiprocessing", "cpu_count", "as", "pool", "for", "_", "in", "pool", "imap_unordered", "generate_parse_fixture", "parse_success_examples", "pass"], "doc_len": 33}
{"doc_id": "test/test_testing.py::test_assert_rule_fail_in_sql_handle_parse_error", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_assert_rule_fail_in_sql_handle_parse_error", "text": "文件路径: test/test_testing.py\ndef test_assert_rule_fail_in_sql_handle_parse_error():\n    \"\"\"Util assert_rule_fail_in_sql should handle parse errors.\"\"\"\n    with pytest.raises(Failed) as failed_test:\n        assert_rule_fail_in_sql(code=\"L000\", sql=\"select from\")\n    failed_test.match(\"Found the following parse errors in test case:\")\n", "tokens": ["test", "test_testing", "py", "def", "test_assert_rule_fail_in_sql_handle_parse_error", "util", "assert_rule_fail_in_sql", "should", "handle", "parse", "errors", "with", "pytest", "raises", "failed", "as", "failed_test", "assert_rule_fail_in_sql", "code", "l000", "sql", "select", "from", "failed_test", "match", "found", "the", "following", "parse", "errors", "in", "test", "case"], "doc_len": 33}
{"doc_id": "test/test_testing.py::test_assert_rule_fail_in_sql_should_fail_queries_that_unexpectedly_pass", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_assert_rule_fail_in_sql_should_fail_queries_that_unexpectedly_pass", "text": "文件路径: test/test_testing.py\ndef test_assert_rule_fail_in_sql_should_fail_queries_that_unexpectedly_pass():\n    \"\"\"Util assert_rule_fail_in_sql should fail tests when a query passes rules that it violates.\"\"\"\n    with pytest.raises(Failed) as failed_test:\n        assert_rule_fail_in_sql(code=\"L001\", sql=\"select 1\")\n    failed_test.match(\"No L001 failures found in query which should fail\")\n", "tokens": ["test", "test_testing", "py", "def", "test_assert_rule_fail_in_sql_should_fail_queries_that_unexpectedly_pass", "util", "assert_rule_fail_in_sql", "should", "fail", "tests", "when", "a", "query", "passes", "rules", "that", "it", "violates", "with", "pytest", "raises", "failed", "as", "failed_test", "assert_rule_fail_in_sql", "code", "l001", "sql", "select", "1", "failed_test", "match", "no", "l001", "failures", "found", "in", "query", "which", "should", "fail"], "doc_len": 41}
{"doc_id": "test/test_testing.py::test_assert_rule_pass_in_sql_should_handle_parse_error", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_assert_rule_pass_in_sql_should_handle_parse_error", "text": "文件路径: test/test_testing.py\ndef test_assert_rule_pass_in_sql_should_handle_parse_error():\n    \"\"\"Util assert_rule_pass_in_sql should handle parse errors.\"\"\"\n    with pytest.raises(Failed) as failed_test:\n        assert_rule_pass_in_sql(code=\"L001\", sql=\"select from\")\n    failed_test.match(\"Found unparsable section:\")\n", "tokens": ["test", "test_testing", "py", "def", "test_assert_rule_pass_in_sql_should_handle_parse_error", "util", "assert_rule_pass_in_sql", "should", "handle", "parse", "errors", "with", "pytest", "raises", "failed", "as", "failed_test", "assert_rule_pass_in_sql", "code", "l001", "sql", "select", "from", "failed_test", "match", "found", "unparsable", "section"], "doc_len": 28}
{"doc_id": "test/test_testing.py::test_assert_rule_pass_in_sql_should_fail_when_there_are_violations", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_assert_rule_pass_in_sql_should_fail_when_there_are_violations", "text": "文件路径: test/test_testing.py\ndef test_assert_rule_pass_in_sql_should_fail_when_there_are_violations():\n    \"\"\"Util assert_rule_pass_in_sql should fail when there are violations.\"\"\"\n    with pytest.raises(Failed) as failed_test:\n        assert_rule_pass_in_sql(code=\"L005\", sql=\"select a , b from t\")\n    failed_test.match(\"Found L005 failures in query which should pass\")\n", "tokens": ["test", "test_testing", "py", "def", "test_assert_rule_pass_in_sql_should_fail_when_there_are_violations", "util", "assert_rule_pass_in_sql", "should", "fail", "when", "there", "are", "violations", "with", "pytest", "raises", "failed", "as", "failed_test", "assert_rule_pass_in_sql", "code", "l005", "sql", "select", "a", "b", "from", "t", "failed_test", "match", "found", "l005", "failures", "in", "query", "which", "should", "pass"], "doc_len": 38}
{"doc_id": "test/test_testing.py::test_rules__test_helper_skipped_when_test_case_skipped", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_rules__test_helper_skipped_when_test_case_skipped", "text": "文件路径: test/test_testing.py\ndef test_rules__test_helper_skipped_when_test_case_skipped():\n    \"\"\"Util rules__test_helper should skip the test when test case is \"skipped\".\"\"\"\n    rule_test_case = RuleTestCase(skip=\"Skip this one for now\")\n    with pytest.raises(Skipped) as skipped_test:\n        rules__test_helper(rule_test_case)\n    skipped_test.match(\"Skip this one for now\")\n", "tokens": ["test", "test_testing", "py", "def", "test_rules__test_helper_skipped_when_test_case_skipped", "util", "rules__test_helper", "should", "skip", "the", "test", "when", "test", "case", "is", "skipped", "rule_test_case", "ruletestcase", "skip", "skip", "this", "one", "for", "now", "with", "pytest", "raises", "skipped", "as", "skipped_test", "rules__test_helper", "rule_test_case", "skipped_test", "match", "skip", "this", "one", "for", "now"], "doc_len": 39}
{"doc_id": "test/test_testing.py::test_rules__test_helper_has_variable_introspection", "file_path": "test/test_testing.py", "class_name": null, "func_name": "test_rules__test_helper_has_variable_introspection", "text": "文件路径: test/test_testing.py\ndef test_rules__test_helper_has_variable_introspection():\n    \"\"\"Make sure the helper gives variable introspection information on failure.\"\"\"\n    rule_test_case = RuleTestCase(\n        rule=\"L003\",\n        fail_str=\"\"\"\n            select\n                a,\n                    b\n            from table\n        \"\"\",\n        # extra comma on purpose\n        fix_str=\"\"\"\n            select\n                a,\n                b,\n            from table\n        \"\"\",\n    )\n    with pytest.raises(AssertionError) as skipped_test:\n        rules__test_helper(rule_test_case)\n    # Enough to check that a query diff is displayed\n    skipped_test.match(\"select\")\n", "tokens": ["test", "test_testing", "py", "def", "test_rules__test_helper_has_variable_introspection", "make", "sure", "the", "helper", "gives", "variable", "introspection", "information", "on", "failure", "rule_test_case", "ruletestcase", "rule", "l003", "fail_str", "select", "a", "b", "from", "table", "extra", "comma", "on", "purpose", "fix_str", "select", "a", "b", "from", "table", "with", "pytest", "raises", "assertionerror", "as", "skipped_test", "rules__test_helper", "rule_test_case", "enough", "to", "check", "that", "a", "query", "diff", "is", "displayed", "skipped_test", "match", "select"], "doc_len": 55}
{"doc_id": "test/api/classes_test.py::test__api__lexer", "file_path": "test/api/classes_test.py", "class_name": null, "func_name": "test__api__lexer", "text": "文件路径: test/api/classes_test.py\ndef test__api__lexer():\n    \"\"\"Basic checking of lexing functionality.\"\"\"\n    tokens, violations = Lexer().lex(test_query)\n    assert violations == []\n    assert isinstance(tokens, tuple)\n    assert [elem.raw for elem in tokens] == [\"SELECt\", \" \", \"1\"]\n", "tokens": ["test", "api", "classes_test", "py", "def", "test__api__lexer", "basic", "checking", "of", "lexing", "functionality", "tokens", "violations", "lexer", "lex", "test_query", "assert", "violations", "assert", "isinstance", "tokens", "tuple", "assert", "elem", "raw", "for", "elem", "in", "tokens", "select", "1"], "doc_len": 31}
{"doc_id": "test/api/classes_test.py::test__api__parser", "file_path": "test/api/classes_test.py", "class_name": null, "func_name": "test__api__parser", "text": "文件路径: test/api/classes_test.py\ndef test__api__parser():\n    \"\"\"Basic checking of parsing functionality.\"\"\"\n    tokens, _ = Lexer().lex(test_query)\n    parsed = Parser().parse(tokens)\n    assert parsed.raw == test_query\n", "tokens": ["test", "api", "classes_test", "py", "def", "test__api__parser", "basic", "checking", "of", "parsing", "functionality", "tokens", "_", "lexer", "lex", "test_query", "parsed", "parser", "parse", "tokens", "assert", "parsed", "raw", "test_query"], "doc_len": 24}
{"doc_id": "test/api/classes_test.py::test__api__linter_lint", "file_path": "test/api/classes_test.py", "class_name": null, "func_name": "test__api__linter_lint", "text": "文件路径: test/api/classes_test.py\ndef test__api__linter_lint():\n    \"\"\"Basic checking of parsing functionality.\"\"\"\n    tokens, _ = Lexer().lex(test_query)\n    parsed = Parser().parse(tokens)\n    violations = Linter().lint(parsed)\n    assert [v.rule.code for v in violations] == [\"L009\", \"L010\"]\n", "tokens": ["test", "api", "classes_test", "py", "def", "test__api__linter_lint", "basic", "checking", "of", "parsing", "functionality", "tokens", "_", "lexer", "lex", "test_query", "parsed", "parser", "parse", "tokens", "violations", "linter", "lint", "parsed", "assert", "v", "rule", "code", "for", "v", "in", "violations", "l009", "l010"], "doc_len": 34}
{"doc_id": "test/api/classes_test.py::test__api__linter_fix", "file_path": "test/api/classes_test.py", "class_name": null, "func_name": "test__api__linter_fix", "text": "文件路径: test/api/classes_test.py\ndef test__api__linter_fix():\n    \"\"\"Basic checking of parsing functionality.\"\"\"\n    tokens, _ = Lexer().lex(test_query)\n    parsed = Parser().parse(tokens)\n    fixed, _ = Linter().fix(parsed)\n    assert fixed.raw == \"SELECT 1\\n\"\n", "tokens": ["test", "api", "classes_test", "py", "def", "test__api__linter_fix", "basic", "checking", "of", "parsing", "functionality", "tokens", "_", "lexer", "lex", "test_query", "parsed", "parser", "parse", "tokens", "fixed", "_", "linter", "fix", "parsed", "assert", "fixed", "raw", "select", "1", "n"], "doc_len": 31}
{"doc_id": "test/api/info_test.py::test__api__info_dialects", "file_path": "test/api/info_test.py", "class_name": null, "func_name": "test__api__info_dialects", "text": "文件路径: test/api/info_test.py\ndef test__api__info_dialects():\n    \"\"\"Basic linting of dialects.\"\"\"\n    dialects = sqlfluff.list_dialects()\n    assert isinstance(dialects, list)\n    assert (\"ansi\", \"ansi\", \"nothing\") in dialects\n", "tokens": ["test", "api", "info_test", "py", "def", "test__api__info_dialects", "basic", "linting", "of", "dialects", "dialects", "sqlfluff", "list_dialects", "assert", "isinstance", "dialects", "list", "assert", "ansi", "ansi", "nothing", "in", "dialects"], "doc_len": 23}
{"doc_id": "test/api/info_test.py::test__api__info_rules", "file_path": "test/api/info_test.py", "class_name": null, "func_name": "test__api__info_rules", "text": "文件路径: test/api/info_test.py\ndef test__api__info_rules():\n    \"\"\"Basic linting of dialects.\"\"\"\n    rules = sqlfluff.list_rules()\n    assert isinstance(rules, list)\n    assert (\"L001\", \"Unnecessary trailing whitespace.\") in rules\n", "tokens": ["test", "api", "info_test", "py", "def", "test__api__info_rules", "basic", "linting", "of", "dialects", "rules", "sqlfluff", "list_rules", "assert", "isinstance", "rules", "list", "assert", "l001", "unnecessary", "trailing", "whitespace", "in", "rules"], "doc_len": 24}
{"doc_id": "test/api/simple_test.py::test__api__lint_string_without_violations", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__lint_string_without_violations", "text": "文件路径: test/api/simple_test.py\ndef test__api__lint_string_without_violations():\n    \"\"\"Check lint functionality when there is no violation.\"\"\"\n    result = sqlfluff.lint(\"select column from table\\n\")\n    assert result == []\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__lint_string_without_violations", "check", "lint", "functionality", "when", "there", "is", "no", "violation", "result", "sqlfluff", "lint", "select", "column", "from", "table", "n", "assert", "result"], "doc_len": 24}
{"doc_id": "test/api/simple_test.py::test__api__lint_string", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__lint_string", "text": "文件路径: test/api/simple_test.py\ndef test__api__lint_string():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = sqlfluff.lint(my_bad_query)\n    # Check return types.\n    assert isinstance(result, list)\n    assert all(isinstance(elem, dict) for elem in result)\n    # Check actual result\n    assert result == lint_result\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__lint_string", "basic", "checking", "of", "lint", "functionality", "result", "sqlfluff", "lint", "my_bad_query", "check", "return", "types", "assert", "isinstance", "result", "list", "assert", "all", "isinstance", "elem", "dict", "for", "elem", "in", "result", "check", "actual", "result", "assert", "result", "lint_result"], "doc_len": 37}
{"doc_id": "test/api/simple_test.py::test__api__lint_file", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__lint_file", "text": "文件路径: test/api/simple_test.py\ndef test__api__lint_file():\n    \"\"\"Basic checking of lint functionality from a file object.\"\"\"\n    string_buffer = io.StringIO(my_bad_query)\n    result = sqlfluff.lint(string_buffer)\n    # Check actual result\n    assert result == lint_result\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__lint_file", "basic", "checking", "of", "lint", "functionality", "from", "a", "file", "object", "string_buffer", "io", "stringio", "my_bad_query", "result", "sqlfluff", "lint", "string_buffer", "check", "actual", "result", "assert", "result", "lint_result"], "doc_len": 29}
{"doc_id": "test/api/simple_test.py::test__api__lint_string_specific", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__lint_string_specific", "text": "文件路径: test/api/simple_test.py\ndef test__api__lint_string_specific():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    rules = [\"L014\", \"L009\"]\n    result = sqlfluff.lint(my_bad_query, rules=rules)\n    # Check which rules are found\n    assert all(elem[\"code\"] in rules for elem in result)\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__lint_string_specific", "basic", "checking", "of", "lint", "functionality", "rules", "l014", "l009", "result", "sqlfluff", "lint", "my_bad_query", "rules", "rules", "check", "which", "rules", "are", "found", "assert", "all", "elem", "code", "in", "rules", "for", "elem", "in", "result"], "doc_len": 35}
{"doc_id": "test/api/simple_test.py::test__api__fix_string", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__fix_string", "text": "文件路径: test/api/simple_test.py\ndef test__api__fix_string():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = sqlfluff.fix(my_bad_query)\n    # Check return types.\n    assert isinstance(result, str)\n    # Check actual result\n    assert (\n        result\n        == \"\"\"SELECT\n    *,\n    1,\n    blah AS foo FROM mytable\n\"\"\"\n    )\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__fix_string", "basic", "checking", "of", "lint", "functionality", "result", "sqlfluff", "fix", "my_bad_query", "check", "return", "types", "assert", "isinstance", "result", "str", "check", "actual", "result", "assert", "result", "select", "1", "blah", "as", "foo", "from", "mytable"], "doc_len": 34}
{"doc_id": "test/api/simple_test.py::test__api__fix_string_specific", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__fix_string_specific", "text": "文件路径: test/api/simple_test.py\ndef test__api__fix_string_specific():\n    \"\"\"Basic checking of lint functionality with a specific rule.\"\"\"\n    result = sqlfluff.fix(my_bad_query, rules=\"L010\")\n    # Check actual result\n    assert result == \"SELECT  *, 1, blah AS  fOO  FROM myTable\"\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__fix_string_specific", "basic", "checking", "of", "lint", "functionality", "with", "a", "specific", "rule", "result", "sqlfluff", "fix", "my_bad_query", "rules", "l010", "check", "actual", "result", "assert", "result", "select", "1", "blah", "as", "foo", "from", "mytable"], "doc_len": 33}
{"doc_id": "test/api/simple_test.py::test__api__parse_string", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__parse_string", "text": "文件路径: test/api/simple_test.py\ndef test__api__parse_string():\n    \"\"\"Basic checking of parse functionality.\"\"\"\n    parsed = sqlfluff.parse(my_bad_query)\n    # Check we can call `to_tuple` on the result\n    assert isinstance(parsed, ParsedString)\n    # Check we can iterate objects within it\n    keywords = [keyword.raw for keyword in parsed.tree.recursive_crawl(\"keyword\")]\n    assert keywords == [\"SeLEct\", \"as\", \"from\"]\n    # Check we can get columns from it\n    col_refs = [\n        col_ref.raw for col_ref in parsed.tree.recursive_crawl(\"column_reference\")\n    ]\n    assert col_refs == [\"blah\"]\n    # Check we can get table from it\n    tbl_refs = [\n        tbl_ref.raw for tbl_ref in parsed.tree.recursive_crawl(\"table_reference\")\n    ]\n    assert tbl_refs == [\"myTable\"]\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__parse_string", "basic", "checking", "of", "parse", "functionality", "parsed", "sqlfluff", "parse", "my_bad_query", "check", "we", "can", "call", "to_tuple", "on", "the", "result", "assert", "isinstance", "parsed", "parsedstring", "check", "we", "can", "iterate", "objects", "within", "it", "keywords", "keyword", "raw", "for", "keyword", "in", "parsed", "tree", "recursive_crawl", "keyword", "assert", "keywords", "select", "as", "from", "check", "we", "can", "get", "columns", "from", "it", "col_refs", "col_ref", "raw", "for", "col_ref", "in", "parsed", "tree", "recursive_crawl", "column_reference", "assert", "col_refs", "blah", "check", "we", "can", "get", "table", "from", "it", "tbl_refs", "tbl_ref", "raw", "for", "tbl_ref", "in", "parsed", "tree", "recursive_crawl", "table_reference", "assert", "tbl_refs", "mytable"], "doc_len": 89}
{"doc_id": "test/api/simple_test.py::test__api__parse_fail", "file_path": "test/api/simple_test.py", "class_name": null, "func_name": "test__api__parse_fail", "text": "文件路径: test/api/simple_test.py\ndef test__api__parse_fail():\n    \"\"\"Basic failure mode of parse functionality.\"\"\"\n    try:\n        sqlfluff.parse(\"Select (1 + 2 +++) FROM mytable as blah blah\")\n        pytest.fail(\"sqlfluff.parse should have raised an exception.\")\n    except Exception as err:\n        # Check it's the right kind of exception\n        assert isinstance(err, sqlfluff.api.APIParsingError)\n        # Check there are two violations in there.\n        assert len(err.violations) == 2\n        # Check it prints nicely.\n        assert (\n            str(err)\n            == \"\"\"Found 2 issues while parsing string.\nLine 1, Position 14: Found unparsable section: ' +++'\nLine 1, Position 41: Found unparsable section: 'blah'\"\"\"\n        )\n", "tokens": ["test", "api", "simple_test", "py", "def", "test__api__parse_fail", "basic", "failure", "mode", "of", "parse", "functionality", "try", "sqlfluff", "parse", "select", "1", "2", "from", "mytable", "as", "blah", "blah", "pytest", "fail", "sqlfluff", "parse", "should", "have", "raised", "an", "exception", "except", "exception", "as", "err", "check", "it", "s", "the", "right", "kind", "of", "exception", "assert", "isinstance", "err", "sqlfluff", "api", "apiparsingerror", "check", "there", "are", "two", "violations", "in", "there", "assert", "len", "err", "violations", "2", "check", "it", "prints", "nicely", "assert", "str", "err", "found", "2", "issues", "while", "parsing", "string", "line", "1", "position", "14", "found", "unparsable", "section", "line", "1", "position", "41", "found", "unparsable", "section", "blah"], "doc_len": 90}
{"doc_id": "test/api/util_test.py::test__api__util_get_table_references", "file_path": "test/api/util_test.py", "class_name": null, "func_name": "test__api__util_get_table_references", "text": "文件路径: test/api/util_test.py\ndef test__api__util_get_table_references(sql, table_refs, dialect):\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    parsed = sqlfluff.parse(sql, dialect=dialect)\n    external_tables = parsed.tree.get_table_references()\n    assert external_tables == table_refs\n", "tokens": ["test", "api", "util_test", "py", "def", "test__api__util_get_table_references", "sql", "table_refs", "dialect", "basic", "checking", "of", "lint", "functionality", "parsed", "sqlfluff", "parse", "sql", "dialect", "dialect", "external_tables", "parsed", "tree", "get_table_references", "assert", "external_tables", "table_refs"], "doc_len": 27}
{"doc_id": "test/cli/commands_test.py::invoke_assert_code", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "invoke_assert_code", "text": "文件路径: test/cli/commands_test.py\ndef invoke_assert_code(\n    ret_code=0,\n    args=None,\n    kwargs=None,\n    cli_input=None,\n    mix_stderr=True,\n    output_contains=\"\",\n):\n    \"\"\"Invoke a command and check return code.\"\"\"\n    args = args or []\n    kwargs = kwargs or {}\n    if cli_input:\n        kwargs[\"input\"] = cli_input\n    runner = CliRunner(mix_stderr=mix_stderr)\n    result = runner.invoke(*args, **kwargs)\n    # Output the CLI code for debugging\n    print(result.output)\n    # Check return codes\n    if output_contains != \"\":\n        assert output_contains in result.output\n    if ret_code == 0:\n        if result.exception:\n            raise result.exception\n    assert ret_code == result.exit_code\n    return result\n", "tokens": ["test", "cli", "commands_test", "py", "def", "invoke_assert_code", "ret_code", "0", "args", "none", "kwargs", "none", "cli_input", "none", "mix_stderr", "true", "output_contains", "invoke", "a", "command", "and", "check", "return", "code", "args", "args", "or", "kwargs", "kwargs", "or", "if", "cli_input", "kwargs", "input", "cli_input", "runner", "clirunner", "mix_stderr", "mix_stderr", "result", "runner", "invoke", "args", "kwargs", "output", "the", "cli", "code", "for", "debugging", "print", "result", "output", "check", "return", "codes", "if", "output_contains", "assert", "output_contains", "in", "result", "output", "if", "ret_code", "0", "if", "result", "exception", "raise", "result", "exception", "assert", "ret_code", "result", "exit_code", "return", "result"], "doc_len": 78}
{"doc_id": "test/cli/commands_test.py::test__cli__command_directed", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_directed", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_directed():\n    \"\"\"Basic checking of lint functionality.\"\"\"\n    result = invoke_assert_code(\n        ret_code=65,\n        args=[lint, [\"test/fixtures/linter/indentation_error_simple.sql\"]],\n    )\n    # We should get a readout of what the error was\n    check_a = \"L:   2 | P:   4 | L003\"\n    # NB: Skip the number at the end because it's configurable\n    check_b = \"Indentation\"\n    assert check_a in result.output\n    assert check_b in result.output\n    # Finally check the WHOLE output to make sure that unexpected newlines are not added.\n    # The replace command just accounts for cross platform testing.\n    assert result.output.replace(\"\\\\\", \"/\").startswith(expected_output)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_directed", "basic", "checking", "of", "lint", "functionality", "result", "invoke_assert_code", "ret_code", "65", "args", "lint", "test", "fixtures", "linter", "indentation_error_simple", "sql", "we", "should", "get", "a", "readout", "of", "what", "the", "error", "was", "check_a", "l", "2", "p", "4", "l003", "nb", "skip", "the", "number", "at", "the", "end", "because", "it", "s", "configurable", "check_b", "indentation", "assert", "check_a", "in", "result", "output", "assert", "check_b", "in", "result", "output", "finally", "check", "the", "whole", "output", "to", "make", "sure", "that", "unexpected", "newlines", "are", "not", "added", "the", "replace", "command", "just", "accounts", "for", "cross", "platform", "testing", "assert", "result", "output", "replace", "startswith", "expected_output"], "doc_len": 90}
{"doc_id": "test/cli/commands_test.py::test__cli__command_dialect", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_dialect", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_dialect():\n    \"\"\"Check the script raises the right exception on an unknown dialect.\"\"\"\n    # The dialect is unknown should be a non-zero exit code\n    invoke_assert_code(\n        ret_code=66,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"faslkjh\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n    )\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_dialect", "check", "the", "script", "raises", "the", "right", "exception", "on", "an", "unknown", "dialect", "the", "dialect", "is", "unknown", "should", "be", "a", "non", "zero", "exit", "code", "invoke_assert_code", "ret_code", "66", "args", "lint", "n", "dialect", "faslkjh", "test", "fixtures", "linter", "indentation_error_simple", "sql"], "doc_len": 41}
{"doc_id": "test/cli/commands_test.py::test__cli__command_dialect_legacy", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_dialect_legacy", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_dialect_legacy():\n    \"\"\"Check the script raises the right exception on a legacy dialect.\"\"\"\n    result = invoke_assert_code(\n        ret_code=66,\n        args=[\n            lint,\n            [\n                \"-n\",\n                \"--dialect\",\n                \"exasol_fs\",\n                \"test/fixtures/linter/indentation_error_simple.sql\",\n            ],\n        ],\n    )\n    assert \"Please use the 'exasol' dialect instead.\" in result.stdout\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_dialect_legacy", "check", "the", "script", "raises", "the", "right", "exception", "on", "a", "legacy", "dialect", "result", "invoke_assert_code", "ret_code", "66", "args", "lint", "n", "dialect", "exasol_fs", "test", "fixtures", "linter", "indentation_error_simple", "sql", "assert", "please", "use", "the", "exasol", "dialect", "instead", "in", "result", "stdout"], "doc_len": 41}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_stdin", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_stdin", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_stdin(command):\n    \"\"\"Check basic commands on a simple script using stdin.\n\n    The subprocess command should exit without errors, as no issues should be found.\n    \"\"\"\n    with open(\"test/fixtures/cli/passing_a.sql\") as test_file:\n        sql = test_file.read()\n    invoke_assert_code(args=[lint, command], cli_input=sql)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_stdin", "command", "check", "basic", "commands", "on", "a", "simple", "script", "using", "stdin", "the", "subprocess", "command", "should", "exit", "without", "errors", "as", "no", "issues", "should", "be", "found", "with", "open", "test", "fixtures", "cli", "passing_a", "sql", "as", "test_file", "sql", "test_file", "read", "invoke_assert_code", "args", "lint", "command", "cli_input", "sql"], "doc_len": 47}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_parse", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_parse", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_parse(command):\n    \"\"\"Check basic commands on a more complicated script.\"\"\"\n    invoke_assert_code(args=command)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_parse", "command", "check", "basic", "commands", "on", "a", "more", "complicated", "script", "invoke_assert_code", "args", "command"], "doc_len": 18}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_parse_with_retcode", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_parse_with_retcode", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_parse_with_retcode(command, ret_code):\n    \"\"\"Check commands expecting a non-zero ret code.\"\"\"\n    invoke_assert_code(ret_code=ret_code, args=command)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_parse_with_retcode", "command", "ret_code", "check", "commands", "expecting", "a", "non", "zero", "ret", "code", "invoke_assert_code", "ret_code", "ret_code", "args", "command"], "doc_len": 21}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_warning_explicit_file_ignored", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_warning_explicit_file_ignored", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_warning_explicit_file_ignored():\n    \"\"\"Check ignoring file works when passed explicitly and ignore file is in the same directory.\"\"\"\n    runner = CliRunner()\n    result = runner.invoke(\n        lint, [\"test/fixtures/linter/sqlfluffignore/path_b/query_c.sql\"]\n    )\n    assert result.exit_code == 0\n    assert (\n        \"Exact file path test/fixtures/linter/sqlfluffignore/path_b/query_c.sql \"\n        \"was given but it was ignored\"\n    ) in result.output.strip()\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_warning_explicit_file_ignored", "check", "ignoring", "file", "works", "when", "passed", "explicitly", "and", "ignore", "file", "is", "in", "the", "same", "directory", "runner", "clirunner", "result", "runner", "invoke", "lint", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "query_c", "sql", "assert", "result", "exit_code", "0", "assert", "exact", "file", "path", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "query_c", "sql", "was", "given", "but", "it", "was", "ignored", "in", "result", "output", "strip"], "doc_len": 59}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_skip_ignore_files", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_skip_ignore_files", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_skip_ignore_files():\n    \"\"\"Check \"ignore file\" is skipped when --disregard-sqlfluffignores flag is set.\"\"\"\n    runner = CliRunner()\n    result = runner.invoke(\n        lint,\n        [\n            \"test/fixtures/linter/sqlfluffignore/path_b/query_c.sql\",\n            \"--disregard-sqlfluffignores\",\n        ],\n    )\n    assert result.exit_code == 65\n    assert \"L009\" in result.output.strip()\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_skip_ignore_files", "check", "ignore", "file", "is", "skipped", "when", "disregard", "sqlfluffignores", "flag", "is", "set", "runner", "clirunner", "result", "runner", "invoke", "lint", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "query_c", "sql", "disregard", "sqlfluffignores", "assert", "result", "exit_code", "65", "assert", "l009", "in", "result", "output", "strip"], "doc_len": 42}
{"doc_id": "test/cli/commands_test.py::test__cli__command_versioning", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_versioning", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_versioning():\n    \"\"\"Check version command.\"\"\"\n    # Get the package version info\n    pkg_version = sqlfluff.__version__\n    # Get the version info from the config file\n    config = configparser.ConfigParser()\n    config.read_file(open(\"src/sqlfluff/config.ini\"))\n    config_version = config[\"sqlfluff\"][\"version\"]\n    assert pkg_version == config_version\n    # Get the version from the cli\n    runner = CliRunner()\n    result = runner.invoke(version)\n    assert result.exit_code == 0\n    # We need to strip to remove the newline characters\n    assert result.output.strip() == pkg_version\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_versioning", "check", "version", "command", "get", "the", "package", "version", "info", "pkg_version", "sqlfluff", "__version__", "get", "the", "version", "info", "from", "the", "config", "file", "config", "configparser", "configparser", "config", "read_file", "open", "src", "sqlfluff", "config", "ini", "config_version", "config", "sqlfluff", "version", "assert", "pkg_version", "config_version", "get", "the", "version", "from", "the", "cli", "runner", "clirunner", "result", "runner", "invoke", "version", "assert", "result", "exit_code", "0", "we", "need", "to", "strip", "to", "remove", "the", "newline", "characters", "assert", "result", "output", "strip", "pkg_version"], "doc_len": 72}
{"doc_id": "test/cli/commands_test.py::test__cli__command_version", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_version", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_version():\n    \"\"\"Just check version command for exceptions.\"\"\"\n    # Get the package version info\n    pkg_version = sqlfluff.__version__\n    runner = CliRunner()\n    result = runner.invoke(version)\n    assert result.exit_code == 0\n    assert pkg_version in result.output\n    # Check a verbose version\n    result = runner.invoke(version, [\"-v\"])\n    assert result.exit_code == 0\n    assert pkg_version in result.output\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_version", "just", "check", "version", "command", "for", "exceptions", "get", "the", "package", "version", "info", "pkg_version", "sqlfluff", "__version__", "runner", "clirunner", "result", "runner", "invoke", "version", "assert", "result", "exit_code", "0", "assert", "pkg_version", "in", "result", "output", "check", "a", "verbose", "version", "result", "runner", "invoke", "version", "v", "assert", "result", "exit_code", "0", "assert", "pkg_version", "in", "result", "output"], "doc_len": 53}
{"doc_id": "test/cli/commands_test.py::test__cli__command_rules", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_rules", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_rules():\n    \"\"\"Check rules command for exceptions.\"\"\"\n    invoke_assert_code(args=[rules])\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_rules", "check", "rules", "command", "for", "exceptions", "invoke_assert_code", "args", "rules"], "doc_len": 14}
{"doc_id": "test/cli/commands_test.py::test__cli__command_dialects", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_dialects", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_dialects():\n    \"\"\"Check dialects command for exceptions.\"\"\"\n    invoke_assert_code(args=[dialects])\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_dialects", "check", "dialects", "command", "for", "exceptions", "invoke_assert_code", "args", "dialects"], "doc_len": 14}
{"doc_id": "test/cli/commands_test.py::generic_roundtrip_test", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "generic_roundtrip_test", "text": "文件路径: test/cli/commands_test.py\ndef generic_roundtrip_test(\n    source_file,\n    rulestring,\n    final_exit_code=0,\n    force=True,\n    fix_input=None,\n    fix_exit_code=0,\n    input_file_encoding=\"utf-8\",\n    output_file_encoding=None,\n):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    # Open the example file and write the content to it\n    with open(filepath, mode=\"w\", encoding=input_file_encoding) as dest_file:\n        for line in source_file:\n            dest_file.write(line)\n    # Check that we first detect the issue\n    invoke_assert_code(ret_code=65, args=[lint, [\"--rules\", rulestring, filepath]])\n    # Fix the file (in force mode)\n    if force:\n        fix_args = [\"--rules\", rulestring, \"-f\", filepath]\n    else:\n        fix_args = [\"--rules\", rulestring, filepath]\n    invoke_assert_code(\n        ret_code=fix_exit_code, args=[fix, fix_args], cli_input=fix_input\n    )\n    # Now lint the file and check for exceptions\n    invoke_assert_code(\n        ret_code=final_exit_code, args=[lint, [\"--rules\", rulestring, filepath]]\n    )\n    # Check the output file has the correct encoding after fix\n    if output_file_encoding:\n        with open(filepath, mode=\"rb\") as f:\n            data = f.read()\n        assert chardet.detect(data)[\"encoding\"] == output_file_encoding\n    shutil.rmtree(tempdir_path)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "generic_roundtrip_test", "source_file", "rulestring", "final_exit_code", "0", "force", "true", "fix_input", "none", "fix_exit_code", "0", "input_file_encoding", "utf", "8", "output_file_encoding", "none", "a", "test", "for", "roundtrip", "testing", "take", "a", "file", "buffer", "lint", "fix", "and", "lint", "this", "is", "explicitly", "different", "from", "the", "linter", "version", "of", "this", "in", "that", "it", "uses", "the", "command", "line", "rather", "than", "the", "direct", "api", "filename", "testing", "sql", "lets", "get", "the", "path", "of", "a", "file", "to", "use", "tempdir_path", "tempfile", "mkdtemp", "filepath", "os", "path", "join", "tempdir_path", "filename", "open", "the", "example", "file", "and", "write", "the", "content", "to", "it", "with", "open", "filepath", "mode", "w", "encoding", "input_file_encoding", "as", "dest_file", "for", "line", "in", "source_file", "dest_file", "write", "line", "check", "that", "we", "first", "detect", "the", "issue", "invoke_assert_code", "ret_code", "65", "args", "lint", "rules", "rulestring", "filepath", "fix", "the", "file", "in", "force", "mode", "if", "force", "fix_args", "rules", "rulestring", "f", "filepath", "else", "fix_args", "rules", "rulestring", "filepath", "invoke_assert_code", "ret_code", "fix_exit_code", "args", "fix", "fix_args", "cli_input", "fix_input", "now", "lint", "the", "file", "and", "check", "for", "exceptions", "invoke_assert_code", "ret_code", "final_exit_code", "args", "lint", "rules", "rulestring", "filepath", "check", "the", "output", "file", "has", "the", "correct", "encoding", "after", "fix", "if", "output_file_encoding", "with", "open", "filepath", "mode", "rb", "as", "f", "data", "f", "read", "assert", "chardet", "detect", "data", "encoding", "output_file_encoding", "shutil", "rmtree", "tempdir_path"], "doc_len": 191}
{"doc_id": "test/cli/commands_test.py::test__cli__command__fix", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command__fix", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command__fix(rule, fname):\n    \"\"\"Test the round trip of detecting, fixing and then not detecting the rule.\"\"\"\n    with open(fname) as test_file:\n        generic_roundtrip_test(test_file, rule)\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command__fix", "rule", "fname", "test", "the", "round", "trip", "of", "detecting", "fixing", "and", "then", "not", "detecting", "the", "rule", "with", "open", "fname", "as", "test_file", "generic_roundtrip_test", "test_file", "rule"], "doc_len": 29}
{"doc_id": "test/cli/commands_test.py::test__cli__command_fix_stdin", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_fix_stdin", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_fix_stdin(stdin, rules, stdout):\n    \"\"\"Check stdin input for fix works.\"\"\"\n    result = invoke_assert_code(args=[fix, (\"-\", \"--rules\", rules)], cli_input=stdin)\n    assert result.output == stdout\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_fix_stdin", "stdin", "rules", "stdout", "check", "stdin", "input", "for", "fix", "works", "result", "invoke_assert_code", "args", "fix", "rules", "rules", "cli_input", "stdin", "assert", "result", "output", "stdout"], "doc_len": 27}
{"doc_id": "test/cli/commands_test.py::test__cli__command_fix_stdin_logging_to_stderr", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_fix_stdin_logging_to_stderr", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_fix_stdin_logging_to_stderr(monkeypatch):\n    \"\"\"Check that logging goes to stderr when stdin is passed to fix.\"\"\"\n    perfect_sql = \"select col from table\"\n\n    class MockLinter(sqlfluff.core.Linter):\n        @classmethod\n        def lint_fix_parsed(cls, *args, **kwargs):\n            cls._warn_unfixable(\"<FAKE CODE>\")\n            return super().lint_fix_parsed(*args, **kwargs)\n\n    monkeypatch.setattr(sqlfluff.cli.commands, \"Linter\", MockLinter)\n    result = invoke_assert_code(\n        args=[fix, (\"-\", \"--rules=L003\")], cli_input=perfect_sql, mix_stderr=False\n    )\n\n    assert result.stdout == perfect_sql\n    assert \"<FAKE CODE>\" in result.stderr\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_fix_stdin_logging_to_stderr", "monkeypatch", "check", "that", "logging", "goes", "to", "stderr", "when", "stdin", "is", "passed", "to", "fix", "perfect_sql", "select", "col", "from", "table", "class", "mocklinter", "sqlfluff", "core", "linter", "classmethod", "def", "lint_fix_parsed", "cls", "args", "kwargs", "cls", "_warn_unfixable", "fake", "code", "return", "super", "lint_fix_parsed", "args", "kwargs", "monkeypatch", "setattr", "sqlfluff", "cli", "commands", "linter", "mocklinter", "result", "invoke_assert_code", "args", "fix", "rules", "l003", "cli_input", "perfect_sql", "mix_stderr", "false", "assert", "result", "stdout", "perfect_sql", "assert", "fake", "code", "in", "result", "stderr"], "doc_len": 71}
{"doc_id": "test/cli/commands_test.py::test__cli__command_fix_stdin_safety", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_fix_stdin_safety", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_fix_stdin_safety():\n    \"\"\"Check edge cases regarding safety when fixing stdin.\"\"\"\n    perfect_sql = \"select col from table\"\n\n    # just prints the very same thing\n    result = invoke_assert_code(args=[fix, (\"-\",)], cli_input=perfect_sql)\n    assert result.output.strip() == perfect_sql\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_fix_stdin_safety", "check", "edge", "cases", "regarding", "safety", "when", "fixing", "stdin", "perfect_sql", "select", "col", "from", "table", "just", "prints", "the", "very", "same", "thing", "result", "invoke_assert_code", "args", "fix", "cli_input", "perfect_sql", "assert", "result", "output", "strip", "perfect_sql"], "doc_len": 36}
{"doc_id": "test/cli/commands_test.py::test__cli__command_fix_stdin_error_exit_code", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_fix_stdin_error_exit_code", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_fix_stdin_error_exit_code(\n    sql, exit_code, params, output_contains\n):\n    \"\"\"Check that the CLI fails nicely if fixing a templated stdin.\"\"\"\n    if exit_code == 0:\n        invoke_assert_code(\n            args=[fix, (\"-\")],\n            cli_input=sql,\n        )\n    else:\n        with pytest.raises(SystemExit) as exc_info:\n            invoke_assert_code(\n                args=[fix, (params, \"-\")],\n                cli_input=sql,\n                output_contains=output_contains,\n            )\n        assert exc_info.value.args[0] == exit_code\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_fix_stdin_error_exit_code", "sql", "exit_code", "params", "output_contains", "check", "that", "the", "cli", "fails", "nicely", "if", "fixing", "a", "templated", "stdin", "if", "exit_code", "0", "invoke_assert_code", "args", "fix", "cli_input", "sql", "else", "with", "pytest", "raises", "systemexit", "as", "exc_info", "invoke_assert_code", "args", "fix", "params", "cli_input", "sql", "output_contains", "output_contains", "assert", "exc_info", "value", "args", "0", "exit_code"], "doc_len": 50}
{"doc_id": "test/cli/commands_test.py::test__cli__command__fix_no_force", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command__fix_no_force", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command__fix_no_force(rule, fname, prompt, exit_code, fix_exit_code):\n    \"\"\"Round trip test, using the prompts.\"\"\"\n    with open(fname) as test_file:\n        generic_roundtrip_test(\n            test_file,\n            rule,\n            force=False,\n            final_exit_code=exit_code,\n            fix_input=prompt,\n            fix_exit_code=fix_exit_code,\n        )\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command__fix_no_force", "rule", "fname", "prompt", "exit_code", "fix_exit_code", "round", "trip", "test", "using", "the", "prompts", "with", "open", "fname", "as", "test_file", "generic_roundtrip_test", "test_file", "rule", "force", "false", "final_exit_code", "exit_code", "fix_input", "prompt", "fix_exit_code", "fix_exit_code"], "doc_len": 33}
{"doc_id": "test/cli/commands_test.py::test__cli__command_parse_serialize_from_stdin", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_parse_serialize_from_stdin", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_parse_serialize_from_stdin(serialize):\n    \"\"\"Check that the parser serialized output option is working.\n\n    Not going to test for the content of the output as that is subject to change.\n    \"\"\"\n    result = invoke_assert_code(\n        args=[parse, (\"-\", \"--format\", serialize)],\n        cli_input=\"select * from tbl\",\n    )\n    if serialize == \"json\":\n        result = json.loads(result.output)\n    elif serialize == \"yaml\":\n        result = yaml.safe_load(result.output)\n    else:\n        raise Exception\n    result = result[0]  # only one file\n    assert result[\"filepath\"] == \"stdin\"\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_parse_serialize_from_stdin", "serialize", "check", "that", "the", "parser", "serialized", "output", "option", "is", "working", "not", "going", "to", "test", "for", "the", "content", "of", "the", "output", "as", "that", "is", "subject", "to", "change", "result", "invoke_assert_code", "args", "parse", "format", "serialize", "cli_input", "select", "from", "tbl", "if", "serialize", "json", "result", "json", "loads", "result", "output", "elif", "serialize", "yaml", "result", "yaml", "safe_load", "result", "output", "else", "raise", "exception", "result", "result", "0", "only", "one", "file", "assert", "result", "filepath", "stdin"], "doc_len": 71}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_serialize_from_stdin", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_serialize_from_stdin", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_serialize_from_stdin(serialize, sql, expected, exit_code):\n    \"\"\"Check an explicit serialized return value for a single error.\"\"\"\n    result = invoke_assert_code(\n        args=[lint, (\"-\", \"--rules\", \"L010\", \"--format\", serialize)],\n        cli_input=sql,\n        ret_code=exit_code,\n    )\n\n    if serialize == \"json\":\n        assert json.loads(result.output) == expected\n    elif serialize == \"yaml\":\n        assert yaml.safe_load(result.output) == expected\n    else:\n        raise Exception\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_serialize_from_stdin", "serialize", "sql", "expected", "exit_code", "check", "an", "explicit", "serialized", "return", "value", "for", "a", "single", "error", "result", "invoke_assert_code", "args", "lint", "rules", "l010", "format", "serialize", "cli_input", "sql", "ret_code", "exit_code", "if", "serialize", "json", "assert", "json", "loads", "result", "output", "expected", "elif", "serialize", "yaml", "assert", "yaml", "safe_load", "result", "output", "expected", "else", "raise", "exception"], "doc_len": 53}
{"doc_id": "test/cli/commands_test.py::test__cli__command_fail_nice_not_found", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_fail_nice_not_found", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_fail_nice_not_found(command):\n    \"\"\"Check commands fail as expected when then don't find files.\"\"\"\n    result = invoke_assert_code(args=command, ret_code=1)\n    assert \"could not be accessed\" in result.output\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_fail_nice_not_found", "command", "check", "commands", "fail", "as", "expected", "when", "then", "don", "t", "find", "files", "result", "invoke_assert_code", "args", "command", "ret_code", "1", "assert", "could", "not", "be", "accessed", "in", "result", "output"], "doc_len": 32}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_serialize_multiple_files", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_serialize_multiple_files", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_serialize_multiple_files(serialize):\n    \"\"\"Check the general format of JSON output for multiple files.\"\"\"\n    fpath = \"test/fixtures/linter/indentation_errors.sql\"\n\n    # note the file is in here twice. two files = two payloads.\n    result = invoke_assert_code(\n        args=[lint, (fpath, fpath, \"--format\", serialize)],\n        ret_code=65,\n    )\n\n    if serialize == \"json\":\n        result = json.loads(result.output)\n        assert len(result) == 2\n    elif serialize == \"yaml\":\n        result = yaml.safe_load(result.output)\n        assert len(result) == 2\n    elif serialize == \"github-annotation\":\n        result = json.loads(result.output)\n        filepaths = {r[\"file\"] for r in result}\n        assert len(filepaths) == 1\n    else:\n        raise Exception\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_serialize_multiple_files", "serialize", "check", "the", "general", "format", "of", "json", "output", "for", "multiple", "files", "fpath", "test", "fixtures", "linter", "indentation_errors", "sql", "note", "the", "file", "is", "in", "here", "twice", "two", "files", "two", "payloads", "result", "invoke_assert_code", "args", "lint", "fpath", "fpath", "format", "serialize", "ret_code", "65", "if", "serialize", "json", "result", "json", "loads", "result", "output", "assert", "len", "result", "2", "elif", "serialize", "yaml", "result", "yaml", "safe_load", "result", "output", "assert", "len", "result", "2", "elif", "serialize", "github", "annotation", "result", "json", "loads", "result", "output", "filepaths", "r", "file", "for", "r", "in", "result", "assert", "len", "filepaths", "1", "else", "raise", "exception"], "doc_len": 91}
{"doc_id": "test/cli/commands_test.py::test__cli__command_lint_serialize_github_annotation", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test__cli__command_lint_serialize_github_annotation", "text": "文件路径: test/cli/commands_test.py\ndef test__cli__command_lint_serialize_github_annotation():\n    \"\"\"Test format of github-annotation output.\"\"\"\n    fpath = \"test/fixtures/linter/identifier_capitalisation.sql\"\n    result = invoke_assert_code(\n        args=[\n            lint,\n            (fpath, \"--format\", \"github-annotation\", \"--annotation-level\", \"warning\"),\n        ],\n        ret_code=65,\n    )\n    result = json.loads(result.output)\n    assert result == [\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 1,\n            \"message\": \"L036: Select targets should be on a new line unless there is \"\n            \"only one select target.\",\n            \"start_column\": 1,\n            \"end_column\": 1,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 2,\n            \"message\": \"L027: Unqualified reference 'foo' found in select with more than \"\n            \"one referenced table/view.\",\n            \"start_column\": 5,\n            \"end_column\": 5,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 3,\n            \"message\": \"L012: Implicit/explicit aliasing of columns.\",\n            \"start_column\": 5,\n            \"end_column\": 5,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 3,\n            \"message\": \"L014: Unquoted identifiers must be consistently lower case.\",\n            \"start_column\": 5,\n            \"end_column\": 5,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 4,\n            \"message\": \"L010: Keywords must be consistently lower case.\",\n            \"start_column\": 1,\n            \"end_column\": 1,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 4,\n            \"message\": \"L014: Unquoted identifiers must be consistently lower case.\",\n            \"start_column\": 12,\n            \"end_column\": 12,\n            \"title\": \"SQLFluff\",\n        },\n        {\n            \"annotation_level\": \"warning\",\n            # Normalise paths to control for OS variance\n            \"file\": os.path.normpath(\n                \"test/fixtures/linter/identifier_capitalisation.sql\"\n            ),\n            \"line\": 4,\n            \"message\": \"L014: Unquoted identifiers must be consistently lower case.\",\n            \"start_column\": 18,\n            \"end_column\": 18,\n            \"title\": \"SQLFluff\",\n        },\n    ]\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test__cli__command_lint_serialize_github_annotation", "test", "format", "of", "github", "annotation", "output", "fpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "result", "invoke_assert_code", "args", "lint", "fpath", "format", "github", "annotation", "annotation", "level", "warning", "ret_code", "65", "result", "json", "loads", "result", "output", "assert", "result", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "1", "message", "l036", "select", "targets", "should", "be", "on", "a", "new", "line", "unless", "there", "is", "only", "one", "select", "target", "start_column", "1", "end_column", "1", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "2", "message", "l027", "unqualified", "reference", "foo", "found", "in", "select", "with", "more", "than", "one", "referenced", "table", "view", "start_column", "5", "end_column", "5", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "3", "message", "l012", "implicit", "explicit", "aliasing", "of", "columns", "start_column", "5", "end_column", "5", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "3", "message", "l014", "unquoted", "identifiers", "must", "be", "consistently", "lower", "case", "start_column", "5", "end_column", "5", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "4", "message", "l010", "keywords", "must", "be", "consistently", "lower", "case", "start_column", "1", "end_column", "1", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "4", "message", "l014", "unquoted", "identifiers", "must", "be", "consistently", "lower", "case", "start_column", "12", "end_column", "12", "title", "sqlfluff", "annotation_level", "warning", "normalise", "paths", "to", "control", "for", "os", "variance", "file", "os", "path", "normpath", "test", "fixtures", "linter", "identifier_capitalisation", "sql", "line", "4", "message", "l014", "unquoted", "identifiers", "must", "be", "consistently", "lower", "case", "start_column", "18", "end_column", "18", "title", "sqlfluff"], "doc_len": 294}
{"doc_id": "test/cli/commands_test.py::test___main___help", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test___main___help", "text": "文件路径: test/cli/commands_test.py\ndef test___main___help():\n    \"\"\"Test that the CLI can be access via __main__.\"\"\"\n    # nonzero exit is good enough\n    subprocess.check_output(\n        [sys.executable, \"-m\", \"sqlfluff\", \"--help\"], env=os.environ\n    )\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test___main___help", "test", "that", "the", "cli", "can", "be", "access", "via", "__main__", "nonzero", "exit", "is", "good", "enough", "subprocess", "check_output", "sys", "executable", "m", "sqlfluff", "help", "env", "os", "environ"], "doc_len": 30}
{"doc_id": "test/cli/commands_test.py::test_encoding", "file_path": "test/cli/commands_test.py", "class_name": null, "func_name": "test_encoding", "text": "文件路径: test/cli/commands_test.py\ndef test_encoding(encoding_in, encoding_out):\n    \"\"\"Check the encoding of the test file remains the same after fix is applied.\"\"\"\n    with open(\"test/fixtures/linter/indentation_errors.sql\", \"r\") as testfile:\n        generic_roundtrip_test(\n            testfile,\n            \"L001\",\n            input_file_encoding=encoding_in,\n            output_file_encoding=encoding_out,\n        )\n", "tokens": ["test", "cli", "commands_test", "py", "def", "test_encoding", "encoding_in", "encoding_out", "check", "the", "encoding", "of", "the", "test", "file", "remains", "the", "same", "after", "fix", "is", "applied", "with", "open", "test", "fixtures", "linter", "indentation_errors", "sql", "r", "as", "testfile", "generic_roundtrip_test", "testfile", "l001", "input_file_encoding", "encoding_in", "output_file_encoding", "encoding_out"], "doc_len": 39}
{"doc_id": "test/cli/formatters_test.py::escape_ansi", "file_path": "test/cli/formatters_test.py", "class_name": null, "func_name": "escape_ansi", "text": "文件路径: test/cli/formatters_test.py\ndef escape_ansi(line):\n    \"\"\"Remove ANSI color codes for testing.\"\"\"\n    ansi_escape = re.compile(\"\\u001b\\\\[[0-9]+(;[0-9]+)?m\")\n    return ansi_escape.sub(\"\", line)\n", "tokens": ["test", "cli", "formatters_test", "py", "def", "escape_ansi", "line", "remove", "ansi", "color", "codes", "for", "testing", "ansi_escape", "re", "compile", "u001b", "0", "9", "0", "9", "m", "return", "ansi_escape", "sub", "line"], "doc_len": 26}
{"doc_id": "test/cli/formatters_test.py::test__cli__formatters__filename_nocol", "file_path": "test/cli/formatters_test.py", "class_name": null, "func_name": "test__cli__formatters__filename_nocol", "text": "文件路径: test/cli/formatters_test.py\ndef test__cli__formatters__filename_nocol():\n    \"\"\"Test formatting filenames.\"\"\"\n    res = format_filename(\"blahblah\", success=True)\n    assert escape_ansi(res) == \"== [blahblah] PASS\"\n", "tokens": ["test", "cli", "formatters_test", "py", "def", "test__cli__formatters__filename_nocol", "test", "formatting", "filenames", "res", "format_filename", "blahblah", "success", "true", "assert", "escape_ansi", "res", "blahblah", "pass"], "doc_len": 19}
{"doc_id": "test/cli/formatters_test.py::test__cli__formatters__violation", "file_path": "test/cli/formatters_test.py", "class_name": null, "func_name": "test__cli__formatters__violation", "text": "文件路径: test/cli/formatters_test.py\ndef test__cli__formatters__violation():\n    \"\"\"Test formatting violations.\n\n    NB Position is 1 + start_pos.\n    \"\"\"\n    s = RawSegment(\n        \"foobarbar\",\n        PositionMarker(\n            slice(10, 19),\n            slice(10, 19),\n            TemplatedFile.from_string(\"      \\n\\n  foobarbar\"),\n        ),\n    )\n    r = RuleGhost(\"A\", \"DESC\")\n    v = SQLLintError(segment=s, rule=r)\n    f = format_violation(v)\n    # Position is 3, 3 becase foobarbar is on the third\n    # line (i.e. it has two newlines preceding it) and\n    # it's at the third position in that line (i.e. there\n    # are two characters between it and the preceding\n    # newline).\n    assert escape_ansi(f) == \"L:   3 | P:   3 |    A | DESC\"\n", "tokens": ["test", "cli", "formatters_test", "py", "def", "test__cli__formatters__violation", "test", "formatting", "violations", "nb", "position", "is", "1", "start_pos", "s", "rawsegment", "foobarbar", "positionmarker", "slice", "10", "19", "slice", "10", "19", "templatedfile", "from_string", "n", "n", "foobarbar", "r", "ruleghost", "a", "desc", "v", "sqllinterror", "segment", "s", "rule", "r", "f", "format_violation", "v", "position", "is", "3", "3", "becase", "foobarbar", "is", "on", "the", "third", "line", "i", "e", "it", "has", "two", "newlines", "preceding", "it", "and", "it", "s", "at", "the", "third", "position", "in", "that", "line", "i", "e", "there", "are", "two", "characters", "between", "it", "and", "the", "preceding", "newline", "assert", "escape_ansi", "f", "l", "3", "p", "3", "a", "desc"], "doc_len": 92}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__colorize", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__colorize", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__colorize():\n    \"\"\"Test ANSI colouring.\"\"\"\n    assert colorize(\"foo\", Color.red) == \"\\u001b[31mfoo\\u001b[0m\"\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__colorize", "test", "ansi", "colouring", "assert", "colorize", "foo", "color", "red", "u001b", "31mfoo", "u001b", "0m"], "doc_len": 18}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__cli_table", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__cli_table", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__cli_table():\n    \"\"\"Test making tables.\"\"\"\n    vals = [(\"a\", 3), (\"b\", \"c\"), (\"d\", 4.7654), (\"e\", 9)]\n    txt = cli_table(vals, col_width=7, divider_char=\"|\", label_color=None)\n    # NB: No trailing newline\n    assert txt == \"a:    3|b:    c\\nd: 4.77|e:    9\"\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__cli_table", "test", "making", "tables", "vals", "a", "3", "b", "c", "d", "4", "7654", "e", "9", "txt", "cli_table", "vals", "col_width", "7", "divider_char", "label_color", "none", "nb", "no", "trailing", "newline", "assert", "txt", "a", "3", "b", "c", "nd", "4", "77", "e", "9"], "doc_len": 42}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__wrap_elem", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__wrap_elem", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__wrap_elem(in_str, length, res):\n    \"\"\"Test wrapping.\"\"\"\n    str_list = wrap_elem(in_str, length)\n    assert str_list == res\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__wrap_elem", "in_str", "length", "res", "test", "wrapping", "str_list", "wrap_elem", "in_str", "length", "assert", "str_list", "res"], "doc_len": 18}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__wrap_field_a", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__wrap_field_a", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__wrap_field_a():\n    \"\"\"Test simple wrapping.\"\"\"\n    dct = wrap_field(\"abc\", \"How Now Brown Cow\", width=40)\n    assert dct[\"label_list\"] == [\"abc\"]\n    assert dct[\"val_list\"] == [\"How Now Brown Cow\"]\n    assert \"sep_char\" in dct\n    assert dct[\"lines\"] == 1\n    assert dct[\"label_width\"] == 3\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__wrap_field_a", "test", "simple", "wrapping", "dct", "wrap_field", "abc", "how", "now", "brown", "cow", "width", "40", "assert", "dct", "label_list", "abc", "assert", "dct", "val_list", "how", "now", "brown", "cow", "assert", "sep_char", "in", "dct", "assert", "dct", "lines", "1", "assert", "dct", "label_width", "3"], "doc_len": 41}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__wrap_field_b", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__wrap_field_b", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__wrap_field_b():\n    \"\"\"Test simple wrapping with overlap avoidance.\"\"\"\n    dct = wrap_field(\"abc\", \"How Now Brown Cow\", width=23)\n    assert dct[\"label_list\"] == [\"abc\"]\n    assert dct[\"val_list\"] == [\"How Now Brown Cow\"]\n    assert dct[\"label_width\"] == 3\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__wrap_field_b", "test", "simple", "wrapping", "with", "overlap", "avoidance", "dct", "wrap_field", "abc", "how", "now", "brown", "cow", "width", "23", "assert", "dct", "label_list", "abc", "assert", "dct", "val_list", "how", "now", "brown", "cow", "assert", "dct", "label_width", "3"], "doc_len": 36}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__wrap_field_c", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__wrap_field_c", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__wrap_field_c():\n    \"\"\"Test simple wrapping.\"\"\"\n    dct = wrap_field(\"how now brn cow\", \"How Now Brown Cow\", width=25)\n    assert dct[\"label_list\"] == [\"how now\", \"brn cow\"]\n    assert dct[\"label_width\"] == 7\n    assert dct[\"val_list\"] == [\"How Now Brown\", \"Cow\"]\n    assert dct[\"lines\"] == 2\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__wrap_field_c", "test", "simple", "wrapping", "dct", "wrap_field", "how", "now", "brn", "cow", "how", "now", "brown", "cow", "width", "25", "assert", "dct", "label_list", "how", "now", "brn", "cow", "assert", "dct", "label_width", "7", "assert", "dct", "val_list", "how", "now", "brown", "cow", "assert", "dct", "lines", "2"], "doc_len": 43}
{"doc_id": "test/cli/helpers_test.py::test__cli__helpers__pad_line", "file_path": "test/cli/helpers_test.py", "class_name": null, "func_name": "test__cli__helpers__pad_line", "text": "文件路径: test/cli/helpers_test.py\ndef test__cli__helpers__pad_line():\n    \"\"\"Test line padding.\"\"\"\n    assert pad_line(\"abc\", 5) == \"abc  \"\n    assert pad_line(\"abcdef\", 10, align=\"right\") == \"    abcdef\"\n", "tokens": ["test", "cli", "helpers_test", "py", "def", "test__cli__helpers__pad_line", "test", "line", "padding", "assert", "pad_line", "abc", "5", "abc", "assert", "pad_line", "abcdef", "10", "align", "right", "abcdef"], "doc_len": 21}
{"doc_id": "test/core/config_test.py::mock_xdg_home", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "mock_xdg_home", "text": "文件路径: test/core/config_test.py\ndef mock_xdg_home(monkeypatch):\n    \"\"\"Sets the XDG_CONFIG_HOME variable.\"\"\"\n    monkeypatch.setenv(\"XDG_CONFIG_HOME\", \"~/.config/my/special/path\")\n", "tokens": ["test", "core", "config_test", "py", "def", "mock_xdg_home", "monkeypatch", "sets", "the", "xdg_config_home", "variable", "monkeypatch", "setenv", "xdg_config_home", "config", "my", "special", "path"], "doc_len": 18}
{"doc_id": "test/core/config_test.py::test__config__nested_combine", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__nested_combine", "text": "文件路径: test/core/config_test.py\ndef test__config__nested_combine():\n    \"\"\"Test combination of two config dicts.\"\"\"\n    a = {\"a\": {\"b\": {\"c\": 123, \"d\": 456}}}\n    b = {\"b\": {\"b\": {\"c\": 123, \"d\": 456}}}\n    c = {\"a\": {\"b\": {\"c\": 234, \"e\": 456}}}\n    r = nested_combine(a, b, c)\n    assert r == {\n        \"a\": {\"b\": {\"c\": 234, \"e\": 456, \"d\": 456}},\n        \"b\": {\"b\": {\"c\": 123, \"d\": 456}},\n    }\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__nested_combine", "test", "combination", "of", "two", "config", "dicts", "a", "a", "b", "c", "123", "d", "456", "b", "b", "b", "c", "123", "d", "456", "c", "a", "b", "c", "234", "e", "456", "r", "nested_combine", "a", "b", "c", "assert", "r", "a", "b", "c", "234", "e", "456", "d", "456", "b", "b", "c", "123", "d", "456"], "doc_len": 54}
{"doc_id": "test/core/config_test.py::test__config__dict_diff", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__dict_diff", "text": "文件路径: test/core/config_test.py\ndef test__config__dict_diff():\n    \"\"\"Test diffs between two config dicts.\"\"\"\n    a = {\"a\": {\"b\": {\"c\": 123, \"d\": 456, \"f\": 6}}}\n    b = {\"b\": {\"b\": {\"c\": 123, \"d\": 456}}}\n    c = {\"a\": {\"b\": {\"c\": 234, \"e\": 456, \"f\": 6}}}\n    assert dict_diff(a, b) == a\n    assert dict_diff(a, c) == {\"a\": {\"b\": {\"c\": 123, \"d\": 456}}}\n    assert dict_diff(c, a) == {\"a\": {\"b\": {\"c\": 234, \"e\": 456}}}\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__dict_diff", "test", "diffs", "between", "two", "config", "dicts", "a", "a", "b", "c", "123", "d", "456", "f", "6", "b", "b", "b", "c", "123", "d", "456", "c", "a", "b", "c", "234", "e", "456", "f", "6", "assert", "dict_diff", "a", "b", "a", "assert", "dict_diff", "a", "c", "a", "b", "c", "123", "d", "456", "assert", "dict_diff", "c", "a", "a", "b", "c", "234", "e", "456"], "doc_len": 62}
{"doc_id": "test/core/config_test.py::test__config__load_file_dir", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__load_file_dir", "text": "文件路径: test/core/config_test.py\ndef test__config__load_file_dir():\n    \"\"\"Test loading config from a directory path.\"\"\"\n    c = ConfigLoader()\n    cfg = c.load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\")\n    )\n    assert cfg == config_a\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__load_file_dir", "test", "loading", "config", "from", "a", "directory", "path", "c", "configloader", "cfg", "c", "load_config_at_path", "os", "path", "join", "test", "fixtures", "config", "inheritance_a", "assert", "cfg", "config_a"], "doc_len": 28}
{"doc_id": "test/core/config_test.py::test__config__load_file_f", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__load_file_f", "text": "文件路径: test/core/config_test.py\ndef test__config__load_file_f():\n    \"\"\"Test loading config from a file path.\"\"\"\n    c = ConfigLoader()\n    cfg = c.load_config_at_path(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"inheritance_a\", \"testing.sql\")\n    )\n    assert cfg == config_a\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__load_file_f", "test", "loading", "config", "from", "a", "file", "path", "c", "configloader", "cfg", "c", "load_config_at_path", "os", "path", "join", "test", "fixtures", "config", "inheritance_a", "testing", "sql", "assert", "cfg", "config_a"], "doc_len": 30}
{"doc_id": "test/core/config_test.py::test__config__load_nested", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__load_nested", "text": "文件路径: test/core/config_test.py\ndef test__config__load_nested():\n    \"\"\"Test nested overwrite and order of precedence of config files in the same directory.\"\"\"\n    c = ConfigLoader()\n    cfg = c.load_config_up_to_path(\n        os.path.join(\n            \"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\", \"blah.sql\"\n        )\n    )\n    assert cfg == {\n        \"core\": {\"testing_val\": \"foobar\", \"testing_int\": 1, \"testing_bar\": 7.698},\n        \"bar\": {\"foo\": \"foobar\"},\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__load_nested", "test", "nested", "overwrite", "and", "order", "of", "precedence", "of", "config", "files", "in", "the", "same", "directory", "c", "configloader", "cfg", "c", "load_config_up_to_path", "os", "path", "join", "test", "fixtures", "config", "inheritance_a", "nested", "blah", "sql", "assert", "cfg", "core", "testing_val", "foobar", "testing_int", "1", "testing_bar", "7", "698", "bar", "foo", "foobar", "fnarr", "fnarr", "foo", "foobar"], "doc_len": 52}
{"doc_id": "test/core/config_test.py::test__config__load_toml", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__load_toml", "text": "文件路径: test/core/config_test.py\ndef test__config__load_toml():\n    \"\"\"Test loading config from a pyproject.toml file.\"\"\"\n    c = ConfigLoader()\n    cfg = c.load_default_config_file(\n        os.path.join(\"test\", \"fixtures\", \"config\", \"toml\"),\n        \"pyproject.toml\",\n    )\n    assert cfg == {\n        \"core\": {\n            \"testing_int\": 5,\n            \"testing_bar\": 7.698,\n            \"testing_bool\": False,\n            \"testing_arr\": [\"a\", \"b\", \"c\"],\n            \"testing_inline_table\": {\"x\": 1},\n        },\n        \"bar\": {\"foo\": \"foobar\"},\n        \"fnarr\": {\"fnarr\": {\"foo\": \"foobar\"}},\n    }\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__load_toml", "test", "loading", "config", "from", "a", "pyproject", "toml", "file", "c", "configloader", "cfg", "c", "load_default_config_file", "os", "path", "join", "test", "fixtures", "config", "toml", "pyproject", "toml", "assert", "cfg", "core", "testing_int", "5", "testing_bar", "7", "698", "testing_bool", "false", "testing_arr", "a", "b", "c", "testing_inline_table", "x", "1", "bar", "foo", "foobar", "fnarr", "fnarr", "foo", "foobar"], "doc_len": 52}
{"doc_id": "test/core/config_test.py::test__config__iter_config_paths_right_order", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__iter_config_paths_right_order", "text": "文件路径: test/core/config_test.py\ndef test__config__iter_config_paths_right_order():\n    \"\"\"Test that config paths are fetched ordered by priority.\"\"\"\n    c = ConfigLoader()\n    cfg_paths = c.iter_config_locations_up_to_path(\n        os.path.join(\n            \"test\", \"fixtures\", \"config\", \"inheritance_a\", \"nested\", \"blah.sql\"\n        ),\n        working_path=\"test/fixtures\",\n    )\n    assert list(cfg_paths) == [\n        str(Path(p).resolve())\n        for p in [\n            \"test/fixtures\",\n            \"test/fixtures/config\",\n            \"test/fixtures/config/inheritance_a\",\n            \"test/fixtures/config/inheritance_a/nested\",\n        ]\n    ]\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__iter_config_paths_right_order", "test", "that", "config", "paths", "are", "fetched", "ordered", "by", "priority", "c", "configloader", "cfg_paths", "c", "iter_config_locations_up_to_path", "os", "path", "join", "test", "fixtures", "config", "inheritance_a", "nested", "blah", "sql", "working_path", "test", "fixtures", "assert", "list", "cfg_paths", "str", "path", "p", "resolve", "for", "p", "in", "test", "fixtures", "test", "fixtures", "config", "test", "fixtures", "config", "inheritance_a", "test", "fixtures", "config", "inheritance_a", "nested"], "doc_len": 57}
{"doc_id": "test/core/config_test.py::test__config__find_sqlfluffignore_in_same_directory", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__find_sqlfluffignore_in_same_directory", "text": "文件路径: test/core/config_test.py\ndef test__config__find_sqlfluffignore_in_same_directory():\n    \"\"\"Test find ignore file in the same directory as sql file.\"\"\"\n    ignore_files = ConfigLoader.find_ignore_config_files(\n        path=\"test/fixtures/linter/sqlfluffignore/path_b/query_b.sql\",\n        working_path=\"test/fixtures/linter/sqlfluffignore/\",\n    )\n    assert ignore_files == {\n        os.path.abspath(\"test/fixtures/linter/sqlfluffignore/path_b/.sqlfluffignore\"),\n        os.path.abspath(\"test/fixtures/linter/sqlfluffignore/.sqlfluffignore\"),\n    }\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__find_sqlfluffignore_in_same_directory", "test", "find", "ignore", "file", "in", "the", "same", "directory", "as", "sql", "file", "ignore_files", "configloader", "find_ignore_config_files", "path", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "query_b", "sql", "working_path", "test", "fixtures", "linter", "sqlfluffignore", "assert", "ignore_files", "os", "path", "abspath", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "sqlfluffignore", "os", "path", "abspath", "test", "fixtures", "linter", "sqlfluffignore", "sqlfluffignore"], "doc_len": 52}
{"doc_id": "test/core/config_test.py::test__config__nested_config_tests", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__nested_config_tests", "text": "文件路径: test/core/config_test.py\ndef test__config__nested_config_tests():\n    \"\"\"Test linting with overriden config in nested paths.\n\n    This looks like a linter test but it's actually a config\n    test.\n    \"\"\"\n    lntr = Linter(config=FluffConfig(overrides=dict(exclude_rules=\"L002\")))\n    lnt = lntr.lint_path(\"test/fixtures/config/inheritance_b\")\n    violations = lnt.check_tuples(by_path=True)\n    for k in violations:\n        if k.endswith(\"nested\\\\example.sql\"):\n            assert (\"L003\", 1, 4) in violations[k]\n            assert (\"L009\", 1, 12) in violations[k]\n            assert \"L002\" not in [c[0] for c in violations[k]]\n        elif k.endswith(\"inheritance_b\\\\example.sql\"):\n            assert (\"L003\", 1, 4) in violations[k]\n            assert \"L002\" not in [c[0] for c in violations[k]]\n            assert \"L009\" not in [c[0] for c in violations[k]]\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__nested_config_tests", "test", "linting", "with", "overriden", "config", "in", "nested", "paths", "this", "looks", "like", "a", "linter", "test", "but", "it", "s", "actually", "a", "config", "test", "lntr", "linter", "config", "fluffconfig", "overrides", "dict", "exclude_rules", "l002", "lnt", "lntr", "lint_path", "test", "fixtures", "config", "inheritance_b", "violations", "lnt", "check_tuples", "by_path", "true", "for", "k", "in", "violations", "if", "k", "endswith", "nested", "example", "sql", "assert", "l003", "1", "4", "in", "violations", "k", "assert", "l009", "1", "12", "in", "violations", "k", "assert", "l002", "not", "in", "c", "0", "for", "c", "in", "violations", "k", "elif", "k", "endswith", "inheritance_b", "example", "sql", "assert", "l003", "1", "4", "in", "violations", "k", "assert", "l002", "not", "in", "c", "0", "for", "c", "in", "violations", "k", "assert", "l009", "not", "in", "c", "0", "for", "c", "in", "violations", "k"], "doc_len": 117}
{"doc_id": "test/core/config_test.py::test__config__load_user_appdir_config", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__load_user_appdir_config", "text": "文件路径: test/core/config_test.py\ndef test__config__load_user_appdir_config(\n    mock_listdir, mock_path_exists, mock_xdg_home\n):\n    \"\"\"Test loading config from user appdir.\"\"\"\n    xdg_config_path = os.environ.get(\"XDG_CONFIG_HOME\") + \"/sqlfluff\"\n\n    def path_exists(x):\n        if x == os.path.expanduser(\"~/.config/sqlfluff\"):\n            return False\n        if x == xdg_config_path:\n            return False\n        else:\n            return True\n\n    mock_path_exists.side_effect = path_exists\n\n    c = ConfigLoader()\n\n    with patch.object(appdirs, attribute=\"system\", new=\"darwin\"):\n        resolved_path = c._get_user_config_dir_path()\n        c.load_user_appdir_config()\n    assert resolved_path == os.path.expanduser(\"~/Library/Application Support/sqlfluff\")\n\n    mock_path_exists.assert_has_calls(\n        [\n            call(xdg_config_path),\n            call(os.path.expanduser(\"~/Library/Application Support/sqlfluff\")),\n        ]\n    )\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__load_user_appdir_config", "mock_listdir", "mock_path_exists", "mock_xdg_home", "test", "loading", "config", "from", "user", "appdir", "xdg_config_path", "os", "environ", "get", "xdg_config_home", "sqlfluff", "def", "path_exists", "x", "if", "x", "os", "path", "expanduser", "config", "sqlfluff", "return", "false", "if", "x", "xdg_config_path", "return", "false", "else", "return", "true", "mock_path_exists", "side_effect", "path_exists", "c", "configloader", "with", "patch", "object", "appdirs", "attribute", "system", "new", "darwin", "resolved_path", "c", "_get_user_config_dir_path", "c", "load_user_appdir_config", "assert", "resolved_path", "os", "path", "expanduser", "library", "application", "support", "sqlfluff", "mock_path_exists", "assert_has_calls", "call", "xdg_config_path", "call", "os", "path", "expanduser", "library", "application", "support", "sqlfluff"], "doc_len": 80}
{"doc_id": "test/core/config_test.py::test__config__split_comma_separated_string", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__split_comma_separated_string", "text": "文件路径: test/core/config_test.py\ndef test__config__split_comma_separated_string(raw_str, expected):\n    \"\"\"Tests that comma separated string config is handled correctly.\"\"\"\n    assert FluffConfig._split_comma_separated_string(raw_str) == expected\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__split_comma_separated_string", "raw_str", "expected", "tests", "that", "comma", "separated", "string", "config", "is", "handled", "correctly", "assert", "fluffconfig", "_split_comma_separated_string", "raw_str", "expected"], "doc_len": 22}
{"doc_id": "test/core/config_test.py::test__config__templater_selection", "file_path": "test/core/config_test.py", "class_name": null, "func_name": "test__config__templater_selection", "text": "文件路径: test/core/config_test.py\ndef test__config__templater_selection():\n    \"\"\"Test template selection by name.\"\"\"\n    cfg = FluffConfig()\n    assert cfg.get_templater().__class__ is JinjaTemplater\n    assert cfg.get_templater(\"raw\").__class__ is RawTemplater\n    assert cfg.get_templater(\"python\").__class__ is PythonTemplater\n    assert cfg.get_templater(\"jinja\").__class__ is JinjaTemplater\n    with pytest.raises(ValueError):\n        cfg.get_templater(\"afefhlsakufe\")\n", "tokens": ["test", "core", "config_test", "py", "def", "test__config__templater_selection", "test", "template", "selection", "by", "name", "cfg", "fluffconfig", "assert", "cfg", "get_templater", "__class__", "is", "jinjatemplater", "assert", "cfg", "get_templater", "raw", "__class__", "is", "rawtemplater", "assert", "cfg", "get_templater", "python", "__class__", "is", "pythontemplater", "assert", "cfg", "get_templater", "jinja", "__class__", "is", "jinjatemplater", "with", "pytest", "raises", "valueerror", "cfg", "get_templater", "afefhlsakufe"], "doc_len": 47}
{"doc_id": "test/core/file_helpers_test.py::test__parser__helper_get_encoding", "file_path": "test/core/file_helpers_test.py", "class_name": null, "func_name": "test__parser__helper_get_encoding", "text": "文件路径: test/core/file_helpers_test.py\ndef test__parser__helper_get_encoding(fname, config_encoding, result):\n    \"\"\"Test get_encoding.\"\"\"\n    assert (\n        get_encoding(\n            fname=fname, config=FluffConfig(overrides={\"encoding\": config_encoding})\n        )\n        == result\n    )\n", "tokens": ["test", "core", "file_helpers_test", "py", "def", "test__parser__helper_get_encoding", "fname", "config_encoding", "result", "test", "get_encoding", "assert", "get_encoding", "fname", "fname", "config", "fluffconfig", "overrides", "encoding", "config_encoding", "result"], "doc_len": 21}
{"doc_id": "test/core/linter_test.py::DummyLintError.__init__", "file_path": "test/core/linter_test.py", "class_name": "DummyLintError", "func_name": "__init__", "text": "文件路径: test/core/linter_test.py, 类名: DummyLintError\n    def __init__(self, line_no: int, code: str = \"L001\"):\n        self._code = code\n        super().__init__(line_no=line_no)\n", "tokens": ["test", "core", "linter_test", "py", "dummylinterror", "def", "__init__", "self", "line_no", "int", "code", "str", "l001", "self", "_code", "code", "super", "__init__", "line_no", "line_no"], "doc_len": 20}
{"doc_id": "test/core/linter_test.py::normalise_paths", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "normalise_paths", "text": "文件路径: test/core/linter_test.py\ndef normalise_paths(paths):\n    \"\"\"Test normalising paths.\n\n    NB Paths on difference platforms might look different, so this\n    makes them comparable.\n    \"\"\"\n    return {pth.replace(\"/\", \".\").replace(\"\\\\\", \".\") for pth in paths}\n", "tokens": ["test", "core", "linter_test", "py", "def", "normalise_paths", "paths", "test", "normalising", "paths", "nb", "paths", "on", "difference", "platforms", "might", "look", "different", "so", "this", "makes", "them", "comparable", "return", "pth", "replace", "replace", "for", "pth", "in", "paths"], "doc_len": 31}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__dir", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__dir", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__dir():\n    \"\"\"Test extracting paths from directories.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(\"test/fixtures/lexer\")\n    assert normalise_paths(paths) == {\n        \"test.fixtures.lexer.block_comment.sql\",\n        \"test.fixtures.lexer.inline_comment.sql\",\n        \"test.fixtures.lexer.basic.sql\",\n    }\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__dir", "test", "extracting", "paths", "from", "directories", "lntr", "linter", "paths", "lntr", "paths_from_path", "test", "fixtures", "lexer", "assert", "normalise_paths", "paths", "test", "fixtures", "lexer", "block_comment", "sql", "test", "fixtures", "lexer", "inline_comment", "sql", "test", "fixtures", "lexer", "basic", "sql"], "doc_len": 37}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__default", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__default", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__default():\n    \"\"\"Test .sql files are found by default.\"\"\"\n    lntr = Linter()\n    paths = normalise_paths(lntr.paths_from_path(\"test/fixtures/linter\"))\n    assert \"test.fixtures.linter.passing.sql\" in paths\n    assert \"test.fixtures.linter.discovery_file.txt\" not in paths\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__default", "test", "sql", "files", "are", "found", "by", "default", "lntr", "linter", "paths", "normalise_paths", "lntr", "paths_from_path", "test", "fixtures", "linter", "assert", "test", "fixtures", "linter", "passing", "sql", "in", "paths", "assert", "test", "fixtures", "linter", "discovery_file", "txt", "not", "in", "paths"], "doc_len": 39}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__exts", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__exts", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__exts():\n    \"\"\"Test configuration of file discovery.\"\"\"\n    lntr = Linter(config=FluffConfig(overrides={\"sql_file_exts\": \".txt\"}))\n    paths = normalise_paths(lntr.paths_from_path(\"test/fixtures/linter\"))\n    assert \"test.fixtures.linter.passing.sql\" not in paths\n    assert \"test.fixtures.linter.discovery_file.txt\" in paths\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__exts", "test", "configuration", "of", "file", "discovery", "lntr", "linter", "config", "fluffconfig", "overrides", "sql_file_exts", "txt", "paths", "normalise_paths", "lntr", "paths_from_path", "test", "fixtures", "linter", "assert", "test", "fixtures", "linter", "passing", "sql", "not", "in", "paths", "assert", "test", "fixtures", "linter", "discovery_file", "txt", "in", "paths"], "doc_len": 42}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__file", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__file", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__file():\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(\"test/fixtures/linter/indentation_errors.sql\")\n    assert normalise_paths(paths) == {\"test.fixtures.linter.indentation_errors.sql\"}\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__file", "test", "extracting", "paths", "from", "a", "file", "path", "lntr", "linter", "paths", "lntr", "paths_from_path", "test", "fixtures", "linter", "indentation_errors", "sql", "assert", "normalise_paths", "paths", "test", "fixtures", "linter", "indentation_errors", "sql"], "doc_len": 31}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__not_exist", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__not_exist", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__not_exist():\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    lntr = Linter()\n    with pytest.raises(IOError):\n        lntr.paths_from_path(\"asflekjfhsakuefhse\")\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__not_exist", "test", "extracting", "paths", "from", "a", "file", "path", "lntr", "linter", "with", "pytest", "raises", "ioerror", "lntr", "paths_from_path", "asflekjfhsakuefhse"], "doc_len": 22}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__not_exist_ignore", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__not_exist_ignore", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__not_exist_ignore():\n    \"\"\"Test extracting paths from a file path.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(\"asflekjfhsakuefhse\", ignore_non_existent_files=True)\n    assert len(paths) == 0\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__not_exist_ignore", "test", "extracting", "paths", "from", "a", "file", "path", "lntr", "linter", "paths", "lntr", "paths_from_path", "asflekjfhsakuefhse", "ignore_non_existent_files", "true", "assert", "len", "paths", "0"], "doc_len": 25}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__explicit_ignore", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__explicit_ignore", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__explicit_ignore():\n    \"\"\"Test ignoring files that were passed explicitly.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(\n        \"test/fixtures/linter/sqlfluffignore/path_a/query_a.sql\",\n        ignore_non_existent_files=True,\n        ignore_files=True,\n        working_path=\"test/fixtures/linter/sqlfluffignore/\",\n    )\n    assert len(paths) == 0\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__explicit_ignore", "test", "ignoring", "files", "that", "were", "passed", "explicitly", "lntr", "linter", "paths", "lntr", "paths_from_path", "test", "fixtures", "linter", "sqlfluffignore", "path_a", "query_a", "sql", "ignore_non_existent_files", "true", "ignore_files", "true", "working_path", "test", "fixtures", "linter", "sqlfluffignore", "assert", "len", "paths", "0"], "doc_len": 38}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__dot", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__dot", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__dot():\n    \"\"\"Test extracting paths from a dot.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(\".\")\n    # Use set theory to check that we get AT LEAST these files\n    assert normalise_paths(paths) >= {\n        \"test.fixtures.lexer.block_comment.sql\",\n        \"test.fixtures.lexer.inline_comment.sql\",\n        \"test.fixtures.lexer.basic.sql\",\n    }\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__dot", "test", "extracting", "paths", "from", "a", "dot", "lntr", "linter", "paths", "lntr", "paths_from_path", "use", "set", "theory", "to", "check", "that", "we", "get", "at", "least", "these", "files", "assert", "normalise_paths", "paths", "test", "fixtures", "lexer", "block_comment", "sql", "test", "fixtures", "lexer", "inline_comment", "sql", "test", "fixtures", "lexer", "basic", "sql"], "doc_len": 47}
{"doc_id": "test/core/linter_test.py::test__linter__path_from_paths__ignore", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__path_from_paths__ignore", "text": "文件路径: test/core/linter_test.py\ndef test__linter__path_from_paths__ignore(path):\n    \"\"\"Test extracting paths from a dot.\"\"\"\n    lntr = Linter()\n    paths = lntr.paths_from_path(path)\n    # We should only get query_b, because of the sqlfluffignore files.\n    assert normalise_paths(paths) == {\n        \"test.fixtures.linter.sqlfluffignore.path_b.query_b.sql\"\n    }\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__path_from_paths__ignore", "path", "test", "extracting", "paths", "from", "a", "dot", "lntr", "linter", "paths", "lntr", "paths_from_path", "path", "we", "should", "only", "get", "query_b", "because", "of", "the", "sqlfluffignore", "files", "assert", "normalise_paths", "paths", "test", "fixtures", "linter", "sqlfluffignore", "path_b", "query_b", "sql"], "doc_len": 39}
{"doc_id": "test/core/linter_test.py::test__linter__lint_string_vs_file", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__lint_string_vs_file", "text": "文件路径: test/core/linter_test.py\ndef test__linter__lint_string_vs_file(path):\n    \"\"\"Test the linter finds the same things on strings and files.\"\"\"\n    with open(path) as f:\n        sql_str = f.read()\n    lntr = Linter()\n    assert (\n        lntr.lint_string(sql_str).check_tuples() == lntr.lint_path(path).check_tuples()\n    )\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__lint_string_vs_file", "path", "test", "the", "linter", "finds", "the", "same", "things", "on", "strings", "and", "files", "with", "open", "path", "as", "f", "sql_str", "f", "read", "lntr", "linter", "assert", "lntr", "lint_string", "sql_str", "check_tuples", "lntr", "lint_path", "path", "check_tuples"], "doc_len": 37}
{"doc_id": "test/core/linter_test.py::test__linter__get_violations_filter_rules", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__get_violations_filter_rules", "text": "文件路径: test/core/linter_test.py\ndef test__linter__get_violations_filter_rules(rules, num_violations):\n    \"\"\"Test filtering violations by which rules were violated.\"\"\"\n    lntr = Linter()\n    lint_result = lntr.lint_string(\"select a, b FROM tbl c order BY d\")\n\n    assert len(lint_result.get_violations(rules=rules)) == num_violations\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__get_violations_filter_rules", "rules", "num_violations", "test", "filtering", "violations", "by", "which", "rules", "were", "violated", "lntr", "linter", "lint_result", "lntr", "lint_string", "select", "a", "b", "from", "tbl", "c", "order", "by", "d", "assert", "len", "lint_result", "get_violations", "rules", "rules", "num_violations"], "doc_len": 37}
{"doc_id": "test/core/linter_test.py::test__linter__linting_result__sum_dicts", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_result__sum_dicts", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_result__sum_dicts():\n    \"\"\"Test the summing of dictionaries in the linter.\"\"\"\n    lr = LintingResult()\n    i = {}\n    a = dict(a=3, b=123, f=876.321)\n    b = dict(a=19, b=321.0, g=23478)\n    r = dict(a=22, b=444.0, f=876.321, g=23478)\n    assert lr.sum_dicts(a, b) == r\n    # Check the identity too\n    assert lr.sum_dicts(r, i) == r\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_result__sum_dicts", "test", "the", "summing", "of", "dictionaries", "in", "the", "linter", "lr", "lintingresult", "i", "a", "dict", "a", "3", "b", "123", "f", "876", "321", "b", "dict", "a", "19", "b", "321", "0", "g", "23478", "r", "dict", "a", "22", "b", "444", "0", "f", "876", "321", "g", "23478", "assert", "lr", "sum_dicts", "a", "b", "r", "check", "the", "identity", "too", "assert", "lr", "sum_dicts", "r", "i", "r"], "doc_len": 63}
{"doc_id": "test/core/linter_test.py::test__linter__linting_result__combine_dicts", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_result__combine_dicts", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_result__combine_dicts():\n    \"\"\"Test the combination of dictionaries in the linter.\"\"\"\n    lr = LintingResult()\n    a = dict(a=3, b=123, f=876.321)\n    b = dict(h=19, i=321.0, j=23478)\n    r = dict(z=22)\n    assert lr.combine_dicts(a, b, r) == dict(\n        a=3, b=123, f=876.321, h=19, i=321.0, j=23478, z=22\n    )\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_result__combine_dicts", "test", "the", "combination", "of", "dictionaries", "in", "the", "linter", "lr", "lintingresult", "a", "dict", "a", "3", "b", "123", "f", "876", "321", "b", "dict", "h", "19", "i", "321", "0", "j", "23478", "r", "dict", "z", "22", "assert", "lr", "combine_dicts", "a", "b", "r", "dict", "a", "3", "b", "123", "f", "876", "321", "h", "19", "i", "321", "0", "j", "23478", "z", "22"], "doc_len": 61}
{"doc_id": "test/core/linter_test.py::test__linter__linting_result_check_tuples_by_path", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_result_check_tuples_by_path", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_result_check_tuples_by_path(by_path, result_type):\n    \"\"\"Test that a LintingResult can partition violations by the source files.\"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths(\n        [\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ]\n    )\n    check_tuples = result.check_tuples(by_path=by_path)\n    isinstance(check_tuples, result_type)\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_result_check_tuples_by_path", "by_path", "result_type", "test", "that", "a", "lintingresult", "can", "partition", "violations", "by", "the", "source", "files", "lntr", "linter", "result", "lntr", "lint_paths", "test", "fixtures", "linter", "comma_errors", "sql", "test", "fixtures", "linter", "whitespace_errors", "sql", "check_tuples", "result", "check_tuples", "by_path", "by_path", "isinstance", "check_tuples", "result_type"], "doc_len": 42}
{"doc_id": "test/core/linter_test.py::test__linter__linting_result_get_violations", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_result_get_violations", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_result_get_violations(processes):\n    \"\"\"Test that we can get violations from a LintingResult.\"\"\"\n    lntr = Linter()\n    result = lntr.lint_paths(\n        [\n            \"test/fixtures/linter/comma_errors.sql\",\n            \"test/fixtures/linter/whitespace_errors.sql\",\n        ],\n        processes=processes,\n    )\n\n    all([type(v) == SQLLintError for v in result.get_violations()])\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_result_get_violations", "processes", "test", "that", "we", "can", "get", "violations", "from", "a", "lintingresult", "lntr", "linter", "result", "lntr", "lint_paths", "test", "fixtures", "linter", "comma_errors", "sql", "test", "fixtures", "linter", "whitespace_errors", "sql", "processes", "processes", "all", "type", "v", "sqllinterror", "for", "v", "in", "result", "get_violations"], "doc_len": 42}
{"doc_id": "test/core/linter_test.py::test__linter__linting_parallel_thread", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_parallel_thread", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_parallel_thread(force_error, monkeypatch):\n    \"\"\"Run linter in parallel mode using threads.\n\n    Similar to test__linter__linting_result_get_violations but uses a thread\n    pool of 1 worker to test parallel mode without subprocesses. This lets the\n    tests capture code coverage information for the backend parts of parallel\n    execution without having to jump through hoops.\n    \"\"\"\n    if not force_error:\n\n        monkeypatch.setattr(Linter, \"allow_process_parallelism\", False)\n\n    else:\n\n        def _create_pool(*args, **kwargs):\n            class ErrorPool:\n                def __enter__(self):\n                    return self\n\n                def __exit__(self, exc_type, exc_val, exc_tb):\n                    pass\n\n                def imap_unordered(self, *args, **kwargs):\n                    yield runner.DelayedException(ValueError())\n\n            return ErrorPool()\n\n        monkeypatch.setattr(runner.MultiProcessRunner, \"_create_pool\", _create_pool)\n\n    lntr = Linter(formatter=CallbackFormatter(callback=lambda m: None, verbosity=0))\n    result = lntr.lint_paths(\n        (\"test/fixtures/linter/comma_errors.sql\",),\n        processes=2,\n    )\n\n    all([type(v) == SQLLintError for v in result.get_violations()])\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_parallel_thread", "force_error", "monkeypatch", "run", "linter", "in", "parallel", "mode", "using", "threads", "similar", "to", "test__linter__linting_result_get_violations", "but", "uses", "a", "thread", "pool", "of", "1", "worker", "to", "test", "parallel", "mode", "without", "subprocesses", "this", "lets", "the", "tests", "capture", "code", "coverage", "information", "for", "the", "backend", "parts", "of", "parallel", "execution", "without", "having", "to", "jump", "through", "hoops", "if", "not", "force_error", "monkeypatch", "setattr", "linter", "allow_process_parallelism", "false", "else", "def", "_create_pool", "args", "kwargs", "class", "errorpool", "def", "__enter__", "self", "return", "self", "def", "__exit__", "self", "exc_type", "exc_val", "exc_tb", "pass", "def", "imap_unordered", "self", "args", "kwargs", "yield", "runner", "delayedexception", "valueerror", "return", "errorpool", "monkeypatch", "setattr", "runner", "multiprocessrunner", "_create_pool", "_create_pool", "lntr", "linter", "formatter", "callbackformatter", "callback", "lambda", "m", "none", "verbosity", "0", "result", "lntr", "lint_paths", "test", "fixtures", "linter", "comma_errors", "sql", "processes", "2", "all", "type", "v", "sqllinterror", "for", "v", "in", "result", "get_violations"], "doc_len": 126}
{"doc_id": "test/core/linter_test.py::test_lint_path_parallel_wrapper_exception", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_lint_path_parallel_wrapper_exception", "text": "文件路径: test/core/linter_test.py\ndef test_lint_path_parallel_wrapper_exception(patched_lint):\n    \"\"\"Tests the error catching behavior of _lint_path_parallel_wrapper().\n\n    Test on MultiThread runner because otherwise we have pickling issues.\n    \"\"\"\n    patched_lint.side_effect = ValueError(\"Something unexpected happened\")\n    for result in runner.MultiThreadRunner(Linter(), FluffConfig(), processes=1).run(\n        [\"test/fixtures/linter/passing.sql\"], fix=False\n    ):\n        assert isinstance(result, runner.DelayedException)\n        with pytest.raises(ValueError):\n            result.reraise()\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_lint_path_parallel_wrapper_exception", "patched_lint", "tests", "the", "error", "catching", "behavior", "of", "_lint_path_parallel_wrapper", "test", "on", "multithread", "runner", "because", "otherwise", "we", "have", "pickling", "issues", "patched_lint", "side_effect", "valueerror", "something", "unexpected", "happened", "for", "result", "in", "runner", "multithreadrunner", "linter", "fluffconfig", "processes", "1", "run", "test", "fixtures", "linter", "passing", "sql", "fix", "false", "assert", "isinstance", "result", "runner", "delayedexception", "with", "pytest", "raises", "valueerror", "result", "reraise"], "doc_len": 58}
{"doc_id": "test/core/linter_test.py::test__linter__linting_unexpected_error_handled_gracefully", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__linting_unexpected_error_handled_gracefully", "text": "文件路径: test/core/linter_test.py\ndef test__linter__linting_unexpected_error_handled_gracefully(\n    patched_lint, patched_logger\n):\n    \"\"\"Test that an unexpected internal error is handled gracefully and returns the issue-surfacing file.\"\"\"\n    patched_lint.side_effect = Exception(\"Something unexpected happened\")\n    lntr = Linter()\n    lntr.lint_paths((\"test/fixtures/linter/passing.sql\",))\n    assert (\n        \"Unable to lint test/fixtures/linter/passing.sql due to an internal error.\"\n        # NB: Replace is to handle windows-style paths.\n        in patched_logger.warning.call_args[0][0].replace(\"\\\\\", \"/\")\n        and \"Exception: Something unexpected happened\"\n        in patched_logger.warning.call_args[0][0]\n    )\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__linting_unexpected_error_handled_gracefully", "patched_lint", "patched_logger", "test", "that", "an", "unexpected", "internal", "error", "is", "handled", "gracefully", "and", "returns", "the", "issue", "surfacing", "file", "patched_lint", "side_effect", "exception", "something", "unexpected", "happened", "lntr", "linter", "lntr", "lint_paths", "test", "fixtures", "linter", "passing", "sql", "assert", "unable", "to", "lint", "test", "fixtures", "linter", "passing", "sql", "due", "to", "an", "internal", "error", "nb", "replace", "is", "to", "handle", "windows", "style", "paths", "in", "patched_logger", "warning", "call_args", "0", "0", "replace", "and", "exception", "something", "unexpected", "happened", "in", "patched_logger", "warning", "call_args", "0", "0"], "doc_len": 78}
{"doc_id": "test/core/linter_test.py::test__linter__raises_malformed_noqa", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__raises_malformed_noqa", "text": "文件路径: test/core/linter_test.py\ndef test__linter__raises_malformed_noqa():\n    \"\"\"A badly formatted noqa gets raised as a parsing error.\"\"\"\n    lntr = Linter()\n    result = lntr.lint_string_wrapped(\"select 1 --noqa missing semicolon\")\n\n    with pytest.raises(SQLParseError):\n        result.check_tuples()\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__raises_malformed_noqa", "a", "badly", "formatted", "noqa", "gets", "raised", "as", "a", "parsing", "error", "lntr", "linter", "result", "lntr", "lint_string_wrapped", "select", "1", "noqa", "missing", "semicolon", "with", "pytest", "raises", "sqlparseerror", "result", "check_tuples"], "doc_len": 32}
{"doc_id": "test/core/linter_test.py::test__linter__empty_file", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__empty_file", "text": "文件路径: test/core/linter_test.py\ndef test__linter__empty_file():\n    \"\"\"Test linter behaves nicely with an empty string.\"\"\"\n    lntr = Linter()\n    # Make sure no exceptions raised and no violations found in empty file.\n    parsed = lntr.parse_string(\"\")\n    assert not parsed.violations\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__empty_file", "test", "linter", "behaves", "nicely", "with", "an", "empty", "string", "lntr", "linter", "make", "sure", "no", "exceptions", "raised", "and", "no", "violations", "found", "in", "empty", "file", "parsed", "lntr", "parse_string", "assert", "not", "parsed", "violations"], "doc_len": 35}
{"doc_id": "test/core/linter_test.py::test__linter__mask_templated_violations", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__mask_templated_violations", "text": "文件路径: test/core/linter_test.py\ndef test__linter__mask_templated_violations(ignore_templated_areas, check_tuples):\n    \"\"\"Test linter masks files properly around templated content.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"L006\",\n                \"ignore_templated_areas\": ignore_templated_areas,\n            }\n        )\n    )\n    linted = lntr.lint_path(path=\"test/fixtures/templater/jinja_h_macros/jinja.sql\")\n    assert linted.check_tuples() == check_tuples\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__mask_templated_violations", "ignore_templated_areas", "check_tuples", "test", "linter", "masks", "files", "properly", "around", "templated", "content", "lntr", "linter", "config", "fluffconfig", "overrides", "rules", "l006", "ignore_templated_areas", "ignore_templated_areas", "linted", "lntr", "lint_path", "path", "test", "fixtures", "templater", "jinja_h_macros", "jinja", "sql", "assert", "linted", "check_tuples", "check_tuples"], "doc_len": 39}
{"doc_id": "test/core/linter_test.py::test__linter__encoding", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__linter__encoding", "text": "文件路径: test/core/linter_test.py\ndef test__linter__encoding(fname, config_encoding, lexerror):\n    \"\"\"Test linter deals with files with different encoding.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"L001\",\n                \"encoding\": config_encoding,\n            }\n        )\n    )\n    result = lntr.lint_paths([fname])\n    assert lexerror == (SQLLexError in [type(v) for v in result.get_violations()])\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__linter__encoding", "fname", "config_encoding", "lexerror", "test", "linter", "deals", "with", "files", "with", "different", "encoding", "lntr", "linter", "config", "fluffconfig", "overrides", "rules", "l001", "encoding", "config_encoding", "result", "lntr", "lint_paths", "fname", "assert", "lexerror", "sqllexerror", "in", "type", "v", "for", "v", "in", "result", "get_violations"], "doc_len": 41}
{"doc_id": "test/core/linter_test.py::test_parse_noqa", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_parse_noqa", "text": "文件路径: test/core/linter_test.py\ndef test_parse_noqa(input, expected):\n    \"\"\"Test correct of \"noqa\" comments.\"\"\"\n    result = Linter.parse_noqa(input, 0)\n    if not isinstance(expected, type):\n        assert result == expected\n    else:\n        # With exceptions, just check the type, not the contents.\n        assert isinstance(result, expected)\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_parse_noqa", "input", "expected", "test", "correct", "of", "noqa", "comments", "result", "linter", "parse_noqa", "input", "0", "if", "not", "isinstance", "expected", "type", "assert", "result", "expected", "else", "with", "exceptions", "just", "check", "the", "type", "not", "the", "contents", "assert", "isinstance", "result", "expected"], "doc_len": 40}
{"doc_id": "test/core/linter_test.py::test_linted_file_ignore_masked_violations", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_linted_file_ignore_masked_violations", "text": "文件路径: test/core/linter_test.py\ndef test_linted_file_ignore_masked_violations(\n    noqa: dict, violations: List[SQLBaseError], expected\n):\n    \"\"\"Test that _ignore_masked_violations() correctly filters violations.\"\"\"\n    ignore_mask = [Linter.parse_noqa(**c) for c in noqa]\n    lf = linter.LintedFile(\n        path=\"\",\n        violations=violations,\n        time_dict={},\n        tree=None,\n        ignore_mask=ignore_mask,\n        templated_file=TemplatedFile.from_string(\"\"),\n        encoding=\"utf8\",\n    )\n    result = lf.ignore_masked_violations(violations, ignore_mask)\n    expected_violations = [v for i, v in enumerate(violations) if i in expected]\n    assert expected_violations == result\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_linted_file_ignore_masked_violations", "noqa", "dict", "violations", "list", "sqlbaseerror", "expected", "test", "that", "_ignore_masked_violations", "correctly", "filters", "violations", "ignore_mask", "linter", "parse_noqa", "c", "for", "c", "in", "noqa", "lf", "linter", "lintedfile", "path", "violations", "violations", "time_dict", "tree", "none", "ignore_mask", "ignore_mask", "templated_file", "templatedfile", "from_string", "encoding", "utf8", "result", "lf", "ignore_masked_violations", "violations", "ignore_mask", "expected_violations", "v", "for", "i", "v", "in", "enumerate", "violations", "if", "i", "in", "expected", "assert", "expected_violations", "result"], "doc_len": 62}
{"doc_id": "test/core/linter_test.py::test_linter_noqa", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_linter_noqa", "text": "文件路径: test/core/linter_test.py\ndef test_linter_noqa():\n    \"\"\"Test \"noqa\" feature at the higher \"Linter\" level.\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"rules\": \"L012\",\n            }\n        )\n    )\n    sql = \"\"\"\n    SELECT\n        col_a a,\n        col_b b, --noqa: disable=L012\n        col_c c,\n        col_d d, --noqa: enable=L012\n        col_e e,\n        col_f f,\n        col_g g,  --noqa\n        col_h h,\n        col_i i, --noqa:L012\n        col_j j,\n        col_k k, --noqa:L013\n        col_l l,\n        col_m m,\n        col_n n, --noqa: disable=all\n        col_o o,\n        col_p p --noqa: enable=all\n    FROM foo\n        \"\"\"\n    result = lntr.lint_string(sql)\n    violations = result.get_violations()\n    assert {3, 6, 7, 8, 10, 12, 13, 14, 15, 18} == {v.line_no for v in violations}\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_linter_noqa", "test", "noqa", "feature", "at", "the", "higher", "linter", "level", "lntr", "linter", "config", "fluffconfig", "overrides", "rules", "l012", "sql", "select", "col_a", "a", "col_b", "b", "noqa", "disable", "l012", "col_c", "c", "col_d", "d", "noqa", "enable", "l012", "col_e", "e", "col_f", "f", "col_g", "g", "noqa", "col_h", "h", "col_i", "i", "noqa", "l012", "col_j", "j", "col_k", "k", "noqa", "l013", "col_l", "l", "col_m", "m", "col_n", "n", "noqa", "disable", "all", "col_o", "o", "col_p", "p", "noqa", "enable", "all", "from", "foo", "result", "lntr", "lint_string", "sql", "violations", "result", "get_violations", "assert", "3", "6", "7", "8", "10", "12", "13", "14", "15", "18", "v", "line_no", "for", "v", "in", "violations"], "doc_len": 98}
{"doc_id": "test/core/linter_test.py::test_linter_noqa_with_templating", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_linter_noqa_with_templating", "text": "文件路径: test/core/linter_test.py\ndef test_linter_noqa_with_templating():\n    \"\"\"Similar to test_linter_noqa, but uses templating (Jinja).\"\"\"\n    lntr = Linter(\n        config=FluffConfig(\n            overrides={\n                \"templater\": \"jinja\",\n                \"rules\": \"L016\",\n            }\n        )\n    )\n    sql = \"\"\"\n    {%- set a_var = [\"1\", \"2\"] -%}\n    SELECT\n      this_is_just_a_very_long_line_for_demonstration_purposes_of_a_bug_involving_templated_sql_files, --noqa: L016\n      this_is_not_so_big\n    FROM\n      a_table\n        \"\"\"\n    result = lntr.lint_string(sql)\n    assert not result.get_violations()\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_linter_noqa_with_templating", "similar", "to", "test_linter_noqa", "but", "uses", "templating", "jinja", "lntr", "linter", "config", "fluffconfig", "overrides", "templater", "jinja", "rules", "l016", "sql", "set", "a_var", "1", "2", "select", "this_is_just_a_very_long_line_for_demonstration_purposes_of_a_bug_involving_templated_sql_files", "noqa", "l016", "this_is_not_so_big", "from", "a_table", "result", "lntr", "lint_string", "sql", "assert", "not", "result", "get_violations"], "doc_len": 42}
{"doc_id": "test/core/linter_test.py::test_delayed_exception", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test_delayed_exception", "text": "文件路径: test/core/linter_test.py\ndef test_delayed_exception():\n    \"\"\"Test that DelayedException stores and reraises a stored exception.\"\"\"\n    ve = ValueError()\n    de = runner.DelayedException(ve)\n    with pytest.raises(ValueError):\n        de.reraise()\n", "tokens": ["test", "core", "linter_test", "py", "def", "test_delayed_exception", "test", "that", "delayedexception", "stores", "and", "reraises", "a", "stored", "exception", "ve", "valueerror", "de", "runner", "delayedexception", "ve", "with", "pytest", "raises", "valueerror", "de", "reraise"], "doc_len": 27}
{"doc_id": "test/core/linter_test.py::test__attempt_to_change_templater_warning", "file_path": "test/core/linter_test.py", "class_name": null, "func_name": "test__attempt_to_change_templater_warning", "text": "文件路径: test/core/linter_test.py\ndef test__attempt_to_change_templater_warning(caplog):\n    \"\"\"Test warning if user tries to change templater in .sqlfluff file in subdirectory.\"\"\"\n    initial_config = FluffConfig(configs={\"core\": {\"templater\": \"jinja\"}})\n    lntr = Linter(config=initial_config)\n    updated_config = FluffConfig(configs={\"core\": {\"templater\": \"python\"}})\n    logger = logging.getLogger(\"sqlfluff\")\n    original_propagate_value = logger.propagate\n    try:\n        logger.propagate = True\n        with caplog.at_level(logging.WARNING, logger=\"sqlfluff.linter\"):\n            lntr.render_string(\n                in_str=\"select * from table\",\n                fname=\"test.sql\",\n                config=updated_config,\n                encoding=\"utf-8\",\n            )\n        assert \"Attempt to set templater to \" in caplog.text\n    finally:\n        logger.propagate = original_propagate_value\n", "tokens": ["test", "core", "linter_test", "py", "def", "test__attempt_to_change_templater_warning", "caplog", "test", "warning", "if", "user", "tries", "to", "change", "templater", "in", "sqlfluff", "file", "in", "subdirectory", "initial_config", "fluffconfig", "configs", "core", "templater", "jinja", "lntr", "linter", "config", "initial_config", "updated_config", "fluffconfig", "configs", "core", "templater", "python", "logger", "logging", "getlogger", "sqlfluff", "original_propagate_value", "logger", "propagate", "try", "logger", "propagate", "true", "with", "caplog", "at_level", "logging", "warning", "logger", "sqlfluff", "linter", "lntr", "render_string", "in_str", "select", "from", "table", "fname", "test", "sql", "config", "updated_config", "encoding", "utf", "8", "assert", "attempt", "to", "set", "templater", "to", "in", "caplog", "text", "finally", "logger", "propagate", "original_propagate_value"], "doc_len": 82}
{"doc_id": "test/core/plugin_test.py::test__plugin_manager_registers_example_plugin", "file_path": "test/core/plugin_test.py", "class_name": null, "func_name": "test__plugin_manager_registers_example_plugin", "text": "文件路径: test/core/plugin_test.py\ndef test__plugin_manager_registers_example_plugin():\n    \"\"\"Test that the example plugin is registered.\"\"\"\n    plugin_manager = get_plugin_manager()\n    # The plugin import order is non-deterministic\n    assert sorted(\n        [plugin_module.__name__ for plugin_module in plugin_manager.get_plugins()]\n    ) == [\n        \"example.rules\",\n        \"sqlfluff.core.plugin.lib\",\n    ]\n", "tokens": ["test", "core", "plugin_test", "py", "def", "test__plugin_manager_registers_example_plugin", "test", "that", "the", "example", "plugin", "is", "registered", "plugin_manager", "get_plugin_manager", "the", "plugin", "import", "order", "is", "non", "deterministic", "assert", "sorted", "plugin_module", "__name__", "for", "plugin_module", "in", "plugin_manager", "get_plugins", "example", "rules", "sqlfluff", "core", "plugin", "lib"], "doc_len": 37}
{"doc_id": "test/core/plugin_test.py::test__plugin_example_rules_returned", "file_path": "test/core/plugin_test.py", "class_name": null, "func_name": "test__plugin_example_rules_returned", "text": "文件路径: test/core/plugin_test.py\ndef test__plugin_example_rules_returned():\n    \"\"\"Test that the example rules from the plugin are returned.\"\"\"\n    plugin_manager = get_plugin_manager()\n    # The plugin import order is non-deterministic\n    assert \"Rule_Example_L001\" in [\n        rule.__name__ for rules in plugin_manager.hook.get_rules() for rule in rules\n    ]\n", "tokens": ["test", "core", "plugin_test", "py", "def", "test__plugin_example_rules_returned", "test", "that", "the", "example", "rules", "from", "the", "plugin", "are", "returned", "plugin_manager", "get_plugin_manager", "the", "plugin", "import", "order", "is", "non", "deterministic", "assert", "rule_example_l001", "in", "rule", "__name__", "for", "rules", "in", "plugin_manager", "hook", "get_rules", "for", "rule", "in", "rules"], "doc_len": 40}
{"doc_id": "test/core/plugin_test.py::test__plugin_default_config_read", "file_path": "test/core/plugin_test.py", "class_name": null, "func_name": "test__plugin_default_config_read", "text": "文件路径: test/core/plugin_test.py\ndef test__plugin_default_config_read():\n    \"\"\"Test that the example plugin default config is merged into FluffConfig.\"\"\"\n    fluff_config = FluffConfig()\n    # The plugin import order is non-deterministic\n    assert \"forbidden_columns\" in fluff_config._configs[\"rules\"][\"Example_L001\"]\n", "tokens": ["test", "core", "plugin_test", "py", "def", "test__plugin_default_config_read", "test", "that", "the", "example", "plugin", "default", "config", "is", "merged", "into", "fluffconfig", "fluff_config", "fluffconfig", "the", "plugin", "import", "order", "is", "non", "deterministic", "assert", "forbidden_columns", "in", "fluff_config", "_configs", "rules", "example_l001"], "doc_len": 33}
{"doc_id": "test/core/string_helpers_test.py::test__parser__helper_findall", "file_path": "test/core/string_helpers_test.py", "class_name": null, "func_name": "test__parser__helper_findall", "text": "文件路径: test/core/string_helpers_test.py\ndef test__parser__helper_findall(mainstr, substr, positions):\n    \"\"\"Test _findall.\"\"\"\n    assert list(findall(substr, mainstr)) == positions\n", "tokens": ["test", "core", "string_helpers_test", "py", "def", "test__parser__helper_findall", "mainstr", "substr", "positions", "test", "_findall", "assert", "list", "findall", "substr", "mainstr", "positions"], "doc_len": 17}
{"doc_id": "test/core/parser/conftest.py::fresh_ansi_dialect", "file_path": "test/core/parser/conftest.py", "class_name": null, "func_name": "fresh_ansi_dialect", "text": "文件路径: test/core/parser/conftest.py\ndef fresh_ansi_dialect():\n    \"\"\"Expand the ansi dialect for use.\"\"\"\n    return dialect_selector(\"ansi\")\n", "tokens": ["test", "core", "parser", "conftest", "py", "def", "fresh_ansi_dialect", "expand", "the", "ansi", "dialect", "for", "use", "return", "dialect_selector", "ansi"], "doc_len": 16}
{"doc_id": "test/core/parser/conftest.py::seg_list", "file_path": "test/core/parser/conftest.py", "class_name": null, "func_name": "seg_list", "text": "文件路径: test/core/parser/conftest.py\ndef seg_list(generate_test_segments):\n    \"\"\"A preset list of segments for testing.\n\n    Includes a templated segment for completeness.\n    \"\"\"\n    main_list = generate_test_segments([\"bar\", \" \\t \", \"foo\", \"baar\", \" \\t \"])\n    ts = TemplateSegment(\n        pos_marker=main_list[-1].get_end_point_marker(),\n        source_str=\"{# comment #}\",\n        block_type=\"comment\",\n    )\n    return main_list + (ts,)\n", "tokens": ["test", "core", "parser", "conftest", "py", "def", "seg_list", "generate_test_segments", "a", "preset", "list", "of", "segments", "for", "testing", "includes", "a", "templated", "segment", "for", "completeness", "main_list", "generate_test_segments", "bar", "t", "foo", "baar", "t", "ts", "templatesegment", "pos_marker", "main_list", "1", "get_end_point_marker", "source_str", "comment", "block_type", "comment", "return", "main_list", "ts"], "doc_len": 41}
{"doc_id": "test/core/parser/conftest.py::bracket_seg_list", "file_path": "test/core/parser/conftest.py", "class_name": null, "func_name": "bracket_seg_list", "text": "文件路径: test/core/parser/conftest.py\ndef bracket_seg_list(generate_test_segments):\n    \"\"\"Another preset list of segments for testing.\"\"\"\n    return generate_test_segments(\n        [\"bar\", \" \\t \", \"(\", \"foo\", \"    \", \")\", \"baar\", \" \\t \", \"foo\"]\n    )\n", "tokens": ["test", "core", "parser", "conftest", "py", "def", "bracket_seg_list", "generate_test_segments", "another", "preset", "list", "of", "segments", "for", "testing", "return", "generate_test_segments", "bar", "t", "foo", "baar", "t", "foo"], "doc_len": 23}
{"doc_id": "test/core/parser/grammar_test.py::make_result_tuple", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "make_result_tuple", "text": "文件路径: test/core/parser/grammar_test.py\ndef make_result_tuple(result_slice, matcher_keywords, seg_list):\n    \"\"\"Make a comparison tuple for test matching.\"\"\"\n    # No result slice means no match.\n    if not result_slice:\n        return ()\n\n    return tuple(\n        KeywordSegment(elem.raw, pos_marker=elem.pos_marker)\n        if elem.raw in matcher_keywords\n        else elem\n        for elem in seg_list[result_slice]\n    )\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "make_result_tuple", "result_slice", "matcher_keywords", "seg_list", "make", "a", "comparison", "tuple", "for", "test", "matching", "no", "result", "slice", "means", "no", "match", "if", "not", "result_slice", "return", "return", "tuple", "keywordsegment", "elem", "raw", "pos_marker", "elem", "pos_marker", "if", "elem", "raw", "in", "matcher_keywords", "else", "elem", "for", "elem", "in", "seg_list", "result_slice"], "doc_len": 47}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__longest_trimmed_match__basic", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__longest_trimmed_match__basic", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__longest_trimmed_match__basic(\n    seg_list, seg_list_slice, matcher_keywords, trim_noncode, result_slice\n):\n    \"\"\"Test the _longest_trimmed_match method of the BaseGrammar.\"\"\"\n    # Make the matcher keywords\n    matchers = [StringParser(keyword, KeywordSegment) for keyword in matcher_keywords]\n\n    with RootParseContext(dialect=None) as ctx:\n        m, _ = BaseGrammar._longest_trimmed_match(\n            seg_list[seg_list_slice], matchers, ctx, trim_noncode=trim_noncode\n        )\n\n    # Make the check tuple\n    expected_result = make_result_tuple(\n        result_slice=result_slice,\n        matcher_keywords=matcher_keywords,\n        seg_list=seg_list,\n    )\n\n    assert m.matched_segments == expected_result\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__longest_trimmed_match__basic", "seg_list", "seg_list_slice", "matcher_keywords", "trim_noncode", "result_slice", "test", "the", "_longest_trimmed_match", "method", "of", "the", "basegrammar", "make", "the", "matcher", "keywords", "matchers", "stringparser", "keyword", "keywordsegment", "for", "keyword", "in", "matcher_keywords", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "m", "_", "basegrammar", "_longest_trimmed_match", "seg_list", "seg_list_slice", "matchers", "ctx", "trim_noncode", "trim_noncode", "make", "the", "check", "tuple", "expected_result", "make_result_tuple", "result_slice", "result_slice", "matcher_keywords", "matcher_keywords", "seg_list", "seg_list", "assert", "m", "matched_segments", "expected_result"], "doc_len": 63}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__longest_trimmed_match__adv", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__longest_trimmed_match__adv", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__longest_trimmed_match__adv(seg_list, caplog):\n    \"\"\"Test the _longest_trimmed_match method of the BaseGrammar.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    matchers = [\n        bs,\n        fs,\n        Sequence(bs, fs),  # This should be the winner.\n        OneOf(bs, fs),\n        Sequence(bs, fs),  # Another to check we return the first\n    ]\n    with RootParseContext(dialect=None) as ctx:\n        # Matching the first element of the list\n        with caplog.at_level(logging.DEBUG, logger=\"sqluff.parser\"):\n            match, matcher = BaseGrammar._longest_trimmed_match(seg_list, matchers, ctx)\n    # Check we got a match\n    assert match\n    # Check we got the right one.\n    assert matcher is matchers[2]\n    # And it matched the first three segments\n    assert len(match) == 3\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__longest_trimmed_match__adv", "seg_list", "caplog", "test", "the", "_longest_trimmed_match", "method", "of", "the", "basegrammar", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "matchers", "bs", "fs", "sequence", "bs", "fs", "this", "should", "be", "the", "winner", "oneof", "bs", "fs", "sequence", "bs", "fs", "another", "to", "check", "we", "return", "the", "first", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "matching", "the", "first", "element", "of", "the", "list", "with", "caplog", "at_level", "logging", "debug", "logger", "sqluff", "parser", "match", "matcher", "basegrammar", "_longest_trimmed_match", "seg_list", "matchers", "ctx", "check", "we", "got", "a", "match", "assert", "match", "check", "we", "got", "the", "right", "one", "assert", "matcher", "is", "matchers", "2", "and", "it", "matched", "the", "first", "three", "segments", "assert", "len", "match", "3"], "doc_len": 105}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__look_ahead_match", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__look_ahead_match", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__look_ahead_match(\n    seg_list_slice,\n    matcher_keywords,\n    result_slice,\n    winning_matcher,\n    pre_match_slice,\n    seg_list,\n):\n    \"\"\"Test the _look_ahead_match method of the BaseGrammar.\"\"\"\n    # Make the matcher keywords\n    matchers = [StringParser(keyword, KeywordSegment) for keyword in matcher_keywords]\n    # Fetch the matching keyword from above by index\n    winning_matcher = matchers[matcher_keywords.index(winning_matcher)]\n\n    with RootParseContext(dialect=None) as ctx:\n        m = BaseGrammar._look_ahead_match(\n            seg_list[seg_list_slice],\n            matchers,\n            ctx,\n        )\n\n    # Check structure of the response.\n    assert isinstance(m, tuple)\n    assert len(m) == 3\n    # Unpack\n    result_pre_match, result_match, result_matcher = m\n\n    # Check the right matcher won\n    assert result_matcher == winning_matcher\n\n    # Make check tuple for the pre-match section\n    if pre_match_slice:\n        pre_match_slice = seg_list[pre_match_slice]\n    else:\n        pre_match_slice = ()\n    assert result_pre_match == pre_match_slice\n\n    # Make the check tuple\n    expected_result = make_result_tuple(\n        result_slice=result_slice,\n        matcher_keywords=matcher_keywords,\n        seg_list=seg_list,\n    )\n    assert result_match.matched_segments == expected_result\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__look_ahead_match", "seg_list_slice", "matcher_keywords", "result_slice", "winning_matcher", "pre_match_slice", "seg_list", "test", "the", "_look_ahead_match", "method", "of", "the", "basegrammar", "make", "the", "matcher", "keywords", "matchers", "stringparser", "keyword", "keywordsegment", "for", "keyword", "in", "matcher_keywords", "fetch", "the", "matching", "keyword", "from", "above", "by", "index", "winning_matcher", "matchers", "matcher_keywords", "index", "winning_matcher", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "m", "basegrammar", "_look_ahead_match", "seg_list", "seg_list_slice", "matchers", "ctx", "check", "structure", "of", "the", "response", "assert", "isinstance", "m", "tuple", "assert", "len", "m", "3", "unpack", "result_pre_match", "result_match", "result_matcher", "m", "check", "the", "right", "matcher", "won", "assert", "result_matcher", "winning_matcher", "make", "check", "tuple", "for", "the", "pre", "match", "section", "if", "pre_match_slice", "pre_match_slice", "seg_list", "pre_match_slice", "else", "pre_match_slice", "assert", "result_pre_match", "pre_match_slice", "make", "the", "check", "tuple", "expected_result", "make_result_tuple", "result_slice", "result_slice", "matcher_keywords", "matcher_keywords", "seg_list", "seg_list", "assert", "result_match", "matched_segments", "expected_result"], "doc_len": 118}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__ephemeral_segment", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__ephemeral_segment", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__ephemeral_segment(seg_list):\n    \"\"\"Test the ephemeral features on BaseGrammar.\n\n    Normally you cant call .match() on a BaseGrammar, but\n    if things are set up right, then it should be possible\n    in the case that the ephemeral_name is set.\n\n    This indirectly tests the allow_ephemeral decorator.\n    \"\"\"\n    g = BaseGrammar(ephemeral_name=\"TestGrammar\")\n\n    with RootParseContext(dialect=None) as ctx:\n        m = g.match(seg_list, ctx)\n        # Check we get an ephemeral segment\n        assert isinstance(m.matched_segments[0], EphemeralSegment)\n        assert len(m.matched_segments) == 1\n        chkpoint = m.matched_segments[0]\n        # Check it's got the same content.\n        assert chkpoint.segments == seg_list\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__ephemeral_segment", "seg_list", "test", "the", "ephemeral", "features", "on", "basegrammar", "normally", "you", "cant", "call", "match", "on", "a", "basegrammar", "but", "if", "things", "are", "set", "up", "right", "then", "it", "should", "be", "possible", "in", "the", "case", "that", "the", "ephemeral_name", "is", "set", "this", "indirectly", "tests", "the", "allow_ephemeral", "decorator", "g", "basegrammar", "ephemeral_name", "testgrammar", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "m", "g", "match", "seg_list", "ctx", "check", "we", "get", "an", "ephemeral", "segment", "assert", "isinstance", "m", "matched_segments", "0", "ephemeralsegment", "assert", "len", "m", "matched_segments", "1", "chkpoint", "m", "matched_segments", "0", "check", "it", "s", "got", "the", "same", "content", "assert", "chkpoint", "segments", "seg_list"], "doc_len": 95}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__oneof__ephemeral_segment", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__oneof__ephemeral_segment", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__oneof__ephemeral_segment(seg_list):\n    \"\"\"A realistic full test of ephemeral segments.\"\"\"\n\n    class TestSegment(BaseSegment):\n        match_grammar = OneOf(\n            StringParser(\"bar\", KeywordSegment), ephemeral_name=\"foofoo\"\n        )\n\n    with RootParseContext(dialect=None) as ctx:\n        m = TestSegment.match(seg_list[:1], ctx)\n        # Make sure we've matched\n        assert m\n        seg = m.matched_segments[0]\n        assert isinstance(seg, TestSegment)\n        # Check the content is ephemeral\n        assert isinstance(seg.segments[0], EphemeralSegment)\n        assert seg.segments[0].name == \"foofoo\"\n        # Expand the segment\n        res = seg.parse(ctx)\n        # Check we still have a test segment\n        assert isinstance(res, TestSegment)\n        # But that it contains a keyword segment now\n        assert isinstance(res.segments[0], KeywordSegment)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__oneof__ephemeral_segment", "seg_list", "a", "realistic", "full", "test", "of", "ephemeral", "segments", "class", "testsegment", "basesegment", "match_grammar", "oneof", "stringparser", "bar", "keywordsegment", "ephemeral_name", "foofoo", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "m", "testsegment", "match", "seg_list", "1", "ctx", "make", "sure", "we", "ve", "matched", "assert", "m", "seg", "m", "matched_segments", "0", "assert", "isinstance", "seg", "testsegment", "check", "the", "content", "is", "ephemeral", "assert", "isinstance", "seg", "segments", "0", "ephemeralsegment", "assert", "seg", "segments", "0", "name", "foofoo", "expand", "the", "segment", "res", "seg", "parse", "ctx", "check", "we", "still", "have", "a", "test", "segment", "assert", "isinstance", "res", "testsegment", "but", "that", "it", "contains", "a", "keyword", "segment", "now", "assert", "isinstance", "res", "segments", "0", "keywordsegment"], "doc_len": 101}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__bracket_sensitive_look_ahead_match", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__bracket_sensitive_look_ahead_match", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__bracket_sensitive_look_ahead_match(\n    bracket_seg_list, fresh_ansi_dialect\n):\n    \"\"\"Test the _bracket_sensitive_look_ahead_match method of the BaseGrammar.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    # We need a dialect here to do bracket matching\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        # Basic version, we should find bar first\n        pre_section, match, matcher = BaseGrammar._bracket_sensitive_look_ahead_match(\n            bracket_seg_list, [fs, bs], ctx\n        )\n        assert pre_section == ()\n        assert matcher == bs\n        # NB the middle element is a match object\n        assert match.matched_segments == (\n            KeywordSegment(\"bar\", bracket_seg_list[0].pos_marker),\n        )\n\n        # Look ahead for foo, we should find the one AFTER the brackets, not the\n        # on IN the brackets.\n        pre_section, match, matcher = BaseGrammar._bracket_sensitive_look_ahead_match(\n            bracket_seg_list, [fs], ctx\n        )\n        # NB: The bracket segments will have been mutated, so we can't directly compare.\n        # Make sure we've got a bracketed section in there.\n        assert len(pre_section) == 5\n        assert pre_section[2].is_type(\"bracketed\")\n        assert len(pre_section[2].segments) == 4\n        assert matcher == fs\n        # We shouldn't match the whitespace with the keyword\n        assert match.matched_segments == (\n            KeywordSegment(\"foo\", bracket_seg_list[8].pos_marker),\n        )\n        # Check that the unmatched segments are nothing.\n        assert not match.unmatched_segments\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__bracket_sensitive_look_ahead_match", "bracket_seg_list", "fresh_ansi_dialect", "test", "the", "_bracket_sensitive_look_ahead_match", "method", "of", "the", "basegrammar", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "we", "need", "a", "dialect", "here", "to", "do", "bracket", "matching", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "basic", "version", "we", "should", "find", "bar", "first", "pre_section", "match", "matcher", "basegrammar", "_bracket_sensitive_look_ahead_match", "bracket_seg_list", "fs", "bs", "ctx", "assert", "pre_section", "assert", "matcher", "bs", "nb", "the", "middle", "element", "is", "a", "match", "object", "assert", "match", "matched_segments", "keywordsegment", "bar", "bracket_seg_list", "0", "pos_marker", "look", "ahead", "for", "foo", "we", "should", "find", "the", "one", "after", "the", "brackets", "not", "the", "on", "in", "the", "brackets", "pre_section", "match", "matcher", "basegrammar", "_bracket_sensitive_look_ahead_match", "bracket_seg_list", "fs", "ctx", "nb", "the", "bracket", "segments", "will", "have", "been", "mutated", "so", "we", "can", "t", "directly", "compare", "make", "sure", "we", "ve", "got", "a", "bracketed", "section", "in", "there", "assert", "len", "pre_section", "5", "assert", "pre_section", "2", "is_type", "bracketed", "assert", "len", "pre_section", "2", "segments", "4", "assert", "matcher", "fs", "we", "shouldn", "t", "match", "the", "whitespace", "with", "the", "keyword", "assert", "match", "matched_segments", "keywordsegment", "foo", "bracket_seg_list", "8", "pos_marker", "check", "that", "the", "unmatched", "segments", "are", "nothing", "assert", "not", "match", "unmatched_segments"], "doc_len": 172}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__bracket_fail_with_open_paren_close_square_mismatch", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__bracket_fail_with_open_paren_close_square_mismatch", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__bracket_fail_with_open_paren_close_square_mismatch(\n    generate_test_segments, fresh_ansi_dialect\n):\n    \"\"\"Test _bracket_sensitive_look_ahead_match failure case.\n\n    Should fail when the type of a close bracket doesn't match the type of the\n    corresponding open bracket, but both are \"definite\" brackets.\n    \"\"\"\n    fs = StringParser(\"foo\", KeywordSegment)\n    # We need a dialect here to do bracket matching\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        # Basic version, we should find bar first\n        with pytest.raises(SQLParseError) as sql_parse_error:\n            BaseGrammar._bracket_sensitive_look_ahead_match(\n                generate_test_segments(\n                    [\n                        \"select\",\n                        \" \",\n                        \"*\",\n                        \" \",\n                        \"from\",\n                        \"(\",\n                        \"foo\",\n                        \"]\",  # Bracket types don't match (parens vs square)\n                    ]\n                ),\n                [fs],\n                ctx,\n            )\n        assert sql_parse_error.match(\"Found unexpected end bracket\")\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__bracket_fail_with_open_paren_close_square_mismatch", "generate_test_segments", "fresh_ansi_dialect", "test", "_bracket_sensitive_look_ahead_match", "failure", "case", "should", "fail", "when", "the", "type", "of", "a", "close", "bracket", "doesn", "t", "match", "the", "type", "of", "the", "corresponding", "open", "bracket", "but", "both", "are", "definite", "brackets", "fs", "stringparser", "foo", "keywordsegment", "we", "need", "a", "dialect", "here", "to", "do", "bracket", "matching", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "basic", "version", "we", "should", "find", "bar", "first", "with", "pytest", "raises", "sqlparseerror", "as", "sql_parse_error", "basegrammar", "_bracket_sensitive_look_ahead_match", "generate_test_segments", "select", "from", "foo", "bracket", "types", "don", "t", "match", "parens", "vs", "square", "fs", "ctx", "assert", "sql_parse_error", "match", "found", "unexpected", "end", "bracket"], "doc_len": 92}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__base__bracket_fail_with_unexpected_end_bracket", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__base__bracket_fail_with_unexpected_end_bracket", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__base__bracket_fail_with_unexpected_end_bracket(\n    generate_test_segments, fresh_ansi_dialect\n):\n    \"\"\"Test _bracket_sensitive_look_ahead_match edge case.\n\n    Should fail gracefully and stop matching if we find a trailing unmatched.\n    \"\"\"\n    fs = StringParser(\"foo\", KeywordSegment)\n    # We need a dialect here to do bracket matching\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        _, match, _ = BaseGrammar._bracket_sensitive_look_ahead_match(\n            generate_test_segments(\n                [\n                    \"bar\",\n                    \"(\",  # This bracket pair should be mutated\n                    \")\",\n                    \" \",\n                    \")\",  # This is the unmatched bracket\n                    \" \",\n                    \"foo\",\n                ]\n            ),\n            [fs],\n            ctx,\n        )\n        # Check we don't match (even though there's a foo at the end)\n        assert not match\n        # Check the first bracket pair have been mutated.\n        segs = match.unmatched_segments\n        assert segs[1].is_type(\"bracketed\")\n        assert segs[1].raw == \"()\"\n        assert len(segs[1].segments) == 2\n        # Check the trailing foo hasn't been mutated\n        assert segs[5].raw == \"foo\"\n        assert not isinstance(segs[5], KeywordSegment)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__base__bracket_fail_with_unexpected_end_bracket", "generate_test_segments", "fresh_ansi_dialect", "test", "_bracket_sensitive_look_ahead_match", "edge", "case", "should", "fail", "gracefully", "and", "stop", "matching", "if", "we", "find", "a", "trailing", "unmatched", "fs", "stringparser", "foo", "keywordsegment", "we", "need", "a", "dialect", "here", "to", "do", "bracket", "matching", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "_", "match", "_", "basegrammar", "_bracket_sensitive_look_ahead_match", "generate_test_segments", "bar", "this", "bracket", "pair", "should", "be", "mutated", "this", "is", "the", "unmatched", "bracket", "foo", "fs", "ctx", "check", "we", "don", "t", "match", "even", "though", "there", "s", "a", "foo", "at", "the", "end", "assert", "not", "match", "check", "the", "first", "bracket", "pair", "have", "been", "mutated", "segs", "match", "unmatched_segments", "assert", "segs", "1", "is_type", "bracketed", "assert", "segs", "1", "raw", "assert", "len", "segs", "1", "segments", "2", "check", "the", "trailing", "foo", "hasn", "t", "been", "mutated", "assert", "segs", "5", "raw", "foo", "assert", "not", "isinstance", "segs", "5", "keywordsegment"], "doc_len": 127}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__ref_eq", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__ref_eq", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__ref_eq():\n    \"\"\"Test equality of Ref Grammars.\"\"\"\n    r1 = Ref(\"foo\")\n    r2 = Ref(\"foo\")\n    assert r1 is not r2\n    assert r1 == r2\n    check_list = [1, 2, r2, 3]\n    # Check we can find it in lists\n    assert r1 in check_list\n    # Check we can get it's position\n    assert check_list.index(r1) == 2\n    # Check we can remove it from a list\n    check_list.remove(r1)\n    assert r1 not in check_list\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__ref_eq", "test", "equality", "of", "ref", "grammars", "r1", "ref", "foo", "r2", "ref", "foo", "assert", "r1", "is", "not", "r2", "assert", "r1", "r2", "check_list", "1", "2", "r2", "3", "check", "we", "can", "find", "it", "in", "lists", "assert", "r1", "in", "check_list", "check", "we", "can", "get", "it", "s", "position", "assert", "check_list", "index", "r1", "2", "check", "we", "can", "remove", "it", "from", "a", "list", "check_list", "remove", "r1", "assert", "r1", "not", "in", "check_list"], "doc_len": 70}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar__oneof__copy", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar__oneof__copy", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar__oneof__copy():\n    \"\"\"Test grammar copying.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g1 = OneOf(fs, bs)\n    # Check copy\n    g2 = g1.copy()\n    assert g1 == g2\n    assert g1 is not g2\n    # Check copy insert (start)\n    g3 = g1.copy(insert=[bs], at=0)\n    assert g3 == OneOf(bs, fs, bs)\n    # Check copy insert (mid)\n    g4 = g1.copy(insert=[bs], at=1)\n    assert g4 == OneOf(fs, bs, bs)\n    # Check copy insert (end)\n    g5 = g1.copy(insert=[bs], at=-1)\n    assert g5 == OneOf(fs, bs, bs)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar__oneof__copy", "test", "grammar", "copying", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g1", "oneof", "fs", "bs", "check", "copy", "g2", "g1", "copy", "assert", "g1", "g2", "assert", "g1", "is", "not", "g2", "check", "copy", "insert", "start", "g3", "g1", "copy", "insert", "bs", "at", "0", "assert", "g3", "oneof", "bs", "fs", "bs", "check", "copy", "insert", "mid", "g4", "g1", "copy", "insert", "bs", "at", "1", "assert", "g4", "oneof", "fs", "bs", "bs", "check", "copy", "insert", "end", "g5", "g1", "copy", "insert", "bs", "at", "1", "assert", "g5", "oneof", "fs", "bs", "bs"], "doc_len": 86}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_oneof", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_oneof", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_oneof(seg_list, allow_gaps):\n    \"\"\"Test the OneOf grammar.\n\n    NB: Should behave the same regardless of code_only.\n\n    \"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = OneOf(fs, bs, allow_gaps=allow_gaps)\n    with RootParseContext(dialect=None) as ctx:\n        # Check directly\n        assert g.match(seg_list, parse_context=ctx).matched_segments == (\n            KeywordSegment(\"bar\", seg_list[0].pos_marker),\n        )\n        # Check with a bit of whitespace\n        assert not g.match(seg_list[1:], parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_oneof", "seg_list", "allow_gaps", "test", "the", "oneof", "grammar", "nb", "should", "behave", "the", "same", "regardless", "of", "code_only", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g", "oneof", "fs", "bs", "allow_gaps", "allow_gaps", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "check", "directly", "assert", "g", "match", "seg_list", "parse_context", "ctx", "matched_segments", "keywordsegment", "bar", "seg_list", "0", "pos_marker", "check", "with", "a", "bit", "of", "whitespace", "assert", "not", "g", "match", "seg_list", "1", "parse_context", "ctx"], "doc_len": 69}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_oneof_templated", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_oneof_templated", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_oneof_templated(seg_list):\n    \"\"\"Test the OneOf grammar.\n\n    NB: Should behave the same regardless of code_only.\n\n    \"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = OneOf(fs, bs)\n    with RootParseContext(dialect=None) as ctx:\n        # This shouldn't match, but it *ALSO* shouldn't raise an exception.\n        # https://github.com/sqlfluff/sqlfluff/issues/780\n        assert not g.match(seg_list[5:], parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_oneof_templated", "seg_list", "test", "the", "oneof", "grammar", "nb", "should", "behave", "the", "same", "regardless", "of", "code_only", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g", "oneof", "fs", "bs", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "this", "shouldn", "t", "match", "but", "it", "also", "shouldn", "t", "raise", "an", "exception", "https", "github", "com", "sqlfluff", "sqlfluff", "issues", "780", "assert", "not", "g", "match", "seg_list", "5", "parse_context", "ctx"], "doc_len": 65}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_oneof_exclude", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_oneof_exclude", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_oneof_exclude(seg_list):\n    \"\"\"Test the OneOf grammar exclude option.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = OneOf(bs, exclude=Sequence(bs, fs))\n    with RootParseContext(dialect=None) as ctx:\n        # Just against the first alone\n        assert g.match(seg_list[:1], parse_context=ctx)\n        # Now with the bit to exclude included\n        assert not g.match(seg_list, parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_oneof_exclude", "seg_list", "test", "the", "oneof", "grammar", "exclude", "option", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g", "oneof", "bs", "exclude", "sequence", "bs", "fs", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "just", "against", "the", "first", "alone", "assert", "g", "match", "seg_list", "1", "parse_context", "ctx", "now", "with", "the", "bit", "to", "exclude", "included", "assert", "not", "g", "match", "seg_list", "parse_context", "ctx"], "doc_len": 61}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_oneof_take_longest_match", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_oneof_take_longest_match", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_oneof_take_longest_match(seg_list):\n    \"\"\"Test that the OneOf grammar takes the longest match.\"\"\"\n    fooRegex = RegexParser(r\"fo{2}\", KeywordSegment)\n    baar = StringParser(\"baar\", KeywordSegment)\n    foo = StringParser(\"foo\", KeywordSegment)\n    fooBaar = Sequence(\n        foo,\n        baar,\n    )\n\n    # Even if fooRegex comes first, fooBaar\n    # is a longer match and should be taken\n    g = OneOf(fooRegex, fooBaar)\n    with RootParseContext(dialect=None) as ctx:\n        assert fooRegex.match(seg_list[2:], parse_context=ctx).matched_segments == (\n            KeywordSegment(\"foo\", seg_list[2].pos_marker),\n        )\n        assert g.match(seg_list[2:], parse_context=ctx).matched_segments == (\n            KeywordSegment(\"foo\", seg_list[2].pos_marker),\n            KeywordSegment(\"baar\", seg_list[3].pos_marker),\n        )\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_oneof_take_longest_match", "seg_list", "test", "that", "the", "oneof", "grammar", "takes", "the", "longest", "match", "fooregex", "regexparser", "r", "fo", "2", "keywordsegment", "baar", "stringparser", "baar", "keywordsegment", "foo", "stringparser", "foo", "keywordsegment", "foobaar", "sequence", "foo", "baar", "even", "if", "fooregex", "comes", "first", "foobaar", "is", "a", "longer", "match", "and", "should", "be", "taken", "g", "oneof", "fooregex", "foobaar", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "assert", "fooregex", "match", "seg_list", "2", "parse_context", "ctx", "matched_segments", "keywordsegment", "foo", "seg_list", "2", "pos_marker", "assert", "g", "match", "seg_list", "2", "parse_context", "ctx", "matched_segments", "keywordsegment", "foo", "seg_list", "2", "pos_marker", "keywordsegment", "baar", "seg_list", "3", "pos_marker"], "doc_len": 90}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_oneof_take_first", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_oneof_take_first", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_oneof_take_first(seg_list):\n    \"\"\"Test that the OneOf grammar takes first match in case they are of same length.\"\"\"\n    fooRegex = RegexParser(r\"fo{2}\", KeywordSegment)\n    foo = StringParser(\"foo\", KeywordSegment)\n\n    # Both segments would match \"foo\"\n    # so we test that order matters\n    g1 = OneOf(fooRegex, foo)\n    g2 = OneOf(foo, fooRegex)\n    with RootParseContext(dialect=None) as ctx:\n        assert g1.match(seg_list[2:], parse_context=ctx).matched_segments == (\n            KeywordSegment(\"foo\", seg_list[2].pos_marker),\n        )\n        assert g2.match(seg_list[2:], parse_context=ctx).matched_segments == (\n            KeywordSegment(\"foo\", seg_list[2].pos_marker),\n        )\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_oneof_take_first", "seg_list", "test", "that", "the", "oneof", "grammar", "takes", "first", "match", "in", "case", "they", "are", "of", "same", "length", "fooregex", "regexparser", "r", "fo", "2", "keywordsegment", "foo", "stringparser", "foo", "keywordsegment", "both", "segments", "would", "match", "foo", "so", "we", "test", "that", "order", "matters", "g1", "oneof", "fooregex", "foo", "g2", "oneof", "foo", "fooregex", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "assert", "g1", "match", "seg_list", "2", "parse_context", "ctx", "matched_segments", "keywordsegment", "foo", "seg_list", "2", "pos_marker", "assert", "g2", "match", "seg_list", "2", "parse_context", "ctx", "matched_segments", "keywordsegment", "foo", "seg_list", "2", "pos_marker"], "doc_len": 84}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_startswith_a", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_startswith_a", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_startswith_a(\n    keyword, match_truthy, seg_list, fresh_ansi_dialect, caplog\n):\n    \"\"\"Test the StartsWith grammar simply.\"\"\"\n    Keyword = StringParser(keyword, KeywordSegment)\n    grammar = StartsWith(Keyword)\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            m = grammar.match(seg_list, parse_context=ctx)\n            assert bool(m) is match_truthy\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_startswith_a", "keyword", "match_truthy", "seg_list", "fresh_ansi_dialect", "caplog", "test", "the", "startswith", "grammar", "simply", "keyword", "stringparser", "keyword", "keywordsegment", "grammar", "startswith", "keyword", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "m", "grammar", "match", "seg_list", "parse_context", "ctx", "assert", "bool", "m", "is", "match_truthy"], "doc_len": 49}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_startswith_b", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_startswith_b", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_startswith_b(\n    include_terminator, match_length, seg_list, fresh_ansi_dialect, caplog\n):\n    \"\"\"Test the StartsWith grammar with a terminator (included & exluded).\"\"\"\n    baar = StringParser(\"baar\", KeywordSegment)\n    bar = StringParser(\"bar\", KeywordSegment)\n    grammar = StartsWith(bar, terminator=baar, include_terminator=include_terminator)\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            m = grammar.match(seg_list, parse_context=ctx)\n            assert len(m) == match_length\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_startswith_b", "include_terminator", "match_length", "seg_list", "fresh_ansi_dialect", "caplog", "test", "the", "startswith", "grammar", "with", "a", "terminator", "included", "exluded", "baar", "stringparser", "baar", "keywordsegment", "bar", "stringparser", "bar", "keywordsegment", "grammar", "startswith", "bar", "terminator", "baar", "include_terminator", "include_terminator", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "m", "grammar", "match", "seg_list", "parse_context", "ctx", "assert", "len", "m", "match_length"], "doc_len": 60}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_sequence", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_sequence", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_sequence(seg_list, caplog):\n    \"\"\"Test the Sequence grammar.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = Sequence(bs, fs)\n    gc = Sequence(bs, fs, allow_gaps=False)\n    with RootParseContext(dialect=None) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            # Should be able to match the list using the normal matcher\n            logging.info(\"#### TEST 1\")\n            m = g.match(seg_list, parse_context=ctx)\n            assert m\n            assert len(m) == 3\n            assert m.matched_segments == (\n                KeywordSegment(\"bar\", seg_list[0].pos_marker),\n                seg_list[1],  # This will be the whitespace segment\n                KeywordSegment(\"foo\", seg_list[2].pos_marker),\n            )\n            # Shouldn't with the allow_gaps matcher\n            logging.info(\"#### TEST 2\")\n            assert not gc.match(seg_list, parse_context=ctx)\n            # Shouldn't match even on the normal one if we don't start at the beginning\n            logging.info(\"#### TEST 2\")\n            assert not g.match(seg_list[1:], parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_sequence", "seg_list", "caplog", "test", "the", "sequence", "grammar", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g", "sequence", "bs", "fs", "gc", "sequence", "bs", "fs", "allow_gaps", "false", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "should", "be", "able", "to", "match", "the", "list", "using", "the", "normal", "matcher", "logging", "info", "test", "1", "m", "g", "match", "seg_list", "parse_context", "ctx", "assert", "m", "assert", "len", "m", "3", "assert", "m", "matched_segments", "keywordsegment", "bar", "seg_list", "0", "pos_marker", "seg_list", "1", "this", "will", "be", "the", "whitespace", "segment", "keywordsegment", "foo", "seg_list", "2", "pos_marker", "shouldn", "t", "with", "the", "allow_gaps", "matcher", "logging", "info", "test", "2", "assert", "not", "gc", "match", "seg_list", "parse_context", "ctx", "shouldn", "t", "match", "even", "on", "the", "normal", "one", "if", "we", "don", "t", "start", "at", "the", "beginning", "logging", "info", "test", "2", "assert", "not", "g", "match", "seg_list", "1", "parse_context", "ctx"], "doc_len": 138}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_sequence_nested", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_sequence_nested", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_sequence_nested(seg_list, caplog):\n    \"\"\"Test the Sequence grammar when nested.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    bas = StringParser(\"baar\", KeywordSegment)\n    g = Sequence(Sequence(bs, fs), bas)\n    with RootParseContext(dialect=None) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            # Matching the start of the list shouldn't work\n            logging.info(\"#### TEST 1\")\n            assert not g.match(seg_list[:2], parse_context=ctx)\n            # Matching the whole list should, and the result should be flat\n            logging.info(\"#### TEST 2\")\n            assert g.match(seg_list, parse_context=ctx).matched_segments == (\n                KeywordSegment(\"bar\", seg_list[0].pos_marker),\n                seg_list[1],  # This will be the whitespace segment\n                KeywordSegment(\"foo\", seg_list[2].pos_marker),\n                KeywordSegment(\"baar\", seg_list[3].pos_marker)\n                # NB: No whitespace at the end, this shouldn't be consumed.\n            )\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_sequence_nested", "seg_list", "caplog", "test", "the", "sequence", "grammar", "when", "nested", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "bas", "stringparser", "baar", "keywordsegment", "g", "sequence", "sequence", "bs", "fs", "bas", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "matching", "the", "start", "of", "the", "list", "shouldn", "t", "work", "logging", "info", "test", "1", "assert", "not", "g", "match", "seg_list", "2", "parse_context", "ctx", "matching", "the", "whole", "list", "should", "and", "the", "result", "should", "be", "flat", "logging", "info", "test", "2", "assert", "g", "match", "seg_list", "parse_context", "ctx", "matched_segments", "keywordsegment", "bar", "seg_list", "0", "pos_marker", "seg_list", "1", "this", "will", "be", "the", "whitespace", "segment", "keywordsegment", "foo", "seg_list", "2", "pos_marker", "keywordsegment", "baar", "seg_list", "3", "pos_marker", "nb", "no", "whitespace", "at", "the", "end", "this", "shouldn", "t", "be", "consumed"], "doc_len": 124}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_sequence_indent", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_sequence_indent", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_sequence_indent(seg_list, caplog):\n    \"\"\"Test the Sequence grammar with indents.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = Sequence(Indent, bs, fs)\n    with RootParseContext(dialect=None) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            m = g.match(seg_list, parse_context=ctx)\n            assert m\n            # check we get an indent.\n            assert isinstance(m.matched_segments[0], Indent)\n            assert isinstance(m.matched_segments[1], KeywordSegment)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_sequence_indent", "seg_list", "caplog", "test", "the", "sequence", "grammar", "with", "indents", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "g", "sequence", "indent", "bs", "fs", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "m", "g", "match", "seg_list", "parse_context", "ctx", "assert", "m", "check", "we", "get", "an", "indent", "assert", "isinstance", "m", "matched_segments", "0", "indent", "assert", "isinstance", "m", "matched_segments", "1", "keywordsegment"], "doc_len": 67}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_sequence_indent_conditional", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_sequence_indent_conditional", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_sequence_indent_conditional(seg_list, caplog):\n    \"\"\"Test the Sequence grammar with indents.\"\"\"\n    bs = StringParser(\"bar\", KeywordSegment)\n    fs = StringParser(\"foo\", KeywordSegment)\n    # We will assume the default config has indented_joins = False.\n    # We're testing without explictly setting the `config_type` because\n    # that's the assumed way of using the grammar in practice.\n    g = Sequence(\n        Conditional(Indent, indented_joins=False),\n        bs,\n        Conditional(Indent, indented_joins=True),\n        fs,\n    )\n    with RootParseContext(dialect=None) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            m = g.match(seg_list, parse_context=ctx)\n            assert m\n            # Check we get an Indent.\n            assert isinstance(m.matched_segments[0], Indent)\n            assert isinstance(m.matched_segments[1], KeywordSegment)\n            # check the whitespace is still there\n            assert isinstance(m.matched_segments[2], WhitespaceSegment)\n            # Check the second Indent does not appear\n            assert not isinstance(m.matched_segments[3], Indent)\n            assert isinstance(m.matched_segments[3], KeywordSegment)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_sequence_indent_conditional", "seg_list", "caplog", "test", "the", "sequence", "grammar", "with", "indents", "bs", "stringparser", "bar", "keywordsegment", "fs", "stringparser", "foo", "keywordsegment", "we", "will", "assume", "the", "default", "config", "has", "indented_joins", "false", "we", "re", "testing", "without", "explictly", "setting", "the", "config_type", "because", "that", "s", "the", "assumed", "way", "of", "using", "the", "grammar", "in", "practice", "g", "sequence", "conditional", "indent", "indented_joins", "false", "bs", "conditional", "indent", "indented_joins", "true", "fs", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "m", "g", "match", "seg_list", "parse_context", "ctx", "assert", "m", "check", "we", "get", "an", "indent", "assert", "isinstance", "m", "matched_segments", "0", "indent", "assert", "isinstance", "m", "matched_segments", "1", "keywordsegment", "check", "the", "whitespace", "is", "still", "there", "assert", "isinstance", "m", "matched_segments", "2", "whitespacesegment", "check", "the", "second", "indent", "does", "not", "appear", "assert", "not", "isinstance", "m", "matched_segments", "3", "indent", "assert", "isinstance", "m", "matched_segments", "3", "keywordsegment"], "doc_len": 135}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_delimited", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_delimited", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_delimited(\n    min_delimiters,\n    allow_gaps,\n    allow_trailing,\n    token_list,\n    match_len,\n    caplog,\n    generate_test_segments,\n    fresh_ansi_dialect,\n):\n    \"\"\"Test the Delimited grammar when not code_only.\"\"\"\n    seg_list = generate_test_segments(token_list)\n    g = Delimited(\n        StringParser(\"bar\", KeywordSegment),\n        delimiter=StringParser(\".\", SymbolSegment, name=\"dot\"),\n        allow_gaps=allow_gaps,\n        allow_trailing=allow_trailing,\n        min_delimiters=min_delimiters,\n    )\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.parser\"):\n            # Matching with whitespace shouldn't match if we need at least one delimiter\n            m = g.match(seg_list, parse_context=ctx)\n            assert len(m) == match_len\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_delimited", "min_delimiters", "allow_gaps", "allow_trailing", "token_list", "match_len", "caplog", "generate_test_segments", "fresh_ansi_dialect", "test", "the", "delimited", "grammar", "when", "not", "code_only", "seg_list", "generate_test_segments", "token_list", "g", "delimited", "stringparser", "bar", "keywordsegment", "delimiter", "stringparser", "symbolsegment", "name", "dot", "allow_gaps", "allow_gaps", "allow_trailing", "allow_trailing", "min_delimiters", "min_delimiters", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "parser", "matching", "with", "whitespace", "shouldn", "t", "match", "if", "we", "need", "at", "least", "one", "delimiter", "m", "g", "match", "seg_list", "parse_context", "ctx", "assert", "len", "m", "match_len"], "doc_len": 78}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_greedyuntil", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_greedyuntil", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_greedyuntil(\n    keyword, seg_list, enforce_ws, slice_len, fresh_ansi_dialect\n):\n    \"\"\"Test the GreedyUntil grammar.\"\"\"\n    grammar = GreedyUntil(\n        StringParser(keyword, KeywordSegment),\n        enforce_whitespace_preceding_terminator=enforce_ws,\n    )\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        assert (\n            grammar.match(seg_list, parse_context=ctx).matched_segments\n            == seg_list[:slice_len]\n        )\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_greedyuntil", "keyword", "seg_list", "enforce_ws", "slice_len", "fresh_ansi_dialect", "test", "the", "greedyuntil", "grammar", "grammar", "greedyuntil", "stringparser", "keyword", "keywordsegment", "enforce_whitespace_preceding_terminator", "enforce_ws", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "assert", "grammar", "match", "seg_list", "parse_context", "ctx", "matched_segments", "seg_list", "slice_len"], "doc_len": 38}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_greedyuntil_bracketed", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_greedyuntil_bracketed", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_greedyuntil_bracketed(bracket_seg_list, fresh_ansi_dialect):\n    \"\"\"Test the GreedyUntil grammar with brackets.\"\"\"\n    fs = StringParser(\"foo\", KeywordSegment)\n    g = GreedyUntil(fs)\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        # Check that we can make it past the brackets\n        match = g.match(bracket_seg_list, parse_context=ctx)\n        assert len(match) == 4\n        # Check we successfully constructed a bracketed segment\n        assert match.matched_segments[2].is_type(\"bracketed\")\n        assert match.matched_segments[2].raw == \"(foo    )\"\n        # Check that the unmatched segments is foo AND the whitespace\n        assert len(match.unmatched_segments) == 2\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_greedyuntil_bracketed", "bracket_seg_list", "fresh_ansi_dialect", "test", "the", "greedyuntil", "grammar", "with", "brackets", "fs", "stringparser", "foo", "keywordsegment", "g", "greedyuntil", "fs", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "check", "that", "we", "can", "make", "it", "past", "the", "brackets", "match", "g", "match", "bracket_seg_list", "parse_context", "ctx", "assert", "len", "match", "4", "check", "we", "successfully", "constructed", "a", "bracketed", "segment", "assert", "match", "matched_segments", "2", "is_type", "bracketed", "assert", "match", "matched_segments", "2", "raw", "foo", "check", "that", "the", "unmatched", "segments", "is", "foo", "and", "the", "whitespace", "assert", "len", "match", "unmatched_segments", "2"], "doc_len": 81}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_anything", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_anything", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_anything(seg_list, fresh_ansi_dialect):\n    \"\"\"Test the Anything grammar.\"\"\"\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        assert Anything().match(seg_list, parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_anything", "seg_list", "fresh_ansi_dialect", "test", "the", "anything", "grammar", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "assert", "anything", "match", "seg_list", "parse_context", "ctx"], "doc_len": 25}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_nothing", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_nothing", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_nothing(seg_list, fresh_ansi_dialect):\n    \"\"\"Test the Nothing grammar.\"\"\"\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        assert not Nothing().match(seg_list, parse_context=ctx)\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_nothing", "seg_list", "fresh_ansi_dialect", "test", "the", "nothing", "grammar", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "assert", "not", "nothing", "match", "seg_list", "parse_context", "ctx"], "doc_len": 26}
{"doc_id": "test/core/parser/grammar_test.py::test__parser__grammar_noncode", "file_path": "test/core/parser/grammar_test.py", "class_name": null, "func_name": "test__parser__grammar_noncode", "text": "文件路径: test/core/parser/grammar_test.py\ndef test__parser__grammar_noncode(seg_list, fresh_ansi_dialect):\n    \"\"\"Test the NonCodeMatcher.\"\"\"\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        m = NonCodeMatcher().match(seg_list[1:], parse_context=ctx)\n    # We should match one and only one segment\n    assert len(m) == 1\n", "tokens": ["test", "core", "parser", "grammar_test", "py", "def", "test__parser__grammar_noncode", "seg_list", "fresh_ansi_dialect", "test", "the", "noncodematcher", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "m", "noncodematcher", "match", "seg_list", "1", "parse_context", "ctx", "we", "should", "match", "one", "and", "only", "one", "segment", "assert", "len", "m", "1"], "doc_len": 37}
{"doc_id": "test/core/parser/helpers_test.py::test__parser__helper_trim_non_code_segments", "file_path": "test/core/parser/helpers_test.py", "class_name": null, "func_name": "test__parser__helper_trim_non_code_segments", "text": "文件路径: test/core/parser/helpers_test.py\ndef test__parser__helper_trim_non_code_segments(\n    token_list,\n    pre_len,\n    mid_len,\n    post_len,\n    generate_test_segments,\n):\n    \"\"\"Test trim_non_code_segments.\"\"\"\n    seg_list = generate_test_segments(token_list)\n    pre, mid, post = trim_non_code_segments(seg_list)\n    # Assert lengths\n    assert (len(pre), len(mid), len(post)) == (pre_len, mid_len, post_len)\n    # Assert content\n    assert [elem.raw for elem in pre] == list(token_list[:pre_len])\n    assert [elem.raw for elem in mid] == list(token_list[pre_len : pre_len + mid_len])\n    assert [elem.raw for elem in post] == list(token_list[len(seg_list) - post_len :])\n", "tokens": ["test", "core", "parser", "helpers_test", "py", "def", "test__parser__helper_trim_non_code_segments", "token_list", "pre_len", "mid_len", "post_len", "generate_test_segments", "test", "trim_non_code_segments", "seg_list", "generate_test_segments", "token_list", "pre", "mid", "post", "trim_non_code_segments", "seg_list", "assert", "lengths", "assert", "len", "pre", "len", "mid", "len", "post", "pre_len", "mid_len", "post_len", "assert", "content", "assert", "elem", "raw", "for", "elem", "in", "pre", "list", "token_list", "pre_len", "assert", "elem", "raw", "for", "elem", "in", "mid", "list", "token_list", "pre_len", "pre_len", "mid_len", "assert", "elem", "raw", "for", "elem", "in", "post", "list", "token_list", "len", "seg_list", "post_len"], "doc_len": 70}
{"doc_id": "test/core/parser/helpers_test.py::test__parser__helper_iter_indices", "file_path": "test/core/parser/helpers_test.py", "class_name": null, "func_name": "test__parser__helper_iter_indices", "text": "文件路径: test/core/parser/helpers_test.py\ndef test__parser__helper_iter_indices(seq, val, indices):\n    \"\"\"Test iter_indices.\"\"\"\n    res = list(iter_indices(seq, val))\n    assert res == indices\n", "tokens": ["test", "core", "parser", "helpers_test", "py", "def", "test__parser__helper_iter_indices", "seq", "val", "indices", "test", "iter_indices", "res", "list", "iter_indices", "seq", "val", "assert", "res", "indices"], "doc_len": 20}
{"doc_id": "test/core/parser/lexer_test.py::assert_matches", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "assert_matches", "text": "文件路径: test/core/parser/lexer_test.py\ndef assert_matches(instring, matcher, matchstring):\n    \"\"\"Assert that a matcher does or doesn't work on a string.\n\n    The optional `matchstring` argument, which can optionally\n    be None, allows to either test positive matching of a\n    particular string or negative matching (that it explicitly)\n    doesn't match.\n    \"\"\"\n    res = matcher.match(instring)\n    # Check we've got the right type\n    assert isinstance(res, LexMatch)\n    if matchstring is None:\n        assert res.forward_string == instring\n        assert res.elements == []\n    else:\n        assert res.forward_string == instring[len(matchstring) :]\n        assert len(res.elements) == 1\n        assert res.elements[0].raw == matchstring\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "assert_matches", "instring", "matcher", "matchstring", "assert", "that", "a", "matcher", "does", "or", "doesn", "t", "work", "on", "a", "string", "the", "optional", "matchstring", "argument", "which", "can", "optionally", "be", "none", "allows", "to", "either", "test", "positive", "matching", "of", "a", "particular", "string", "or", "negative", "matching", "that", "it", "explicitly", "doesn", "t", "match", "res", "matcher", "match", "instring", "check", "we", "ve", "got", "the", "right", "type", "assert", "isinstance", "res", "lexmatch", "if", "matchstring", "is", "none", "assert", "res", "forward_string", "instring", "assert", "res", "elements", "else", "assert", "res", "forward_string", "instring", "len", "matchstring", "assert", "len", "res", "elements", "1", "assert", "res", "elements", "0", "raw", "matchstring"], "doc_len": 94}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_obj", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_obj", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_obj(raw, res, caplog):\n    \"\"\"Test the lexer splits as expected in a selection of cases.\"\"\"\n    lex = Lexer(config=FluffConfig())\n    with caplog.at_level(logging.DEBUG):\n        lexing_segments, _ = lex.lex(raw)\n        assert [seg.raw for seg in lexing_segments] == res\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_obj", "raw", "res", "caplog", "test", "the", "lexer", "splits", "as", "expected", "in", "a", "selection", "of", "cases", "lex", "lexer", "config", "fluffconfig", "with", "caplog", "at_level", "logging", "debug", "lexing_segments", "_", "lex", "lex", "raw", "assert", "seg", "raw", "for", "seg", "in", "lexing_segments", "res"], "doc_len": 43}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_string", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_string", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_string(raw, res):\n    \"\"\"Test the StringLexer.\"\"\"\n    matcher = StringLexer(\"dot\", \".\", CodeSegment)\n    assert_matches(raw, matcher, res)\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_string", "raw", "res", "test", "the", "stringlexer", "matcher", "stringlexer", "dot", "codesegment", "assert_matches", "raw", "matcher", "res"], "doc_len": 20}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_regex", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_regex", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_regex(raw, reg, res, caplog):\n    \"\"\"Test the RegexLexer.\"\"\"\n    matcher = RegexLexer(\"test\", reg, CodeSegment)\n    with caplog.at_level(logging.DEBUG):\n        assert_matches(raw, matcher, res)\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_regex", "raw", "reg", "res", "caplog", "test", "the", "regexlexer", "matcher", "regexlexer", "test", "reg", "codesegment", "with", "caplog", "at_level", "logging", "debug", "assert_matches", "raw", "matcher", "res"], "doc_len": 28}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_lex_match", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_lex_match", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_lex_match(caplog):\n    \"\"\"Test the RepeatedMultiMatcher.\"\"\"\n    matchers = [\n        StringLexer(\"dot\", \".\", CodeSegment),\n        RegexLexer(\"test\", r\"#[^#]*#\", CodeSegment),\n    ]\n    with caplog.at_level(logging.DEBUG):\n        res = Lexer.lex_match(\"..#..#..#\", matchers)\n        assert res.forward_string == \"#\"  # Should match right up to the final element\n        assert len(res.elements) == 5\n        assert res.elements[2].raw == \"#..#\"\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_lex_match", "caplog", "test", "the", "repeatedmultimatcher", "matchers", "stringlexer", "dot", "codesegment", "regexlexer", "test", "r", "codesegment", "with", "caplog", "at_level", "logging", "debug", "res", "lexer", "lex_match", "matchers", "assert", "res", "forward_string", "should", "match", "right", "up", "to", "the", "final", "element", "assert", "len", "res", "elements", "5", "assert", "res", "elements", "2", "raw"], "doc_len": 49}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_fail", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_fail", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_fail():\n    \"\"\"Test the how the lexer fails and reports errors.\"\"\"\n    lex = Lexer(config=FluffConfig())\n\n    _, vs = lex.lex(\"Select \\u0394\")\n\n    assert len(vs) == 1\n    err = vs[0]\n    assert isinstance(err, SQLLexError)\n    assert err.line_pos == 8\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_fail", "test", "the", "how", "the", "lexer", "fails", "and", "reports", "errors", "lex", "lexer", "config", "fluffconfig", "_", "vs", "lex", "lex", "select", "u0394", "assert", "len", "vs", "1", "err", "vs", "0", "assert", "isinstance", "err", "sqllexerror", "assert", "err", "line_pos", "8"], "doc_len": 41}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_fail_via_parse", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_fail_via_parse", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_fail_via_parse():\n    \"\"\"Test the how the parser fails and reports errors while lexing.\"\"\"\n    lexer = Lexer(config=FluffConfig())\n    _, vs = lexer.lex(\"Select \\u0394\")\n    assert vs\n    assert len(vs) == 1\n    err = vs[0]\n    assert isinstance(err, SQLLexError)\n    assert err.line_pos == 8\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_fail_via_parse", "test", "the", "how", "the", "parser", "fails", "and", "reports", "errors", "while", "lexing", "lexer", "lexer", "config", "fluffconfig", "_", "vs", "lexer", "lex", "select", "u0394", "assert", "vs", "assert", "len", "vs", "1", "err", "vs", "0", "assert", "isinstance", "err", "sqllexerror", "assert", "err", "line_pos", "8"], "doc_len": 45}
{"doc_id": "test/core/parser/lexer_test.py::test__parser__lexer_trim_post_subdivide", "file_path": "test/core/parser/lexer_test.py", "class_name": null, "func_name": "test__parser__lexer_trim_post_subdivide", "text": "文件路径: test/core/parser/lexer_test.py\ndef test__parser__lexer_trim_post_subdivide(caplog):\n    \"\"\"Test a RegexLexer with a trim_post_subdivide function.\"\"\"\n    matcher = [\n        RegexLexer(\n            \"function_script_terminator\",\n            r\";\\s+(?!\\*)\\/(?!\\*)|\\s+(?!\\*)\\/(?!\\*)\",\n            CodeSegment,\n            segment_kwargs={\"type\": \"function_script_terminator\"},\n            subdivider=StringLexer(\n                \"semicolon\", \";\", CodeSegment, segment_kwargs={\"type\": \"semicolon\"}\n            ),\n            trim_post_subdivide=RegexLexer(\n                \"newline\",\n                r\"(\\n|\\r\\n)+\",\n                NewlineSegment,\n            ),\n        )\n    ]\n    with caplog.at_level(logging.DEBUG):\n        res = Lexer.lex_match(\";\\n/\\n\", matcher)\n        assert res.elements[0].raw == \";\"\n        assert res.elements[1].raw == \"\\n\"\n        assert res.elements[2].raw == \"/\"\n        assert len(res.elements) == 3\n", "tokens": ["test", "core", "parser", "lexer_test", "py", "def", "test__parser__lexer_trim_post_subdivide", "caplog", "test", "a", "regexlexer", "with", "a", "trim_post_subdivide", "function", "matcher", "regexlexer", "function_script_terminator", "r", "s", "s", "codesegment", "segment_kwargs", "type", "function_script_terminator", "subdivider", "stringlexer", "semicolon", "codesegment", "segment_kwargs", "type", "semicolon", "trim_post_subdivide", "regexlexer", "newline", "r", "n", "r", "n", "newlinesegment", "with", "caplog", "at_level", "logging", "debug", "res", "lexer", "lex_match", "n", "n", "matcher", "assert", "res", "elements", "0", "raw", "assert", "res", "elements", "1", "raw", "n", "assert", "res", "elements", "2", "raw", "assert", "len", "res", "elements", "3"], "doc_len": 72}
{"doc_id": "test/core/parser/markers_test.py::test_markers__infer_next_position", "file_path": "test/core/parser/markers_test.py", "class_name": null, "func_name": "test_markers__infer_next_position", "text": "文件路径: test/core/parser/markers_test.py\ndef test_markers__infer_next_position(raw, start_pos, end_pos):\n    \"\"\"Test that we can correctly infer positions from strings.\"\"\"\n    assert end_pos == PositionMarker.infer_next_position(raw, *start_pos)\n", "tokens": ["test", "core", "parser", "markers_test", "py", "def", "test_markers__infer_next_position", "raw", "start_pos", "end_pos", "test", "that", "we", "can", "correctly", "infer", "positions", "from", "strings", "assert", "end_pos", "positionmarker", "infer_next_position", "raw", "start_pos"], "doc_len": 25}
{"doc_id": "test/core/parser/markers_test.py::test_markers__setting_position_raw", "file_path": "test/core/parser/markers_test.py", "class_name": null, "func_name": "test_markers__setting_position_raw", "text": "文件路径: test/core/parser/markers_test.py\ndef test_markers__setting_position_raw():\n    \"\"\"Test that we can correctly infer positions from strings & locations.\"\"\"\n    templ = TemplatedFile.from_string(\"foobar\")\n    # Check inference in the template\n    assert templ.get_line_pos_of_char_pos(2, source=True) == (1, 3)\n    assert templ.get_line_pos_of_char_pos(2, source=False) == (1, 3)\n    # Now check it passes through\n    pos = PositionMarker(slice(2, 5), slice(2, 5), templ)\n    # Can we infer positions correctly?\n    assert pos.working_loc == (1, 3)\n", "tokens": ["test", "core", "parser", "markers_test", "py", "def", "test_markers__setting_position_raw", "test", "that", "we", "can", "correctly", "infer", "positions", "from", "strings", "locations", "templ", "templatedfile", "from_string", "foobar", "check", "inference", "in", "the", "template", "assert", "templ", "get_line_pos_of_char_pos", "2", "source", "true", "1", "3", "assert", "templ", "get_line_pos_of_char_pos", "2", "source", "false", "1", "3", "now", "check", "it", "passes", "through", "pos", "positionmarker", "slice", "2", "5", "slice", "2", "5", "templ", "can", "we", "infer", "positions", "correctly", "assert", "pos", "working_loc", "1", "3"], "doc_len": 66}
{"doc_id": "test/core/parser/markers_test.py::test_markers__setting_position_working", "file_path": "test/core/parser/markers_test.py", "class_name": null, "func_name": "test_markers__setting_position_working", "text": "文件路径: test/core/parser/markers_test.py\ndef test_markers__setting_position_working():\n    \"\"\"Test that we can correctly set positions manually.\"\"\"\n    templ = TemplatedFile.from_string(\"foobar\")\n    pos = PositionMarker(slice(2, 5), slice(2, 5), templ, 4, 4)\n    # Can we NOT infer when we're told.\n    assert pos.working_loc == (4, 4)\n", "tokens": ["test", "core", "parser", "markers_test", "py", "def", "test_markers__setting_position_working", "test", "that", "we", "can", "correctly", "set", "positions", "manually", "templ", "templatedfile", "from_string", "foobar", "pos", "positionmarker", "slice", "2", "5", "slice", "2", "5", "templ", "4", "4", "can", "we", "not", "infer", "when", "we", "re", "told", "assert", "pos", "working_loc", "4", "4"], "doc_len": 43}
{"doc_id": "test/core/parser/parse_test.py::test__parser__parse_match", "file_path": "test/core/parser/parse_test.py", "class_name": null, "func_name": "test__parser__parse_match", "text": "文件路径: test/core/parser/parse_test.py\ndef test__parser__parse_match(seg_list):\n    \"\"\"Test match method on a real segment.\"\"\"\n    with RootParseContext(dialect=None) as ctx:\n        # This should match and have consumed everything, which should\n        # now be part of a BasicSegment.\n        m = BasicSegment.match(seg_list[:1], parse_context=ctx)\n        assert m\n        assert len(m.matched_segments) == 1\n        assert isinstance(m.matched_segments[0], BasicSegment)\n        assert m.matched_segments[0].segments[0].type == \"raw\"\n", "tokens": ["test", "core", "parser", "parse_test", "py", "def", "test__parser__parse_match", "seg_list", "test", "match", "method", "on", "a", "real", "segment", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "this", "should", "match", "and", "have", "consumed", "everything", "which", "should", "now", "be", "part", "of", "a", "basicsegment", "m", "basicsegment", "match", "seg_list", "1", "parse_context", "ctx", "assert", "m", "assert", "len", "m", "matched_segments", "1", "assert", "isinstance", "m", "matched_segments", "0", "basicsegment", "assert", "m", "matched_segments", "0", "segments", "0", "type", "raw"], "doc_len": 64}
{"doc_id": "test/core/parser/parse_test.py::test__parser__parse_parse", "file_path": "test/core/parser/parse_test.py", "class_name": null, "func_name": "test__parser__parse_parse", "text": "文件路径: test/core/parser/parse_test.py\ndef test__parser__parse_parse(seg_list, caplog):\n    \"\"\"Test parse method on a real segment.\"\"\"\n    with RootParseContext(dialect=None) as ctx:\n        # Match the segment, and get the inner segment\n        seg = BasicSegment.match(seg_list[:1], parse_context=ctx).matched_segments[0]\n        # Remind ourselves that this should be an unparsed BasicSegment\n        assert isinstance(seg, BasicSegment)\n\n        # Now parse that segment, with debugging because this is\n        # where we'll need to debug if things fail.\n        with caplog.at_level(logging.DEBUG):\n            res = seg.parse(parse_context=ctx)\n        # Check it's still a BasicSegment\n        assert isinstance(res, BasicSegment)\n        # Check that we now have a keyword inside\n        assert isinstance(res.segments[0], KeywordSegment)\n", "tokens": ["test", "core", "parser", "parse_test", "py", "def", "test__parser__parse_parse", "seg_list", "caplog", "test", "parse", "method", "on", "a", "real", "segment", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "match", "the", "segment", "and", "get", "the", "inner", "segment", "seg", "basicsegment", "match", "seg_list", "1", "parse_context", "ctx", "matched_segments", "0", "remind", "ourselves", "that", "this", "should", "be", "an", "unparsed", "basicsegment", "assert", "isinstance", "seg", "basicsegment", "now", "parse", "that", "segment", "with", "debugging", "because", "this", "is", "where", "we", "ll", "need", "to", "debug", "if", "things", "fail", "with", "caplog", "at_level", "logging", "debug", "res", "seg", "parse", "parse_context", "ctx", "check", "it", "s", "still", "a", "basicsegment", "assert", "isinstance", "res", "basicsegment", "check", "that", "we", "now", "have", "a", "keyword", "inside", "assert", "isinstance", "res", "segments", "0", "keywordsegment"], "doc_len": 104}
{"doc_id": "test/core/parser/parse_test.py::test__parser__parse_expand", "file_path": "test/core/parser/parse_test.py", "class_name": null, "func_name": "test__parser__parse_expand", "text": "文件路径: test/core/parser/parse_test.py\ndef test__parser__parse_expand(seg_list):\n    \"\"\"Test expand method on a real segment.\"\"\"\n    with RootParseContext(dialect=None) as ctx:\n        # Match the segment, and get the matched segments\n        segments = BasicSegment.match(seg_list[:1], parse_context=ctx).matched_segments\n        # Remind ourselves that this should be tuple containing a BasicSegment\n        assert isinstance(segments[0], BasicSegment)\n\n        # Now expand those segments, using the base class version (not that it should matter)\n        res = BasicSegment.expand(segments, parse_context=ctx)\n        # Check we get an iterable containing a BasicSegment\n        assert isinstance(res[0], BasicSegment)\n        # Check that we now have a keyword inside\n        assert isinstance(res[0].segments[0], KeywordSegment)\n", "tokens": ["test", "core", "parser", "parse_test", "py", "def", "test__parser__parse_expand", "seg_list", "test", "expand", "method", "on", "a", "real", "segment", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "match", "the", "segment", "and", "get", "the", "matched", "segments", "segments", "basicsegment", "match", "seg_list", "1", "parse_context", "ctx", "matched_segments", "remind", "ourselves", "that", "this", "should", "be", "tuple", "containing", "a", "basicsegment", "assert", "isinstance", "segments", "0", "basicsegment", "now", "expand", "those", "segments", "using", "the", "base", "class", "version", "not", "that", "it", "should", "matter", "res", "basicsegment", "expand", "segments", "parse_context", "ctx", "check", "we", "get", "an", "iterable", "containing", "a", "basicsegment", "assert", "isinstance", "res", "0", "basicsegment", "check", "that", "we", "now", "have", "a", "keyword", "inside", "assert", "isinstance", "res", "0", "segments", "0", "keywordsegment"], "doc_len": 100}
{"doc_id": "test/core/parser/segments_base_test.py::raw_seg_list", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "raw_seg_list", "text": "文件路径: test/core/parser/segments_base_test.py\ndef raw_seg_list(generate_test_segments):\n    \"\"\"Construct a list of raw segments as a fixture.\"\"\"\n    return generate_test_segments([\"foobar\", \".barfoo\"])\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "raw_seg_list", "generate_test_segments", "construct", "a", "list", "of", "raw", "segments", "as", "a", "fixture", "return", "generate_test_segments", "foobar", "barfoo"], "doc_len": 21}
{"doc_id": "test/core/parser/segments_base_test.py::raw_seg", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "raw_seg", "text": "文件路径: test/core/parser/segments_base_test.py\ndef raw_seg(raw_seg_list):\n    \"\"\"Construct a raw segment as a fixture.\"\"\"\n    return raw_seg_list[0]\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "raw_seg", "raw_seg_list", "construct", "a", "raw", "segment", "as", "a", "fixture", "return", "raw_seg_list", "0"], "doc_len": 18}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_type", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_type", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_type():\n    \"\"\"Test the .is_type() method.\"\"\"\n    assert BaseSegment.class_is_type(\"base\")\n    assert not BaseSegment.class_is_type(\"foo\")\n    assert not BaseSegment.class_is_type(\"foo\", \"bar\")\n    assert DummySegment.class_is_type(\"dummy\")\n    assert DummySegment.class_is_type(\"base\")\n    assert DummySegment.class_is_type(\"base\", \"foo\", \"bar\")\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_type", "test", "the", "is_type", "method", "assert", "basesegment", "class_is_type", "base", "assert", "not", "basesegment", "class_is_type", "foo", "assert", "not", "basesegment", "class_is_type", "foo", "bar", "assert", "dummysegment", "class_is_type", "dummy", "assert", "dummysegment", "class_is_type", "base", "assert", "dummysegment", "class_is_type", "base", "foo", "bar"], "doc_len": 40}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_stubs", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_stubs", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_stubs():\n    \"\"\"Test stub methods that have no implementation in base class.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    base_segment = BaseSegment(segments=[rs1])\n\n    with pytest.raises(NotImplementedError):\n        base_segment.edit(\"foo\")\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_stubs", "test", "stub", "methods", "that", "have", "no", "implementation", "in", "base", "class", "template", "templatedfile", "from_string", "foobar", "rs1", "rawsegment", "foobar", "positionmarker", "slice", "0", "6", "slice", "0", "6", "template", "base_segment", "basesegment", "segments", "rs1", "with", "pytest", "raises", "notimplementederror", "base_segment", "edit", "foo"], "doc_len": 43}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_raw", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_raw", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_raw(raw_seg):\n    \"\"\"Test raw segments behave as expected.\"\"\"\n    # Check Segment Return\n    assert raw_seg.segments == []\n    assert raw_seg.raw == \"foobar\"\n    # Check Formatting and Stringification\n    assert str(raw_seg) == repr(raw_seg) == \"<CodeSegment: ([L:  1, P:  1]) 'foobar'>\"\n    assert (\n        raw_seg.stringify(ident=1, tabsize=2)\n        == \"[L:  1, P:  1]      |  raw:                                                        'foobar'\\n\"\n    )\n    # Check tuple\n    assert raw_seg.to_tuple() == (\"raw\", ())\n    # Check tuple\n    assert raw_seg.to_tuple(show_raw=True) == (\"raw\", \"foobar\")\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_raw", "raw_seg", "test", "raw", "segments", "behave", "as", "expected", "check", "segment", "return", "assert", "raw_seg", "segments", "assert", "raw_seg", "raw", "foobar", "check", "formatting", "and", "stringification", "assert", "str", "raw_seg", "repr", "raw_seg", "codesegment", "l", "1", "p", "1", "foobar", "assert", "raw_seg", "stringify", "ident", "1", "tabsize", "2", "l", "1", "p", "1", "raw", "foobar", "n", "check", "tuple", "assert", "raw_seg", "to_tuple", "raw", "check", "tuple", "assert", "raw_seg", "to_tuple", "show_raw", "true", "raw", "foobar"], "doc_len": 68}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_base", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_base", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_base(raw_seg_list, fresh_ansi_dialect):\n    \"\"\"Test base segments behave as expected.\"\"\"\n    base_seg = DummySegment(raw_seg_list)\n    # Check we assume the position correctly\n    assert (\n        base_seg.pos_marker.start_point_marker()\n        == raw_seg_list[0].pos_marker.start_point_marker()\n    )\n    assert (\n        base_seg.pos_marker.end_point_marker()\n        == raw_seg_list[-1].pos_marker.end_point_marker()\n    )\n    with RootParseContext(dialect=fresh_ansi_dialect) as ctx:\n        # Expand and given we don't have a grammar we should get the same thing\n        assert base_seg.parse(parse_context=ctx) == base_seg\n    # Check that we correctly reconstruct the raw\n    assert base_seg.raw == \"foobar.barfoo\"\n    # Check tuple\n    assert base_seg.to_tuple() == (\n        \"dummy\",\n        (raw_seg_list[0].to_tuple(), raw_seg_list[1].to_tuple()),\n    )\n    # Check Formatting and Stringification\n    assert str(base_seg) == repr(base_seg) == \"<DummySegment: ([L:  1, P:  1])>\"\n    assert base_seg.stringify(ident=1, tabsize=2) == (\n        \"[L:  1, P:  1]      |  dummy:\\n\"\n        \"[L:  1, P:  1]      |    raw:                                                      'foobar'\\n\"\n        \"[L:  1, P:  7]      |    raw:                                                      '.barfoo'\\n\"\n    )\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_base", "raw_seg_list", "fresh_ansi_dialect", "test", "base", "segments", "behave", "as", "expected", "base_seg", "dummysegment", "raw_seg_list", "check", "we", "assume", "the", "position", "correctly", "assert", "base_seg", "pos_marker", "start_point_marker", "raw_seg_list", "0", "pos_marker", "start_point_marker", "assert", "base_seg", "pos_marker", "end_point_marker", "raw_seg_list", "1", "pos_marker", "end_point_marker", "with", "rootparsecontext", "dialect", "fresh_ansi_dialect", "as", "ctx", "expand", "and", "given", "we", "don", "t", "have", "a", "grammar", "we", "should", "get", "the", "same", "thing", "assert", "base_seg", "parse", "parse_context", "ctx", "base_seg", "check", "that", "we", "correctly", "reconstruct", "the", "raw", "assert", "base_seg", "raw", "foobar", "barfoo", "check", "tuple", "assert", "base_seg", "to_tuple", "dummy", "raw_seg_list", "0", "to_tuple", "raw_seg_list", "1", "to_tuple", "check", "formatting", "and", "stringification", "assert", "str", "base_seg", "repr", "base_seg", "dummysegment", "l", "1", "p", "1", "assert", "base_seg", "stringify", "ident", "1", "tabsize", "2", "l", "1", "p", "1", "dummy", "n", "l", "1", "p", "1", "raw", "foobar", "n", "l", "1", "p", "7", "raw", "barfoo", "n"], "doc_len": 132}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_raw_compare", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_raw_compare", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_raw_compare():\n    \"\"\"Test comparison of raw segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    assert rs1 == rs2\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_raw_compare", "test", "comparison", "of", "raw", "segments", "template", "templatedfile", "from_string", "foobar", "rs1", "rawsegment", "foobar", "positionmarker", "slice", "0", "6", "slice", "0", "6", "template", "rs2", "rawsegment", "foobar", "positionmarker", "slice", "0", "6", "slice", "0", "6", "template", "assert", "rs1", "rs2"], "doc_len": 41}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_base_compare", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_base_compare", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_base_compare():\n    \"\"\"Test comparison of base segments.\"\"\"\n    template = TemplatedFile.from_string(\"foobar\")\n    rs1 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n    rs2 = RawSegment(\"foobar\", PositionMarker(slice(0, 6), slice(0, 6), template))\n\n    ds1 = DummySegment([rs1])\n    ds2 = DummySegment([rs2])\n    dsa2 = DummyAuxSegment([rs2])\n\n    # Check for equality\n    assert ds1 == ds2\n    # Check a different match on the same details are not the same\n    assert ds1 != dsa2\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_base_compare", "test", "comparison", "of", "base", "segments", "template", "templatedfile", "from_string", "foobar", "rs1", "rawsegment", "foobar", "positionmarker", "slice", "0", "6", "slice", "0", "6", "template", "rs2", "rawsegment", "foobar", "positionmarker", "slice", "0", "6", "slice", "0", "6", "template", "ds1", "dummysegment", "rs1", "ds2", "dummysegment", "rs2", "dsa2", "dummyauxsegment", "rs2", "check", "for", "equality", "assert", "ds1", "ds2", "check", "a", "different", "match", "on", "the", "same", "details", "are", "not", "the", "same", "assert", "ds1", "dsa2"], "doc_len": 68}
{"doc_id": "test/core/parser/segments_base_test.py::test__parser__base_segments_file", "file_path": "test/core/parser/segments_base_test.py", "class_name": null, "func_name": "test__parser__base_segments_file", "text": "文件路径: test/core/parser/segments_base_test.py\ndef test__parser__base_segments_file(raw_seg_list):\n    \"\"\"Test BaseFileSegment to behave as expected.\"\"\"\n    base_seg = BaseFileSegment(raw_seg_list, fname=\"/some/dir/file.sql\")\n    assert base_seg.type == \"file\"\n    assert base_seg.file_path == \"/some/dir/file.sql\"\n    assert base_seg.can_start_end_non_code\n    assert base_seg.allow_empty\n", "tokens": ["test", "core", "parser", "segments_base_test", "py", "def", "test__parser__base_segments_file", "raw_seg_list", "test", "basefilesegment", "to", "behave", "as", "expected", "base_seg", "basefilesegment", "raw_seg_list", "fname", "some", "dir", "file", "sql", "assert", "base_seg", "type", "file", "assert", "base_seg", "file_path", "some", "dir", "file", "sql", "assert", "base_seg", "can_start_end_non_code", "assert", "base_seg", "allow_empty"], "doc_len": 39}
{"doc_id": "test/core/parser/segments_common_test.py::raw_seg_list", "file_path": "test/core/parser/segments_common_test.py", "class_name": null, "func_name": "raw_seg_list", "text": "文件路径: test/core/parser/segments_common_test.py\ndef raw_seg_list(generate_test_segments):\n    \"\"\"A generic list of raw segments to test against.\"\"\"\n    return generate_test_segments([\"bar\", \"foo\", \"bar\"])\n", "tokens": ["test", "core", "parser", "segments_common_test", "py", "def", "raw_seg_list", "generate_test_segments", "a", "generic", "list", "of", "raw", "segments", "to", "test", "against", "return", "generate_test_segments", "bar", "foo", "bar"], "doc_len": 22}
{"doc_id": "test/core/parser/segments_common_test.py::test__parser__core_keyword", "file_path": "test/core/parser/segments_common_test.py", "class_name": null, "func_name": "test__parser__core_keyword", "text": "文件路径: test/core/parser/segments_common_test.py\ndef test__parser__core_keyword(raw_seg_list):\n    \"\"\"Test the Mystical KeywordSegment.\"\"\"\n    # First make a keyword\n    FooKeyword = StringParser(\"foo\", KeywordSegment)\n    # Check it looks as expected\n    assert FooKeyword.template == \"FOO\"\n    with RootParseContext(dialect=None) as ctx:\n        # Match it against a list and check it doesn't match\n        assert not FooKeyword.match(raw_seg_list, parse_context=ctx)\n        # Match it against a the first element and check it doesn't match\n        assert not FooKeyword.match(raw_seg_list[0], parse_context=ctx)\n        # Match it against a the first element as a list and check it doesn't match\n        assert not FooKeyword.match([raw_seg_list[0]], parse_context=ctx)\n        # Match it against the final element (returns tuple)\n        m = FooKeyword.match(raw_seg_list[1], parse_context=ctx)\n        assert m\n        assert m.matched_segments[0].raw == \"foo\"\n        assert isinstance(m.matched_segments[0], KeywordSegment)\n        # Match it against the final element as a list\n        assert FooKeyword.match([raw_seg_list[1]], parse_context=ctx)\n        # Match it against a list slice and check it still works\n        assert FooKeyword.match(raw_seg_list[1:], parse_context=ctx)\n", "tokens": ["test", "core", "parser", "segments_common_test", "py", "def", "test__parser__core_keyword", "raw_seg_list", "test", "the", "mystical", "keywordsegment", "first", "make", "a", "keyword", "fookeyword", "stringparser", "foo", "keywordsegment", "check", "it", "looks", "as", "expected", "assert", "fookeyword", "template", "foo", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "match", "it", "against", "a", "list", "and", "check", "it", "doesn", "t", "match", "assert", "not", "fookeyword", "match", "raw_seg_list", "parse_context", "ctx", "match", "it", "against", "a", "the", "first", "element", "and", "check", "it", "doesn", "t", "match", "assert", "not", "fookeyword", "match", "raw_seg_list", "0", "parse_context", "ctx", "match", "it", "against", "a", "the", "first", "element", "as", "a", "list", "and", "check", "it", "doesn", "t", "match", "assert", "not", "fookeyword", "match", "raw_seg_list", "0", "parse_context", "ctx", "match", "it", "against", "the", "final", "element", "returns", "tuple", "m", "fookeyword", "match", "raw_seg_list", "1", "parse_context", "ctx", "assert", "m", "assert", "m", "matched_segments", "0", "raw", "foo", "assert", "isinstance", "m", "matched_segments", "0", "keywordsegment", "match", "it", "against", "the", "final", "element", "as", "a", "list", "assert", "fookeyword", "match", "raw_seg_list", "1", "parse_context", "ctx", "match", "it", "against", "a", "list", "slice", "and", "check", "it", "still", "works", "assert", "fookeyword", "match", "raw_seg_list", "1", "parse_context", "ctx"], "doc_len": 161}
{"doc_id": "test/core/parser/segments_common_test.py::test__parser__core_ephemeral_segment", "file_path": "test/core/parser/segments_common_test.py", "class_name": null, "func_name": "test__parser__core_ephemeral_segment", "text": "文件路径: test/core/parser/segments_common_test.py\ndef test__parser__core_ephemeral_segment(raw_seg_list):\n    \"\"\"Test the Mystical KeywordSegment.\"\"\"\n    # First make a keyword\n    BarKeyword = StringParser(\"bar\", KeywordSegment)\n\n    ephemeral_segment = EphemeralSegment(\n        segments=raw_seg_list[:1],\n        pos_marker=None,\n        parse_grammar=BarKeyword,\n        name=\"foobar\",\n    )\n\n    with RootParseContext(dialect=None) as ctx:\n        # Parse it and make sure we don't get an EphemeralSegment back\n        res = ephemeral_segment.parse(ctx)\n        assert isinstance(res, tuple)\n        elem = res[0]\n        assert not isinstance(elem, EphemeralSegment)\n        assert isinstance(elem, KeywordSegment)\n", "tokens": ["test", "core", "parser", "segments_common_test", "py", "def", "test__parser__core_ephemeral_segment", "raw_seg_list", "test", "the", "mystical", "keywordsegment", "first", "make", "a", "keyword", "barkeyword", "stringparser", "bar", "keywordsegment", "ephemeral_segment", "ephemeralsegment", "segments", "raw_seg_list", "1", "pos_marker", "none", "parse_grammar", "barkeyword", "name", "foobar", "with", "rootparsecontext", "dialect", "none", "as", "ctx", "parse", "it", "and", "make", "sure", "we", "don", "t", "get", "an", "ephemeralsegment", "back", "res", "ephemeral_segment", "parse", "ctx", "assert", "isinstance", "res", "tuple", "elem", "res", "0", "assert", "not", "isinstance", "elem", "ephemeralsegment", "assert", "isinstance", "elem", "keywordsegment"], "doc_len": 69}
{"doc_id": "test/core/rules/config_test.py::Rule_T042._eval", "file_path": "test/core/rules/config_test.py", "class_name": "Rule_T042", "func_name": "_eval", "text": "文件路径: test/core/rules/config_test.py, 类名: Rule_T042\n    def _eval(self, context):\n        pass\n", "tokens": ["test", "core", "rules", "config_test", "py", "rule_t042", "def", "_eval", "self", "context", "pass"], "doc_len": 11}
{"doc_id": "test/core/rules/config_test.py::Rule_T001._eval", "file_path": "test/core/rules/config_test.py", "class_name": "Rule_T001", "func_name": "_eval", "text": "文件路径: test/core/rules/config_test.py, 类名: Rule_T001\n    def _eval(self, context):\n        \"\"\"Stars make newlines.\"\"\"\n        if context.segment.is_type(\"star\"):\n            return LintResult(\n                anchor=context.segment,\n                fixes=[LintFix(\"create\", context.segment, NewlineSegment())],\n            )\n", "tokens": ["test", "core", "rules", "config_test", "py", "rule_t001", "def", "_eval", "self", "context", "stars", "make", "newlines", "if", "context", "segment", "is_type", "star", "return", "lintresult", "anchor", "context", "segment", "fixes", "lintfix", "create", "context", "segment", "newlinesegment"], "doc_len": 29}
{"doc_id": "test/core/rules/config_test.py::test__rules__user_rules", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test__rules__user_rules", "text": "文件路径: test/core/rules/config_test.py\ndef test__rules__user_rules():\n    \"\"\"Test that can safely add user rules.\"\"\"\n    # Set up a linter with the user rule\n    linter = Linter(user_rules=[Rule_T042])\n    # Make sure the new one is in there.\n    assert (\"T042\", \"A dummy rule.\") in linter.rule_tuples()\n    # Instantiate a second linter and check it's NOT in there.\n    # This tests that copying and isolation works.\n    linter = Linter()\n    assert not any(rule[0] == \"T042\" for rule in linter.rule_tuples())\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test__rules__user_rules", "test", "that", "can", "safely", "add", "user", "rules", "set", "up", "a", "linter", "with", "the", "user", "rule", "linter", "linter", "user_rules", "rule_t042", "make", "sure", "the", "new", "one", "is", "in", "there", "assert", "t042", "a", "dummy", "rule", "in", "linter", "rule_tuples", "instantiate", "a", "second", "linter", "and", "check", "it", "s", "not", "in", "there", "this", "tests", "that", "copying", "and", "isolation", "works", "linter", "linter", "assert", "not", "any", "rule", "0", "t042", "for", "rule", "in", "linter", "rule_tuples"], "doc_len": 73}
{"doc_id": "test/core/rules/config_test.py::test__rules__runaway_fail_catch", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test__rules__runaway_fail_catch", "text": "文件路径: test/core/rules/config_test.py\ndef test__rules__runaway_fail_catch():\n    \"\"\"Test that we catch runaway rules.\"\"\"\n    runaway_limit = 5\n    my_query = \"SELECT * FROM foo\"\n    # Set up the config to only use the rule we are testing.\n    cfg = FluffConfig(overrides={\"rules\": \"T001\", \"runaway_limit\": runaway_limit})\n    # Lint it using the current config (while in fix mode)\n    linter = Linter(config=cfg, user_rules=[Rule_T001])\n    # In theory this step should result in an infinite\n    # loop, but the loop limit should catch it.\n    linted = linter.lint_string(my_query, fix=True)\n    # We should have a lot of newlines in there.\n    # The number should equal the runaway limit\n    assert linted.tree.raw.count(\"\\n\") == runaway_limit\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test__rules__runaway_fail_catch", "test", "that", "we", "catch", "runaway", "rules", "runaway_limit", "5", "my_query", "select", "from", "foo", "set", "up", "the", "config", "to", "only", "use", "the", "rule", "we", "are", "testing", "cfg", "fluffconfig", "overrides", "rules", "t001", "runaway_limit", "runaway_limit", "lint", "it", "using", "the", "current", "config", "while", "in", "fix", "mode", "linter", "linter", "config", "cfg", "user_rules", "rule_t001", "in", "theory", "this", "step", "should", "result", "in", "an", "infinite", "loop", "but", "the", "loop", "limit", "should", "catch", "it", "linted", "linter", "lint_string", "my_query", "fix", "true", "we", "should", "have", "a", "lot", "of", "newlines", "in", "there", "the", "number", "should", "equal", "the", "runaway", "limit", "assert", "linted", "tree", "raw", "count", "n", "runaway_limit"], "doc_len": 100}
{"doc_id": "test/core/rules/config_test.py::test_rules_cannot_be_instantiated_without_declared_configs", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test_rules_cannot_be_instantiated_without_declared_configs", "text": "文件路径: test/core/rules/config_test.py\ndef test_rules_cannot_be_instantiated_without_declared_configs():\n    \"\"\"Ensure that new rules must be instantiated with config values.\"\"\"\n\n    class NewRule(BaseRule):\n        config_keywords = [\"comma_style\"]\n\n    new_rule = NewRule(code=\"L000\", description=\"\", comma_style=\"trailing\")\n    assert new_rule.comma_style == \"trailing\"\n    # Error is thrown since \"comma_style\" is defined in class,\n    # but not upon instantiation\n    with pytest.raises(ValueError):\n        new_rule = NewRule(code=\"L000\", description=\"\")\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test_rules_cannot_be_instantiated_without_declared_configs", "ensure", "that", "new", "rules", "must", "be", "instantiated", "with", "config", "values", "class", "newrule", "baserule", "config_keywords", "comma_style", "new_rule", "newrule", "code", "l000", "description", "comma_style", "trailing", "assert", "new_rule", "comma_style", "trailing", "error", "is", "thrown", "since", "comma_style", "is", "defined", "in", "class", "but", "not", "upon", "instantiation", "with", "pytest", "raises", "valueerror", "new_rule", "newrule", "code", "l000", "description"], "doc_len": 55}
{"doc_id": "test/core/rules/config_test.py::test_rules_configs_are_dynamically_documented", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test_rules_configs_are_dynamically_documented", "text": "文件路径: test/core/rules/config_test.py\ndef test_rules_configs_are_dynamically_documented():\n    \"\"\"Ensure that rule configurations are added to the class docstring.\"\"\"\n\n    @document_configuration\n    class RuleWithConfig(BaseRule):\n        \"\"\"A new rule with configuration.\"\"\"\n\n        config_keywords = [\"comma_style\"]\n\n    assert \"comma_style\" in RuleWithConfig.__doc__\n\n    @document_configuration\n    class RuleWithoutConfig(BaseRule):\n        \"\"\"A new rule without configuration.\"\"\"\n\n        pass\n\n    assert \"Configuration\" not in RuleWithoutConfig.__doc__\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test_rules_configs_are_dynamically_documented", "ensure", "that", "rule", "configurations", "are", "added", "to", "the", "class", "docstring", "document_configuration", "class", "rulewithconfig", "baserule", "a", "new", "rule", "with", "configuration", "config_keywords", "comma_style", "assert", "comma_style", "in", "rulewithconfig", "__doc__", "document_configuration", "class", "rulewithoutconfig", "baserule", "a", "new", "rule", "without", "configuration", "pass", "assert", "configuration", "not", "in", "rulewithoutconfig", "__doc__"], "doc_len": 49}
{"doc_id": "test/core/rules/config_test.py::test_rule_exception_is_caught_to_validation", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test_rule_exception_is_caught_to_validation", "text": "文件路径: test/core/rules/config_test.py\ndef test_rule_exception_is_caught_to_validation():\n    \"\"\"Assert that a rule that throws an exception on _eval returns it as a validation.\"\"\"\n    std_rule_set = get_ruleset()\n\n    @std_rule_set.register\n    class Rule_T000(BaseRule):\n        \"\"\"Rule that throws an exception.\"\"\"\n\n        def _eval(self, segment, parent_stack, **kwargs):\n            raise Exception(\"Catch me or I'll deny any linting results from you\")\n\n    linter = Linter(\n        config=FluffConfig(overrides=dict(rules=\"T000\")),\n        user_rules=[Rule_T000],\n    )\n\n    assert linter.lint_string(\"select 1\").check_tuples() == [(\"T000\", 1, 1)]\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test_rule_exception_is_caught_to_validation", "assert", "that", "a", "rule", "that", "throws", "an", "exception", "on", "_eval", "returns", "it", "as", "a", "validation", "std_rule_set", "get_ruleset", "std_rule_set", "register", "class", "rule_t000", "baserule", "rule", "that", "throws", "an", "exception", "def", "_eval", "self", "segment", "parent_stack", "kwargs", "raise", "exception", "catch", "me", "or", "i", "ll", "deny", "any", "linting", "results", "from", "you", "linter", "linter", "config", "fluffconfig", "overrides", "dict", "rules", "t000", "user_rules", "rule_t000", "assert", "linter", "lint_string", "select", "1", "check_tuples", "t000", "1", "1"], "doc_len": 72}
{"doc_id": "test/core/rules/config_test.py::test_std_rule_import_fail_bad_naming", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test_std_rule_import_fail_bad_naming", "text": "文件路径: test/core/rules/config_test.py\ndef test_std_rule_import_fail_bad_naming():\n    \"\"\"Check that rule import from file works.\"\"\"\n    assert (\n        get_rules_from_path(\n            rules_path=\"test/fixtures/rules/custom/*.py\",\n            base_module=\"test.fixtures.rules.custom\",\n        )\n        == [Rule_L000, Rule_S000]\n    )\n\n    with pytest.raises(AttributeError) as e:\n        get_rules_from_path(\n            rules_path=\"test/fixtures/rules/custom/bad_rule_name/*.py\",\n            base_module=\"test.fixtures.rules.custom.bad_rule_name\",\n        )\n\n    e.match(\"Rule classes must be named in the format of\")\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test_std_rule_import_fail_bad_naming", "check", "that", "rule", "import", "from", "file", "works", "assert", "get_rules_from_path", "rules_path", "test", "fixtures", "rules", "custom", "py", "base_module", "test", "fixtures", "rules", "custom", "rule_l000", "rule_s000", "with", "pytest", "raises", "attributeerror", "as", "e", "get_rules_from_path", "rules_path", "test", "fixtures", "rules", "custom", "bad_rule_name", "py", "base_module", "test", "fixtures", "rules", "custom", "bad_rule_name", "e", "match", "rule", "classes", "must", "be", "named", "in", "the", "format", "of"], "doc_len": 60}
{"doc_id": "test/core/rules/config_test.py::test_rule_set_return_informative_error_when_rule_not_registered", "file_path": "test/core/rules/config_test.py", "class_name": null, "func_name": "test_rule_set_return_informative_error_when_rule_not_registered", "text": "文件路径: test/core/rules/config_test.py\ndef test_rule_set_return_informative_error_when_rule_not_registered():\n    \"\"\"Assert that a rule that throws an exception on _eval returns it as a validation.\"\"\"\n    cfg = FluffConfig()\n    with pytest.raises(ValueError) as e:\n        get_rule_from_set(\"L000\", config=cfg)\n\n    e.match(\"'L000' not in\")\n", "tokens": ["test", "core", "rules", "config_test", "py", "def", "test_rule_set_return_informative_error_when_rule_not_registered", "assert", "that", "a", "rule", "that", "throws", "an", "exception", "on", "_eval", "returns", "it", "as", "a", "validation", "cfg", "fluffconfig", "with", "pytest", "raises", "valueerror", "as", "e", "get_rule_from_set", "l000", "config", "cfg", "e", "match", "l000", "not", "in"], "doc_len": 39}
{"doc_id": "test/core/rules/docstring_test.py::test_content_count", "file_path": "test/core/rules/docstring_test.py", "class_name": null, "func_name": "test_content_count", "text": "文件路径: test/core/rules/docstring_test.py\ndef test_content_count(content, min_count):\n    \"\"\"Test docstring have specific content.\"\"\"\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            if rule._check_docstring is True:\n                assert (\n                    rule.__doc__.count(content) >= min_count\n                ), f\"{rule.__name__} content {content} does not occur at less {min_count} times\"\n", "tokens": ["test", "core", "rules", "docstring_test", "py", "def", "test_content_count", "content", "min_count", "test", "docstring", "have", "specific", "content", "for", "plugin_rules", "in", "get_plugin_manager", "hook", "get_rules", "for", "rule", "in", "plugin_rules", "if", "rule", "_check_docstring", "is", "true", "assert", "rule", "__doc__", "count", "content", "min_count", "f", "rule", "__name__", "content", "content", "does", "not", "occur", "at", "less", "min_count", "times"], "doc_len": 47}
{"doc_id": "test/core/rules/docstring_test.py::test_keyword_anti_before_best", "file_path": "test/core/rules/docstring_test.py", "class_name": null, "func_name": "test_keyword_anti_before_best", "text": "文件路径: test/core/rules/docstring_test.py\ndef test_keyword_anti_before_best():\n    \"\"\"Test docstring anti pattern before best pattern.\"\"\"\n    for plugin_rules in get_plugin_manager().hook.get_rules():\n        for rule in plugin_rules:\n            if rule._check_docstring is True:\n                assert rule.__doc__.index(KEYWORD_ANTI) < rule.__doc__.index(\n                    KEYWORD_BEST\n                ), f\"{rule.__name__} keyword {KEYWORD_BEST} appears before {KEYWORD_ANTI}\"\n", "tokens": ["test", "core", "rules", "docstring_test", "py", "def", "test_keyword_anti_before_best", "test", "docstring", "anti", "pattern", "before", "best", "pattern", "for", "plugin_rules", "in", "get_plugin_manager", "hook", "get_rules", "for", "rule", "in", "plugin_rules", "if", "rule", "_check_docstring", "is", "true", "assert", "rule", "__doc__", "index", "keyword_anti", "rule", "__doc__", "index", "keyword_best", "f", "rule", "__name__", "keyword", "keyword_best", "appears", "before", "keyword_anti"], "doc_len": 46}
{"doc_id": "test/core/templaters/base_test.py::test__indices_of_newlines", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__indices_of_newlines", "text": "文件路径: test/core/templaters/base_test.py\ndef test__indices_of_newlines(raw_str, positions):\n    \"\"\"Test iter_indices_of_newlines.\"\"\"\n    assert list(iter_indices_of_newlines(raw_str)) == positions\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__indices_of_newlines", "raw_str", "positions", "test", "iter_indices_of_newlines", "assert", "list", "iter_indices_of_newlines", "raw_str", "positions"], "doc_len": 16}
{"doc_id": "test/core/templaters/base_test.py::test__templater_raw", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__templater_raw", "text": "文件路径: test/core/templaters/base_test.py\ndef test__templater_raw():\n    \"\"\"Test the raw templater.\"\"\"\n    t = RawTemplater()\n    instr = \"SELECT * FROM {{blah}}\"\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert instr == str(outstr)\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__templater_raw", "test", "the", "raw", "templater", "t", "rawtemplater", "instr", "select", "from", "blah", "outstr", "_", "t", "process", "in_str", "instr", "fname", "test", "assert", "instr", "str", "outstr"], "doc_len": 29}
{"doc_id": "test/core/templaters/base_test.py::test__templated_file_get_line_pos_of_char_pos", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__templated_file_get_line_pos_of_char_pos", "text": "文件路径: test/core/templaters/base_test.py\ndef test__templated_file_get_line_pos_of_char_pos(\n    source_str, templated_str, file_slices, in_charpos, out_line_no, out_line_pos\n):\n    \"\"\"Test TemplatedFile.get_line_pos_of_char_pos.\"\"\"\n    file = TemplatedFile(\n        source_str=source_str,\n        templated_str=templated_str,\n        sliced_file=file_slices,\n        fname=\"test\",\n    )\n    res_line_no, res_line_pos = file.get_line_pos_of_char_pos(in_charpos)\n    assert res_line_no == out_line_no\n    assert res_line_pos == out_line_pos\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__templated_file_get_line_pos_of_char_pos", "source_str", "templated_str", "file_slices", "in_charpos", "out_line_no", "out_line_pos", "test", "templatedfile", "get_line_pos_of_char_pos", "file", "templatedfile", "source_str", "source_str", "templated_str", "templated_str", "sliced_file", "file_slices", "fname", "test", "res_line_no", "res_line_pos", "file", "get_line_pos_of_char_pos", "in_charpos", "assert", "res_line_no", "out_line_no", "assert", "res_line_pos", "out_line_pos"], "doc_len": 37}
{"doc_id": "test/core/templaters/base_test.py::test__templated_file_find_slice_indices_of_templated_pos", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__templated_file_find_slice_indices_of_templated_pos", "text": "文件路径: test/core/templaters/base_test.py\ndef test__templated_file_find_slice_indices_of_templated_pos(\n    templated_position, inclusive, file_slices, sliced_idx_start, sliced_idx_stop\n):\n    \"\"\"Test TemplatedFile._find_slice_indices_of_templated_pos.\"\"\"\n    file = TemplatedFile(\n        source_str=\"Dummy String\", sliced_file=file_slices, fname=\"test\"\n    )\n    res_start, res_stop = file._find_slice_indices_of_templated_pos(\n        templated_position, inclusive=inclusive\n    )\n    assert res_start == sliced_idx_start\n    assert res_stop == sliced_idx_stop\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__templated_file_find_slice_indices_of_templated_pos", "templated_position", "inclusive", "file_slices", "sliced_idx_start", "sliced_idx_stop", "test", "templatedfile", "_find_slice_indices_of_templated_pos", "file", "templatedfile", "source_str", "dummy", "string", "sliced_file", "file_slices", "fname", "test", "res_start", "res_stop", "file", "_find_slice_indices_of_templated_pos", "templated_position", "inclusive", "inclusive", "assert", "res_start", "sliced_idx_start", "assert", "res_stop", "sliced_idx_stop"], "doc_len": 37}
{"doc_id": "test/core/templaters/base_test.py::test__templated_file_templated_slice_to_source_slice", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__templated_file_templated_slice_to_source_slice", "text": "文件路径: test/core/templaters/base_test.py\ndef test__templated_file_templated_slice_to_source_slice(\n    in_slice, out_slice, is_literal, file_slices, raw_slices\n):\n    \"\"\"Test TemplatedFile.templated_slice_to_source_slice.\"\"\"\n    file = TemplatedFile(\n        source_str=\"Dummy String\",\n        sliced_file=file_slices,\n        raw_sliced=[\n            rs if isinstance(rs, RawFileSlice) else RawFileSlice(*rs)\n            for rs in raw_slices\n        ],\n        fname=\"test\",\n    )\n    source_slice = file.templated_slice_to_source_slice(in_slice)\n    literal_test = file.is_source_slice_literal(source_slice)\n    assert (is_literal, source_slice) == (literal_test, out_slice)\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__templated_file_templated_slice_to_source_slice", "in_slice", "out_slice", "is_literal", "file_slices", "raw_slices", "test", "templatedfile", "templated_slice_to_source_slice", "file", "templatedfile", "source_str", "dummy", "string", "sliced_file", "file_slices", "raw_sliced", "rs", "if", "isinstance", "rs", "rawfileslice", "else", "rawfileslice", "rs", "for", "rs", "in", "raw_slices", "fname", "test", "source_slice", "file", "templated_slice_to_source_slice", "in_slice", "literal_test", "file", "is_source_slice_literal", "source_slice", "assert", "is_literal", "source_slice", "literal_test", "out_slice"], "doc_len": 50}
{"doc_id": "test/core/templaters/base_test.py::test__templated_file_source_only_slices", "file_path": "test/core/templaters/base_test.py", "class_name": null, "func_name": "test__templated_file_source_only_slices", "text": "文件路径: test/core/templaters/base_test.py\ndef test__templated_file_source_only_slices():\n    \"\"\"Test TemplatedFile.source_only_slices.\"\"\"\n    file = TemplatedFile(\n        source_str=\" Dummy String again \",  # NB: has length 20\n        fname=\"test\",\n        raw_sliced=[\n            RawFileSlice(\"a\" * 10, \"literal\", 0),\n            RawFileSlice(\"b\" * 7, \"comment\", 10),\n            RawFileSlice(\"a\" * 10, \"literal\", 17),\n        ],\n    )\n    assert file.source_only_slices() == [RawFileSlice(\"b\" * 7, \"comment\", 10)]\n", "tokens": ["test", "core", "templaters", "base_test", "py", "def", "test__templated_file_source_only_slices", "test", "templatedfile", "source_only_slices", "file", "templatedfile", "source_str", "dummy", "string", "again", "nb", "has", "length", "20", "fname", "test", "raw_sliced", "rawfileslice", "a", "10", "literal", "0", "rawfileslice", "b", "7", "comment", "10", "rawfileslice", "a", "10", "literal", "17", "assert", "file", "source_only_slices", "rawfileslice", "b", "7", "comment", "10"], "doc_len": 46}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja(instr, expected_outstr):\n    \"\"\"Test jinja templating and the treatment of whitespace.\"\"\"\n    t = JinjaTemplater(override_context=dict(blah=\"foo\", condition=\"a < 10\"))\n    outstr, _ = t.process(in_str=instr, fname=\"test\", config=FluffConfig())\n    assert str(outstr) == expected_outstr\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja", "instr", "expected_outstr", "test", "jinja", "templating", "and", "the", "treatment", "of", "whitespace", "t", "jinjatemplater", "override_context", "dict", "blah", "foo", "condition", "a", "10", "outstr", "_", "t", "process", "in_str", "instr", "fname", "test", "config", "fluffconfig", "assert", "str", "outstr", "expected_outstr"], "doc_len": 40}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_slices", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_slices", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_slices(case: RawTemplatedTestCase):\n    \"\"\"Test that Jinja templater slices raw and templated file correctly.\"\"\"\n    t = JinjaTemplater()\n    templated_file, _ = t.process(in_str=case.instr, fname=\"test\", config=FluffConfig())\n    assert templated_file.source_str == case.instr\n    assert templated_file.templated_str == case.templated_str\n    # Build and check the list of source strings referenced by \"sliced_file\".\n    actual_ts_source_list = [\n        case.instr[ts.source_slice] for ts in templated_file.sliced_file\n    ]\n    assert actual_ts_source_list == case.expected_templated_sliced__source_list\n\n    # Build and check the list of templated strings referenced by \"sliced_file\".\n    actual_ts_templated_list = [\n        templated_file.templated_str[ts.templated_slice]\n        for ts in templated_file.sliced_file\n    ]\n    assert actual_ts_templated_list == case.expected_templated_sliced__templated_list\n\n    # Build and check the list of source strings referenced by \"raw_sliced\".\n    previous_rs = None\n    actual_rs_source_list = []\n    for rs in templated_file.raw_sliced + [None]:\n        if previous_rs:\n            if rs:\n                actual_source = case.instr[previous_rs.source_idx : rs.source_idx]\n            else:\n                actual_source = case.instr[previous_rs.source_idx :]\n            actual_rs_source_list.append(actual_source)\n        previous_rs = rs\n    assert actual_rs_source_list == case.expected_raw_sliced__source_list\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_slices", "case", "rawtemplatedtestcase", "test", "that", "jinja", "templater", "slices", "raw", "and", "templated", "file", "correctly", "t", "jinjatemplater", "templated_file", "_", "t", "process", "in_str", "case", "instr", "fname", "test", "config", "fluffconfig", "assert", "templated_file", "source_str", "case", "instr", "assert", "templated_file", "templated_str", "case", "templated_str", "build", "and", "check", "the", "list", "of", "source", "strings", "referenced", "by", "sliced_file", "actual_ts_source_list", "case", "instr", "ts", "source_slice", "for", "ts", "in", "templated_file", "sliced_file", "assert", "actual_ts_source_list", "case", "expected_templated_sliced__source_list", "build", "and", "check", "the", "list", "of", "templated", "strings", "referenced", "by", "sliced_file", "actual_ts_templated_list", "templated_file", "templated_str", "ts", "templated_slice", "for", "ts", "in", "templated_file", "sliced_file", "assert", "actual_ts_templated_list", "case", "expected_templated_sliced__templated_list", "build", "and", "check", "the", "list", "of", "source", "strings", "referenced", "by", "raw_sliced", "previous_rs", "none", "actual_rs_source_list", "for", "rs", "in", "templated_file", "raw_sliced", "none", "if", "previous_rs", "if", "rs", "actual_source", "case", "instr", "previous_rs", "source_idx", "rs", "source_idx", "else", "actual_source", "case", "instr", "previous_rs", "source_idx", "actual_rs_source_list", "append", "actual_source", "previous_rs", "rs", "assert", "actual_rs_source_list", "case", "expected_raw_sliced__source_list"], "doc_len": 138}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_error_variable", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_error_variable", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_error_variable():\n    \"\"\"Test missing variable error handling in the jinja templater.\"\"\"\n    t = JinjaTemplater(override_context=dict(blah=\"foo\"))\n    instr = JINJA_STRING\n    outstr, vs = t.process(in_str=instr, fname=\"test\", config=FluffConfig())\n    assert str(outstr) == \"SELECT * FROM f, o, o WHERE \\n\\n\"\n    # Check we have violations.\n    assert len(vs) > 0\n    # Check one of them is a templating error on line 1\n    assert any(v.rule_code() == \"TMP\" and v.line_no == 1 for v in vs)\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_error_variable", "test", "missing", "variable", "error", "handling", "in", "the", "jinja", "templater", "t", "jinjatemplater", "override_context", "dict", "blah", "foo", "instr", "jinja_string", "outstr", "vs", "t", "process", "in_str", "instr", "fname", "test", "config", "fluffconfig", "assert", "str", "outstr", "select", "from", "f", "o", "o", "where", "n", "n", "check", "we", "have", "violations", "assert", "len", "vs", "0", "check", "one", "of", "them", "is", "a", "templating", "error", "on", "line", "1", "assert", "any", "v", "rule_code", "tmp", "and", "v", "line_no", "1", "for", "v", "in", "vs"], "doc_len": 77}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_error_syntax", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_error_syntax", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_error_syntax():\n    \"\"\"Test syntax problems in the jinja templater.\"\"\"\n    t = JinjaTemplater()\n    instr = \"SELECT {{foo} FROM jinja_error\\n\"\n    outstr, vs = t.process(in_str=instr, fname=\"test\", config=FluffConfig())\n    # Check we just skip templating.\n    assert str(outstr) == instr\n    # Check we have violations.\n    assert len(vs) > 0\n    # Check one of them is a templating error on line 1\n    assert any(v.rule_code() == \"TMP\" and v.line_no == 1 for v in vs)\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_error_syntax", "test", "syntax", "problems", "in", "the", "jinja", "templater", "t", "jinjatemplater", "instr", "select", "foo", "from", "jinja_error", "n", "outstr", "vs", "t", "process", "in_str", "instr", "fname", "test", "config", "fluffconfig", "check", "we", "just", "skip", "templating", "assert", "str", "outstr", "instr", "check", "we", "have", "violations", "assert", "len", "vs", "0", "check", "one", "of", "them", "is", "a", "templating", "error", "on", "line", "1", "assert", "any", "v", "rule_code", "tmp", "and", "v", "line_no", "1", "for", "v", "in", "vs"], "doc_len": 73}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_error_catatrophic", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_error_catatrophic", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_error_catatrophic():\n    \"\"\"Test error handling in the jinja templater.\"\"\"\n    t = JinjaTemplater(override_context=dict(blah=7))\n    instr = JINJA_STRING\n    outstr, vs = t.process(in_str=instr, fname=\"test\", config=FluffConfig())\n    assert not outstr\n    assert len(vs) > 0\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_error_catatrophic", "test", "error", "handling", "in", "the", "jinja", "templater", "t", "jinjatemplater", "override_context", "dict", "blah", "7", "instr", "jinja_string", "outstr", "vs", "t", "process", "in_str", "instr", "fname", "test", "config", "fluffconfig", "assert", "not", "outstr", "assert", "len", "vs", "0"], "doc_len": 39}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_lint_empty", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_lint_empty", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_lint_empty():\n    \"\"\"Check that parsing a file which renders to an empty string.\n\n    No exception should be raised, but the parsed tree should be None.\n    \"\"\"\n    lntr = Linter()\n    parsed = lntr.parse_string(in_str='{{ \"\" }}')\n    assert parsed.templated_file.source_str == '{{ \"\" }}'\n    assert parsed.templated_file.templated_str == \"\"\n    assert parsed.tree is None\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_lint_empty", "check", "that", "parsing", "a", "file", "which", "renders", "to", "an", "empty", "string", "no", "exception", "should", "be", "raised", "but", "the", "parsed", "tree", "should", "be", "none", "lntr", "linter", "parsed", "lntr", "parse_string", "in_str", "assert", "parsed", "templated_file", "source_str", "assert", "parsed", "templated_file", "templated_str", "assert", "parsed", "tree", "is", "none"], "doc_len": 49}
{"doc_id": "test/core/templaters/jinja_test.py::assert_structure", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "assert_structure", "text": "文件路径: test/core/templaters/jinja_test.py\ndef assert_structure(yaml_loader, path, code_only=True, include_meta=False):\n    \"\"\"Check that a parsed sql file matches the yaml file with the same name.\"\"\"\n    lntr = Linter()\n    p = list(lntr.parse_path(path + \".sql\"))\n    parsed = p[0][0]\n    if parsed is None:\n        print(p)\n        raise RuntimeError(p[0][1])\n    # Whitespace is important here to test how that's treated\n    tpl = parsed.to_tuple(code_only=code_only, show_raw=True, include_meta=include_meta)\n    # Check nothing unparsable\n    if \"unparsable\" in parsed.type_set():\n        print(parsed.stringify())\n        raise ValueError(\"Input file is contains unparsable.\")\n    _hash, expected = yaml_loader(path + \".yml\")\n    assert tpl == expected\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "assert_structure", "yaml_loader", "path", "code_only", "true", "include_meta", "false", "check", "that", "a", "parsed", "sql", "file", "matches", "the", "yaml", "file", "with", "the", "same", "name", "lntr", "linter", "p", "list", "lntr", "parse_path", "path", "sql", "parsed", "p", "0", "0", "if", "parsed", "is", "none", "print", "p", "raise", "runtimeerror", "p", "0", "1", "whitespace", "is", "important", "here", "to", "test", "how", "that", "s", "treated", "tpl", "parsed", "to_tuple", "code_only", "code_only", "show_raw", "true", "include_meta", "include_meta", "check", "nothing", "unparsable", "if", "unparsable", "in", "parsed", "type_set", "print", "parsed", "stringify", "raise", "valueerror", "input", "file", "is", "contains", "unparsable", "_hash", "expected", "yaml_loader", "path", "yml", "assert", "tpl", "expected"], "doc_len": 95}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_full", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_full", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_full(subpath, code_only, include_meta, yaml_loader, caplog):\n    \"\"\"Check structure can be parsed from jinja templated files.\"\"\"\n    # Log the templater and lexer throughout this test\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n\n    assert_structure(\n        yaml_loader,\n        \"test/fixtures/templater/\" + subpath,\n        code_only=code_only,\n        include_meta=include_meta,\n    )\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_full", "subpath", "code_only", "include_meta", "yaml_loader", "caplog", "check", "structure", "can", "be", "parsed", "from", "jinja", "templated", "files", "log", "the", "templater", "and", "lexer", "throughout", "this", "test", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "templater", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "lexer", "assert_structure", "yaml_loader", "test", "fixtures", "templater", "subpath", "code_only", "code_only", "include_meta", "include_meta"], "doc_len": 53}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_slice_template", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_slice_template", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_slice_template(test, result):\n    \"\"\"Test _slice_template.\"\"\"\n    resp = list(JinjaTemplater._slice_template(test))\n    # check contigious (unless there's a comment in it)\n    if \"{#\" not in test:\n        assert \"\".join(elem[0] for elem in resp) == test\n        # check indices\n        idx = 0\n        for literal, _, pos, _ in resp:\n            assert pos == idx\n            idx += len(literal)\n    # Check total result\n    assert [r[:3] for r in resp] == result\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_slice_template", "test", "result", "test", "_slice_template", "resp", "list", "jinjatemplater", "_slice_template", "test", "check", "contigious", "unless", "there", "s", "a", "comment", "in", "it", "if", "not", "in", "test", "assert", "join", "elem", "0", "for", "elem", "in", "resp", "test", "check", "indices", "idx", "0", "for", "literal", "_", "pos", "_", "in", "resp", "assert", "pos", "idx", "idx", "len", "literal", "check", "total", "result", "assert", "r", "3", "for", "r", "in", "resp", "result"], "doc_len": 66}
{"doc_id": "test/core/templaters/jinja_test.py::test__templater_jinja_slice_file", "file_path": "test/core/templaters/jinja_test.py", "class_name": null, "func_name": "test__templater_jinja_slice_file", "text": "文件路径: test/core/templaters/jinja_test.py\ndef test__templater_jinja_slice_file(raw_file, templated_file, result, caplog):\n    \"\"\"Test slice_file.\"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.templater\"):\n        _, resp, _ = JinjaTemplater.slice_file(\n            raw_file,\n            templated_file,\n        )\n    # Check contigious on the TEMPLATED VERSION\n    print(resp)\n    prev_slice = None\n    for elem in resp:\n        print(elem)\n        if prev_slice:\n            assert elem[2].start == prev_slice.stop\n        prev_slice = elem[2]\n    # Check that all literal segments have a raw slice\n    for elem in resp:\n        if elem[0] == \"literal\":\n            assert elem[1] is not None\n    # check result\n    assert resp == result\n", "tokens": ["test", "core", "templaters", "jinja_test", "py", "def", "test__templater_jinja_slice_file", "raw_file", "templated_file", "result", "caplog", "test", "slice_file", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "templater", "_", "resp", "_", "jinjatemplater", "slice_file", "raw_file", "templated_file", "check", "contigious", "on", "the", "templated", "version", "print", "resp", "prev_slice", "none", "for", "elem", "in", "resp", "print", "elem", "if", "prev_slice", "assert", "elem", "2", "start", "prev_slice", "stop", "prev_slice", "elem", "2", "check", "that", "all", "literal", "segments", "have", "a", "raw", "slice", "for", "elem", "in", "resp", "if", "elem", "0", "literal", "assert", "elem", "1", "is", "not", "none", "check", "result", "assert", "resp", "result"], "doc_len": 83}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python():\n    \"\"\"Test the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(blah=\"foo\"))\n    instr = PYTHON_STRING\n    outstr, _ = t.process(in_str=instr, fname=\"test\")\n    assert str(outstr) == \"SELECT * FROM foo\"\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python", "test", "the", "python", "templater", "t", "pythontemplater", "override_context", "dict", "blah", "foo", "instr", "python_string", "outstr", "_", "t", "process", "in_str", "instr", "fname", "test", "assert", "str", "outstr", "select", "from", "foo"], "doc_len": 33}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_error", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_error", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_error():\n    \"\"\"Test error handling in the python templater.\"\"\"\n    t = PythonTemplater(override_context=dict(noblah=\"foo\"))\n    instr = PYTHON_STRING\n    with pytest.raises(SQLTemplaterError):\n        t.process(in_str=instr, fname=\"test\")\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_error", "test", "error", "handling", "in", "the", "python", "templater", "t", "pythontemplater", "override_context", "dict", "noblah", "foo", "instr", "python_string", "with", "pytest", "raises", "sqltemplatererror", "t", "process", "in_str", "instr", "fname", "test"], "doc_len": 32}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_intermediate__trim", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_intermediate__trim", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_intermediate__trim(\n    int_slice, templated_str, head_test, tail_test, int_test\n):\n    \"\"\"Test trimming IntermediateFileSlice.\"\"\"\n    h, i, t = int_slice.trim_ends(templated_str=templated_str)\n    assert h == head_test\n    assert t == tail_test\n    assert i == int_test\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_intermediate__trim", "int_slice", "templated_str", "head_test", "tail_test", "int_test", "test", "trimming", "intermediatefileslice", "h", "i", "t", "int_slice", "trim_ends", "templated_str", "templated_str", "assert", "h", "head_test", "assert", "t", "tail_test", "assert", "i", "int_test"], "doc_len": 31}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_substring_occurrences", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_substring_occurrences", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_substring_occurrences(mainstr, substrings, positions):\n    \"\"\"Test _substring_occurrences.\"\"\"\n    occurrences = PythonTemplater._substring_occurrences(mainstr, substrings)\n    assert isinstance(occurrences, dict)\n    pos_test = [occurrences[substring] for substring in substrings]\n    assert pos_test == positions\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_substring_occurrences", "mainstr", "substrings", "positions", "test", "_substring_occurrences", "occurrences", "pythontemplater", "_substring_occurrences", "mainstr", "substrings", "assert", "isinstance", "occurrences", "dict", "pos_test", "occurrences", "substring", "for", "substring", "in", "substrings", "assert", "pos_test", "positions"], "doc_len": 31}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_sorted_occurrence_tuples", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_sorted_occurrence_tuples", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_sorted_occurrence_tuples(test, result):\n    \"\"\"Test _sorted_occurrence_tuples.\"\"\"\n    assert PythonTemplater._sorted_occurrence_tuples(test) == result\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_sorted_occurrence_tuples", "test", "result", "test", "_sorted_occurrence_tuples", "assert", "pythontemplater", "_sorted_occurrence_tuples", "test", "result"], "doc_len": 16}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_slice_template", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_slice_template", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_slice_template(test, result):\n    \"\"\"Test _slice_template.\"\"\"\n    resp = list(PythonTemplater._slice_template(test))\n    # check contigious\n    assert \"\".join(elem[0] for elem in resp) == test\n    # check indices\n    idx = 0\n    for literal, _, pos, _ in resp:\n        assert pos == idx\n        idx += len(literal)\n    # Check total result\n    assert resp == result\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_slice_template", "test", "result", "test", "_slice_template", "resp", "list", "pythontemplater", "_slice_template", "test", "check", "contigious", "assert", "join", "elem", "0", "for", "elem", "in", "resp", "test", "check", "indices", "idx", "0", "for", "literal", "_", "pos", "_", "in", "resp", "assert", "pos", "idx", "idx", "len", "literal", "check", "total", "result", "assert", "resp", "result"], "doc_len": 50}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_split_invariants", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_split_invariants", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_split_invariants(\n    raw_sliced,\n    literals,\n    raw_occurrences,\n    templated_occurrences,\n    templated_length,\n    result,\n):\n    \"\"\"Test _split_invariants.\"\"\"\n    resp = list(\n        PythonTemplater._split_invariants(\n            raw_sliced,\n            literals,\n            raw_occurrences,\n            templated_occurrences,\n            templated_length,\n        )\n    )\n    # check result\n    assert resp == result\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_split_invariants", "raw_sliced", "literals", "raw_occurrences", "templated_occurrences", "templated_length", "result", "test", "_split_invariants", "resp", "list", "pythontemplater", "_split_invariants", "raw_sliced", "literals", "raw_occurrences", "templated_occurrences", "templated_length", "check", "result", "assert", "resp", "result"], "doc_len": 29}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_split_uniques_coalesce_rest", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_split_uniques_coalesce_rest", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_split_uniques_coalesce_rest(\n    split_file, raw_occurrences, templated_occurrences, templated_str, result, caplog\n):\n    \"\"\"Test _split_uniques_coalesce_rest.\"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.templater\"):\n        resp = list(\n            PythonTemplater._split_uniques_coalesce_rest(\n                split_file,\n                raw_occurrences,\n                templated_occurrences,\n                templated_str,\n            )\n        )\n    # Check contigious\n    prev_slice = None\n    for elem in result:\n        if prev_slice:\n            assert elem[1].start == prev_slice[0].stop\n            assert elem[2].start == prev_slice[1].stop\n        prev_slice = (elem[1], elem[2])\n    # check result\n    assert resp == result\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_split_uniques_coalesce_rest", "split_file", "raw_occurrences", "templated_occurrences", "templated_str", "result", "caplog", "test", "_split_uniques_coalesce_rest", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "templater", "resp", "list", "pythontemplater", "_split_uniques_coalesce_rest", "split_file", "raw_occurrences", "templated_occurrences", "templated_str", "check", "contigious", "prev_slice", "none", "for", "elem", "in", "result", "if", "prev_slice", "assert", "elem", "1", "start", "prev_slice", "0", "stop", "assert", "elem", "2", "start", "prev_slice", "1", "stop", "prev_slice", "elem", "1", "elem", "2", "check", "result", "assert", "resp", "result"], "doc_len": 65}
{"doc_id": "test/core/templaters/python_test.py::test__templater_python_slice_file", "file_path": "test/core/templaters/python_test.py", "class_name": null, "func_name": "test__templater_python_slice_file", "text": "文件路径: test/core/templaters/python_test.py\ndef test__templater_python_slice_file(raw_file, templated_file, unwrap_wrapped, result):\n    \"\"\"Test slice_file.\"\"\"\n    _, resp, _ = PythonTemplater.slice_file(\n        raw_file,\n        templated_file,\n        config=FluffConfig(\n            configs={\"templater\": {\"unwrap_wrapped_queries\": unwrap_wrapped}}\n        ),\n    )\n    # Check contigious\n    prev_slice = None\n    for templated_slice in resp:\n        if prev_slice:\n            assert templated_slice.source_slice.start == prev_slice[0].stop\n            assert templated_slice.templated_slice.start == prev_slice[1].stop\n        prev_slice = (templated_slice.source_slice, templated_slice.templated_slice)\n    # check result\n    assert resp == result\n", "tokens": ["test", "core", "templaters", "python_test", "py", "def", "test__templater_python_slice_file", "raw_file", "templated_file", "unwrap_wrapped", "result", "test", "slice_file", "_", "resp", "_", "pythontemplater", "slice_file", "raw_file", "templated_file", "config", "fluffconfig", "configs", "templater", "unwrap_wrapped_queries", "unwrap_wrapped", "check", "contigious", "prev_slice", "none", "for", "templated_slice", "in", "resp", "if", "prev_slice", "assert", "templated_slice", "source_slice", "start", "prev_slice", "0", "stop", "assert", "templated_slice", "templated_slice", "start", "prev_slice", "1", "stop", "prev_slice", "templated_slice", "source_slice", "templated_slice", "templated_slice", "check", "result", "assert", "resp", "result"], "doc_len": 60}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi__file_lex", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi__file_lex", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi__file_lex(raw, res, caplog):\n    \"\"\"Test we don't drop bits on simple examples.\"\"\"\n    config = FluffConfig(overrides=dict(dialect=\"ansi\"))\n    lexer = Lexer(config=config)\n    with caplog.at_level(logging.DEBUG):\n        tokens, _ = lexer.lex(raw)\n    # From just the initial parse, check we're all there\n    raw_list = [token.raw for token in tokens]\n    assert \"\".join(token.raw for token in tokens) == raw\n    assert raw_list == res\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi__file_lex", "raw", "res", "caplog", "test", "we", "don", "t", "drop", "bits", "on", "simple", "examples", "config", "fluffconfig", "overrides", "dict", "dialect", "ansi", "lexer", "lexer", "config", "config", "with", "caplog", "at_level", "logging", "debug", "tokens", "_", "lexer", "lex", "raw", "from", "just", "the", "initial", "parse", "check", "we", "re", "all", "there", "raw_list", "token", "raw", "for", "token", "in", "tokens", "assert", "join", "token", "raw", "for", "token", "in", "tokens", "raw", "assert", "raw_list", "res"], "doc_len": 67}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_parses", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_specific_segment_parses", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_specific_segment_parses(\n    segmentref, raw, caplog, dialect_specific_segment_parses\n):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    dialect_specific_segment_parses(\"ansi\", segmentref, raw, caplog)\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_specific_segment_parses", "segmentref", "raw", "caplog", "dialect_specific_segment_parses", "test", "that", "specific", "segments", "parse", "as", "expected", "nb", "we", "re", "testing", "the", "parse", "function", "not", "the", "match", "function", "although", "this", "will", "be", "a", "recursive", "parse", "and", "so", "the", "match", "function", "of", "subsections", "will", "be", "tested", "if", "present", "the", "match", "function", "of", "the", "parent", "will", "not", "be", "tested", "dialect_specific_segment_parses", "ansi", "segmentref", "raw", "caplog"], "doc_len": 62}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_not_match", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_specific_segment_not_match", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_specific_segment_not_match(\n    segmentref, raw, caplog, dialect_specific_segment_not_match\n):\n    \"\"\"Test that specific segments do not match.\n\n    NB: We're testing the MATCH function not the PARSE function.\n    This is the opposite to the above.\n    \"\"\"\n    dialect_specific_segment_not_match(\"ansi\", segmentref, raw, caplog)\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_specific_segment_not_match", "segmentref", "raw", "caplog", "dialect_specific_segment_not_match", "test", "that", "specific", "segments", "do", "not", "match", "nb", "we", "re", "testing", "the", "match", "function", "not", "the", "parse", "function", "this", "is", "the", "opposite", "to", "the", "above", "dialect_specific_segment_not_match", "ansi", "segmentref", "raw", "caplog"], "doc_len": 40}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_specific_segment_not_parse", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_specific_segment_not_parse", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_specific_segment_not_parse(raw, err_locations, caplog):\n    \"\"\"Test queries do not parse, with parsing errors raised properly.\"\"\"\n    lnt = Linter()\n    parsed = lnt.parse_string(raw)\n    assert len(parsed.violations) > 0\n    print(parsed.violations)\n    locs = [(v.line_no, v.line_pos) for v in parsed.violations]\n    assert locs == err_locations\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_specific_segment_not_parse", "raw", "err_locations", "caplog", "test", "queries", "do", "not", "parse", "with", "parsing", "errors", "raised", "properly", "lnt", "linter", "parsed", "lnt", "parse_string", "raw", "assert", "len", "parsed", "violations", "0", "print", "parsed", "violations", "locs", "v", "line_no", "v", "line_pos", "for", "v", "in", "parsed", "violations", "assert", "locs", "err_locations"], "doc_len": 46}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_is_whitespace", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_is_whitespace", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_is_whitespace():\n    \"\"\"Test proper tagging with is_whitespace.\"\"\"\n    lnt = Linter()\n    with open(\"test/fixtures/dialects/ansi/select_in_multiline_comment.sql\") as f:\n        parsed = lnt.parse_string(f.read())\n    # Check all the segments that *should* be whitespace, ARE\n    for raw_seg in parsed.tree.iter_raw_seg():\n        if raw_seg.is_type(\"whitespace\", \"newline\"):\n            assert raw_seg.is_whitespace\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_is_whitespace", "test", "proper", "tagging", "with", "is_whitespace", "lnt", "linter", "with", "open", "test", "fixtures", "dialects", "ansi", "select_in_multiline_comment", "sql", "as", "f", "parsed", "lnt", "parse_string", "f", "read", "check", "all", "the", "segments", "that", "should", "be", "whitespace", "are", "for", "raw_seg", "in", "parsed", "tree", "iter_raw_seg", "if", "raw_seg", "is_type", "whitespace", "newline", "assert", "raw_seg", "is_whitespace"], "doc_len": 51}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_parse_indented_joins", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_parse_indented_joins", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_parse_indented_joins(sql_string, indented_joins, meta_loc):\n    \"\"\"Test parsing of meta segments using Conditional works with indented_joins.\"\"\"\n    lnt = Linter(\n        config=FluffConfig(configs={\"indentation\": {\"indented_joins\": indented_joins}})\n    )\n    parsed = lnt.parse_string(sql_string)\n    # Check that there's nothing unparsable\n    assert \"unparsable\" not in parsed.tree.type_set()\n    # Check all the segments that *should* be whitespace, ARE\n    res_meta_locs = tuple(\n        idx for idx, raw_seg in enumerate(parsed.tree.iter_raw_seg()) if raw_seg.is_meta\n    )\n    assert res_meta_locs == meta_loc\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_parse_indented_joins", "sql_string", "indented_joins", "meta_loc", "test", "parsing", "of", "meta", "segments", "using", "conditional", "works", "with", "indented_joins", "lnt", "linter", "config", "fluffconfig", "configs", "indentation", "indented_joins", "indented_joins", "parsed", "lnt", "parse_string", "sql_string", "check", "that", "there", "s", "nothing", "unparsable", "assert", "unparsable", "not", "in", "parsed", "tree", "type_set", "check", "all", "the", "segments", "that", "should", "be", "whitespace", "are", "res_meta_locs", "tuple", "idx", "for", "idx", "raw_seg", "in", "enumerate", "parsed", "tree", "iter_raw_seg", "if", "raw_seg", "is_meta", "assert", "res_meta_locs", "meta_loc"], "doc_len": 70}
{"doc_id": "test/dialects/ansi_test.py::test__dialect__ansi_multiple_semicolons", "file_path": "test/dialects/ansi_test.py", "class_name": null, "func_name": "test__dialect__ansi_multiple_semicolons", "text": "文件路径: test/dialects/ansi_test.py\ndef test__dialect__ansi_multiple_semicolons(raw: str, expected_message: str) -> None:\n    \"\"\"Multiple semicolons should be properly handled.\"\"\"\n    lnt = Linter()\n    parsed = lnt.parse_string(raw)\n\n    assert len(parsed.violations) == (1 if expected_message else 0)\n    if expected_message:\n        violation = parsed.violations[0]\n        assert isinstance(violation, SQLParseError)\n        assert violation.desc() == expected_message\n", "tokens": ["test", "dialects", "ansi_test", "py", "def", "test__dialect__ansi_multiple_semicolons", "raw", "str", "expected_message", "str", "none", "multiple", "semicolons", "should", "be", "properly", "handled", "lnt", "linter", "parsed", "lnt", "parse_string", "raw", "assert", "len", "parsed", "violations", "1", "if", "expected_message", "else", "0", "if", "expected_message", "violation", "parsed", "violations", "0", "assert", "isinstance", "violation", "sqlparseerror", "assert", "violation", "desc", "expected_message"], "doc_len": 46}
{"doc_id": "test/dialects/bigquery_test.py::test_bigquery_relational_operator_parsing", "file_path": "test/dialects/bigquery_test.py", "class_name": null, "func_name": "test_bigquery_relational_operator_parsing", "text": "文件路径: test/dialects/bigquery_test.py\ndef test_bigquery_relational_operator_parsing(data):\n    \"\"\"Tests queries with a diverse mixture of relational operators.\"\"\"\n    # Generate a simple SELECT query with relational operators and conjunctions\n    # as specified in 'data'. Note the conjunctions are used as separators\n    # between comparisons, sn the conjunction in the first item is not used.\n    filter = []\n    for i, (relation, conjunction) in enumerate(data):\n        if i:\n            filter.append(f\" {conjunction} \")\n        filter.append(f\"a {relation} b\")\n    raw = f'SELECT * FROM t WHERE {\"\".join(filter)}'\n    note(f\"query: {raw}\")\n    # Load the right dialect\n    config = FluffConfig(overrides=dict(dialect=\"bigquery\"))\n    tokens, lex_vs = Lexer(config=config).lex(raw)\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in tokens) == raw\n    # Check we don't have lexing issues\n    assert not lex_vs\n\n    # Do the parse WITHOUT lots of logging\n    # The logs get too long here to be useful. We should use\n    # specfic segment tests if we want to debug logs.\n    parsed = Parser(config=config).parse(tokens)\n    print(f\"Post-parse structure: {parsed.to_tuple(show_raw=True)}\")\n    print(f\"Post-parse structure: {parsed.stringify()}\")\n    # Check we're all there.\n    assert parsed.raw == raw\n    # Check that there's nothing un parsable\n    typs = parsed.type_set()\n    assert \"unparsable\" not in typs\n", "tokens": ["test", "dialects", "bigquery_test", "py", "def", "test_bigquery_relational_operator_parsing", "data", "tests", "queries", "with", "a", "diverse", "mixture", "of", "relational", "operators", "generate", "a", "simple", "select", "query", "with", "relational", "operators", "and", "conjunctions", "as", "specified", "in", "data", "note", "the", "conjunctions", "are", "used", "as", "separators", "between", "comparisons", "sn", "the", "conjunction", "in", "the", "first", "item", "is", "not", "used", "filter", "for", "i", "relation", "conjunction", "in", "enumerate", "data", "if", "i", "filter", "append", "f", "conjunction", "filter", "append", "f", "a", "relation", "b", "raw", "f", "select", "from", "t", "where", "join", "filter", "note", "f", "query", "raw", "load", "the", "right", "dialect", "config", "fluffconfig", "overrides", "dict", "dialect", "bigquery", "tokens", "lex_vs", "lexer", "config", "config", "lex", "raw", "from", "just", "the", "initial", "parse", "check", "we", "re", "all", "there", "assert", "join", "token", "raw", "for", "token", "in", "tokens", "raw", "check", "we", "don", "t", "have", "lexing", "issues", "assert", "not", "lex_vs", "do", "the", "parse", "without", "lots", "of", "logging", "the", "logs", "get", "too", "long", "here", "to", "be", "useful", "we", "should", "use", "specfic", "segment", "tests", "if", "we", "want", "to", "debug", "logs", "parsed", "parser", "config", "config", "parse", "tokens", "print", "f", "post", "parse", "structure", "parsed", "to_tuple", "show_raw", "true", "print", "f", "post", "parse", "structure", "parsed", "stringify", "check", "we", "re", "all", "there", "assert", "parsed", "raw", "raw", "check", "that", "there", "s", "nothing", "un", "parsable", "typs", "parsed", "type_set", "assert", "unparsable", "not", "in", "typs"], "doc_len": 201}
{"doc_id": "test/dialects/conftest.py::lex", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "lex", "text": "文件路径: test/dialects/conftest.py\ndef lex(raw, config):\n    \"\"\"Basic parsing for the tests below.\"\"\"\n    # Set up the lexer\n    lex = Lexer(config=config)\n    # Lex the string for matching. For a good test, this would\n    # arguably happen as a fixture, but it's easier to pass strings\n    # as parameters than pre-lexed segment strings.\n    seg_list, vs = lex.lex(raw)\n    assert not vs\n    print(seg_list)\n    return seg_list\n", "tokens": ["test", "dialects", "conftest", "py", "def", "lex", "raw", "config", "basic", "parsing", "for", "the", "tests", "below", "set", "up", "the", "lexer", "lex", "lexer", "config", "config", "lex", "the", "string", "for", "matching", "for", "a", "good", "test", "this", "would", "arguably", "happen", "as", "a", "fixture", "but", "it", "s", "easier", "to", "pass", "strings", "as", "parameters", "than", "pre", "lexed", "segment", "strings", "seg_list", "vs", "lex", "lex", "raw", "assert", "not", "vs", "print", "seg_list", "return", "seg_list"], "doc_len": 64}
{"doc_id": "test/dialects/conftest.py::validate_segment", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "validate_segment", "text": "文件路径: test/dialects/conftest.py\ndef validate_segment(segmentref, config):\n    \"\"\"Get and validate segment for tests below.\"\"\"\n    Seg = config.get(\"dialect_obj\").ref(segmentref)\n    if isinstance(Seg, StringParser):\n        return Seg\n    try:\n        if issubclass(Seg, BaseSegment):\n            return Seg\n    except TypeError:\n        pass\n    raise TypeError(\n        \"{} is not of type Segment or StringParser. Test is invalid.\".format(segmentref)\n    )\n", "tokens": ["test", "dialects", "conftest", "py", "def", "validate_segment", "segmentref", "config", "get", "and", "validate", "segment", "for", "tests", "below", "seg", "config", "get", "dialect_obj", "ref", "segmentref", "if", "isinstance", "seg", "stringparser", "return", "seg", "try", "if", "issubclass", "seg", "basesegment", "return", "seg", "except", "typeerror", "pass", "raise", "typeerror", "is", "not", "of", "type", "segment", "or", "stringparser", "test", "is", "invalid", "format", "segmentref"], "doc_len": 51}
{"doc_id": "test/dialects/conftest.py::_dialect_specific_segment_parses", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "_dialect_specific_segment_parses", "text": "文件路径: test/dialects/conftest.py\ndef _dialect_specific_segment_parses(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    seg_list = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    # This test is different if we're working with RawSegment\n    # derivatives or not.\n    if isinstance(Seg, StringParser) or issubclass(Seg, RawSegment):\n        print(\"Raw/Parser route...\")\n        with RootParseContext.from_config(config) as ctx:\n            with caplog.at_level(logging.DEBUG):\n                parsed = Seg.match(segments=seg_list, parse_context=ctx)\n        assert isinstance(parsed, MatchResult)\n        assert len(parsed.matched_segments) == 1\n        print(parsed)\n        parsed = parsed.matched_segments[0]\n        print(parsed)\n    else:\n        print(\"Base route...\")\n        # Construct an unparsed segment\n        seg = Seg(seg_list, pos_marker=seg_list[0].pos_marker)\n        # Perform the match (THIS IS THE MEAT OF THE TEST)\n        with RootParseContext.from_config(config) as ctx:\n            with caplog.at_level(logging.DEBUG):\n                parsed = seg.parse(parse_context=ctx)\n        print(parsed)\n        assert isinstance(parsed, Seg)\n\n    # Check we get a good response\n    print(parsed)\n    print(type(parsed))\n    # print(type(parsed._reconstruct()))\n    print(type(parsed.raw))\n    # Check we're all there.\n    assert parsed.raw == raw\n    # Check that there's nothing un parsable\n    typs = parsed.type_set()\n    assert \"unparsable\" not in typs\n", "tokens": ["test", "dialects", "conftest", "py", "def", "_dialect_specific_segment_parses", "dialect", "segmentref", "raw", "caplog", "test", "that", "specific", "segments", "parse", "as", "expected", "nb", "we", "re", "testing", "the", "parse", "function", "not", "the", "match", "function", "although", "this", "will", "be", "a", "recursive", "parse", "and", "so", "the", "match", "function", "of", "subsections", "will", "be", "tested", "if", "present", "the", "match", "function", "of", "the", "parent", "will", "not", "be", "tested", "config", "fluffconfig", "overrides", "dict", "dialect", "dialect", "seg_list", "lex", "raw", "config", "config", "seg", "validate_segment", "segmentref", "config", "config", "this", "test", "is", "different", "if", "we", "re", "working", "with", "rawsegment", "derivatives", "or", "not", "if", "isinstance", "seg", "stringparser", "or", "issubclass", "seg", "rawsegment", "print", "raw", "parser", "route", "with", "rootparsecontext", "from_config", "config", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "parsed", "seg", "match", "segments", "seg_list", "parse_context", "ctx", "assert", "isinstance", "parsed", "matchresult", "assert", "len", "parsed", "matched_segments", "1", "print", "parsed", "parsed", "parsed", "matched_segments", "0", "print", "parsed", "else", "print", "base", "route", "construct", "an", "unparsed", "segment", "seg", "seg", "seg_list", "pos_marker", "seg_list", "0", "pos_marker", "perform", "the", "match", "this", "is", "the", "meat", "of", "the", "test", "with", "rootparsecontext", "from_config", "config", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "parsed", "seg", "parse", "parse_context", "ctx", "print", "parsed", "assert", "isinstance", "parsed", "seg", "check", "we", "get", "a", "good", "response", "print", "parsed", "print", "type", "parsed", "print", "type", "parsed", "_reconstruct", "print", "type", "parsed", "raw", "check", "we", "re", "all", "there", "assert", "parsed", "raw", "raw", "check", "that", "there", "s", "nothing", "un", "parsable", "typs", "parsed", "type_set", "assert", "unparsable", "not", "in", "typs"], "doc_len": 223}
{"doc_id": "test/dialects/conftest.py::_dialect_specific_segment_not_match", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "_dialect_specific_segment_not_match", "text": "文件路径: test/dialects/conftest.py\ndef _dialect_specific_segment_not_match(dialect, segmentref, raw, caplog):\n    \"\"\"Test that specific segments do not match.\n\n    NB: We're testing the MATCH function not the PARSE function.\n    This is the opposite to the above.\n    \"\"\"\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    seg_list = lex(raw, config=config)\n    Seg = validate_segment(segmentref, config=config)\n\n    with RootParseContext.from_config(config) as ctx:\n        with caplog.at_level(logging.DEBUG):\n            match = Seg.match(segments=seg_list, parse_context=ctx)\n\n    assert not match\n", "tokens": ["test", "dialects", "conftest", "py", "def", "_dialect_specific_segment_not_match", "dialect", "segmentref", "raw", "caplog", "test", "that", "specific", "segments", "do", "not", "match", "nb", "we", "re", "testing", "the", "match", "function", "not", "the", "parse", "function", "this", "is", "the", "opposite", "to", "the", "above", "config", "fluffconfig", "overrides", "dict", "dialect", "dialect", "seg_list", "lex", "raw", "config", "config", "seg", "validate_segment", "segmentref", "config", "config", "with", "rootparsecontext", "from_config", "config", "as", "ctx", "with", "caplog", "at_level", "logging", "debug", "match", "seg", "match", "segments", "seg_list", "parse_context", "ctx", "assert", "not", "match"], "doc_len": 72}
{"doc_id": "test/dialects/conftest.py::_validate_dialect_specific_statements", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "_validate_dialect_specific_statements", "text": "文件路径: test/dialects/conftest.py\ndef _validate_dialect_specific_statements(dialect, segment_cls, raw, stmt_count):\n    \"\"\"This validates one or multiple statements against specified segment class.\n\n    It even validates the number of parsed statements with the number of expected statements.\n    \"\"\"\n    lnt = Linter(dialect=dialect)\n    parsed = lnt.parse_string(raw)\n    assert len(parsed.violations) == 0\n\n    # Find any unparsable statements\n    typs = parsed.tree.type_set()\n    assert \"unparsable\" not in typs\n\n    # Find the expected type in the parsed segment\n    child_segments = [seg for seg in parsed.tree.recursive_crawl(segment_cls.type)]\n    assert len(child_segments) == stmt_count\n\n    # Check if all child segments are the correct type\n    for c in child_segments:\n        assert isinstance(c, segment_cls)\n", "tokens": ["test", "dialects", "conftest", "py", "def", "_validate_dialect_specific_statements", "dialect", "segment_cls", "raw", "stmt_count", "this", "validates", "one", "or", "multiple", "statements", "against", "specified", "segment", "class", "it", "even", "validates", "the", "number", "of", "parsed", "statements", "with", "the", "number", "of", "expected", "statements", "lnt", "linter", "dialect", "dialect", "parsed", "lnt", "parse_string", "raw", "assert", "len", "parsed", "violations", "0", "find", "any", "unparsable", "statements", "typs", "parsed", "tree", "type_set", "assert", "unparsable", "not", "in", "typs", "find", "the", "expected", "type", "in", "the", "parsed", "segment", "child_segments", "seg", "for", "seg", "in", "parsed", "tree", "recursive_crawl", "segment_cls", "type", "assert", "len", "child_segments", "stmt_count", "check", "if", "all", "child", "segments", "are", "the", "correct", "type", "for", "c", "in", "child_segments", "assert", "isinstance", "c", "segment_cls"], "doc_len": 99}
{"doc_id": "test/dialects/conftest.py::dialect_specific_segment_parses", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "dialect_specific_segment_parses", "text": "文件路径: test/dialects/conftest.py\ndef dialect_specific_segment_parses():\n    \"\"\"Fixture to check specific segments of a dialect.\"\"\"\n    return _dialect_specific_segment_parses\n", "tokens": ["test", "dialects", "conftest", "py", "def", "dialect_specific_segment_parses", "fixture", "to", "check", "specific", "segments", "of", "a", "dialect", "return", "_dialect_specific_segment_parses"], "doc_len": 16}
{"doc_id": "test/dialects/conftest.py::dialect_specific_segment_not_match", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "dialect_specific_segment_not_match", "text": "文件路径: test/dialects/conftest.py\ndef dialect_specific_segment_not_match():\n    \"\"\"Fixture to check specific segments of a dialect which will not match to a segment.\"\"\"\n    return _dialect_specific_segment_not_match\n", "tokens": ["test", "dialects", "conftest", "py", "def", "dialect_specific_segment_not_match", "fixture", "to", "check", "specific", "segments", "of", "a", "dialect", "which", "will", "not", "match", "to", "a", "segment", "return", "_dialect_specific_segment_not_match"], "doc_len": 23}
{"doc_id": "test/dialects/conftest.py::validate_dialect_specific_statements", "file_path": "test/dialects/conftest.py", "class_name": null, "func_name": "validate_dialect_specific_statements", "text": "文件路径: test/dialects/conftest.py\ndef validate_dialect_specific_statements():\n    \"\"\"This validates one or multiple statements against specified segment class.\n\n    It even validates the number of parsed statements with the number of expected statements.\n    \"\"\"\n    return _validate_dialect_specific_statements\n", "tokens": ["test", "dialects", "conftest", "py", "def", "validate_dialect_specific_statements", "this", "validates", "one", "or", "multiple", "statements", "against", "specified", "segment", "class", "it", "even", "validates", "the", "number", "of", "parsed", "statements", "with", "the", "number", "of", "expected", "statements", "return", "_validate_dialect_specific_statements"], "doc_len": 32}
{"doc_id": "test/dialects/dialects_test.py::test__dialect__base_file_parse", "file_path": "test/dialects/dialects_test.py", "class_name": null, "func_name": "test__dialect__base_file_parse", "text": "文件路径: test/dialects/dialects_test.py\ndef test__dialect__base_file_parse(dialect, file):\n    \"\"\"For given test examples, check successful parsing.\"\"\"\n    raw = load_file(dialect, file)\n    # Load the right dialect\n    config = FluffConfig(overrides=dict(dialect=dialect))\n    tokens, lex_vs = Lexer(config=config).lex(raw)\n    # From just the initial parse, check we're all there\n    assert \"\".join(token.raw for token in tokens) == raw\n    # Check we don't have lexing issues\n    assert not lex_vs\n\n    # Do the parse WITHOUT lots of logging\n    # The logs get too long here to be useful. We should use\n    # specific segment tests if we want to debug logs.\n    if raw:\n        parsed = Parser(config=config).parse(tokens)\n        print(f\"Post-parse structure: {parsed.to_tuple(show_raw=True)}\")\n        print(f\"Post-parse structure: {parsed.stringify()}\")\n        # Check we're all there.\n        assert parsed.raw == raw\n        # Check that there's nothing unparsable\n        typs = parsed.type_set()\n        assert \"unparsable\" not in typs\n", "tokens": ["test", "dialects", "dialects_test", "py", "def", "test__dialect__base_file_parse", "dialect", "file", "for", "given", "test", "examples", "check", "successful", "parsing", "raw", "load_file", "dialect", "file", "load", "the", "right", "dialect", "config", "fluffconfig", "overrides", "dict", "dialect", "dialect", "tokens", "lex_vs", "lexer", "config", "config", "lex", "raw", "from", "just", "the", "initial", "parse", "check", "we", "re", "all", "there", "assert", "join", "token", "raw", "for", "token", "in", "tokens", "raw", "check", "we", "don", "t", "have", "lexing", "issues", "assert", "not", "lex_vs", "do", "the", "parse", "without", "lots", "of", "logging", "the", "logs", "get", "too", "long", "here", "to", "be", "useful", "we", "should", "use", "specific", "segment", "tests", "if", "we", "want", "to", "debug", "logs", "if", "raw", "parsed", "parser", "config", "config", "parse", "tokens", "print", "f", "post", "parse", "structure", "parsed", "to_tuple", "show_raw", "true", "print", "f", "post", "parse", "structure", "parsed", "stringify", "check", "we", "re", "all", "there", "assert", "parsed", "raw", "raw", "check", "that", "there", "s", "nothing", "unparsable", "typs", "parsed", "type_set", "assert", "unparsable", "not", "in", "typs"], "doc_len": 140}
{"doc_id": "test/dialects/dialects_test.py::test__dialect__base_parse_struct", "file_path": "test/dialects/dialects_test.py", "class_name": null, "func_name": "test__dialect__base_parse_struct", "text": "文件路径: test/dialects/dialects_test.py\ndef test__dialect__base_parse_struct(\n    dialect, sqlfile, code_only, yamlfile, yaml_loader\n):\n    \"\"\"For given test examples, check parsed structure against yaml.\"\"\"\n    parsed = parse_example_file(dialect, sqlfile)\n    actual_hash = compute_parse_tree_hash(parsed)\n    # Load the YAML\n    expected_hash, res = yaml_loader(make_dialect_path(dialect, yamlfile))\n    if parsed:\n        # Verify the current parse tree matches the historic parse tree.\n        parsed_tree = parsed.to_tuple(code_only=code_only, show_raw=True)\n        # The prased tree consists of a tuple of \"File:\", followed by the\n        # statements. So only compare when there is at least one statement.\n        if parsed_tree[1] or res[1]:\n            assert parsed_tree == res\n        # Verify the current hash matches the historic hash. The main purpose of\n        # this check is to force contributors to use the generator script to\n        # to create these files. New contributors have sometimes been unaware of\n        # this tool and have attempted to craft the YAML files manually. This\n        # can lead to slight differences, confusion, and errors.\n        assert expected_hash == actual_hash, (\n            \"Parse tree hash does not match. Please run \"\n            \"'python test/generate_parse_fixture_yml.py' to create YAML files \"\n            \"in test/fixtures/dialects.\"\n        )\n    else:\n        assert parsed == res\n", "tokens": ["test", "dialects", "dialects_test", "py", "def", "test__dialect__base_parse_struct", "dialect", "sqlfile", "code_only", "yamlfile", "yaml_loader", "for", "given", "test", "examples", "check", "parsed", "structure", "against", "yaml", "parsed", "parse_example_file", "dialect", "sqlfile", "actual_hash", "compute_parse_tree_hash", "parsed", "load", "the", "yaml", "expected_hash", "res", "yaml_loader", "make_dialect_path", "dialect", "yamlfile", "if", "parsed", "verify", "the", "current", "parse", "tree", "matches", "the", "historic", "parse", "tree", "parsed_tree", "parsed", "to_tuple", "code_only", "code_only", "show_raw", "true", "the", "prased", "tree", "consists", "of", "a", "tuple", "of", "file", "followed", "by", "the", "statements", "so", "only", "compare", "when", "there", "is", "at", "least", "one", "statement", "if", "parsed_tree", "1", "or", "res", "1", "assert", "parsed_tree", "res", "verify", "the", "current", "hash", "matches", "the", "historic", "hash", "the", "main", "purpose", "of", "this", "check", "is", "to", "force", "contributors", "to", "use", "the", "generator", "script", "to", "to", "create", "these", "files", "new", "contributors", "have", "sometimes", "been", "unaware", "of", "this", "tool", "and", "have", "attempted", "to", "craft", "the", "yaml", "files", "manually", "this", "can", "lead", "to", "slight", "differences", "confusion", "and", "errors", "assert", "expected_hash", "actual_hash", "parse", "tree", "hash", "does", "not", "match", "please", "run", "python", "test", "generate_parse_fixture_yml", "py", "to", "create", "yaml", "files", "in", "test", "fixtures", "dialects", "else", "assert", "parsed", "res"], "doc_len": 169}
{"doc_id": "test/dialects/exasol_test.py::test_dialect_exasol_specific_segment_parses", "file_path": "test/dialects/exasol_test.py", "class_name": null, "func_name": "test_dialect_exasol_specific_segment_parses", "text": "文件路径: test/dialects/exasol_test.py\ndef test_dialect_exasol_specific_segment_parses(\n    segmentref, raw, caplog, dialect_specific_segment_parses\n):\n    \"\"\"Test exasol specific segments.\"\"\"\n    dialect_specific_segment_parses(TEST_DIALECT, segmentref, raw, caplog)\n", "tokens": ["test", "dialects", "exasol_test", "py", "def", "test_dialect_exasol_specific_segment_parses", "segmentref", "raw", "caplog", "dialect_specific_segment_parses", "test", "exasol", "specific", "segments", "dialect_specific_segment_parses", "test_dialect", "segmentref", "raw", "caplog"], "doc_len": 19}
{"doc_id": "test/dialects/postgres_test.py::test_dialect_postgres_specific_segment_parses", "file_path": "test/dialects/postgres_test.py", "class_name": null, "func_name": "test_dialect_postgres_specific_segment_parses", "text": "文件路径: test/dialects/postgres_test.py\ndef test_dialect_postgres_specific_segment_parses(\n    segment_reference: str,\n    raw: str,\n    caplog: LogCaptureFixture,\n    dialect_specific_segment_parses: Callable,\n) -> None:\n    \"\"\"Test that specific segments parse as expected.\n\n    NB: We're testing the PARSE function not the MATCH function\n    although this will be a recursive parse and so the match\n    function of SUBSECTIONS will be tested if present. The match\n    function of the parent will not be tested.\n    \"\"\"\n    dialect_specific_segment_parses(\"postgres\", segment_reference, raw, caplog)\n", "tokens": ["test", "dialects", "postgres_test", "py", "def", "test_dialect_postgres_specific_segment_parses", "segment_reference", "str", "raw", "str", "caplog", "logcapturefixture", "dialect_specific_segment_parses", "callable", "none", "test", "that", "specific", "segments", "parse", "as", "expected", "nb", "we", "re", "testing", "the", "parse", "function", "not", "the", "match", "function", "although", "this", "will", "be", "a", "recursive", "parse", "and", "so", "the", "match", "function", "of", "subsections", "will", "be", "tested", "if", "present", "the", "match", "function", "of", "the", "parent", "will", "not", "be", "tested", "dialect_specific_segment_parses", "postgres", "segment_reference", "raw", "caplog"], "doc_len": 67}
{"doc_id": "test/dialects/postgres_test.py::test_epoch_datetime_unit", "file_path": "test/dialects/postgres_test.py", "class_name": null, "func_name": "test_epoch_datetime_unit", "text": "文件路径: test/dialects/postgres_test.py\ndef test_epoch_datetime_unit(raw: str) -> None:\n    \"\"\"Test the EPOCH keyword for postgres dialect.\"\"\"\n    # Don't test for new lines or capitalisation\n    cfg = FluffConfig(\n        configs={\"core\": {\"exclude_rules\": \"L009,L016,L036\", \"dialect\": \"postgres\"}}\n    )\n    lnt = Linter(config=cfg)\n    result = lnt.lint_string(raw)\n    assert result.num_violations() == 0\n", "tokens": ["test", "dialects", "postgres_test", "py", "def", "test_epoch_datetime_unit", "raw", "str", "none", "test", "the", "epoch", "keyword", "for", "postgres", "dialect", "don", "t", "test", "for", "new", "lines", "or", "capitalisation", "cfg", "fluffconfig", "configs", "core", "exclude_rules", "l009", "l016", "l036", "dialect", "postgres", "lnt", "linter", "config", "cfg", "result", "lnt", "lint_string", "raw", "assert", "result", "num_violations", "0"], "doc_len": 46}
{"doc_id": "test/dialects/postgres_test.py::test_space_is_not_reserved", "file_path": "test/dialects/postgres_test.py", "class_name": null, "func_name": "test_space_is_not_reserved", "text": "文件路径: test/dialects/postgres_test.py\ndef test_space_is_not_reserved(raw: str) -> None:\n    \"\"\"Ensure that SPACE is not treated as reserved.\"\"\"\n    cfg = FluffConfig(\n        configs={\"core\": {\"exclude_rules\": \"L009,L016,L031\", \"dialect\": \"postgres\"}}\n    )\n    lnt = Linter(config=cfg)\n    result = lnt.lint_string(raw)\n    assert result.num_violations() == 0\n", "tokens": ["test", "dialects", "postgres_test", "py", "def", "test_space_is_not_reserved", "raw", "str", "none", "ensure", "that", "space", "is", "not", "treated", "as", "reserved", "cfg", "fluffconfig", "configs", "core", "exclude_rules", "l009", "l016", "l031", "dialect", "postgres", "lnt", "linter", "config", "cfg", "result", "lnt", "lint_string", "raw", "assert", "result", "num_violations", "0"], "doc_len": 39}
{"doc_id": "test/dialects/postgres_test.py::test_priority_keyword_merge", "file_path": "test/dialects/postgres_test.py", "class_name": null, "func_name": "test_priority_keyword_merge", "text": "文件路径: test/dialects/postgres_test.py\ndef test_priority_keyword_merge() -> None:\n    \"\"\"Test merging on keyword lists works as expected.\"\"\"\n    kw_list_1 = [(\"A\", \"not-keyword\"), (\"B\", \"non-reserved\")]\n\n    kw_list_2 = [(\"A\", \"reserved\"), (\"C\", \"non-reserved\")]\n\n    result = priority_keyword_merge(kw_list_1, kw_list_2)\n\n    expected_result = [(\"A\", \"reserved\"), (\"B\", \"non-reserved\"), (\"C\", \"non-reserved\")]\n\n    assert sorted(result) == sorted(expected_result)\n\n    kw_list_1 = [(\"A\", \"not-keyword\"), (\"B\", \"non-reserved\")]\n\n    kw_list_2 = [(\"A\", \"reserved\"), (\"C\", \"non-reserved\")]\n\n    result_2 = priority_keyword_merge(kw_list_2, kw_list_1)\n\n    expected_result_2 = [\n        (\"A\", \"not-keyword\"),\n        (\"B\", \"non-reserved\"),\n        (\"C\", \"non-reserved\"),\n    ]\n\n    assert sorted(result_2) == sorted(expected_result_2)\n\n    kw_list_1 = [(\"A\", \"not-keyword\"), (\"B\", \"non-reserved\")]\n\n    kw_list_2 = [(\"A\", \"reserved\"), (\"C\", \"non-reserved\")]\n\n    kw_list_3 = [(\"B\", \"reserved\")]\n\n    result_3 = priority_keyword_merge(kw_list_2, kw_list_1, kw_list_3)\n\n    expected_result_3 = [(\"A\", \"not-keyword\"), (\"B\", \"reserved\"), (\"C\", \"non-reserved\")]\n\n    assert sorted(result_3) == sorted(expected_result_3)\n\n    kw_list_1 = [(\"A\", \"not-keyword\"), (\"B\", \"non-reserved\")]\n\n    result_4 = priority_keyword_merge(kw_list_1)\n\n    expected_result_4 = kw_list_1\n\n    assert sorted(result_4) == sorted(expected_result_4)\n", "tokens": ["test", "dialects", "postgres_test", "py", "def", "test_priority_keyword_merge", "none", "test", "merging", "on", "keyword", "lists", "works", "as", "expected", "kw_list_1", "a", "not", "keyword", "b", "non", "reserved", "kw_list_2", "a", "reserved", "c", "non", "reserved", "result", "priority_keyword_merge", "kw_list_1", "kw_list_2", "expected_result", "a", "reserved", "b", "non", "reserved", "c", "non", "reserved", "assert", "sorted", "result", "sorted", "expected_result", "kw_list_1", "a", "not", "keyword", "b", "non", "reserved", "kw_list_2", "a", "reserved", "c", "non", "reserved", "result_2", "priority_keyword_merge", "kw_list_2", "kw_list_1", "expected_result_2", "a", "not", "keyword", "b", "non", "reserved", "c", "non", "reserved", "assert", "sorted", "result_2", "sorted", "expected_result_2", "kw_list_1", "a", "not", "keyword", "b", "non", "reserved", "kw_list_2", "a", "reserved", "c", "non", "reserved", "kw_list_3", "b", "reserved", "result_3", "priority_keyword_merge", "kw_list_2", "kw_list_1", "kw_list_3", "expected_result_3", "a", "not", "keyword", "b", "reserved", "c", "non", "reserved", "assert", "sorted", "result_3", "sorted", "expected_result_3", "kw_list_1", "a", "not", "keyword", "b", "non", "reserved", "result_4", "priority_keyword_merge", "kw_list_1", "expected_result_4", "kw_list_1", "assert", "sorted", "result_4", "sorted", "expected_result_4"], "doc_len": 130}
{"doc_id": "test/dialects/postgres_test.py::test_get_keywords", "file_path": "test/dialects/postgres_test.py", "class_name": null, "func_name": "test_get_keywords", "text": "文件路径: test/dialects/postgres_test.py\ndef test_get_keywords() -> None:\n    \"\"\"Test keyword filtering works as expected.\"\"\"\n    kw_list = [\n        (\"A\", \"not-keyword\"),\n        (\"B\", \"reserved\"),\n        (\"C\", \"non-reserved\"),\n        (\"D\", \"not-keyword\"),\n        (\"E\", \"non-reserved-(cannot-be-function-or-type)\"),\n    ]\n\n    expected_result = [\"A\", \"D\"]\n\n    assert sorted(get_keywords(kw_list, \"not-keyword\")) == sorted(expected_result)\n\n    expected_result_2 = [\"C\", \"E\"]\n\n    assert sorted(get_keywords(kw_list, \"non-reserved\")) == sorted(expected_result_2)\n", "tokens": ["test", "dialects", "postgres_test", "py", "def", "test_get_keywords", "none", "test", "keyword", "filtering", "works", "as", "expected", "kw_list", "a", "not", "keyword", "b", "reserved", "c", "non", "reserved", "d", "not", "keyword", "e", "non", "reserved", "cannot", "be", "function", "or", "type", "expected_result", "a", "d", "assert", "sorted", "get_keywords", "kw_list", "not", "keyword", "sorted", "expected_result", "expected_result_2", "c", "e", "assert", "sorted", "get_keywords", "kw_list", "non", "reserved", "sorted", "expected_result_2"], "doc_len": 55}
{"doc_id": "test/dialects/snowflake_test.py::test_snowflake_queries", "file_path": "test/dialects/snowflake_test.py", "class_name": null, "func_name": "test_snowflake_queries", "text": "文件路径: test/dialects/snowflake_test.py\ndef test_snowflake_queries(segment_cls, raw, caplog):\n    \"\"\"Test snowflake specific queries parse.\"\"\"\n    lnt = Linter(dialect=\"snowflake\")\n    parsed = lnt.parse_string(raw)\n    print(parsed.violations)\n    assert len(parsed.violations) == 0\n\n    # Find any unparsable statements\n    typs = parsed.tree.type_set()\n    assert \"unparsable\" not in typs\n\n    # Find the expected type in the parsed segment\n    seg_type = dialect_selector(\"snowflake\").get_segment(segment_cls).type\n    child_segments = [seg for seg in parsed.tree.recursive_crawl(seg_type)]\n    assert len(child_segments) > 0\n", "tokens": ["test", "dialects", "snowflake_test", "py", "def", "test_snowflake_queries", "segment_cls", "raw", "caplog", "test", "snowflake", "specific", "queries", "parse", "lnt", "linter", "dialect", "snowflake", "parsed", "lnt", "parse_string", "raw", "print", "parsed", "violations", "assert", "len", "parsed", "violations", "0", "find", "any", "unparsable", "statements", "typs", "parsed", "tree", "type_set", "assert", "unparsable", "not", "in", "typs", "find", "the", "expected", "type", "in", "the", "parsed", "segment", "seg_type", "dialect_selector", "snowflake", "get_segment", "segment_cls", "type", "child_segments", "seg", "for", "seg", "in", "parsed", "tree", "recursive_crawl", "seg_type", "assert", "len", "child_segments", "0"], "doc_len": 70}
{"doc_id": "test/fixtures/templater/jinja_j_libraries/libs/bar.py::equals", "file_path": "test/fixtures/templater/jinja_j_libraries/libs/bar.py", "class_name": null, "func_name": "equals", "text": "文件路径: test/fixtures/templater/jinja_j_libraries/libs/bar.py\ndef equals(col, val):\n    \"\"\"Return a string that has col = val.\"\"\"\n    return f\"{col} = {val}\"\n", "tokens": ["test", "fixtures", "templater", "jinja_j_libraries", "libs", "bar", "py", "def", "equals", "col", "val", "return", "a", "string", "that", "has", "col", "val", "return", "f", "col", "val"], "doc_len": 22}
{"doc_id": "test/fixtures/templater/jinja_j_libraries/libs/foo.py::table", "file_path": "test/fixtures/templater/jinja_j_libraries/libs/foo.py", "class_name": null, "func_name": "table", "text": "文件路径: test/fixtures/templater/jinja_j_libraries/libs/foo.py\ndef table(name):\n    \"\"\"Return the parameter with foo_ in front of it.\"\"\"\n    return f\"foo_{name}\"\n", "tokens": ["test", "fixtures", "templater", "jinja_j_libraries", "libs", "foo", "py", "def", "table", "name", "return", "the", "parameter", "with", "foo_", "in", "front", "of", "it", "return", "f", "foo_", "name"], "doc_len": 23}
{"doc_id": "test/rules/std_fix_auto_test.py::make_dialect_path", "file_path": "test/rules/std_fix_auto_test.py", "class_name": null, "func_name": "make_dialect_path", "text": "文件路径: test/rules/std_fix_auto_test.py\ndef make_dialect_path(dialect, fname):\n    \"\"\"Work out how to find paths given a dialect and a file name.\"\"\"\n    return os.path.join(\"test\", \"fixtures\", \"dialects\", dialect, fname)\n", "tokens": ["test", "rules", "std_fix_auto_test", "py", "def", "make_dialect_path", "dialect", "fname", "work", "out", "how", "to", "find", "paths", "given", "a", "dialect", "and", "a", "file", "name", "return", "os", "path", "join", "test", "fixtures", "dialects", "dialect", "fname"], "doc_len": 30}
{"doc_id": "test/rules/std_fix_auto_test.py::auto_fix_test", "file_path": "test/rules/std_fix_auto_test.py", "class_name": null, "func_name": "auto_fix_test", "text": "文件路径: test/rules/std_fix_auto_test.py\ndef auto_fix_test(dialect, folder, caplog):\n    \"\"\"A test for roundtrip testing, take a file buffer, lint, fix and lint.\n\n    This is explicitly different from the linter version of this, in that\n    it uses the command line rather than the direct api.\n    \"\"\"\n    # Log just the rules logger for this test.\n    # NOTE: In debugging it may be instructive to enable some of\n    # the other loggers listed here to debug particular issues.\n    # Enabling all of them results in very long logs so use\n    # wisely.\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.templater\")\n    # caplog.set_level(logging.DEBUG, logger=\"sqlfluff.lexer\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.linter\")\n    caplog.set_level(logging.DEBUG, logger=\"sqlfluff.rules\")\n\n    filename = \"testing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    cfgpath = os.path.join(tempdir_path, \".sqlfluff\")\n    src_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"before.sql\")\n    cmp_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"after.sql\")\n    vio_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \"violations.json\")\n    cfg_filepath = os.path.join(*base_auto_fix_path, dialect, folder, \".sqlfluff\")\n    test_conf_filepath = os.path.join(\n        *base_auto_fix_path, dialect, folder, \"test-config.yml\"\n    )\n\n    # Load the config file for the test:\n    with open(test_conf_filepath) as cfg_file:\n        cfg = yaml.safe_load(cfg_file)\n    print(\"## Config: \", cfg)\n    rules = \",\".join(cfg[\"test-config\"][\"rules\"])\n    raise_on_non_linting_violations = cfg[\"test-config\"].get(\n        \"raise_on_non_linting_violations\", True\n    )\n\n    # Open the example file and write the content to it\n    print_buff = \"\"\n    with open(filepath, mode=\"w\") as dest_file:\n        with open(src_filepath) as source_file:\n            for line in source_file:\n                dest_file.write(line)\n                print_buff += line\n    # Copy the config file too\n    try:\n        with open(cfgpath, mode=\"w\") as dest_file:\n            with open(cfg_filepath) as source_file:\n                print(\"## Config File Found.\")\n                for line in source_file:\n                    dest_file.write(line)\n    except FileNotFoundError:\n        # No config file? No biggie\n        print(\"## No Config File Found.\")\n        pass\n    print(f\"## Input file:\\n{print_buff}\")\n    # Do we need to do a violations check?\n    try:\n        with open(vio_filepath) as vio_file:\n            violations = json.load(vio_file)\n    except FileNotFoundError:\n        # No violations file. Let's not worry\n        violations = None\n\n    # Run the fix command\n    cfg = FluffConfig.from_root(overrides=dict(rules=rules, dialect=dialect))\n    lnt = Linter(config=cfg)\n    res = lnt.lint_path(filepath, fix=True)\n\n    print(f\"## Templated file:\\n{res.tree.raw}\")\n\n    # We call the check_tuples here, even to makes sure any non-linting\n    # violations are raised, and the test fails.\n    vs = set(\n        res.check_tuples(\n            raise_on_non_linting_violations=raise_on_non_linting_violations\n        )\n    )\n    # If we have a violations structure, let's enforce it.\n    if violations:\n        # Format the violations file\n        expected_vs = set()\n        for rule_key in violations[\"violations\"][\"linting\"]:\n            for elem in violations[\"violations\"][\"linting\"][rule_key]:\n                expected_vs.add((rule_key, *elem))\n        assert expected_vs == vs\n\n    # Actually do the fixes\n    res = res.persist_changes()\n    # Read the fixed file\n    with open(filepath) as fixed_file:\n        fixed_buff = fixed_file.read()\n    # Clearup once read\n    shutil.rmtree(tempdir_path)\n    # Read the comparison file\n    with open(cmp_filepath) as comp_file:\n        comp_buff = comp_file.read()\n\n    # Make sure we were successful\n    assert res\n    # Assert that we fixed as expected\n    assert fixed_buff == comp_buff\n", "tokens": ["test", "rules", "std_fix_auto_test", "py", "def", "auto_fix_test", "dialect", "folder", "caplog", "a", "test", "for", "roundtrip", "testing", "take", "a", "file", "buffer", "lint", "fix", "and", "lint", "this", "is", "explicitly", "different", "from", "the", "linter", "version", "of", "this", "in", "that", "it", "uses", "the", "command", "line", "rather", "than", "the", "direct", "api", "log", "just", "the", "rules", "logger", "for", "this", "test", "note", "in", "debugging", "it", "may", "be", "instructive", "to", "enable", "some", "of", "the", "other", "loggers", "listed", "here", "to", "debug", "particular", "issues", "enabling", "all", "of", "them", "results", "in", "very", "long", "logs", "so", "use", "wisely", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "templater", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "lexer", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "linter", "caplog", "set_level", "logging", "debug", "logger", "sqlfluff", "rules", "filename", "testing", "sql", "lets", "get", "the", "path", "of", "a", "file", "to", "use", "tempdir_path", "tempfile", "mkdtemp", "filepath", "os", "path", "join", "tempdir_path", "filename", "cfgpath", "os", "path", "join", "tempdir_path", "sqlfluff", "src_filepath", "os", "path", "join", "base_auto_fix_path", "dialect", "folder", "before", "sql", "cmp_filepath", "os", "path", "join", "base_auto_fix_path", "dialect", "folder", "after", "sql", "vio_filepath", "os", "path", "join", "base_auto_fix_path", "dialect", "folder", "violations", "json", "cfg_filepath", "os", "path", "join", "base_auto_fix_path", "dialect", "folder", "sqlfluff", "test_conf_filepath", "os", "path", "join", "base_auto_fix_path", "dialect", "folder", "test", "config", "yml", "load", "the", "config", "file", "for", "the", "test", "with", "open", "test_conf_filepath", "as", "cfg_file", "cfg", "yaml", "safe_load", "cfg_file", "print", "config", "cfg", "rules", "join", "cfg", "test", "config", "rules", "raise_on_non_linting_violations", "cfg", "test", "config", "get", "raise_on_non_linting_violations", "true", "open", "the", "example", "file", "and", "write", "the", "content", "to", "it", "print_buff", "with", "open", "filepath", "mode", "w", "as", "dest_file", "with", "open", "src_filepath", "as", "source_file", "for", "line", "in", "source_file", "dest_file", "write", "line", "print_buff", "line", "copy", "the", "config", "file", "too", "try", "with", "open", "cfgpath", "mode", "w", "as", "dest_file", "with", "open", "cfg_filepath", "as", "source_file", "print", "config", "file", "found", "for", "line", "in", "source_file", "dest_file", "write", "line", "except", "filenotfounderror", "no", "config", "file", "no", "biggie", "print", "no", "config", "file", "found", "pass", "print", "f", "input", "file", "n", "print_buff", "do", "we", "need", "to", "do", "a", "violations", "check", "try", "with", "open", "vio_filepath", "as", "vio_file", "violations", "json", "load", "vio_file", "except", "filenotfounderror", "no", "violations", "file", "let", "s", "not", "worry", "violations", "none", "run", "the", "fix", "command", "cfg", "fluffconfig", "from_root", "overrides", "dict", "rules", "rules", "dialect", "dialect", "lnt", "linter", "config", "cfg", "res", "lnt", "lint_path", "filepath", "fix", "true", "print", "f", "templated", "file", "n", "res", "tree", "raw", "we", "call", "the", "check_tuples", "here", "even", "to", "makes", "sure", "any", "non", "linting", "violations", "are", "raised", "and", "the", "test", "fails", "vs", "set", "res", "check_tuples", "raise_on_non_linting_violations", "raise_on_non_linting_violations", "if", "we", "have", "a", "violations", "structure", "let", "s", "enforce", "it", "if", "violations", "format", "the", "violations", "file", "expected_vs", "set", "for", "rule_key", "in", "violations", "violations", "linting", "for", "elem", "in", "violations", "violations", "linting", "rule_key", "expected_vs", "add", "rule_key", "elem", "assert", "expected_vs", "vs", "actually", "do", "the", "fixes", "res", "res", "persist_changes", "read", "the", "fixed", "file", "with", "open", "filepath", "as", "fixed_file", "fixed_buff", "fixed_file", "read", "clearup", "once", "read", "shutil", "rmtree", "tempdir_path", "read", "the", "comparison", "file", "with", "open", "cmp_filepath", "as", "comp_file", "comp_buff", "comp_file", "read", "make", "sure", "we", "were", "successful", "assert", "res", "assert", "that", "we", "fixed", "as", "expected", "assert", "fixed_buff", "comp_buff"], "doc_len": 472}
{"doc_id": "test/rules/std_fix_auto_test.py::test__std_fix_auto", "file_path": "test/rules/std_fix_auto_test.py", "class_name": null, "func_name": "test__std_fix_auto", "text": "文件路径: test/rules/std_fix_auto_test.py\ndef test__std_fix_auto(dialect, folder, caplog):\n    \"\"\"Automated Fixing Tests.\"\"\"\n    auto_fix_test(dialect=dialect, folder=folder, caplog=caplog)\n", "tokens": ["test", "rules", "std_fix_auto_test", "py", "def", "test__std_fix_auto", "dialect", "folder", "caplog", "automated", "fixing", "tests", "auto_fix_test", "dialect", "dialect", "folder", "folder", "caplog", "caplog"], "doc_len": 19}
{"doc_id": "test/rules/std_L003_test.py::test__rules__std_L003_process_raw_stack", "file_path": "test/rules/std_L003_test.py", "class_name": null, "func_name": "test__rules__std_L003_process_raw_stack", "text": "文件路径: test/rules/std_L003_test.py\ndef test__rules__std_L003_process_raw_stack(generate_test_segments, test_elems, result):\n    \"\"\"Test the _process_raw_stack function.\n\n    Note: This test probably needs expanding. It doesn't\n    really check enough of the full functionality.\n\n    \"\"\"\n    cfg = FluffConfig()\n    r = get_rule_from_set(\"L003\", config=cfg)\n    test_stack = generate_test_segments(test_elems)\n    res = r._process_raw_stack(test_stack, {})\n    print(res)\n    # Verify structure\n    assert isinstance(res, dict)\n    assert all(isinstance(k, int) for k in res.keys())\n    assert all(isinstance(v, dict) for v in res.values())\n    # Check keys are all present\n    assert all(\n        v.keys()\n        == {\n            \"line_no\",\n            \"line_buffer\",\n            \"indent_buffer\",\n            \"indent_size\",\n            \"indent_balance\",\n            \"hanging_indent\",\n            \"clean_indent\",\n        }\n        for v in res.values()\n    )\n    # For testing purposes, we won't be checking the buffer fields. They're just\n    # too hard to create in the test cases and aren't critical in determining\n    # what course of action to take. Most of the logic uses the values which we\n    # *are* still testing.\n    for k in res:\n        del res[k][\"line_buffer\"]\n        del res[k][\"indent_buffer\"]\n    assert res == result\n", "tokens": ["test", "rules", "std_l003_test", "py", "def", "test__rules__std_l003_process_raw_stack", "generate_test_segments", "test_elems", "result", "test", "the", "_process_raw_stack", "function", "note", "this", "test", "probably", "needs", "expanding", "it", "doesn", "t", "really", "check", "enough", "of", "the", "full", "functionality", "cfg", "fluffconfig", "r", "get_rule_from_set", "l003", "config", "cfg", "test_stack", "generate_test_segments", "test_elems", "res", "r", "_process_raw_stack", "test_stack", "print", "res", "verify", "structure", "assert", "isinstance", "res", "dict", "assert", "all", "isinstance", "k", "int", "for", "k", "in", "res", "keys", "assert", "all", "isinstance", "v", "dict", "for", "v", "in", "res", "values", "check", "keys", "are", "all", "present", "assert", "all", "v", "keys", "line_no", "line_buffer", "indent_buffer", "indent_size", "indent_balance", "hanging_indent", "clean_indent", "for", "v", "in", "res", "values", "for", "testing", "purposes", "we", "won", "t", "be", "checking", "the", "buffer", "fields", "they", "re", "just", "too", "hard", "to", "create", "in", "the", "test", "cases", "and", "aren", "t", "critical", "in", "determining", "what", "course", "of", "action", "to", "take", "most", "of", "the", "logic", "uses", "the", "values", "which", "we", "are", "still", "testing", "for", "k", "in", "res", "del", "res", "k", "line_buffer", "del", "res", "k", "indent_buffer", "assert", "res", "result"], "doc_len": 153}
{"doc_id": "test/rules/std_L003_test.py::test__rules__std_L003_make_indent", "file_path": "test/rules/std_L003_test.py", "class_name": null, "func_name": "test__rules__std_L003_make_indent", "text": "文件路径: test/rules/std_L003_test.py\ndef test__rules__std_L003_make_indent(indent_unit, num, tab_space_size, result):\n    \"\"\"Test Rule_L003._make_indent.\"\"\"\n    res = Rule_L003._make_indent(\n        num=num, indent_unit=indent_unit, tab_space_size=tab_space_size\n    )\n    assert res == result\n", "tokens": ["test", "rules", "std_l003_test", "py", "def", "test__rules__std_l003_make_indent", "indent_unit", "num", "tab_space_size", "result", "test", "rule_l003", "_make_indent", "res", "rule_l003", "_make_indent", "num", "num", "indent_unit", "indent_unit", "tab_space_size", "tab_space_size", "assert", "res", "result"], "doc_len": 25}
{"doc_id": "test/rules/std_L003_test.py::test__rules__std_L003_make_indent_invalid_param", "file_path": "test/rules/std_L003_test.py", "class_name": null, "func_name": "test__rules__std_L003_make_indent_invalid_param", "text": "文件路径: test/rules/std_L003_test.py\ndef test__rules__std_L003_make_indent_invalid_param():\n    \"\"\"Test Rule_L003._make_indent with invalid indent_unit parameter.\"\"\"\n    with pytest.raises(ValueError):\n        Rule_L003._make_indent(indent_unit=\"aaa\")\n", "tokens": ["test", "rules", "std_l003_test", "py", "def", "test__rules__std_l003_make_indent_invalid_param", "test", "rule_l003", "_make_indent", "with", "invalid", "indent_unit", "parameter", "with", "pytest", "raises", "valueerror", "rule_l003", "_make_indent", "indent_unit", "aaa"], "doc_len": 21}
{"doc_id": "test/rules/std_L003_test.py::test__rules__std_L003__get_element_template_info", "file_path": "test/rules/std_L003_test.py", "class_name": null, "func_name": "test__rules__std_L003__get_element_template_info", "text": "文件路径: test/rules/std_L003_test.py\ndef test__rules__std_L003__get_element_template_info():\n    \"\"\"Test Rule_L003._get_element_template_info with invalid templated_file parameter.\"\"\"\n    mock_segment = Mock()\n    mock_segment.return_value.is_type = True\n    with pytest.raises(ValueError):\n        Rule_L003._get_element_template_info(elem=mock_segment, templated_file=None)\n", "tokens": ["test", "rules", "std_l003_test", "py", "def", "test__rules__std_l003__get_element_template_info", "test", "rule_l003", "_get_element_template_info", "with", "invalid", "templated_file", "parameter", "mock_segment", "mock", "mock_segment", "return_value", "is_type", "true", "with", "pytest", "raises", "valueerror", "rule_l003", "_get_element_template_info", "elem", "mock_segment", "templated_file", "none"], "doc_len": 29}
{"doc_id": "test/rules/std_L003_test.py::ProtoSeg.__init__", "file_path": "test/rules/std_L003_test.py", "class_name": "ProtoSeg", "func_name": "__init__", "text": "文件路径: test/rules/std_L003_test.py, 类名: ProtoSeg\n    def __init__(self, raw):\n        self.raw = raw\n", "tokens": ["test", "rules", "std_l003_test", "py", "protoseg", "def", "__init__", "self", "raw", "self", "raw", "raw"], "doc_len": 12}
{"doc_id": "test/rules/std_L003_test.py::test__rules__std_L003_indent_size", "file_path": "test/rules/std_L003_test.py", "class_name": null, "func_name": "test__rules__std_L003_indent_size", "text": "文件路径: test/rules/std_L003_test.py\ndef test__rules__std_L003_indent_size(tab_space_size, segments, result):\n    \"\"\"Test Rule_L003._make_indent.\"\"\"\n    res = Rule_L003._indent_size(segments=segments, tab_space_size=tab_space_size)\n    assert res == result\n", "tokens": ["test", "rules", "std_l003_test", "py", "def", "test__rules__std_l003_indent_size", "tab_space_size", "segments", "result", "test", "rule_l003", "_make_indent", "res", "rule_l003", "_indent_size", "segments", "segments", "tab_space_size", "tab_space_size", "assert", "res", "result"], "doc_len": 22}
{"doc_id": "test/rules/std_L007_test.py::test__rules__std_L007_default", "file_path": "test/rules/std_L007_test.py", "class_name": null, "func_name": "test__rules__std_L007_default", "text": "文件路径: test/rules/std_L007_test.py\ndef test__rules__std_L007_default():\n    \"\"\"Verify that L007 returns the correct error message for default (after).\"\"\"\n    sql = \"\"\"\n        SELECT\n            a,\n            b\n        FROM foo\n        WHERE\n            a = 1 AND\n            b = 2\n    \"\"\"\n    result = sqlfluff.lint(sql)\n    assert \"L007\" in [r[\"code\"] for r in result]\n    assert after_description in [r[\"description\"] for r in result]\n", "tokens": ["test", "rules", "std_l007_test", "py", "def", "test__rules__std_l007_default", "verify", "that", "l007", "returns", "the", "correct", "error", "message", "for", "default", "after", "sql", "select", "a", "b", "from", "foo", "where", "a", "1", "and", "b", "2", "result", "sqlfluff", "lint", "sql", "assert", "l007", "in", "r", "code", "for", "r", "in", "result", "assert", "after_description", "in", "r", "description", "for", "r", "in", "result"], "doc_len": 51}
{"doc_id": "test/rules/std_L007_test.py::test__rules__std_L007_after", "file_path": "test/rules/std_L007_test.py", "class_name": null, "func_name": "test__rules__std_L007_after", "text": "文件路径: test/rules/std_L007_test.py\ndef test__rules__std_L007_after():\n    \"\"\"Verify that L007 returns the correct error message when after is explicitly used.\"\"\"\n    sql = \"\"\"\n        SELECT\n            a,\n            b\n        FROM foo\n        WHERE\n            a = 1 AND\n            b = 2\n    \"\"\"\n    config = FluffConfig(configs={\"rules\": {\"L007\": {\"operator_new_lines\": \"after\"}}})\n    # The sqlfluff.lint API doesn't allow us to pass config so need to do what it does\n    linter = Linter(config=config)\n    result_records = linter.lint_string_wrapped(sql).as_records()\n    result = result_records[0][\"violations\"]\n    assert \"L007\" in [r[\"code\"] for r in result]\n    assert after_description in [r[\"description\"] for r in result]\n", "tokens": ["test", "rules", "std_l007_test", "py", "def", "test__rules__std_l007_after", "verify", "that", "l007", "returns", "the", "correct", "error", "message", "when", "after", "is", "explicitly", "used", "sql", "select", "a", "b", "from", "foo", "where", "a", "1", "and", "b", "2", "config", "fluffconfig", "configs", "rules", "l007", "operator_new_lines", "after", "the", "sqlfluff", "lint", "api", "doesn", "t", "allow", "us", "to", "pass", "config", "so", "need", "to", "do", "what", "it", "does", "linter", "linter", "config", "config", "result_records", "linter", "lint_string_wrapped", "sql", "as_records", "result", "result_records", "0", "violations", "assert", "l007", "in", "r", "code", "for", "r", "in", "result", "assert", "after_description", "in", "r", "description", "for", "r", "in", "result"], "doc_len": 87}
{"doc_id": "test/rules/std_L007_test.py::test__rules__std_L007_before", "file_path": "test/rules/std_L007_test.py", "class_name": null, "func_name": "test__rules__std_L007_before", "text": "文件路径: test/rules/std_L007_test.py\ndef test__rules__std_L007_before():\n    \"\"\"Verify that L007 returns the correct error message when before is used.\"\"\"\n    sql = \"\"\"\n        SELECT\n            a,\n            b\n        FROM foo\n        WHERE\n            a = 1\n            AND b = 2\n    \"\"\"\n    config = FluffConfig(configs={\"rules\": {\"L007\": {\"operator_new_lines\": \"before\"}}})\n    # The sqlfluff.lint API doesn't allow us to pass config so need to do what it does\n    linter = Linter(config=config)\n    result_records = linter.lint_string_wrapped(sql).as_records()\n    result = result_records[0][\"violations\"]\n    assert \"L007\" in [r[\"code\"] for r in result]\n    assert before_description in [r[\"description\"] for r in result]\n", "tokens": ["test", "rules", "std_l007_test", "py", "def", "test__rules__std_l007_before", "verify", "that", "l007", "returns", "the", "correct", "error", "message", "when", "before", "is", "used", "sql", "select", "a", "b", "from", "foo", "where", "a", "1", "and", "b", "2", "config", "fluffconfig", "configs", "rules", "l007", "operator_new_lines", "before", "the", "sqlfluff", "lint", "api", "doesn", "t", "allow", "us", "to", "pass", "config", "so", "need", "to", "do", "what", "it", "does", "linter", "linter", "config", "config", "result_records", "linter", "lint_string_wrapped", "sql", "as_records", "result", "result_records", "0", "violations", "assert", "l007", "in", "r", "code", "for", "r", "in", "result", "assert", "before_description", "in", "r", "description", "for", "r", "in", "result"], "doc_len": 86}
{"doc_id": "test/rules/std_L016_L36_combo.py::test__rules__std_L016_L036_long_line_lint", "file_path": "test/rules/std_L016_L36_combo.py", "class_name": null, "func_name": "test__rules__std_L016_L036_long_line_lint", "text": "文件路径: test/rules/std_L016_L36_combo.py\ndef test__rules__std_L016_L036_long_line_lint():\n    \"\"\"Verify that a long line that causes a clash between L016 and L036 is not changed.\"\"\"\n    sql = \"SELECT\\n1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\n\"\n    result = sqlfluff.lint(sql)\n    assert \"L016\" in [r[\"code\"] for r in result]\n    assert \"L036\" in [r[\"code\"] for r in result]\n", "tokens": ["test", "rules", "std_l016_l36_combo", "py", "def", "test__rules__std_l016_l036_long_line_lint", "verify", "that", "a", "long", "line", "that", "causes", "a", "clash", "between", "l016", "and", "l036", "is", "not", "changed", "sql", "select", "n1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "n", "result", "sqlfluff", "lint", "sql", "assert", "l016", "in", "r", "code", "for", "r", "in", "result", "assert", "l036", "in", "r", "code", "for", "r", "in", "result"], "doc_len": 48}
{"doc_id": "test/rules/std_L016_L36_combo.py::test__rules__std_L016_L036_long_line_fix", "file_path": "test/rules/std_L016_L36_combo.py", "class_name": null, "func_name": "test__rules__std_L016_L036_long_line_fix", "text": "文件路径: test/rules/std_L016_L36_combo.py\ndef test__rules__std_L016_L036_long_line_fix():\n    \"\"\"Verify that a long line that causes a clash between L016 and L036 does not add multiple newlines (see #1424).\"\"\"\n    sql = \"SELECT 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\n\"\n    result = sqlfluff.fix(sql)\n    assert (\n        result\n        == \"SELECT\\n    1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\n\"\n    )\n", "tokens": ["test", "rules", "std_l016_l36_combo", "py", "def", "test__rules__std_l016_l036_long_line_fix", "verify", "that", "a", "long", "line", "that", "causes", "a", "clash", "between", "l016", "and", "l036", "does", "not", "add", "multiple", "newlines", "see", "1424", "sql", "select", "1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "n", "result", "sqlfluff", "fix", "sql", "assert", "result", "select", "n", "1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "n"], "doc_len": 40}
{"doc_id": "test/rules/std_L016_L36_combo.py::test__rules__std_L016_L036_long_line_fix2", "file_path": "test/rules/std_L016_L36_combo.py", "class_name": null, "func_name": "test__rules__std_L016_L036_long_line_fix2", "text": "文件路径: test/rules/std_L016_L36_combo.py\ndef test__rules__std_L016_L036_long_line_fix2():\n    \"\"\"Verify that a long line that causes a clash between L016 and L036 does not add multiple newlines (see #1424).\"\"\"\n    sql = \"SELECT\\n    1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\n\"\n    result = sqlfluff.fix(sql)\n    assert (\n        result\n        == \"SELECT 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000\\n\"\n    )\n", "tokens": ["test", "rules", "std_l016_l36_combo", "py", "def", "test__rules__std_l016_l036_long_line_fix2", "verify", "that", "a", "long", "line", "that", "causes", "a", "clash", "between", "l016", "and", "l036", "does", "not", "add", "multiple", "newlines", "see", "1424", "sql", "select", "n", "1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "n", "result", "sqlfluff", "fix", "sql", "assert", "result", "select", "1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000", "n"], "doc_len": 40}
{"doc_id": "test/rules/std_L019_test.py::test__rules__std_L019_unparseable", "file_path": "test/rules/std_L019_test.py", "class_name": null, "func_name": "test__rules__std_L019_unparseable", "text": "文件路径: test/rules/std_L019_test.py\ndef test__rules__std_L019_unparseable():\n    \"\"\"Verify that L019 doesn't try to fix queries with parse errors.\n\n    This has been observed to frequently cause syntax errors, especially in\n    combination with Jinja templating, e.g. undefined template variables.\n    \"\"\"\n    # This example comes almost directly from a real-world example. The user\n    # accidentally ran \"sqlfluff fix\" without defining\n    # \"readability_features_numeric\" and \"readability_features_count_list\", and\n    # doing so corrupted their query.\n    sql = \"\"\"\n        SELECT\n          user_id,\n          campaign_id,\n          business_type,\n          SPLIT(intents, \",\") AS intent_list,\n          {% for feature in readability_features_numeric %}\n            CAST(JSON_EXTRACT(readability_scores,\n            '$.data.{{feature}}') AS float64) AS {{feature}} {% if not loop.last %} , {% endif %}\n          {% endfor %},\n          {% for feature in readability_features_count_list %}\n            CAST(JSON_EXTRACT(asset_structure,\n            '$.{{feature}}') AS float64) AS {{feature}}_count {% if not loop.last %} , {% endif %}\n          {% endfor %},\n            track_clicks_text,\n            track_clicks_html\n        FROM\n          t\n    \"\"\"\n    result = sqlfluff.lint(sql)\n    assert \"L019\" not in [r[\"code\"] for r in result]\n", "tokens": ["test", "rules", "std_l019_test", "py", "def", "test__rules__std_l019_unparseable", "verify", "that", "l019", "doesn", "t", "try", "to", "fix", "queries", "with", "parse", "errors", "this", "has", "been", "observed", "to", "frequently", "cause", "syntax", "errors", "especially", "in", "combination", "with", "jinja", "templating", "e", "g", "undefined", "template", "variables", "this", "example", "comes", "almost", "directly", "from", "a", "real", "world", "example", "the", "user", "accidentally", "ran", "sqlfluff", "fix", "without", "defining", "readability_features_numeric", "and", "readability_features_count_list", "and", "doing", "so", "corrupted", "their", "query", "sql", "select", "user_id", "campaign_id", "business_type", "split", "intents", "as", "intent_list", "for", "feature", "in", "readability_features_numeric", "cast", "json_extract", "readability_scores", "data", "feature", "as", "float64", "as", "feature", "if", "not", "loop", "last", "endif", "endfor", "for", "feature", "in", "readability_features_count_list", "cast", "json_extract", "asset_structure", "feature", "as", "float64", "as", "feature", "_count", "if", "not", "loop", "last", "endif", "endfor", "track_clicks_text", "track_clicks_html", "from", "t", "result", "sqlfluff", "lint", "sql", "assert", "l019", "not", "in", "r", "code", "for", "r", "in", "result"], "doc_len": 130}
{"doc_id": "test/rules/std_L020_test.py::test__rules__std_L020_one_aliases_one_duplicate", "file_path": "test/rules/std_L020_test.py", "class_name": null, "func_name": "test__rules__std_L020_one_aliases_one_duplicate", "text": "文件路径: test/rules/std_L020_test.py\ndef test__rules__std_L020_one_aliases_one_duplicate():\n    \"\"\"Verify that L020 returns the correct error message for one duplicate table aliases occur one times.\"\"\"\n    sql = \"\"\"\n        SELECT\n            a.pk\n        FROM table_1 AS a\n        JOIN table_2 AS a ON a.pk = a.pk\n    \"\"\"\n    result = sqlfluff.lint(sql)\n    assert \"L020\" in [r[\"code\"] for r in result]\n    assert [r[\"code\"] for r in result].count(\"L020\") == 1\n", "tokens": ["test", "rules", "std_l020_test", "py", "def", "test__rules__std_l020_one_aliases_one_duplicate", "verify", "that", "l020", "returns", "the", "correct", "error", "message", "for", "one", "duplicate", "table", "aliases", "occur", "one", "times", "sql", "select", "a", "pk", "from", "table_1", "as", "a", "join", "table_2", "as", "a", "on", "a", "pk", "a", "pk", "result", "sqlfluff", "lint", "sql", "assert", "l020", "in", "r", "code", "for", "r", "in", "result", "assert", "r", "code", "for", "r", "in", "result", "count", "l020", "1"], "doc_len": 62}
{"doc_id": "test/rules/std_L020_test.py::test__rules__std_L020_one_aliases_two_duplicate", "file_path": "test/rules/std_L020_test.py", "class_name": null, "func_name": "test__rules__std_L020_one_aliases_two_duplicate", "text": "文件路径: test/rules/std_L020_test.py\ndef test__rules__std_L020_one_aliases_two_duplicate():\n    \"\"\"Verify that L020 returns the correct error message for one duplicate table aliases occur two times.\"\"\"\n    sql = \"\"\"\n        SELECT\n            a.pk\n        FROM table_1 AS a\n        JOIN table_2 AS a ON a.pk = a.pk\n        JOIN table_3 AS a ON a.pk = a.pk\n    \"\"\"\n    result = sqlfluff.lint(sql)\n    result_filter = [r for r in result if r[\"code\"] == \"L020\"]\n    # Error message only show two times, not three\n    assert len(result_filter) == 2\n    assert (\n        len(\n            [\n                r\n                for r in result_filter\n                if \"Duplicate table alias 'a'\" in r[\"description\"]\n            ]\n        )\n        == 2\n    )\n    # Test specific line number\n    assert result_filter[0][\"line_no\"] == 5\n    assert result_filter[1][\"line_no\"] == 6\n", "tokens": ["test", "rules", "std_l020_test", "py", "def", "test__rules__std_l020_one_aliases_two_duplicate", "verify", "that", "l020", "returns", "the", "correct", "error", "message", "for", "one", "duplicate", "table", "aliases", "occur", "two", "times", "sql", "select", "a", "pk", "from", "table_1", "as", "a", "join", "table_2", "as", "a", "on", "a", "pk", "a", "pk", "join", "table_3", "as", "a", "on", "a", "pk", "a", "pk", "result", "sqlfluff", "lint", "sql", "result_filter", "r", "for", "r", "in", "result", "if", "r", "code", "l020", "error", "message", "only", "show", "two", "times", "not", "three", "assert", "len", "result_filter", "2", "assert", "len", "r", "for", "r", "in", "result_filter", "if", "duplicate", "table", "alias", "a", "in", "r", "description", "2", "test", "specific", "line", "number", "assert", "result_filter", "0", "line_no", "5", "assert", "result_filter", "1", "line_no", "6"], "doc_len": 104}
{"doc_id": "test/rules/std_L020_test.py::test__rules__std_L020_complex", "file_path": "test/rules/std_L020_test.py", "class_name": null, "func_name": "test__rules__std_L020_complex", "text": "文件路径: test/rules/std_L020_test.py\ndef test__rules__std_L020_complex():\n    \"\"\"Verify that L020 returns the correct error message for complex example.\"\"\"\n    sql = \"\"\"\n        SELECT\n            a.pk,\n            b.pk\n        FROM table_1 AS a\n        JOIN table_2 AS a ON a.pk = a.pk\n        JOIN table_3 AS b ON a.pk = b.pk\n        JOIN table_4 AS b ON b.pk = b.pk\n        JOIN table_5 AS a ON b.pk = a.pk\n    \"\"\"\n    result = sqlfluff.lint(sql)\n    result_filter = [r for r in result if r[\"code\"] == \"L020\"]\n    # Error message only show two times, not three\n    assert len(result_filter) == 3\n    assert (\n        len(\n            [\n                r\n                for r in result_filter\n                if \"Duplicate table alias 'a'\" in r[\"description\"]\n            ]\n        )\n        == 2\n    )\n    assert (\n        len(\n            [\n                r\n                for r in result_filter\n                if \"Duplicate table alias 'b'\" in r[\"description\"]\n            ]\n        )\n        == 1\n    )\n    # Test specific line number\n    assert result_filter[0][\"line_no\"] == 6\n    assert result_filter[1][\"line_no\"] == 8\n    assert result_filter[2][\"line_no\"] == 9\n", "tokens": ["test", "rules", "std_l020_test", "py", "def", "test__rules__std_l020_complex", "verify", "that", "l020", "returns", "the", "correct", "error", "message", "for", "complex", "example", "sql", "select", "a", "pk", "b", "pk", "from", "table_1", "as", "a", "join", "table_2", "as", "a", "on", "a", "pk", "a", "pk", "join", "table_3", "as", "b", "on", "a", "pk", "b", "pk", "join", "table_4", "as", "b", "on", "b", "pk", "b", "pk", "join", "table_5", "as", "a", "on", "b", "pk", "a", "pk", "result", "sqlfluff", "lint", "sql", "result_filter", "r", "for", "r", "in", "result", "if", "r", "code", "l020", "error", "message", "only", "show", "two", "times", "not", "three", "assert", "len", "result_filter", "3", "assert", "len", "r", "for", "r", "in", "result_filter", "if", "duplicate", "table", "alias", "a", "in", "r", "description", "2", "assert", "len", "r", "for", "r", "in", "result_filter", "if", "duplicate", "table", "alias", "b", "in", "r", "description", "1", "test", "specific", "line", "number", "assert", "result_filter", "0", "line_no", "6", "assert", "result_filter", "1", "line_no", "8", "assert", "result_filter", "2", "line_no", "9"], "doc_len": 140}
{"doc_id": "test/rules/std_L048_test.py::test__rules__std_L048_raised", "file_path": "test/rules/std_L048_test.py", "class_name": null, "func_name": "test__rules__std_L048_raised", "text": "文件路径: test/rules/std_L048_test.py\ndef test__rules__std_L048_raised() -> None:\n    \"\"\"L048 is raised for quoted literals not surrounded by a single whitespace.\"\"\"\n    sql = \"SELECT a +'b'+'c' FROM tbl;\"\n    result = sqlfluff.lint(sql)\n\n    assert len(result) == 7\n\n    results_l048 = [r for r in result if r[\"code\"] == \"L048\"]\n    assert len(results_l048) == 3\n    assert results_l048[0][\"description\"] == \"Missing whitespace before 'b'\"\n    assert results_l048[1][\"description\"] == \"Missing whitespace after 'b'\"\n    assert results_l048[2][\"description\"] == \"Missing whitespace before 'c'\"\n", "tokens": ["test", "rules", "std_l048_test", "py", "def", "test__rules__std_l048_raised", "none", "l048", "is", "raised", "for", "quoted", "literals", "not", "surrounded", "by", "a", "single", "whitespace", "sql", "select", "a", "b", "c", "from", "tbl", "result", "sqlfluff", "lint", "sql", "assert", "len", "result", "7", "results_l048", "r", "for", "r", "in", "result", "if", "r", "code", "l048", "assert", "len", "results_l048", "3", "assert", "results_l048", "0", "description", "missing", "whitespace", "before", "b", "assert", "results_l048", "1", "description", "missing", "whitespace", "after", "b", "assert", "results_l048", "2", "description", "missing", "whitespace", "before", "c"], "doc_len": 72}
{"doc_id": "test/rules/std_roundtrip_test.py::generic_roundtrip_test", "file_path": "test/rules/std_roundtrip_test.py", "class_name": null, "func_name": "generic_roundtrip_test", "text": "文件路径: test/rules/std_roundtrip_test.py\ndef generic_roundtrip_test(source_file, rulestring):\n    \"\"\"Run a roundtrip test given a sql file and a rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing.\n    \"\"\"\n    if isinstance(source_file, str):\n        # If it's a string, treat it as a path so lets load it.\n        with open(source_file) as f:\n            source_file = StringIO(f.read())\n\n    filename = \"tesing.sql\"\n    # Lets get the path of a file to use\n    tempdir_path = tempfile.mkdtemp()\n    filepath = os.path.join(tempdir_path, filename)\n    # Open the example file and write the content to it\n    with open(filepath, mode=\"w\") as dest_file:\n        for line in source_file:\n            dest_file.write(line)\n    runner = CliRunner()\n    # Check that we first detect the issue\n    result = runner.invoke(lint, [\"--rules\", rulestring, filepath])\n    assert result.exit_code == 65\n    # Fix the file (in force mode)\n    result = runner.invoke(fix, [\"--rules\", rulestring, \"-f\", filepath])\n    assert result.exit_code == 0\n    # Now lint the file and check for exceptions\n    result = runner.invoke(lint, [\"--rules\", rulestring, filepath])\n    assert result.exit_code == 0\n    shutil.rmtree(tempdir_path)\n", "tokens": ["test", "rules", "std_roundtrip_test", "py", "def", "generic_roundtrip_test", "source_file", "rulestring", "run", "a", "roundtrip", "test", "given", "a", "sql", "file", "and", "a", "rule", "we", "take", "a", "file", "buffer", "lint", "fix", "and", "lint", "finally", "checking", "that", "the", "file", "fails", "initially", "but", "not", "after", "fixing", "if", "isinstance", "source_file", "str", "if", "it", "s", "a", "string", "treat", "it", "as", "a", "path", "so", "lets", "load", "it", "with", "open", "source_file", "as", "f", "source_file", "stringio", "f", "read", "filename", "tesing", "sql", "lets", "get", "the", "path", "of", "a", "file", "to", "use", "tempdir_path", "tempfile", "mkdtemp", "filepath", "os", "path", "join", "tempdir_path", "filename", "open", "the", "example", "file", "and", "write", "the", "content", "to", "it", "with", "open", "filepath", "mode", "w", "as", "dest_file", "for", "line", "in", "source_file", "dest_file", "write", "line", "runner", "clirunner", "check", "that", "we", "first", "detect", "the", "issue", "result", "runner", "invoke", "lint", "rules", "rulestring", "filepath", "assert", "result", "exit_code", "65", "fix", "the", "file", "in", "force", "mode", "result", "runner", "invoke", "fix", "rules", "rulestring", "f", "filepath", "assert", "result", "exit_code", "0", "now", "lint", "the", "file", "and", "check", "for", "exceptions", "result", "runner", "invoke", "lint", "rules", "rulestring", "filepath", "assert", "result", "exit_code", "0", "shutil", "rmtree", "tempdir_path"], "doc_len": 171}
{"doc_id": "test/rules/std_roundtrip_test.py::jinja_roundtrip_test", "file_path": "test/rules/std_roundtrip_test.py", "class_name": null, "func_name": "jinja_roundtrip_test", "text": "文件路径: test/rules/std_roundtrip_test.py\ndef jinja_roundtrip_test(\n    source_path, rulestring, sqlfile=\"test.sql\", cfgfile=\".sqlfluff\"\n):\n    \"\"\"Run a roundtrip test path and rule.\n\n    We take a file buffer, lint, fix and lint, finally checking that\n    the file fails initially but not after fixing. Additionally\n    we also check that we haven't messed up the templating tags\n    in the process.\n    \"\"\"\n    tempdir_path = tempfile.mkdtemp()\n    sql_filepath = os.path.join(tempdir_path, sqlfile)\n    cfg_filepath = os.path.join(tempdir_path, cfgfile)\n\n    # Copy the SQL file\n    with open(sql_filepath, mode=\"w\") as dest_file:\n        with open(os.path.join(source_path, sqlfile)) as source_file:\n            for line in source_file:\n                dest_file.write(line)\n    # Copy the Config file\n    with open(cfg_filepath, mode=\"w\") as dest_file:\n        with open(os.path.join(source_path, cfgfile)) as source_file:\n            for line in source_file:\n                dest_file.write(line)\n\n    with open(sql_filepath) as f:\n        # Get a record of the pre-existing jinja tags\n        tags = re.findall(r\"{{[^}]*}}|{%[^}%]*%}\", f.read(), flags=0)\n\n    runner = CliRunner()\n    # Check that we first detect the issue\n    result = runner.invoke(lint, [\"--rules\", rulestring, sql_filepath])\n    assert result.exit_code == 65\n    # Fix the file (in force mode)\n    result = runner.invoke(fix, [\"--rules\", rulestring, \"-f\", sql_filepath])\n    assert result.exit_code == 0\n    # Now lint the file and check for exceptions\n    result = runner.invoke(lint, [\"--rules\", rulestring, sql_filepath])\n    if result.exit_code != 0:\n        # Output the file content for debugging\n        print(\"File content:\")\n        with open(sql_filepath) as f:\n            print(repr(f.read()))\n        print(\"Command output:\")\n        print(result.output)\n    assert result.exit_code == 0\n\n    with open(sql_filepath) as f:\n        # Check that the tags are all still there!\n        new_tags = re.findall(r\"{{[^}]*}}|{%[^}%]*%}\", f.read(), flags=0)\n\n    # Clear up the temp dir\n    shutil.rmtree(tempdir_path)\n\n    # Assert that the tags are the same\n    assert tags == new_tags\n", "tokens": ["test", "rules", "std_roundtrip_test", "py", "def", "jinja_roundtrip_test", "source_path", "rulestring", "sqlfile", "test", "sql", "cfgfile", "sqlfluff", "run", "a", "roundtrip", "test", "path", "and", "rule", "we", "take", "a", "file", "buffer", "lint", "fix", "and", "lint", "finally", "checking", "that", "the", "file", "fails", "initially", "but", "not", "after", "fixing", "additionally", "we", "also", "check", "that", "we", "haven", "t", "messed", "up", "the", "templating", "tags", "in", "the", "process", "tempdir_path", "tempfile", "mkdtemp", "sql_filepath", "os", "path", "join", "tempdir_path", "sqlfile", "cfg_filepath", "os", "path", "join", "tempdir_path", "cfgfile", "copy", "the", "sql", "file", "with", "open", "sql_filepath", "mode", "w", "as", "dest_file", "with", "open", "os", "path", "join", "source_path", "sqlfile", "as", "source_file", "for", "line", "in", "source_file", "dest_file", "write", "line", "copy", "the", "config", "file", "with", "open", "cfg_filepath", "mode", "w", "as", "dest_file", "with", "open", "os", "path", "join", "source_path", "cfgfile", "as", "source_file", "for", "line", "in", "source_file", "dest_file", "write", "line", "with", "open", "sql_filepath", "as", "f", "get", "a", "record", "of", "the", "pre", "existing", "jinja", "tags", "tags", "re", "findall", "r", "f", "read", "flags", "0", "runner", "clirunner", "check", "that", "we", "first", "detect", "the", "issue", "result", "runner", "invoke", "lint", "rules", "rulestring", "sql_filepath", "assert", "result", "exit_code", "65", "fix", "the", "file", "in", "force", "mode", "result", "runner", "invoke", "fix", "rules", "rulestring", "f", "sql_filepath", "assert", "result", "exit_code", "0", "now", "lint", "the", "file", "and", "check", "for", "exceptions", "result", "runner", "invoke", "lint", "rules", "rulestring", "sql_filepath", "if", "result", "exit_code", "0", "output", "the", "file", "content", "for", "debugging", "print", "file", "content", "with", "open", "sql_filepath", "as", "f", "print", "repr", "f", "read", "print", "command", "output", "print", "result", "output", "assert", "result", "exit_code", "0", "with", "open", "sql_filepath", "as", "f", "check", "that", "the", "tags", "are", "all", "still", "there", "new_tags", "re", "findall", "r", "f", "read", "flags", "0", "clear", "up", "the", "temp", "dir", "shutil", "rmtree", "tempdir_path", "assert", "that", "the", "tags", "are", "the", "same", "assert", "tags", "new_tags"], "doc_len": 271}
{"doc_id": "test/rules/std_roundtrip_test.py::test__cli__command__fix", "file_path": "test/rules/std_roundtrip_test.py", "class_name": null, "func_name": "test__cli__command__fix", "text": "文件路径: test/rules/std_roundtrip_test.py\ndef test__cli__command__fix(rule, path):\n    \"\"\"Test the round trip of detecting, fixing and then not detecting given rule.\"\"\"\n    generic_roundtrip_test(path, rule)\n", "tokens": ["test", "rules", "std_roundtrip_test", "py", "def", "test__cli__command__fix", "rule", "path", "test", "the", "round", "trip", "of", "detecting", "fixing", "and", "then", "not", "detecting", "given", "rule", "generic_roundtrip_test", "path", "rule"], "doc_len": 24}
{"doc_id": "test/rules/std_roundtrip_test.py::test__cli__command__fix_templated", "file_path": "test/rules/std_roundtrip_test.py", "class_name": null, "func_name": "test__cli__command__fix_templated", "text": "文件路径: test/rules/std_roundtrip_test.py\ndef test__cli__command__fix_templated(rule):\n    \"\"\"Roundtrip test, making sure that we don't drop tags while templating.\"\"\"\n    jinja_roundtrip_test(\"test/fixtures/templater/jinja_d_roundtrip\", rule)\n", "tokens": ["test", "rules", "std_roundtrip_test", "py", "def", "test__cli__command__fix_templated", "rule", "roundtrip", "test", "making", "sure", "that", "we", "don", "t", "drop", "tags", "while", "templating", "jinja_roundtrip_test", "test", "fixtures", "templater", "jinja_d_roundtrip", "rule"], "doc_len": 25}
{"doc_id": "test/rules/std_test.py::test__rules__std_file", "file_path": "test/rules/std_test.py", "class_name": null, "func_name": "test__rules__std_file", "text": "文件路径: test/rules/std_test.py\ndef test__rules__std_file(rule, path, violations):\n    \"\"\"Test the linter finds the given errors in (and only in) the right places.\"\"\"\n    assert_rule_raises_violations_in_file(\n        rule=rule,\n        fpath=\"test/fixtures/linter/\" + path,\n        violations=violations,\n        fluff_config=FluffConfig(overrides=dict(rules=rule)),\n    )\n", "tokens": ["test", "rules", "std_test", "py", "def", "test__rules__std_file", "rule", "path", "violations", "test", "the", "linter", "finds", "the", "given", "errors", "in", "and", "only", "in", "the", "right", "places", "assert_rule_raises_violations_in_file", "rule", "rule", "fpath", "test", "fixtures", "linter", "path", "violations", "violations", "fluff_config", "fluffconfig", "overrides", "dict", "rules", "rule"], "doc_len": 39}
{"doc_id": "test/rules/std_test.py::test_improper_configs_are_rejected", "file_path": "test/rules/std_test.py", "class_name": null, "func_name": "test_improper_configs_are_rejected", "text": "文件路径: test/rules/std_test.py\ndef test_improper_configs_are_rejected(rule_config_dict):\n    \"\"\"Ensure that unsupported configs raise a ValueError.\"\"\"\n    config = FluffConfig(configs={\"rules\": rule_config_dict})\n    with pytest.raises(ValueError):\n        get_ruleset().get_rulelist(config)\n", "tokens": ["test", "rules", "std_test", "py", "def", "test_improper_configs_are_rejected", "rule_config_dict", "ensure", "that", "unsupported", "configs", "raise", "a", "valueerror", "config", "fluffconfig", "configs", "rules", "rule_config_dict", "with", "pytest", "raises", "valueerror", "get_ruleset", "get_rulelist", "config"], "doc_len": 26}
{"doc_id": "test/rules/yaml_test_cases_test.py::test__rule_test_case", "file_path": "test/rules/yaml_test_cases_test.py", "class_name": null, "func_name": "test__rule_test_case", "text": "文件路径: test/rules/yaml_test_cases_test.py\ndef test__rule_test_case(test_case, caplog):\n    \"\"\"Run the tests.\"\"\"\n    with caplog.at_level(logging.DEBUG, logger=\"sqlfluff.rules\"):\n        res = rules__test_helper(test_case)\n        if res is not None and res != test_case.fail_str:\n            cfg = FluffConfig(configs=test_case.configs)\n            rule = get_rule_from_set(test_case.rule, config=cfg)\n            assert is_fix_compatible(\n                rule\n            ), f'Rule {test_case.rule} returned fixes but does not specify \"@document_fix_compatible\".'\n", "tokens": ["test", "rules", "yaml_test_cases_test", "py", "def", "test__rule_test_case", "test_case", "caplog", "run", "the", "tests", "with", "caplog", "at_level", "logging", "debug", "logger", "sqlfluff", "rules", "res", "rules__test_helper", "test_case", "if", "res", "is", "not", "none", "and", "res", "test_case", "fail_str", "cfg", "fluffconfig", "configs", "test_case", "configs", "rule", "get_rule_from_set", "test_case", "rule", "config", "cfg", "assert", "is_fix_compatible", "rule", "f", "rule", "test_case", "rule", "returned", "fixes", "but", "does", "not", "specify", "document_fix_compatible"], "doc_len": 56}
{"doc_id": "test/rules/yaml_test_cases_test.py::test__rule_test_global_config", "file_path": "test/rules/yaml_test_cases_test.py", "class_name": null, "func_name": "test__rule_test_global_config", "text": "文件路径: test/rules/yaml_test_cases_test.py\ndef test__rule_test_global_config():\n    \"\"\"Test global config in rule test cases.\"\"\"\n    ids, test_cases = load_test_cases(\n        os.path.join(\"test/fixtures/rules/R001_global_config_test.yml\")\n    )\n    assert len(test_cases) == 2\n    # tc1: overwrites global config\n    assert test_cases[0].configs[\"core\"][\"dialect\"] == \"ansi\"\n    # tc2: global config is used\n    assert test_cases[1].configs[\"core\"][\"dialect\"] == \"exasol\"\n", "tokens": ["test", "rules", "yaml_test_cases_test", "py", "def", "test__rule_test_global_config", "test", "global", "config", "in", "rule", "test", "cases", "ids", "test_cases", "load_test_cases", "os", "path", "join", "test", "fixtures", "rules", "r001_global_config_test", "yml", "assert", "len", "test_cases", "2", "tc1", "overwrites", "global", "config", "assert", "test_cases", "0", "configs", "core", "dialect", "ansi", "tc2", "global", "config", "is", "used", "assert", "test_cases", "1", "configs", "core", "dialect", "exasol"], "doc_len": 51}
